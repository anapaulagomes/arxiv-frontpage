{"created":"2025-01-15 18:59:39","title":"Octopus: Scalable Low-Cost CXL Memory Pooling","abstract":"Compute Express Link (CXL) is widely-supported interconnect standard that promises to enable memory disaggregation in data centers. CXL allows for memory pooling, which can be used to create a shared memory space across multiple servers. However, CXL does not specify how to actually build a memory pool. Existing proposals for CXL memory pools are expensive, as they require CXL switches or large multi-headed devices. In this paper, we propose a new design for CXL memory pools that is cost-effective. We call these designs Octopus topologies. Our design uses small CXL devices that can be made cheaply and offer fast access latencies. Specifically, we propose asymmetric CXL topologies where hosts connect to different sets of CXL devices. This enables pooling and sharing memory across multiple hosts even as each individual CXL device is only connected to a small number of hosts. Importantly, this uses hardware that is readily available today. We also show the trade-off in terms of CXL pod size and cost overhead per host. Octopus improves the Pareto frontier defined by prior policies, e.g., offering to connect 3x as many hosts at 17% lower cost per host.","sentences":["Compute Express Link (CXL) is widely-supported interconnect standard that promises to enable memory disaggregation in data centers.","CXL allows for memory pooling, which can be used to create a shared memory space across multiple servers.","However, CXL does not specify how to actually build a memory pool.","Existing proposals for CXL memory pools are expensive, as they require CXL switches or large multi-headed devices.","In this paper, we propose a new design for CXL memory pools that is cost-effective.","We call these designs Octopus topologies.","Our design uses small CXL devices that can be made cheaply and offer fast access latencies.","Specifically, we propose asymmetric CXL topologies where hosts connect to different sets of CXL devices.","This enables pooling and sharing memory across multiple hosts even as each individual CXL device is only connected to a small number of hosts.","Importantly, this uses hardware that is readily available today.","We also show the trade-off in terms of CXL pod size and cost overhead per host.","Octopus improves the Pareto frontier defined by prior policies, e.g., offering to connect 3x as many hosts at 17% lower cost per host."],"url":"http://arxiv.org/abs/2501.09020v1"}
{"created":"2025-01-15 18:59:15","title":"Ouroboros-Diffusion: Exploring Consistent Content Generation in Tuning-free Long Video Diffusion","abstract":"The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation. This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at the queue's head while Gaussian noise is enqueued at the tail. However, FIFO-Diffusion often struggles to keep long-range temporal consistency in the generated videos due to the lack of correspondence modeling across frames. In this paper, we propose Ouroboros-Diffusion, a novel video denoising framework designed to enhance structural and content (subject) consistency, enabling the generation of consistent videos of arbitrary length. Specifically, we introduce a new latent sampling technique at the queue tail to improve structural consistency, ensuring perceptually smooth transitions among frames. To enhance subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA) mechanism, which aligns subjects across frames within short segments to achieve better visual coherence. Furthermore, we introduce self-recurrent guidance. This technique leverages information from all previous cleaner frames at the front of the queue to guide the denoising of noisier frames at the end, fostering rich and contextual global information interaction. Extensive experiments of long video generation on the VBench benchmark demonstrate the superiority of our Ouroboros-Diffusion, particularly in terms of subject consistency, motion smoothness, and temporal consistency.","sentences":["The first-in-first-out (FIFO) video diffusion, built on a pre-trained text-to-video model, has recently emerged as an effective approach for tuning-free long video generation.","This technique maintains a queue of video frames with progressively increasing noise, continuously producing clean frames at the queue's head while Gaussian noise is enqueued at the tail.","However, FIFO-Diffusion often struggles to keep long-range temporal consistency in the generated videos due to the lack of correspondence modeling across frames.","In this paper, we propose Ouroboros-Diffusion, a novel video denoising framework designed to enhance structural and content (subject) consistency, enabling the generation of consistent videos of arbitrary length.","Specifically, we introduce a new latent sampling technique at the queue tail to improve structural consistency, ensuring perceptually smooth transitions among frames.","To enhance subject consistency, we devise a Subject-Aware Cross-Frame Attention (SACFA) mechanism, which aligns subjects across frames within short segments to achieve better visual coherence.","Furthermore, we introduce self-recurrent guidance.","This technique leverages information from all previous cleaner frames at the front of the queue to guide the denoising of noisier frames at the end, fostering rich and contextual global information interaction.","Extensive experiments of long video generation on the VBench benchmark demonstrate the superiority of our Ouroboros-Diffusion, particularly in terms of subject consistency, motion smoothness, and temporal consistency."],"url":"http://arxiv.org/abs/2501.09019v1"}
{"created":"2025-01-15 18:57:17","title":"How Do Generative Models Draw a Software Engineer? A Case Study on Stable Diffusion Bias","abstract":"Generative models are nowadays widely used to generate graphical content used for multiple purposes, e.g. web, art, advertisement. However, it has been shown that the images generated by these models could reinforce societal biases already existing in specific contexts. In this paper, we focus on understanding if this is the case when one generates images related to various software engineering tasks. In fact, the Software Engineering (SE) community is not immune from gender and ethnicity disparities, which could be amplified by the use of these models. Hence, if used without consciousness, artificially generated images could reinforce these biases in the SE domain. Specifically, we perform an extensive empirical evaluation of the gender and ethnicity bias exposed by three versions of the Stable Diffusion (SD) model (a very popular open-source text-to-image model) - SD 2, SD XL, and SD 3 - towards SE tasks. We obtain 6,720 images by feeding each model with two sets of prompts describing different software-related tasks: one set includes the Software Engineer keyword, and one set does not include any specification of the person performing the task. Next, we evaluate the gender and ethnicity disparities in the generated images. Results show how all models are significantly biased towards male figures when representing software engineers. On the contrary, while SD 2 and SD XL are strongly biased towards White figures, SD 3 is slightly more biased towards Asian figures. Nevertheless, all models significantly under-represent Black and Arab figures, regardless of the prompt style used. The results of our analysis highlight severe concerns about adopting those models to generate content for SE tasks and open the field for future research on bias mitigation in this context.","sentences":["Generative models are nowadays widely used to generate graphical content used for multiple purposes, e.g. web, art, advertisement.","However, it has been shown that the images generated by these models could reinforce societal biases already existing in specific contexts.","In this paper, we focus on understanding if this is the case when one generates images related to various software engineering tasks.","In fact, the Software Engineering (SE) community is not immune from gender and ethnicity disparities, which could be amplified by the use of these models.","Hence, if used without consciousness, artificially generated images could reinforce these biases in the SE domain.","Specifically, we perform an extensive empirical evaluation of the gender and ethnicity bias exposed by three versions of the Stable Diffusion (SD) model (a very popular open-source text-to-image model) - SD 2, SD XL, and SD 3 - towards SE tasks.","We obtain 6,720 images by feeding each model with two sets of prompts describing different software-related tasks: one set includes the Software Engineer keyword, and one set does not include any specification of the person performing the task.","Next, we evaluate the gender and ethnicity disparities in the generated images.","Results show how all models are significantly biased towards male figures when representing software engineers.","On the contrary, while SD 2 and SD XL are strongly biased towards White figures, SD 3 is slightly more biased towards Asian figures.","Nevertheless, all models significantly under-represent Black and Arab figures, regardless of the prompt style used.","The results of our analysis highlight severe concerns about adopting those models to generate content for SE tasks and open the field for future research on bias mitigation in this context."],"url":"http://arxiv.org/abs/2501.09014v1"}
{"created":"2025-01-15 18:56:22","title":"Multimodal LLMs Can Reason about Aesthetics in Zero-Shot","abstract":"We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks. To facilitate this investigation, we construct MM-StyleBench, a novel high-quality dataset for benchmarking artistic stylization. We then develop a principled method for human preference modeling and perform a systematic correlation analysis between MLLMs' responses and human preference. Our experiments reveal an inherent hallucination issue of MLLMs in art evaluation, associated with response subjectivity. ArtCoT is proposed, demonstrating that art-specific task decomposition and the use of concrete language boost MLLMs' reasoning ability for aesthetics. Our findings offer valuable insights into MLLMs for art and can benefit a wide range of downstream applications, such as style transfer and artistic image generation. Code available at https://github.com/songrise/MLLM4Art.","sentences":["We present the first study on how Multimodal LLMs' (MLLMs) reasoning ability shall be elicited to evaluate the aesthetics of artworks.","To facilitate this investigation, we construct MM-StyleBench, a novel high-quality dataset for benchmarking artistic stylization.","We then develop a principled method for human preference modeling and perform a systematic correlation analysis between MLLMs' responses and human preference.","Our experiments reveal an inherent hallucination issue of MLLMs in art evaluation, associated with response subjectivity.","ArtCoT is proposed, demonstrating that art-specific task decomposition and the use of concrete language boost MLLMs' reasoning ability for aesthetics.","Our findings offer valuable insights into MLLMs for art and can benefit a wide range of downstream applications, such as style transfer and artistic image generation.","Code available at https://github.com/songrise/MLLM4Art."],"url":"http://arxiv.org/abs/2501.09012v1"}
{"created":"2025-01-15 18:48:38","title":"SimGen: A Diffusion-Based Framework for Simultaneous Surgical Image and Segmentation Mask Generation","abstract":"Acquiring and annotating surgical data is often resource-intensive, ethical constraining, and requiring significant expert involvement. While generative AI models like text-to-image can alleviate data scarcity, incorporating spatial annotations, such as segmentation masks, is crucial for precision-driven surgical applications, simulation, and education. This study introduces both a novel task and method, SimGen, for Simultaneous Image and Mask Generation. SimGen is a diffusion model based on the DDPM framework and Residual U-Net, designed to jointly generate high-fidelity surgical images and their corresponding segmentation masks. The model leverages cross-correlation priors to capture dependencies between continuous image and discrete mask distributions. Additionally, a Canonical Fibonacci Lattice (CFL) is employed to enhance class separability and uniformity in the RGB space of the masks. SimGen delivers high-fidelity images and accurate segmentation masks, outperforming baselines across six public datasets assessed on image and semantic inception distance metrics. Ablation study shows that the CFL improves mask quality and spatial separation. Downstream experiments suggest generated image-mask pairs are usable if regulations limit human data release for research. This work offers a cost-effective solution for generating paired surgical images and complex labels, advancing surgical AI development by reducing the need for expensive manual annotations.","sentences":["Acquiring and annotating surgical data is often resource-intensive, ethical constraining, and requiring significant expert involvement.","While generative AI models like text-to-image can alleviate data scarcity, incorporating spatial annotations, such as segmentation masks, is crucial for precision-driven surgical applications, simulation, and education.","This study introduces both a novel task and method, SimGen, for Simultaneous Image and Mask Generation.","SimGen is a diffusion model based on the DDPM framework and Residual U-Net, designed to jointly generate high-fidelity surgical images and their corresponding segmentation masks.","The model leverages cross-correlation priors to capture dependencies between continuous image and discrete mask distributions.","Additionally, a Canonical Fibonacci Lattice (CFL) is employed to enhance class separability and uniformity in the RGB space of the masks.","SimGen delivers high-fidelity images and accurate segmentation masks, outperforming baselines across six public datasets assessed on image and semantic inception distance metrics.","Ablation study shows that the CFL improves mask quality and spatial separation.","Downstream experiments suggest generated image-mask pairs are usable if regulations limit human data release for research.","This work offers a cost-effective solution for generating paired surgical images and complex labels, advancing surgical AI development by reducing the need for expensive manual annotations."],"url":"http://arxiv.org/abs/2501.09008v1"}
{"created":"2025-01-15 18:47:05","title":"AI-RAN: Transforming RAN with AI-driven Computing Infrastructure","abstract":"The radio access network (RAN) landscape is undergoing a transformative shift from traditional, communication-centric infrastructures towards converged compute-communication platforms. This article introduces AI-RAN which integrates both RAN and artificial intelligence (AI) workloads on the same infrastructure. By doing so, AI-RAN not only meets the performance demands of future networks but also improves asset utilization. We begin by examining how RANs have evolved beyond mobile broadband towards AI-RAN and articulating manifestations of AI-RAN into three forms: AI-for-RAN, AI-on-RAN, and AI-and-RAN. Next, we identify the key requirements and enablers for the convergence of communication and computing in AI-RAN. We then provide a reference architecture for advancing AI-RAN from concept to practice. To illustrate the practical potential of AI-RAN, we present a proof-of-concept that concurrently processes RAN and AI workloads utilizing NVIDIA Grace-Hopper GH200 servers. Finally, we conclude the article by outlining future work directions to guide further developments of AI-RAN.","sentences":["The radio access network (RAN) landscape is undergoing a transformative shift from traditional, communication-centric infrastructures towards converged compute-communication platforms.","This article introduces AI-RAN which integrates both RAN and artificial intelligence (AI) workloads on the same infrastructure.","By doing so, AI-RAN not only meets the performance demands of future networks but also improves asset utilization.","We begin by examining how RANs have evolved beyond mobile broadband towards AI-RAN and articulating manifestations of AI-RAN into three forms: AI-for-RAN, AI-on-RAN, and AI-and-RAN.","Next, we identify the key requirements and enablers for the convergence of communication and computing in AI-RAN.","We then provide a reference architecture for advancing AI-RAN from concept to practice.","To illustrate the practical potential of AI-RAN, we present a proof-of-concept that concurrently processes RAN and AI workloads utilizing NVIDIA Grace-Hopper GH200 servers.","Finally, we conclude the article by outlining future work directions to guide further developments of AI-RAN."],"url":"http://arxiv.org/abs/2501.09007v1"}
{"created":"2025-01-15 18:45:05","title":"Improving Stability Estimates in Adversarial Explainable AI through Alternate Search Methods","abstract":"Advances in the effectiveness of machine learning models have come at the cost of enormous complexity resulting in a poor understanding of how they function. Local surrogate methods have been used to approximate the workings of these complex models, but recent work has revealed their vulnerability to adversarial attacks where the explanation produced is appreciably different while the meaning and structure of the complex model's output remains similar. This prior work has focused on the existence of these weaknesses but not on their magnitude. Here we explore using an alternate search method with the goal of finding minimum viable perturbations, the fewest perturbations necessary to achieve a fixed similarity value between the original and altered text's explanation. Intuitively, a method that requires fewer perturbations to expose a given level of instability is inferior to one which requires more. This nuance allows for superior comparisons of the stability of explainability methods.","sentences":["Advances in the effectiveness of machine learning models have come at the cost of enormous complexity resulting in a poor understanding of how they function.","Local surrogate methods have been used to approximate the workings of these complex models, but recent work has revealed their vulnerability to adversarial attacks where the explanation produced is appreciably different while the meaning and structure of the complex model's output remains similar.","This prior work has focused on the existence of these weaknesses but not on their magnitude.","Here we explore using an alternate search method with the goal of finding minimum viable perturbations, the fewest perturbations necessary to achieve a fixed similarity value between the original and altered text's explanation.","Intuitively, a method that requires fewer perturbations to expose a given level of instability is inferior to one which requires more.","This nuance allows for superior comparisons of the stability of explainability methods."],"url":"http://arxiv.org/abs/2501.09006v1"}
{"created":"2025-01-15 18:43:50","title":"Lightweight Security for Ambient-Powered Programmable Reflections with Reconfigurable Intelligent Surfaces","abstract":"Ambient Internet-of-Things (AIoT) form a new class of emerging technology that promises to deliver pervasive wireless connectivity to previously disconnected devices and products, assisting dependent industries (for example, supply chain, clothing, remote surveillance, climate monitoring, and sensors) to obtain granular real-time service visibility. Such ultra-low complexity and power consumption devices, that are either battery-less or have the capability for limited energy storage, can provide data feeds about the condition of any aspect (e.g., an environment or an item) that is being monitored, enabling proactive or reactive control by any application server. Although the security of data involving AIoT devices is critical for key decisions of any dependent operational system, the implementation of resource intensive cryptographic algorithms and other security mechanisms becomes nearly infeasible, or very challenging, due to the device energy and computational limitations. In this article, we present a lightweight security solution that enables confidentiality, integrity, and privacy protection in wireless links including AIoT. We consider, as a case study, an ambient-powered Reconfigurable Intelligent Surface (RIS) that harvests energy from its incident radio waves to realize programmable reflective beamforming, enabling the communication between a Base Station (BS) and end-user terminals. The proposed lightweight security solution is applied to the control channel between the BS and the RIS controller which is responsible for the metasurface's dynamic management and phase configuration optimization.","sentences":["Ambient Internet-of-Things (AIoT) form a new class of emerging technology that promises to deliver pervasive wireless connectivity to previously disconnected devices and products, assisting dependent industries (for example, supply chain, clothing, remote surveillance, climate monitoring, and sensors) to obtain granular real-time service visibility.","Such ultra-low complexity and power consumption devices, that are either battery-less or have the capability for limited energy storage, can provide data feeds about the condition of any aspect (e.g., an environment or an item) that is being monitored, enabling proactive or reactive control by any application server.","Although the security of data involving AIoT devices is critical for key decisions of any dependent operational system, the implementation of resource intensive cryptographic algorithms and other security mechanisms becomes nearly infeasible, or very challenging, due to the device energy and computational limitations.","In this article, we present a lightweight security solution that enables confidentiality, integrity, and privacy protection in wireless links including AIoT.","We consider, as a case study, an ambient-powered Reconfigurable Intelligent Surface (RIS) that harvests energy from its incident radio waves to realize programmable reflective beamforming, enabling the communication between a Base Station (BS) and end-user terminals.","The proposed lightweight security solution is applied to the control channel between the BS and the RIS controller which is responsible for the metasurface's dynamic management and phase configuration optimization."],"url":"http://arxiv.org/abs/2501.09005v1"}
{"created":"2025-01-15 18:37:08","title":"Aegis2.0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails","abstract":"As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel. Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications. To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into 12 top-level hazard categories with an extension to 9 fine-grained subcategories. This taxonomy is designed to meet the diverse requirements of downstream users, offering more granular and flexible tools for managing various risk types. Using a hybrid data generation pipeline that combines human annotations with a multi-LLM \"jury\" system to assess the safety of responses, we obtain Aegis 2.0, a carefully curated collection of 34,248 samples of human-LLM interactions, annotated according to our proposed taxonomy. To validate its effectiveness, we demonstrate that several lightweight models, trained using parameter-efficient techniques on Aegis 2.0, achieve performance competitive with leading safety models fully fine-tuned on much larger, non-commercial datasets. In addition, we introduce a novel training blend that combines safety with topic following data.This approach enhances the adaptability of guard models, enabling them to generalize to new risk categories defined during inference. We plan to open-source Aegis 2.0 data and models to the research community to aid in the safety guardrailing of LLMs.","sentences":["As Large Language Models (LLMs) and generative AI become increasingly widespread, concerns about content safety have grown in parallel.","Currently, there is a clear lack of high-quality, human-annotated datasets that address the full spectrum of LLM-related safety risks and are usable for commercial applications.","To bridge this gap, we propose a comprehensive and adaptable taxonomy for categorizing safety risks, structured into 12 top-level hazard categories with an extension to 9 fine-grained subcategories.","This taxonomy is designed to meet the diverse requirements of downstream users, offering more granular and flexible tools for managing various risk types.","Using a hybrid data generation pipeline that combines human annotations with a multi-LLM \"jury\" system to assess the safety of responses, we obtain Aegis 2.0, a carefully curated collection of 34,248 samples of human-LLM interactions, annotated according to our proposed taxonomy.","To validate its effectiveness, we demonstrate that several lightweight models, trained using parameter-efficient techniques on Aegis 2.0, achieve performance competitive with leading safety models fully fine-tuned on much larger, non-commercial datasets.","In addition, we introduce a novel training blend that combines safety with topic following data.","This approach enhances the adaptability of guard models, enabling them to generalize to new risk categories defined during inference.","We plan to open-source Aegis 2.0 data and models to the research community to aid in the safety guardrailing of LLMs."],"url":"http://arxiv.org/abs/2501.09004v1"}
{"created":"2025-01-15 18:23:33","title":"VECT-GAN: A variationally encoded generative model for overcoming data scarcity in pharmaceutical science","abstract":"Data scarcity in pharmaceutical research has led to reliance on labour-intensive trial and error approaches for development rather than data driven methods. While Machine Learning offers a solution, existing datasets are often small and noisy, limiting their utility. To address this, we developed a Variationally Encoded Conditional Tabular Generative Adversarial Network (VECT GAN), a novel generative model specifically designed for augmenting small, noisy datasets. We introduce a pipeline where data is augmented before regression model development and demonstrate that this consistently and significantly improves performance over other state of the art tabular generative models. We apply this pipeline across six pharmaceutical datasets, and highlight its real-world applicability by developing novel polymers with medically desirable mucoadhesive properties, which we made and experimentally characterised. Additionally, we pre-train the model on the ChEMBL database of drug-like molecules, leveraging knowledge distillation to enhance its generalisability, making it readily available for use on pharmaceutical datasets containing small molecules, which is an extremely common pharmaceutical task. We demonstrate the power of synthetic data for regularising small tabular datasets, highlighting its potential to become standard practice in pharmaceutical model development, and make our method, including VECT GAN pretrained on ChEMBL available as a pip package.","sentences":["Data scarcity in pharmaceutical research has led to reliance on labour-intensive trial and error approaches for development rather than data driven methods.","While Machine Learning offers a solution, existing datasets are often small and noisy, limiting their utility.","To address this, we developed a Variationally Encoded Conditional Tabular Generative Adversarial Network (VECT GAN), a novel generative model specifically designed for augmenting small, noisy datasets.","We introduce a pipeline where data is augmented before regression model development and demonstrate that this consistently and significantly improves performance over other state of the art tabular generative models.","We apply this pipeline across six pharmaceutical datasets, and highlight its real-world applicability by developing novel polymers with medically desirable mucoadhesive properties, which we made and experimentally characterised.","Additionally, we pre-train the model on the ChEMBL database of drug-like molecules, leveraging knowledge distillation to enhance its generalisability, making it readily available for use on pharmaceutical datasets containing small molecules, which is an extremely common pharmaceutical task.","We demonstrate the power of synthetic data for regularising small tabular datasets, highlighting its potential to become standard practice in pharmaceutical model development, and make our method, including VECT GAN pretrained on ChEMBL available as a pip package."],"url":"http://arxiv.org/abs/2501.08995v1"}
{"created":"2025-01-15 18:20:37","title":"RepVideo: Rethinking Cross-Layer Representation for Video Generation","abstract":"Video generation has achieved remarkable progress with the introduction of diffusion models, which have significantly improved the quality of generated videos. However, recent research has primarily focused on scaling up model training, while offering limited insights into the direct impact of representations on the video generation process. In this paper, we initially investigate the characteristics of features in intermediate layers, finding substantial variations in attention maps across different layers. These variations lead to unstable semantic representations and contribute to cumulative differences between features, which ultimately reduce the similarity between adjacent frames and negatively affect temporal coherence. To address this, we propose RepVideo, an enhanced representation framework for text-to-video diffusion models. By accumulating features from neighboring layers to form enriched representations, this approach captures more stable semantic information. These enhanced representations are then used as inputs to the attention mechanism, thereby improving semantic expressiveness while ensuring feature consistency across adjacent frames. Extensive experiments demonstrate that our RepVideo not only significantly enhances the ability to generate accurate spatial appearances, such as capturing complex spatial relationships between multiple objects, but also improves temporal consistency in video generation.","sentences":["Video generation has achieved remarkable progress with the introduction of diffusion models, which have significantly improved the quality of generated videos.","However, recent research has primarily focused on scaling up model training, while offering limited insights into the direct impact of representations on the video generation process.","In this paper, we initially investigate the characteristics of features in intermediate layers, finding substantial variations in attention maps across different layers.","These variations lead to unstable semantic representations and contribute to cumulative differences between features, which ultimately reduce the similarity between adjacent frames and negatively affect temporal coherence.","To address this, we propose RepVideo, an enhanced representation framework for text-to-video diffusion models.","By accumulating features from neighboring layers to form enriched representations, this approach captures more stable semantic information.","These enhanced representations are then used as inputs to the attention mechanism, thereby improving semantic expressiveness while ensuring feature consistency across adjacent frames.","Extensive experiments demonstrate that our RepVideo not only significantly enhances the ability to generate accurate spatial appearances, such as capturing complex spatial relationships between multiple objects, but also improves temporal consistency in video generation."],"url":"http://arxiv.org/abs/2501.08994v1"}
{"created":"2025-01-15 18:09:33","title":"3GPP Network Architecture Enhancement for Ambient IoT Service","abstract":"Ambient internet of things (A-IoT) paradigm is under study in 3GPP with the intention to provide a sustainable solution for the IoT market without any need to replace the batteries and operate in harsh environments where it is difficult to replenish batteries. This article provides insight on 3rd Generation Partnership Project (3GPP) discussions in Release 18 and 19 with the focus on network architecture aspects. 3GPP has recently decided to start normative work in its Radio Access Network (RAN) Working Group (WG) and discussions are ongoing to start a work item in other WGs with more focus on architecture aspects. We explore and analyze various aspects of system design related to architecture requirements to support A-IoT service, different architecture options to consider, security and authentication mechanisms for A-IoT devices as well as key challenges for standardization of A-IoT service.","sentences":["Ambient internet of things (A-IoT) paradigm is under study in 3GPP with the intention to provide a sustainable solution for the IoT market without any need to replace the batteries and operate in harsh environments where it is difficult to replenish batteries.","This article provides insight on 3rd Generation Partnership Project (3GPP) discussions in Release 18 and 19 with the focus on network architecture aspects.","3GPP has recently decided to start normative work in its Radio Access Network (RAN) Working Group (WG) and discussions are ongoing to start a work item in other WGs with more focus on architecture aspects.","We explore and analyze various aspects of system design related to architecture requirements to support A-IoT service, different architecture options to consider, security and authentication mechanisms for A-IoT devices as well as key challenges for standardization of A-IoT service."],"url":"http://arxiv.org/abs/2501.08990v1"}
{"created":"2025-01-15 18:05:00","title":"Degradedness Under Cooperation","abstract":"We study cooperation problems in broadcast and relay networks, where the receivers do not satisfy the classical physical degradedness assumptions. New notions of degradedness, strongly less noisy and strongly more capable are introduced. We show that under these conditions, decode and forward (D&F) is optimal for classes of cooperative systems with limited conference rates, thus yielding new capacity results for these systems. In particular, we derive bounds on the capacity region of a class of broadcast channels with cooperation, that are tight on part of the capacity region. It is shown that the cut-set bound is tight for classes of primitive relay and diamond channels, beyond the physically or stochastically degraded models.","sentences":["We study cooperation problems in broadcast and relay networks, where the receivers do not satisfy the classical physical degradedness assumptions.","New notions of degradedness, strongly less noisy and strongly more capable are introduced.","We show that under these conditions, decode and forward (D&F) is optimal for classes of cooperative systems with limited conference rates, thus yielding new capacity results for these systems.","In particular, we derive bounds on the capacity region of a class of broadcast channels with cooperation, that are tight on part of the capacity region.","It is shown that the cut-set bound is tight for classes of primitive relay and diamond channels, beyond the physically or stochastically degraded models."],"url":"http://arxiv.org/abs/2501.08987v1"}
{"created":"2025-01-15 18:04:21","title":"Personality Modeling for Persuasion of Misinformation using AI Agent","abstract":"The proliferation of misinformation on social media platforms has highlighted the need to understand how individual personality traits influence susceptibility to and propagation of misinformation. This study employs an innovative agent-based modeling approach to investigate the relationship between personality traits and misinformation dynamics. Using six AI agents embodying different dimensions of the Big Five personality traits (Extraversion, Agreeableness, and Neuroticism), we simulated interactions across six diverse misinformation topics. The experiment, implemented through the AgentScope framework using the GLM-4-Flash model, generated 90 unique interactions, revealing complex patterns in how personality combinations affect persuasion and resistance to misinformation. Our findings demonstrate that analytical and critical personality traits enhance effectiveness in evidence-based discussions, while non-aggressive persuasion strategies show unexpected success in misinformation correction. Notably, agents with critical traits achieved a 59.4% success rate in HIV-related misinformation discussions, while those employing non-aggressive approaches maintained consistent persuasion rates above 40% across different personality combinations. The study also revealed a non-transitive pattern in persuasion effectiveness, challenging conventional assumptions about personality-based influence. These results provide crucial insights for developing personality-aware interventions in digital environments and suggest that effective misinformation countermeasures should prioritize emotional connection and trust-building over confrontational approaches. The findings contribute to both theoretical understanding of personality-misinformation dynamics and practical strategies for combating misinformation in social media contexts.","sentences":["The proliferation of misinformation on social media platforms has highlighted the need to understand how individual personality traits influence susceptibility to and propagation of misinformation.","This study employs an innovative agent-based modeling approach to investigate the relationship between personality traits and misinformation dynamics.","Using six AI agents embodying different dimensions of the Big Five personality traits (Extraversion, Agreeableness, and Neuroticism), we simulated interactions across six diverse misinformation topics.","The experiment, implemented through the AgentScope framework using the GLM-4-Flash model, generated 90 unique interactions, revealing complex patterns in how personality combinations affect persuasion and resistance to misinformation.","Our findings demonstrate that analytical and critical personality traits enhance effectiveness in evidence-based discussions, while non-aggressive persuasion strategies show unexpected success in misinformation correction.","Notably, agents with critical traits achieved a 59.4% success rate in HIV-related misinformation discussions, while those employing non-aggressive approaches maintained consistent persuasion rates above 40% across different personality combinations.","The study also revealed a non-transitive pattern in persuasion effectiveness, challenging conventional assumptions about personality-based influence.","These results provide crucial insights for developing personality-aware interventions in digital environments and suggest that effective misinformation countermeasures should prioritize emotional connection and trust-building over confrontational approaches.","The findings contribute to both theoretical understanding of personality-misinformation dynamics and practical strategies for combating misinformation in social media contexts."],"url":"http://arxiv.org/abs/2501.08985v1"}
{"created":"2025-01-15 17:59:56","title":"CityDreamer4D: Compositional Generative Model of Unbounded 4D Cities","abstract":"3D scene generation has garnered growing attention in recent years and has made significant progress. Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distortions in urban environments. To tackle these issues, we propose CityDreamer4D, a compositional generative model specifically tailored for generating unbounded 4D cities. Our main insights are 1) 4D city generation should separate dynamic objects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2) all objects in the 4D scene should be composed of different types of neural fields for buildings, vehicles, and background stuff. Specifically, we propose Traffic Scenario Generator and Unbounded Layout Generator to produce dynamic traffic scenarios and static city layouts using a highly compact BEV representation. Objects in 4D cities are generated by combining stuff-oriented and instance-oriented neural fields for background stuff, buildings, and vehicles. To suit the distinct characteristics of background stuff and instances, the neural fields employ customized generative hash grids and periodic positional embeddings as scene parameterizations. Furthermore, we offer a comprehensive suite of datasets for city generation, including OSM, GoogleEarth, and CityTopia. The OSM dataset provides a variety of real-world city layouts, while the Google Earth and CityTopia datasets deliver large-scale, high-quality city imagery complete with 3D instance annotations. Leveraging its compositional design, CityDreamer4D supports a range of downstream applications, such as instance editing, city stylization, and urban simulation, while delivering state-of-the-art performance in generating realistic 4D cities.","sentences":["3D scene generation has garnered growing attention in recent years and has made significant progress.","Generating 4D cities is more challenging than 3D scenes due to the presence of structurally complex, visually diverse objects like buildings and vehicles, and heightened human sensitivity to distortions in urban environments.","To tackle these issues, we propose CityDreamer4D, a compositional generative model specifically tailored for generating unbounded 4D cities.","Our main insights are 1) 4D city generation should separate dynamic objects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2) all objects in the 4D scene should be composed of different types of neural fields for buildings, vehicles, and background stuff.","Specifically, we propose Traffic Scenario Generator and Unbounded Layout Generator to produce dynamic traffic scenarios and static city layouts using a highly compact BEV representation.","Objects in 4D cities are generated by combining stuff-oriented and instance-oriented neural fields for background stuff, buildings, and vehicles.","To suit the distinct characteristics of background stuff and instances, the neural fields employ customized generative hash grids and periodic positional embeddings as scene parameterizations.","Furthermore, we offer a comprehensive suite of datasets for city generation, including OSM, GoogleEarth, and CityTopia.","The OSM dataset provides a variety of real-world city layouts, while the Google Earth and CityTopia datasets deliver large-scale, high-quality city imagery complete with 3D instance annotations.","Leveraging its compositional design, CityDreamer4D supports a range of downstream applications, such as instance editing, city stylization, and urban simulation, while delivering state-of-the-art performance in generating realistic 4D cities."],"url":"http://arxiv.org/abs/2501.08983v1"}
{"created":"2025-01-15 17:59:32","title":"CityLoc: 6 DoF Localization of Text Descriptions in Large-Scale Scenes with Gaussian Representation","abstract":"Localizing text descriptions in large-scale 3D scenes is inherently an ambiguous task. This nonetheless arises while describing general concepts, e.g. all traffic lights in a city.   To facilitate reasoning based on such concepts, text localization in the form of distribution is required. In this paper, we generate the distribution of the camera poses conditioned upon the textual description.   To facilitate such generation, we propose a diffusion-based architecture that conditionally diffuses the noisy 6DoF camera poses to their plausible locations.   The conditional signals are derived from the text descriptions, using the pre-trained text encoders. The connection between text descriptions and pose distribution is established through pretrained Vision-Language-Model, i.e. CLIP. Furthermore, we demonstrate that the candidate poses for the distribution can be further refined by rendering potential poses using 3D Gaussian splatting, guiding incorrectly posed samples towards locations that better align with the textual description, through visual reasoning.   We demonstrate the effectiveness of our method by comparing it with both standard retrieval methods and learning-based approaches. Our proposed method consistently outperforms these baselines across all five large-scale datasets. Our source code and dataset will be made publicly available.","sentences":["Localizing text descriptions in large-scale 3D scenes is inherently an ambiguous task.","This nonetheless arises while describing general concepts, e.g. all traffic lights in a city.   ","To facilitate reasoning based on such concepts, text localization in the form of distribution is required.","In this paper, we generate the distribution of the camera poses conditioned upon the textual description.   ","To facilitate such generation, we propose a diffusion-based architecture that conditionally diffuses the noisy 6DoF camera poses to their plausible locations.   ","The conditional signals are derived from the text descriptions, using the pre-trained text encoders.","The connection between text descriptions and pose distribution is established through pretrained Vision-Language-Model, i.e. CLIP.","Furthermore, we demonstrate that the candidate poses for the distribution can be further refined by rendering potential poses using 3D Gaussian splatting, guiding incorrectly posed samples towards locations that better align with the textual description, through visual reasoning.   ","We demonstrate the effectiveness of our method by comparing it with both standard retrieval methods and learning-based approaches.","Our proposed method consistently outperforms these baselines across all five large-scale datasets.","Our source code and dataset will be made publicly available."],"url":"http://arxiv.org/abs/2501.08982v1"}
{"created":"2025-01-15 17:47:57","title":"Development and Validation of the Provider Documentation Summarization Quality Instrument for Large Language Models","abstract":"As Large Language Models (LLMs) are integrated into electronic health record (EHR) workflows, validated instruments are essential to evaluate their performance before implementation. Existing instruments for provider documentation quality are often unsuitable for the complexities of LLM-generated text and lack validation on real-world data. The Provider Documentation Summarization Quality Instrument (PDSQI-9) was developed to evaluate LLM-generated clinical summaries. Multi-document summaries were generated from real-world EHR data across multiple specialties using several LLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b). Validation included Pearson correlation for substantive validity, factor analysis and Cronbach's alpha for structural validity, inter-rater reliability (ICC and Krippendorff's alpha) for generalizability, a semi-Delphi process for content validity, and comparisons of high- versus low-quality summaries for discriminant validity. Seven physician raters evaluated 779 summaries and answered 8,329 questions, achieving over 80% power for inter-rater reliability. The PDSQI-9 demonstrated strong internal consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and high inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting structural validity and generalizability. Factor analysis identified a 4-factor model explaining 58% of the variance, representing organization, clarity, accuracy, and utility. Substantive validity was supported by correlations between note length and scores for Succinct (rho = -0.200, p = 0.029) and Organized (rho = -0.190, p = 0.037). Discriminant validity distinguished high- from low-quality summaries (p < 0.001). The PDSQI-9 demonstrates robust construct validity, supporting its use in clinical practice to evaluate LLM-generated summaries and facilitate safer integration of LLMs into healthcare workflows.","sentences":["As Large Language Models (LLMs) are integrated into electronic health record (EHR) workflows, validated instruments are essential to evaluate their performance before implementation.","Existing instruments for provider documentation quality are often unsuitable for the complexities of LLM-generated text and lack validation on real-world data.","The Provider Documentation Summarization Quality Instrument (PDSQI-9) was developed to evaluate LLM-generated clinical summaries.","Multi-document summaries were generated from real-world EHR data across multiple specialties using several LLMs (GPT-4o, Mixtral 8x7b, and Llama 3-8b).","Validation included Pearson correlation for substantive validity, factor analysis and Cronbach's alpha for structural validity, inter-rater reliability (ICC and Krippendorff's alpha) for generalizability, a semi-Delphi process for content validity, and comparisons of high- versus low-quality summaries for discriminant validity.","Seven physician raters evaluated 779 summaries and answered 8,329 questions, achieving over 80% power for inter-rater reliability.","The PDSQI-9 demonstrated strong internal consistency (Cronbach's alpha = 0.879; 95% CI: 0.867-0.891) and high inter-rater reliability (ICC = 0.867; 95% CI: 0.867-0.868), supporting structural validity and generalizability.","Factor analysis identified a 4-factor model explaining 58% of the variance, representing organization, clarity, accuracy, and utility.","Substantive validity was supported by correlations between note length and scores for Succinct (rho = -0.200, p = 0.029) and Organized (rho = -0.190, p = 0.037).","Discriminant validity distinguished high- from low-quality summaries (p < 0.001).","The PDSQI-9 demonstrates robust construct validity, supporting its use in clinical practice to evaluate LLM-generated summaries and facilitate safer integration of LLMs into healthcare workflows."],"url":"http://arxiv.org/abs/2501.08977v1"}
{"created":"2025-01-15 17:36:56","title":"Learning to Extract Cross-Domain Aspects and Understanding Sentiments Using Large Language Models","abstract":"Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment analysis that aims to extract and classify sentiments based on specific aspects or features of a product, service, or entity. Unlike traditional sentiment analysis, which assigns a general sentiment score to entire reviews or texts, ABSA focuses on breaking down the text into individual components or aspects (e.g., quality, price, service) and evaluating the sentiment towards each. This allows for a more granular level of understanding of customer opinions, enabling businesses to pinpoint specific areas of strength and improvement. The process involves several key steps, including aspect extraction, sentiment classification, and aspect-level sentiment aggregation for a review paragraph or any other form that the users have provided. ABSA has significant applications in areas such as product reviews, social media monitoring, customer feedback analysis, and market research. By leveraging techniques from natural language processing (NLP) and machine learning, ABSA facilitates the extraction of valuable insights, enabling companies to make data-driven decisions that enhance customer satisfaction and optimize offerings. As ABSA evolves, it holds the potential to greatly improve personalized customer experiences by providing a deeper understanding of sentiment across various product aspects. In this work, we have analyzed the strength of LLMs for a complete cross-domain aspect-based sentiment analysis with the aim of defining the framework for certain products and using it for other similar situations. We argue that it is possible to that at an effectiveness of 92\\% accuracy for the Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12.","sentences":["Aspect-based sentiment analysis (ASBA) is a refined approach to sentiment analysis that aims to extract and classify sentiments based on specific aspects or features of a product, service, or entity.","Unlike traditional sentiment analysis, which assigns a general sentiment score to entire reviews or texts, ABSA focuses on breaking down the text into individual components or aspects (e.g., quality, price, service) and evaluating the sentiment towards each.","This allows for a more granular level of understanding of customer opinions, enabling businesses to pinpoint specific areas of strength and improvement.","The process involves several key steps, including aspect extraction, sentiment classification, and aspect-level sentiment aggregation for a review paragraph or any other form that the users have provided.","ABSA has significant applications in areas such as product reviews, social media monitoring, customer feedback analysis, and market research.","By leveraging techniques from natural language processing (NLP) and machine learning, ABSA facilitates the extraction of valuable insights, enabling companies to make data-driven decisions that enhance customer satisfaction and optimize offerings.","As ABSA evolves, it holds the potential to greatly improve personalized customer experiences by providing a deeper understanding of sentiment across various product aspects.","In this work, we have analyzed the strength of LLMs for a complete cross-domain aspect-based sentiment analysis with the aim of defining the framework for certain products and using it for other similar situations.","We argue that it is possible to that at an effectiveness of 92\\% accuracy for the Aspect Based Sentiment Analysis dataset of SemEval-2015 Task 12."],"url":"http://arxiv.org/abs/2501.08974v1"}
{"created":"2025-01-15 17:28:53","title":"Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography","abstract":"We often interact with untrusted parties. Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data. Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptographic protocols that restrict how much data is revealed, such as multi-party computations or zero-knowledge proofs. While significant advances have been made in scaling cryptographic approaches, they remain limited in terms of the size and complexity of applications they can be used for. In this paper, we argue that capable machine learning models can fulfill the role of a trusted third party, thus enabling secure computations for applications that were previously infeasible. In particular, we describe Trusted Capable Model Environments (TCMEs) as an alternative approach for scaling secure computation, where capable machine learning model(s) interact under input/output constraints, with explicit information flow control and explicit statelessness. This approach aims to achieve a balance between privacy and computational efficiency, enabling private inference where classical cryptographic solutions are currently infeasible. We describe a number of use cases that are enabled by TCME, and show that even some simple classic cryptographic problems can already be solved with TCME. Finally, we outline current limitations and discuss the path forward in implementing them.","sentences":["We often interact with untrusted parties.","Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data.","Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptographic protocols that restrict how much data is revealed, such as multi-party computations or zero-knowledge proofs.","While significant advances have been made in scaling cryptographic approaches, they remain limited in terms of the size and complexity of applications they can be used for.","In this paper, we argue that capable machine learning models can fulfill the role of a trusted third party, thus enabling secure computations for applications that were previously infeasible.","In particular, we describe Trusted Capable Model Environments (TCMEs) as an alternative approach for scaling secure computation, where capable machine learning model(s) interact under input/output constraints, with explicit information flow control and explicit statelessness.","This approach aims to achieve a balance between privacy and computational efficiency, enabling private inference where classical cryptographic solutions are currently infeasible.","We describe a number of use cases that are enabled by TCME, and show that even some simple classic cryptographic problems can already be solved with TCME.","Finally, we outline current limitations and discuss the path forward in implementing them."],"url":"http://arxiv.org/abs/2501.08970v1"}
{"created":"2025-01-15 17:19:51","title":"Training-Aware Risk Control for Intensity Modulated Radiation Therapies Quality Assurance with Conformal Prediction","abstract":"Measurement quality assurance (QA) practices play a key role in the safe use of Intensity Modulated Radiation Therapies (IMRT) for cancer treatment. These practices have reduced measurement-based IMRT QA failure below 1%. However, these practices are time and labor intensive which can lead to delays in patient care. In this study, we examine how conformal prediction methodologies can be used to robustly triage plans. We propose a new training-aware conformal risk control method by combining the benefit of conformal risk control and conformal training. We incorporate the decision making thresholds based on the gamma passing rate, along with the risk functions used in clinical evaluation, into the design of the risk control framework. Our method achieves high sensitivity and specificity and significantly reduces the number of plans needing measurement without generating a huge confidence interval. Our results demonstrate the validity and applicability of conformal prediction methods for improving efficiency and reducing the workload of the IMRT QA process.","sentences":["Measurement quality assurance (QA) practices play a key role in the safe use of Intensity Modulated Radiation Therapies (IMRT) for cancer treatment.","These practices have reduced measurement-based IMRT QA failure below 1%.","However, these practices are time and labor intensive which can lead to delays in patient care.","In this study, we examine how conformal prediction methodologies can be used to robustly triage plans.","We propose a new training-aware conformal risk control method by combining the benefit of conformal risk control and conformal training.","We incorporate the decision making thresholds based on the gamma passing rate, along with the risk functions used in clinical evaluation, into the design of the risk control framework.","Our method achieves high sensitivity and specificity and significantly reduces the number of plans needing measurement without generating a huge confidence interval.","Our results demonstrate the validity and applicability of conformal prediction methods for improving efficiency and reducing the workload of the IMRT QA process."],"url":"http://arxiv.org/abs/2501.08963v1"}
{"created":"2025-01-15 17:18:46","title":"An analysis of data variation and bias in image-based dermatological datasets for machine learning classification","abstract":"AI algorithms have become valuable in aiding professionals in healthcare. The increasing confidence obtained by these models is helpful in critical decision demands. In clinical dermatology, classification models can detect malignant lesions on patients' skin using only RGB images as input. However, most learning-based methods employ data acquired from dermoscopic datasets on training, which are large and validated by a gold standard. Clinical models aim to deal with classification on users' smartphone cameras that do not contain the corresponding resolution provided by dermoscopy. Also, clinical applications bring new challenges. It can contain captures from uncontrolled environments, skin tone variations, viewpoint changes, noises in data and labels, and unbalanced classes. A possible alternative would be to use transfer learning to deal with the clinical images. However, as the number of samples is low, it can cause degradations on the model's performance; the source distribution used in training differs from the test set. This work aims to evaluate the gap between dermoscopic and clinical samples and understand how the dataset variations impact training. It assesses the main differences between distributions that disturb the model's prediction. Finally, from experiments on different architectures, we argue how to combine the data from divergent distributions, decreasing the impact on the model's final accuracy.","sentences":["AI algorithms have become valuable in aiding professionals in healthcare.","The increasing confidence obtained by these models is helpful in critical decision demands.","In clinical dermatology, classification models can detect malignant lesions on patients' skin using only RGB images as input.","However, most learning-based methods employ data acquired from dermoscopic datasets on training, which are large and validated by a gold standard.","Clinical models aim to deal with classification on users' smartphone cameras that do not contain the corresponding resolution provided by dermoscopy.","Also, clinical applications bring new challenges.","It can contain captures from uncontrolled environments, skin tone variations, viewpoint changes, noises in data and labels, and unbalanced classes.","A possible alternative would be to use transfer learning to deal with the clinical images.","However, as the number of samples is low, it can cause degradations on the model's performance; the source distribution used in training differs from the test set.","This work aims to evaluate the gap between dermoscopic and clinical samples and understand how the dataset variations impact training.","It assesses the main differences between distributions that disturb the model's prediction.","Finally, from experiments on different architectures, we argue how to combine the data from divergent distributions, decreasing the impact on the model's final accuracy."],"url":"http://arxiv.org/abs/2501.08962v1"}
{"created":"2025-01-15 17:09:07","title":"Kolmogorov-Arnold Networks for Time Series Granger Causality Inference","abstract":"We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an innovative architecture that extends the recently proposed Kolmogorov-Arnold Networks (KAN) to the domain of causal inference. By extracting base weights from KAN layers and incorporating the sparsity-inducing penalty along with ridge regularization, GCKAN infers the Granger causality from time series while enabling automatic time lag selection. Additionally, we propose an algorithm leveraging time-reversed Granger causality to enhance inference accuracy. The algorithm compares prediction and sparse-inducing losses derived from the original and time-reversed series, automatically selecting the casual relationship with the higher score or integrating the results to mitigate spurious connectivities. Comprehensive experiments conducted on Lorenz-96, gene regulatory networks, fMRI BOLD signals, and VAR datasets demonstrate that the proposed model achieves competitive performance to state-of-the-art methods in inferring Granger causality from nonlinear, high-dimensional, and limited-sample time series.","sentences":["We introduce Granger Causality Kolmogorov-Arnold Networks (GCKAN), an innovative architecture that extends the recently proposed Kolmogorov-Arnold Networks (KAN) to the domain of causal inference.","By extracting base weights from KAN layers and incorporating the sparsity-inducing penalty along with ridge regularization, GCKAN infers the Granger causality from time series while enabling automatic time lag selection.","Additionally, we propose an algorithm leveraging time-reversed Granger causality to enhance inference accuracy.","The algorithm compares prediction and sparse-inducing losses derived from the original and time-reversed series, automatically selecting the casual relationship with the higher score or integrating the results to mitigate spurious connectivities.","Comprehensive experiments conducted on Lorenz-96, gene regulatory networks, fMRI BOLD signals, and VAR datasets demonstrate that the proposed model achieves competitive performance to state-of-the-art methods in inferring Granger causality from nonlinear, high-dimensional, and limited-sample time series."],"url":"http://arxiv.org/abs/2501.08958v1"}
{"created":"2025-01-15 16:56:26","title":"Analyzing the Ethical Logic of Six Large Language Models","abstract":"This study examines the ethical reasoning of six prominent generative large language models: OpenAI GPT-4o, Meta LLaMA 3.1, Perplexity, Anthropic Claude 3.5 Sonnet, Google Gemini, and Mistral 7B. The research explores how these models articulate and apply ethical logic, particularly in response to moral dilemmas such as the Trolley Problem, and Heinz Dilemma. Departing from traditional alignment studies, the study adopts an explainability-transparency framework, prompting models to explain their ethical reasoning. This approach is analyzed through three established ethical typologies: the consequentialist-deontological analytic, Moral Foundations Theory, and the Kohlberg Stages of Moral Development Model. Findings reveal that LLMs exhibit largely convergent ethical logic, marked by a rationalist, consequentialist emphasis, with decisions often prioritizing harm minimization and fairness. Despite similarities in pre-training and model architecture, a mixture of nuanced and significant differences in ethical reasoning emerge across models, reflecting variations in fine-tuning and post-training processes. The models consistently display erudition, caution, and self-awareness, presenting ethical reasoning akin to a graduate-level discourse in moral philosophy. In striking uniformity these systems all describe their ethical reasoning as more sophisticated than what is characteristic of typical human moral logic.","sentences":["This study examines the ethical reasoning of six prominent generative large language models: OpenAI GPT-4o, Meta LLaMA 3.1, Perplexity, Anthropic Claude 3.5 Sonnet, Google Gemini, and Mistral 7B.","The research explores how these models articulate and apply ethical logic, particularly in response to moral dilemmas such as the Trolley Problem, and Heinz Dilemma.","Departing from traditional alignment studies, the study adopts an explainability-transparency framework, prompting models to explain their ethical reasoning.","This approach is analyzed through three established ethical typologies: the consequentialist-deontological analytic, Moral Foundations Theory, and the Kohlberg Stages of Moral Development Model.","Findings reveal that LLMs exhibit largely convergent ethical logic, marked by a rationalist, consequentialist emphasis, with decisions often prioritizing harm minimization and fairness.","Despite similarities in pre-training and model architecture, a mixture of nuanced and significant differences in ethical reasoning emerge across models, reflecting variations in fine-tuning and post-training processes.","The models consistently display erudition, caution, and self-awareness, presenting ethical reasoning akin to a graduate-level discourse in moral philosophy.","In striking uniformity these systems all describe their ethical reasoning as more sophisticated than what is characteristic of typical human moral logic."],"url":"http://arxiv.org/abs/2501.08951v1"}
{"created":"2025-01-15 16:52:21","title":"Computing Approximated Fixpoints via Dampened Mann Iteration","abstract":"Fixpoints are ubiquitous in computer science and when dealing with quantitative semantics and verification one is commonly led to consider least fixpoints of (higher-dimensional) functions over the nonnegative reals. We show how to approximate the least fixpoint of such functions, focusing on the case in which they are not known precisely, but represented by a sequence of approximating functions that converge to them. We concentrate on monotone and non-expansive functions, for which uniqueness of fixpoints is not guaranteed and standard fixpoint iteration schemes might get stuck at a fixpoint that is not the least. Our main contribution is the identification of an iteration scheme, a variation of Mann iteration with a dampening factor, which, under suitable conditions, is shown to guarantee convergence to the least fixpoint of the function of interest. We then argue that these results are relevant in the context of model-based reinforcement learning for Markov decision processes (MDPs), showing that the proposed iteration scheme instantiates to MDPs and allows us to derive convergence to the optimal expected return. More generally, we show that our results can be used to iterate to the least fixpoint almost surely for systems where the function of interest can be approximated with given probabilistic error bounds, as it happens for probabilistic systems, such as simple stochastic games, that can be explored via sampling.","sentences":["Fixpoints are ubiquitous in computer science and when dealing with quantitative semantics and verification one is commonly led to consider least fixpoints of (higher-dimensional) functions over the nonnegative reals.","We show how to approximate the least fixpoint of such functions, focusing on the case in which they are not known precisely, but represented by a sequence of approximating functions that converge to them.","We concentrate on monotone and non-expansive functions, for which uniqueness of fixpoints is not guaranteed and standard fixpoint iteration schemes might get stuck at a fixpoint that is not the least.","Our main contribution is the identification of an iteration scheme, a variation of Mann iteration with a dampening factor, which, under suitable conditions, is shown to guarantee convergence to the least fixpoint of the function of interest.","We then argue that these results are relevant in the context of model-based reinforcement learning for Markov decision processes (MDPs), showing that the proposed iteration scheme instantiates to MDPs and allows us to derive convergence to the optimal expected return.","More generally, we show that our results can be used to iterate to the least fixpoint almost surely for systems where the function of interest can be approximated with given probabilistic error bounds, as it happens for probabilistic systems, such as simple stochastic games, that can be explored via sampling."],"url":"http://arxiv.org/abs/2501.08950v1"}
{"created":"2025-01-15 16:49:32","title":"Taint Analysis for Graph APIs Focusing on Broken Access Control","abstract":"Graph APIs are capable of flexibly retrieving or manipulating graph-structured data over the web. This rather novel type of APIs presents new challenges when it comes to properly securing the APIs against the usual web application security risks, e.g., broken access control. A prominent security testing approach is taint analysis, which traces tainted, i.e., security-relevant, data from sources (where tainted data is inserted) to sinks (where the use of tainted data may lead to a security risk), over the information flow in an application.   We present a first systematic approach to static and dynamic taint analysis for Graph APIs focusing on broken access control. The approach comprises the following. We taint nodes in the Graph API if they represent data requiring specific privileges in order to be retrieved or manipulated, and identify API calls which are related to sources and sinks. Then, we statically analyze whether tainted information flow between API source and sink calls occurs. To this end, we model the API calls using graph transformation rules. We subsequently use critical pair analysis to automatically analyze potential dependencies between rules representing source calls and rules representing sink calls. We distinguish direct from indirect tainted information flow and argue under which conditions the CPA is able to detect not only direct, but also indirect tainted flow. The static taint analysis (i) identifies flows that need to be further reviewed, since tainted nodes may be created by an API call and used or manipulated by another API call later without having the necessary privileges, and (ii) can be used to systematically design dynamic security tests for broken access control. The dynamic taint analysis checks if potential broken access control risks detected during the static taint analysis really occur. We apply the approach to a part of the GitHub GraphQL API.","sentences":["Graph APIs are capable of flexibly retrieving or manipulating graph-structured data over the web.","This rather novel type of APIs presents new challenges when it comes to properly securing the APIs against the usual web application security risks, e.g., broken access control.","A prominent security testing approach is taint analysis, which traces tainted, i.e., security-relevant, data from sources (where tainted data is inserted) to sinks (where the use of tainted data may lead to a security risk), over the information flow in an application.   ","We present a first systematic approach to static and dynamic taint analysis for Graph APIs focusing on broken access control.","The approach comprises the following.","We taint nodes in the Graph API if they represent data requiring specific privileges in order to be retrieved or manipulated, and identify API calls which are related to sources and sinks.","Then, we statically analyze whether tainted information flow between API source and sink calls occurs.","To this end, we model the API calls using graph transformation rules.","We subsequently use critical pair analysis to automatically analyze potential dependencies between rules representing source calls and rules representing sink calls.","We distinguish direct from indirect tainted information flow and argue under which conditions the CPA is able to detect not only direct, but also indirect tainted flow.","The static taint analysis (i) identifies flows that need to be further reviewed, since tainted nodes may be created by an API call and used or manipulated by another API call later without having the necessary privileges, and (ii) can be used to systematically design dynamic security tests for broken access control.","The dynamic taint analysis checks if potential broken access control risks detected during the static taint analysis really occur.","We apply the approach to a part of the GitHub GraphQL API."],"url":"http://arxiv.org/abs/2501.08947v1"}
{"created":"2025-01-15 16:49:22","title":"Applying General Turn-taking Models to Conversational Human-Robot Interaction","abstract":"Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions. This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI. These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning. We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions. We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation. The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions.","sentences":["Turn-taking is a fundamental aspect of conversation, but current Human-Robot Interaction (HRI) systems often rely on simplistic, silence-based models, leading to unnatural pauses and interruptions.","This paper investigates, for the first time, the application of general turn-taking models, specifically TurnGPT and Voice Activity Projection (VAP), to improve conversational dynamics in HRI.","These models are trained on human-human dialogue data using self-supervised learning objectives, without requiring domain-specific fine-tuning.","We propose methods for using these models in tandem to predict when a robot should begin preparing responses, take turns, and handle potential interruptions.","We evaluated the proposed system in a within-subject study against a traditional baseline system, using the Furhat robot with 39 adults in a conversational setting, in combination with a large language model for autonomous response generation.","The results show that participants significantly prefer the proposed system, and it significantly reduces response delays and interruptions."],"url":"http://arxiv.org/abs/2501.08946v1"}
{"created":"2025-01-15 16:46:32","title":"Physical AI Agents: Integrating Cognitive Intelligence with Real-World Action","abstract":"Vertical AI Agents are revolutionizing industries by delivering domain-specific intelligence and tailored solutions. However, many sectors, such as manufacturing, healthcare, and logistics, demand AI systems capable of extending their intelligence into the physical world, interacting directly with objects, environments, and dynamic conditions. This need has led to the emergence of Physical AI Agents--systems that integrate cognitive reasoning, powered by specialized LLMs, with precise physical actions to perform real-world tasks.   This work introduces Physical AI Agents as an evolution of shared principles with Vertical AI Agents, tailored for physical interaction. We propose a modular architecture with three core blocks--perception, cognition, and actuation--offering a scalable framework for diverse industries. Additionally, we present the Physical Retrieval Augmented Generation (Ph-RAG) design pattern, which connects physical intelligence to industry-specific LLMs for real-time decision-making and reporting informed by physical context.   Through case studies, we demonstrate how Physical AI Agents and the Ph-RAG framework are transforming industries like autonomous vehicles, warehouse robotics, healthcare, and manufacturing, offering businesses a pathway to integrate embodied AI for operational efficiency and innovation.","sentences":["Vertical AI Agents are revolutionizing industries by delivering domain-specific intelligence and tailored solutions.","However, many sectors, such as manufacturing, healthcare, and logistics, demand AI systems capable of extending their intelligence into the physical world, interacting directly with objects, environments, and dynamic conditions.","This need has led to the emergence of Physical AI Agents--systems that integrate cognitive reasoning, powered by specialized LLMs, with precise physical actions to perform real-world tasks.   ","This work introduces Physical AI Agents as an evolution of shared principles with Vertical AI Agents, tailored for physical interaction.","We propose a modular architecture with three core blocks--perception, cognition, and actuation--offering a scalable framework for diverse industries.","Additionally, we present the Physical Retrieval Augmented Generation (Ph-RAG) design pattern, which connects physical intelligence to industry-specific LLMs for real-time decision-making and reporting informed by physical context.   ","Through case studies, we demonstrate how Physical AI Agents and the Ph-RAG framework are transforming industries like autonomous vehicles, warehouse robotics, healthcare, and manufacturing, offering businesses a pathway to integrate embodied AI for operational efficiency and innovation."],"url":"http://arxiv.org/abs/2501.08944v1"}
{"created":"2025-01-15 16:44:35","title":"A Reinforcement Learning Approach to Quiet and Safe UAM Traffic Management","abstract":"Urban air mobility (UAM) is a transformative system that operates various small aerial vehicles in urban environments to reshape urban transportation. However, integrating UAM into existing urban environments presents a variety of complex challenges. Recent analyses of UAM's operational constraints highlight aircraft noise and system safety as key hurdles to UAM system implementation. Future UAM air traffic management schemes must ensure that the system is both quiet and safe. We propose a multi-agent reinforcement learning approach to manage UAM traffic, aiming at both vertical separation assurance and noise mitigation. Through extensive training, the reinforcement learning agent learns to balance the two primary objectives by employing altitude adjustments in a multi-layer UAM network. The results reveal the tradeoffs among noise impact, traffic congestion, and separation. Overall, our findings demonstrate the potential of reinforcement learning in mitigating UAM's noise impact while maintaining safe separation using altitude adjustments","sentences":["Urban air mobility (UAM) is a transformative system that operates various small aerial vehicles in urban environments to reshape urban transportation.","However, integrating UAM into existing urban environments presents a variety of complex challenges.","Recent analyses of UAM's operational constraints highlight aircraft noise and system safety as key hurdles to UAM system implementation.","Future UAM air traffic management schemes must ensure that the system is both quiet and safe.","We propose a multi-agent reinforcement learning approach to manage UAM traffic, aiming at both vertical separation assurance and noise mitigation.","Through extensive training, the reinforcement learning agent learns to balance the two primary objectives by employing altitude adjustments in a multi-layer UAM network.","The results reveal the tradeoffs among noise impact, traffic congestion, and separation.","Overall, our findings demonstrate the potential of reinforcement learning in mitigating UAM's noise impact while maintaining safe separation using altitude adjustments"],"url":"http://arxiv.org/abs/2501.08941v1"}
{"created":"2025-01-15 16:35:10","title":"Separation Assurance in Urban Air Mobility Systems using Shared Scheduling Protocols","abstract":"Ensuring safe separation between aircraft is a critical challenge in air traffic management, particularly in urban air mobility (UAM) environments where high traffic density and low altitudes require precise control. In these environments, conflicts often arise at the intersections of flight corridors, posing significant risks. We propose a tactical separation approach leveraging shared scheduling protocols, originally designed for Ethernet networks and operating systems, to coordinate access to these intersections. Using a decentralized Markov decision process framework, the proposed approach enables aircraft to autonomously adjust their speed and timing as they navigate these critical areas, maintaining safe separation without a central controller. We evaluate the effectiveness of this approach in simulated UAM scenarios, demonstrating its ability to reduce separation violations to zero while acknowledging trade-offs in flight times as traffic density increases. Additionally, we explore the impact of non-compliant aircraft, showing that while shared scheduling protocols can no longer guarantee safe separation, they still provide significant improvements over systems without scheduling protocols.","sentences":["Ensuring safe separation between aircraft is a critical challenge in air traffic management, particularly in urban air mobility (UAM) environments where high traffic density and low altitudes require precise control.","In these environments, conflicts often arise at the intersections of flight corridors, posing significant risks.","We propose a tactical separation approach leveraging shared scheduling protocols, originally designed for Ethernet networks and operating systems, to coordinate access to these intersections.","Using a decentralized Markov decision process framework, the proposed approach enables aircraft to autonomously adjust their speed and timing as they navigate these critical areas, maintaining safe separation without a central controller.","We evaluate the effectiveness of this approach in simulated UAM scenarios, demonstrating its ability to reduce separation violations to zero while acknowledging trade-offs in flight times as traffic density increases.","Additionally, we explore the impact of non-compliant aircraft, showing that while shared scheduling protocols can no longer guarantee safe separation, they still provide significant improvements over systems without scheduling protocols."],"url":"http://arxiv.org/abs/2501.08933v1"}
{"created":"2025-01-15 16:34:20","title":"Visual WetlandBirds Dataset: Bird Species Identification and Behavior Recognition in Videos","abstract":"The current biodiversity loss crisis makes animal monitoring a relevant field of study. In light of this, data collected through monitoring can provide essential insights, and information for decision-making aimed at preserving global biodiversity. Despite the importance of such data, there is a notable scarcity of datasets featuring videos of birds, and none of the existing datasets offer detailed annotations of bird behaviors in video format. In response to this gap, our study introduces the first fine-grained video dataset specifically designed for bird behavior detection and species classification. This dataset addresses the need for comprehensive bird video datasets and provides detailed data on bird actions, facilitating the development of deep learning models to recognize these, similar to the advancements made in human action recognition. The proposed dataset comprises 178 videos recorded in Spanish wetlands, capturing 13 different bird species performing 7 distinct behavior classes. In addition, we also present baseline results using state of the art models on two tasks: bird behavior recognition and species classification.","sentences":["The current biodiversity loss crisis makes animal monitoring a relevant field of study.","In light of this, data collected through monitoring can provide essential insights, and information for decision-making aimed at preserving global biodiversity.","Despite the importance of such data, there is a notable scarcity of datasets featuring videos of birds, and none of the existing datasets offer detailed annotations of bird behaviors in video format.","In response to this gap, our study introduces the first fine-grained video dataset specifically designed for bird behavior detection and species classification.","This dataset addresses the need for comprehensive bird video datasets and provides detailed data on bird actions, facilitating the development of deep learning models to recognize these, similar to the advancements made in human action recognition.","The proposed dataset comprises 178 videos recorded in Spanish wetlands, capturing 13 different bird species performing 7 distinct behavior classes.","In addition, we also present baseline results using state of the art models on two tasks: bird behavior recognition and species classification."],"url":"http://arxiv.org/abs/2501.08931v1"}
{"created":"2025-01-15 16:32:52","title":"A stochastic programming approach for the scheduling of medical interpreting service under uncertainty","abstract":"Limited English Proficiency (LEP) patients face higher risks of adverse health outcomes due to communication barriers, making timely medical interpreting services essential for mitigating those risks. This paper addresses the scheduling of medical interpreting services under uncertainty. The problem is formulated as a two-stage stochastic programming model that accounts for uncertainties in emergency patients' arrival and service time. The model handles the hiring decisions of part-time interpreters and the assignment of full-time and hired part-time interpreters. The objective is to minimize the total cost, which encompasses full-time interpreters' overtime cost, the fixed and variable costs of part-time interpreters, and the penalty cost for not serving LEP patients on time. The model is solved using the Sample Average Approximation (SAA) algorithm. To overcome the computational burden of the SAA algorithm, a Tabu Search (TS) algorithm was used to solve the model. A real-life case study is used to validate and evaluate the proposed solution algorithms. The results demonstrate the effectiveness of the proposed stochastic programming-based solutions in concurrently reducing both the total cost and the waiting time. Further, sensitivity analysis reveals how the increase in some key parameters, such as the arrival rate of emergency patients with LEP, impacts scheduling outcomes.","sentences":["Limited English Proficiency (LEP) patients face higher risks of adverse health outcomes due to communication barriers, making timely medical interpreting services essential for mitigating those risks.","This paper addresses the scheduling of medical interpreting services under uncertainty.","The problem is formulated as a two-stage stochastic programming model that accounts for uncertainties in emergency patients' arrival and service time.","The model handles the hiring decisions of part-time interpreters and the assignment of full-time and hired part-time interpreters.","The objective is to minimize the total cost, which encompasses full-time interpreters' overtime cost, the fixed and variable costs of part-time interpreters, and the penalty cost for not serving LEP patients on time.","The model is solved using the Sample Average Approximation (SAA) algorithm.","To overcome the computational burden of the SAA algorithm, a Tabu Search (TS) algorithm was used to solve the model.","A real-life case study is used to validate and evaluate the proposed solution algorithms.","The results demonstrate the effectiveness of the proposed stochastic programming-based solutions in concurrently reducing both the total cost and the waiting time.","Further, sensitivity analysis reveals how the increase in some key parameters, such as the arrival rate of emergency patients with LEP, impacts scheduling outcomes."],"url":"http://arxiv.org/abs/2501.08929v1"}
{"created":"2025-01-15 16:32:51","title":"Formulas as Processes, Deadlock-Freedom as Choreographies (Extended Version)","abstract":"We introduce a novel approach to studying properties of processes in the {\\pi}-calculus based on a processes-as-formulas interpretation, by establishing a correspondence between specific sequent calculus derivations and computation trees in the reduction semantics of the recursion-free {\\pi}-calculus. Our method provides a simple logical characterisation of deadlock-freedom for the recursion- and race-free fragment of the {\\pi}-calculus, supporting key features such as cyclic dependencies and an independence of the name restriction and parallel operators. Based on this technique, we establish a strong completeness result for a nontrivial choreographic language: all deadlock-free and race-free finite {\\pi}-calculus processes composed in parallel at the top level can be faithfully represented by a choreography. With these results, we show how the paradigm of computation-as-derivation extends the reach of logical methods for the study of concurrency, by bridging important gaps between logic, the expressiveness of the {\\pi}-calculus, and the expressiveness of choreographic languages.","sentences":["We introduce a novel approach to studying properties of processes in the {\\pi}-calculus based on a processes-as-formulas interpretation, by establishing a correspondence between specific sequent calculus derivations and computation trees in the reduction semantics of the recursion-free {\\pi}-calculus.","Our method provides a simple logical characterisation of deadlock-freedom for the recursion- and race-free fragment of the {\\pi}-calculus, supporting key features such as cyclic dependencies and an independence of the name restriction and parallel operators.","Based on this technique, we establish a strong completeness result for a nontrivial choreographic language: all deadlock-free and race-free finite {\\pi}-calculus processes composed in parallel at the top level can be faithfully represented by a choreography.","With these results, we show how the paradigm of computation-as-derivation extends the reach of logical methods for the study of concurrency, by bridging important gaps between logic, the expressiveness of the {\\pi}-calculus, and the expressiveness of choreographic languages."],"url":"http://arxiv.org/abs/2501.08928v1"}
{"created":"2025-01-15 16:30:29","title":"Disentangling Exploration of Large Language Models by Optimal Exploitation","abstract":"Exploration is a crucial skill for self-improvement and open-ended problem-solving. However, it remains uncertain whether large language models can effectively explore the state-space. Existing evaluations predominantly focus on the trade-off between exploration and exploitation, often assessed in multi-armed bandit problems. In contrast, this work isolates exploration as the sole objective, tasking the agent with delivering information that enhances future returns. For the evaluation, we propose to decompose missing rewards into exploration and exploitation components by measuring the optimal achievable return for the states already explored. Our experiments with various LLMs reveal that most models struggle to sufficiently explore the state-space and that weak exploration is insufficient. We observe a positive correlation between model size and exploration performance, with larger models demonstrating superior capabilities. Furthermore, we show that our decomposition provides insights into differences in behaviors driven by agent instructions during prompt engineering, offering a valuable tool for refining LLM performance in exploratory tasks.","sentences":["Exploration is a crucial skill for self-improvement and open-ended problem-solving.","However, it remains uncertain whether large language models can effectively explore the state-space.","Existing evaluations predominantly focus on the trade-off between exploration and exploitation, often assessed in multi-armed bandit problems.","In contrast, this work isolates exploration as the sole objective, tasking the agent with delivering information that enhances future returns.","For the evaluation, we propose to decompose missing rewards into exploration and exploitation components by measuring the optimal achievable return for the states already explored.","Our experiments with various LLMs reveal that most models struggle to sufficiently explore the state-space and that weak exploration is insufficient.","We observe a positive correlation between model size and exploration performance, with larger models demonstrating superior capabilities.","Furthermore, we show that our decomposition provides insights into differences in behaviors driven by agent instructions during prompt engineering, offering a valuable tool for refining LLM performance in exploratory tasks."],"url":"http://arxiv.org/abs/2501.08925v1"}
{"created":"2025-01-15 16:30:05","title":"Learning Joint Denoising, Demosaicing, and Compression from the Raw Natural Image Noise Dataset","abstract":"This paper introduces the Raw Natural Image Noise Dataset (RawNIND), a diverse collection of paired raw images designed to support the development of denoising models that generalize across sensors, image development workflows, and styles. Two denoising methods are proposed: one operates directly on raw Bayer data, leveraging computational efficiency, while the other processes linear RGB images for improved generalization to different sensors, with both preserving flexibility for subsequent development. Both methods outperform traditional approaches which rely on developed images. Additionally, the integration of denoising and compression at the raw data level significantly enhances rate-distortion performance and computational efficiency. These findings suggest a paradigm shift toward raw data workflows for efficient and flexible image processing.","sentences":["This paper introduces the Raw Natural Image Noise Dataset (RawNIND), a diverse collection of paired raw images designed to support the development of denoising models that generalize across sensors, image development workflows, and styles.","Two denoising methods are proposed: one operates directly on raw Bayer data, leveraging computational efficiency, while the other processes linear RGB images for improved generalization to different sensors, with both preserving flexibility for subsequent development.","Both methods outperform traditional approaches which rely on developed images.","Additionally, the integration of denoising and compression at the raw data level significantly enhances rate-distortion performance and computational efficiency.","These findings suggest a paradigm shift toward raw data workflows for efficient and flexible image processing."],"url":"http://arxiv.org/abs/2501.08924v1"}
{"created":"2025-01-15 16:26:01","title":"Modeling Melt Pool Features and Spatter Using Symbolic Regression and Machine Learning","abstract":"Additive manufacturing (AM) is a rapidly evolving technology that has attracted applications across a wide range of fields due to its ability to fabricate complex geometries. However, one of the key challenges in AM is achieving consistent print quality. This inconsistency is often attributed to uncontrolled melt pool dynamics, partly caused by spatter which can lead to defects. Therefore, capturing and controlling the evolution of the melt pool is crucial for enhancing process stability and part quality. In this study, we developed a framework to support decision-making in AM operations, facilitating quality control and minimizing defects via machine learning (ML) and polynomial symbolic regression models. We implemented experimentally validated computational tools as a cost-effective approach to collect large datasets from laser powder bed fusion (LPBF) processes. For a dataset consisting of 281 process conditions, parameters such as melt pool dimensions (length, width, depth), melt pool geometry (area, volume), and volume indicated as spatter were extracted. Using machine learning (ML) and polynomial symbolic regression models, a high R2 of over 95 % was achieved in predicting the melt pool dimensions and geometry features for both the training and testing datasets, with either process conditions (power and velocity) or melt pool dimensions as the model inputs. In the case of volume indicated as spatter, R2 improved after logarithmic transforming the model inputs, which was either the process conditions or the melt pool dimensions. Among the investigated ML models, the ExtraTree model achieved the highest R2 values of 96.7 % and 87.5 %.","sentences":["Additive manufacturing (AM) is a rapidly evolving technology that has attracted applications across a wide range of fields due to its ability to fabricate complex geometries.","However, one of the key challenges in AM is achieving consistent print quality.","This inconsistency is often attributed to uncontrolled melt pool dynamics, partly caused by spatter which can lead to defects.","Therefore, capturing and controlling the evolution of the melt pool is crucial for enhancing process stability and part quality.","In this study, we developed a framework to support decision-making in AM operations, facilitating quality control and minimizing defects via machine learning (ML) and polynomial symbolic regression models.","We implemented experimentally validated computational tools as a cost-effective approach to collect large datasets from laser powder bed fusion (LPBF) processes.","For a dataset consisting of 281 process conditions, parameters such as melt pool dimensions (length, width, depth), melt pool geometry (area, volume), and volume indicated as spatter were extracted.","Using machine learning (ML) and polynomial symbolic regression models, a high R2 of over 95 % was achieved in predicting the melt pool dimensions and geometry features for both the training and testing datasets, with either process conditions (power and velocity) or melt pool dimensions as the model inputs.","In the case of volume indicated as spatter, R2 improved after logarithmic transforming the model inputs, which was either the process conditions or the melt pool dimensions.","Among the investigated ML models, the ExtraTree model achieved the highest R2 values of 96.7 % and 87.5 %."],"url":"http://arxiv.org/abs/2501.08922v1"}
{"created":"2025-01-15 16:21:09","title":"GenAI Content Detection Task 3: Cross-Domain Machine-Generated Text Detection Challenge","abstract":"Recently there have been many shared tasks targeting the detection of generated text from Large Language Models (LLMs). However, these shared tasks tend to focus either on cases where text is limited to one particular domain or cases where text can be from many domains, some of which may not be seen during test time. In this shared task, using the newly released RAID benchmark, we aim to answer whether or not models can detect generated text from a large, yet fixed, number of domains and LLMs, all of which are seen during training. Over the course of three months, our task was attempted by 9 teams with 23 detector submissions. We find that multiple participants were able to obtain accuracies of over 99% on machine-generated text from RAID while maintaining a 5% False Positive Rate -- suggesting that detectors are able to robustly detect text from many domains and models simultaneously. We discuss potential interpretations of this result and provide directions for future research.","sentences":["Recently there have been many shared tasks targeting the detection of generated text from Large Language Models (LLMs).","However, these shared tasks tend to focus either on cases where text is limited to one particular domain or cases where text can be from many domains, some of which may not be seen during test time.","In this shared task, using the newly released RAID benchmark, we aim to answer whether or not models can detect generated text from a large, yet fixed, number of domains and LLMs, all of which are seen during training.","Over the course of three months, our task was attempted by 9 teams with 23 detector submissions.","We find that multiple participants were able to obtain accuracies of over 99% on machine-generated text from RAID while maintaining a 5% False Positive Rate -- suggesting that detectors are able to robustly detect text from many domains and models simultaneously.","We discuss potential interpretations of this result and provide directions for future research."],"url":"http://arxiv.org/abs/2501.08913v1"}
{"created":"2025-01-15 16:20:26","title":"Empowering Agricultural Insights: RiceLeafBD - A Novel Dataset and Optimal Model Selection for Rice Leaf Disease Diagnosis through Transfer Learning Technique","abstract":"The number of people living in this agricultural nation of ours, which is surrounded by lush greenery, is growing on a daily basis. As a result of this, the level of arable land is decreasing, as well as residential houses and industrial factories. The food crisis is becoming the main threat for us in the upcoming days. Because on the one hand, the population is increasing, and on the other hand, the amount of food crop production is decreasing due to the attack of diseases. Rice is one of the most significant cultivated crops since it provides food for more than half of the world's population. Bangladesh is dependent on rice (Oryza sativa) as a vital crop for its agriculture, but it faces a significant problem as a result of the ongoing decline in rice yield brought on by common diseases. Early disease detection is the main difficulty in rice crop cultivation. In this paper, we proposed our own dataset, which was collected from the Bangladesh field, and also applied deep learning and transfer learning models for the evaluation of the datasets. We elaborately explain our dataset and also give direction for further research work to serve society using this dataset. We applied a light CNN model and pre-trained InceptionNet-V2, EfficientNet-V2, and MobileNet-V2 models, which achieved 91.5% performance for the EfficientNet-V2 model of this work. The results obtained assaulted other models and even exceeded approaches that are considered to be part of the state of the art. It has been demonstrated by this study that it is possible to precisely and effectively identify diseases that affect rice leaves using this unbiased datasets. After analysis of the performance of different models, the proposed datasets are significant for the society for research work to provide solutions for decreasing rice leaf disease.","sentences":["The number of people living in this agricultural nation of ours, which is surrounded by lush greenery, is growing on a daily basis.","As a result of this, the level of arable land is decreasing, as well as residential houses and industrial factories.","The food crisis is becoming the main threat for us in the upcoming days.","Because on the one hand, the population is increasing, and on the other hand, the amount of food crop production is decreasing due to the attack of diseases.","Rice is one of the most significant cultivated crops since it provides food for more than half of the world's population.","Bangladesh is dependent on rice (Oryza sativa) as a vital crop for its agriculture, but it faces a significant problem as a result of the ongoing decline in rice yield brought on by common diseases.","Early disease detection is the main difficulty in rice crop cultivation.","In this paper, we proposed our own dataset, which was collected from the Bangladesh field, and also applied deep learning and transfer learning models for the evaluation of the datasets.","We elaborately explain our dataset and also give direction for further research work to serve society using this dataset.","We applied a light CNN model and pre-trained InceptionNet-V2, EfficientNet-V2, and MobileNet-V2 models, which achieved 91.5% performance for the EfficientNet-V2 model of this work.","The results obtained assaulted other models and even exceeded approaches that are considered to be part of the state of the art.","It has been demonstrated by this study that it is possible to precisely and effectively identify diseases that affect rice leaves using this unbiased datasets.","After analysis of the performance of different models, the proposed datasets are significant for the society for research work to provide solutions for decreasing rice leaf disease."],"url":"http://arxiv.org/abs/2501.08912v1"}
{"created":"2025-01-15 16:19:37","title":"Lights, Camera, Matching: The Role of Image Illumination in Fair Face Recognition","abstract":"Facial brightness is a key image quality factor impacting face recognition accuracy differentials across demographic groups. In this work, we aim to decrease the accuracy gap between the similarity score distributions for Caucasian and African American female mated image pairs, as measured by d' between distributions. To balance brightness across demographic groups, we conduct three experiments, interpreting brightness in the face skin region either as median pixel value or as the distribution of pixel values. Balancing based on median brightness alone yields up to a 46.8% decrease in d', while balancing based on brightness distribution yields up to a 57.6% decrease. In all three cases, the similarity scores of the individual distributions improve, with mean scores maximally improving 5.9% for Caucasian females and 3.7% for African American females.","sentences":["Facial brightness is a key image quality factor impacting face recognition accuracy differentials across demographic groups.","In this work, we aim to decrease the accuracy gap between the similarity score distributions for Caucasian and African American female mated image pairs, as measured by d' between distributions.","To balance brightness across demographic groups, we conduct three experiments, interpreting brightness in the face skin region either as median pixel value or as the distribution of pixel values.","Balancing based on median brightness alone yields up to a 46.8% decrease in d', while balancing based on brightness distribution yields up to a 57.6% decrease.","In all three cases, the similarity scores of the individual distributions improve, with mean scores maximally improving 5.9% for Caucasian females and 3.7% for African American females."],"url":"http://arxiv.org/abs/2501.08910v1"}
{"created":"2025-01-15 16:19:12","title":"Software Testing for Extended Reality Applications: A Systematic Mapping Study","abstract":"Extended Reality (XR) is an emerging technology spanning diverse application domains and offering immersive user experiences. However, its unique characteristics, such as six degrees of freedom interactions, present significant testing challenges distinct from traditional 2D GUI applications, demanding novel testing techniques to build high-quality XR applications. This paper presents the first systematic mapping study on software testing for XR applications. We selected 34 studies focusing on techniques and empirical approaches in XR software testing for detailed examination. The studies are classified and reviewed to address the current research landscape, test facets, and evaluation methodologies in the XR testing domain. Additionally, we provide a repository summarising the mapping study, including datasets and tools referenced in the selected studies, to support future research and practical applications. Our study highlights open challenges in XR testing and proposes actionable future research directions to address the gaps and advance the field of XR software testing.","sentences":["Extended Reality (XR) is an emerging technology spanning diverse application domains and offering immersive user experiences.","However, its unique characteristics, such as six degrees of freedom interactions, present significant testing challenges distinct from traditional 2D GUI applications, demanding novel testing techniques to build high-quality XR applications.","This paper presents the first systematic mapping study on software testing for XR applications.","We selected 34 studies focusing on techniques and empirical approaches in XR software testing for detailed examination.","The studies are classified and reviewed to address the current research landscape, test facets, and evaluation methodologies in the XR testing domain.","Additionally, we provide a repository summarising the mapping study, including datasets and tools referenced in the selected studies, to support future research and practical applications.","Our study highlights open challenges in XR testing and proposes actionable future research directions to address the gaps and advance the field of XR software testing."],"url":"http://arxiv.org/abs/2501.08909v1"}
{"created":"2025-01-15 16:18:13","title":"When Uncertainty Leads to Unsafety: Empirical Insights into the Role of Uncertainty in Unmanned Aerial Vehicle Safety","abstract":"Despite the recent developments in obstacle avoidance and other safety features, autonomous Unmanned Aerial Vehicles (UAVs) continue to face safety challenges. No previous work investigated the relationship between the behavioral uncertainty of a UAV and the unsafety of its flight. By quantifying uncertainty, it is possible to develop a predictor for unsafety, which acts as a flight supervisor. We conducted a large-scale empirical investigation of safety violations using PX4-Autopilot, an open-source UAV software platform. Our dataset of over 5,000 simulated flights, created to challenge obstacle avoidance, allowed us to explore the relation between uncertain UAV decisions and safety violations: up to 89% of unsafe UAV states exhibit significant decision uncertainty, and up to 74% of uncertain decisions lead to unsafe states. Based on these findings, we implemented Superialist (Supervising Autonomous Aerial Vehicles), a runtime uncertainty detector based on autoencoders, the state-of-the-art technology for anomaly detection. Superialist achieved high performance in detecting uncertain behaviors with up to 96% precision and 93% recall. Despite the observed performance degradation when using the same approach for predicting unsafety (up to 74% precision and 87% recall), Superialist enabled early prediction of unsafe states up to 50 seconds in advance.","sentences":["Despite the recent developments in obstacle avoidance and other safety features, autonomous Unmanned Aerial Vehicles (UAVs) continue to face safety challenges.","No previous work investigated the relationship between the behavioral uncertainty of a UAV and the unsafety of its flight.","By quantifying uncertainty, it is possible to develop a predictor for unsafety, which acts as a flight supervisor.","We conducted a large-scale empirical investigation of safety violations using PX4-Autopilot, an open-source UAV software platform.","Our dataset of over 5,000 simulated flights, created to challenge obstacle avoidance, allowed us to explore the relation between uncertain UAV decisions and safety violations: up to 89% of unsafe UAV states exhibit significant decision uncertainty, and up to 74% of uncertain decisions lead to unsafe states.","Based on these findings, we implemented Superialist (Supervising Autonomous Aerial Vehicles), a runtime uncertainty detector based on autoencoders, the state-of-the-art technology for anomaly detection.","Superialist achieved high performance in detecting uncertain behaviors with up to 96% precision and 93% recall.","Despite the observed performance degradation when using the same approach for predicting unsafety (up to 74% precision and 87% recall), Superialist enabled early prediction of unsafe states up to 50 seconds in advance."],"url":"http://arxiv.org/abs/2501.08908v1"}
{"created":"2025-01-15 16:17:02","title":"Projection Implicit Q-Learning with Support Constraint for Offline Reinforcement Learning","abstract":"Offline Reinforcement Learning (RL) faces a critical challenge of extrapolation errors caused by out-of-distribution (OOD) actions. Implicit Q-Learning (IQL) algorithm employs expectile regression to achieve in-sample learning, effectively mitigating the risks associated with OOD actions. However, the fixed hyperparameter in policy evaluation and density-based policy improvement method limit its overall efficiency. In this paper, we propose Proj-IQL, a projective IQL algorithm enhanced with the support constraint. In the policy evaluation phase, Proj-IQL generalizes the one-step approach to a multi-step approach through vector projection, while maintaining in-sample learning and expectile regression framework. In the policy improvement phase, Proj-IQL introduces support constraint that is more aligned with the policy evaluation approach. Furthermore, we theoretically demonstrate that Proj-IQL guarantees monotonic policy improvement and enjoys a progressively more rigorous criterion for superior actions. Empirical results demonstrate the Proj-IQL achieves state-of-the-art performance on D4RL benchmarks, especially in challenging navigation domains.","sentences":["Offline Reinforcement Learning (RL) faces a critical challenge of extrapolation errors caused by out-of-distribution (OOD) actions.","Implicit Q-Learning (IQL) algorithm employs expectile regression to achieve in-sample learning, effectively mitigating the risks associated with OOD actions.","However, the fixed hyperparameter in policy evaluation and density-based policy improvement method limit its overall efficiency.","In this paper, we propose Proj-IQL, a projective IQL algorithm enhanced with the support constraint.","In the policy evaluation phase, Proj-IQL generalizes the one-step approach to a multi-step approach through vector projection, while maintaining in-sample learning and expectile regression framework.","In the policy improvement phase, Proj-IQL introduces support constraint that is more aligned with the policy evaluation approach.","Furthermore, we theoretically demonstrate that Proj-IQL guarantees monotonic policy improvement and enjoys a progressively more rigorous criterion for superior actions.","Empirical results demonstrate the Proj-IQL achieves state-of-the-art performance on D4RL benchmarks, especially in challenging navigation domains."],"url":"http://arxiv.org/abs/2501.08907v1"}
{"created":"2025-01-15 16:15:16","title":"Computing Game Symmetries and Equilibria That Respect Them","abstract":"Strategic interactions can be represented more concisely, and analyzed and solved more efficiently, if we are aware of the symmetries within the multiagent system. Symmetries also have conceptual implications, for example for equilibrium selection. We study the computational complexity of identifying and using symmetries. Using the classical framework of normal-form games, we consider game symmetries that can be across some or all players and/or actions. We find a strong connection between game symmetries and graph automorphisms, yielding graph automorphism and graph isomorphism completeness results for characterizing the symmetries present in a game. On the other hand, we also show that the problem becomes polynomial-time solvable when we restrict the consideration of actions in one of two ways.   Next, we investigate when exactly game symmetries can be successfully leveraged for Nash equilibrium computation. We show that finding a Nash equilibrium that respects a given set of symmetries is PPAD- and CLS-complete in general-sum and team games respectively -- that is, exactly as hard as Brouwer fixed point and gradient descent problems. Finally, we present polynomial-time methods for the special cases where we are aware of a vast number of symmetries, or where the game is two-player zero-sum and we do not even know the symmetries.","sentences":["Strategic interactions can be represented more concisely, and analyzed and solved more efficiently, if we are aware of the symmetries within the multiagent system.","Symmetries also have conceptual implications, for example for equilibrium selection.","We study the computational complexity of identifying and using symmetries.","Using the classical framework of normal-form games, we consider game symmetries that can be across some or all players and/or actions.","We find a strong connection between game symmetries and graph automorphisms, yielding graph automorphism and graph isomorphism completeness results for characterizing the symmetries present in a game.","On the other hand, we also show that the problem becomes polynomial-time solvable when we restrict the consideration of actions in one of two ways.   ","Next, we investigate when exactly game symmetries can be successfully leveraged for Nash equilibrium computation.","We show that finding a Nash equilibrium that respects a given set of symmetries is PPAD- and CLS-complete in general-sum and team games respectively -- that is, exactly as hard as Brouwer fixed point and gradient descent problems.","Finally, we present polynomial-time methods for the special cases where we are aware of a vast number of symmetries, or where the game is two-player zero-sum and we do not even know the symmetries."],"url":"http://arxiv.org/abs/2501.08905v1"}
{"created":"2025-01-15 16:08:25","title":"Enhanced Multi-Scale Cross-Attention for Person Image Generation","abstract":"In this paper, we propose a novel cross-attention-based generative adversarial network (GAN) for the challenging person image generation task. Cross-attention is a novel and intuitive multi-modal fusion method in which an attention/correlation matrix is calculated between two feature maps of different modalities. Specifically, we propose the novel XingGAN (or CrossingGAN), which consists of two generation branches that capture the person's appearance and shape, respectively. Moreover, we propose two novel cross-attention blocks to effectively transfer and update the person's shape and appearance embeddings for mutual improvement. This has not been considered by any other existing GAN-based image generation work. To further learn the long-range correlations between different person poses at different scales and sub-regions, we propose two novel multi-scale cross-attention blocks. To tackle the issue of independent correlation computations within the cross-attention mechanism leading to noisy and ambiguous attention weights, which hinder performance improvements, we propose a module called enhanced attention (EA). Lastly, we introduce a novel densely connected co-attention module to fuse appearance and shape features at different stages effectively. Extensive experiments on two public datasets demonstrate that the proposed method outperforms current GAN-based methods and performs on par with diffusion-based methods. However, our method is significantly faster than diffusion-based methods in both training and inference.","sentences":["In this paper, we propose a novel cross-attention-based generative adversarial network (GAN) for the challenging person image generation task.","Cross-attention is a novel and intuitive multi-modal fusion method in which an attention/correlation matrix is calculated between two feature maps of different modalities.","Specifically, we propose the novel XingGAN (or CrossingGAN), which consists of two generation branches that capture the person's appearance and shape, respectively.","Moreover, we propose two novel cross-attention blocks to effectively transfer and update the person's shape and appearance embeddings for mutual improvement.","This has not been considered by any other existing GAN-based image generation work.","To further learn the long-range correlations between different person poses at different scales and sub-regions, we propose two novel multi-scale cross-attention blocks.","To tackle the issue of independent correlation computations within the cross-attention mechanism leading to noisy and ambiguous attention weights, which hinder performance improvements, we propose a module called enhanced attention (EA).","Lastly, we introduce a novel densely connected co-attention module to fuse appearance and shape features at different stages effectively.","Extensive experiments on two public datasets demonstrate that the proposed method outperforms current GAN-based methods and performs on par with diffusion-based methods.","However, our method is significantly faster than diffusion-based methods in both training and inference."],"url":"http://arxiv.org/abs/2501.08900v1"}
{"created":"2025-01-15 16:06:10","title":"Leveraging Large Language Models as Knowledge-Driven Agents for Reliable Retrosynthesis Planning","abstract":"Identifying reliable synthesis pathways in materials chemistry is a complex task, particularly in polymer science, due to the intricate and often non-unique nomenclature of macromolecules. To address this challenge, we propose an agent system that integrates large language models (LLMs) and knowledge graphs (KGs). By leveraging LLMs' powerful capabilities for extracting and recognizing chemical substance names, and storing the extracted data in a structured knowledge graph, our system fully automates the retrieval of relevant literatures, extraction of reaction data, database querying, construction of retrosynthetic pathway trees, further expansion through the retrieval of additional literature and recommendation of optimal reaction pathways. A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm enables the exploration of all pathways, with a particular focus on multi-branched ones, helping LLMs overcome weak reasoning in multi-branched paths. This work represents the first attempt to develop a fully automated retrosynthesis planning agent tailored specially for macromolecules powered by LLMs. Applied to polyimide synthesis, our new approach constructs a retrosynthetic pathway tree with hundreds of pathways and recommends optimized routes, including both known and novel pathways, demonstrating its effectiveness and potential for broader applications.","sentences":["Identifying reliable synthesis pathways in materials chemistry is a complex task, particularly in polymer science, due to the intricate and often non-unique nomenclature of macromolecules.","To address this challenge, we propose an agent system that integrates large language models (LLMs) and knowledge graphs (KGs).","By leveraging LLMs' powerful capabilities for extracting and recognizing chemical substance names, and storing the extracted data in a structured knowledge graph, our system fully automates the retrieval of relevant literatures, extraction of reaction data, database querying, construction of retrosynthetic pathway trees, further expansion through the retrieval of additional literature and recommendation of optimal reaction pathways.","A novel Multi-branched Reaction Pathway Search (MBRPS) algorithm enables the exploration of all pathways, with a particular focus on multi-branched ones, helping LLMs overcome weak reasoning in multi-branched paths.","This work represents the first attempt to develop a fully automated retrosynthesis planning agent tailored specially for macromolecules powered by LLMs.","Applied to polyimide synthesis, our new approach constructs a retrosynthetic pathway tree with hundreds of pathways and recommends optimized routes, including both known and novel pathways, demonstrating its effectiveness and potential for broader applications."],"url":"http://arxiv.org/abs/2501.08897v1"}
{"created":"2025-01-15 16:05:50","title":"Parallel Query Processing with Heterogeneous Machines","abstract":"We study the problem of computing a full Conjunctive Query in parallel using $p$ heterogeneous machines. Our computational model is similar to the MPC model, but each machine has its own cost function mapping from the number of bits it receives to a cost. An optimal algorithm should minimize the maximum cost across all machines. We consider algorithms over a single communication round and give a lower bound and matching upper bound for databases where each relation has the same cardinality. We do this for both linear cost functions like in previous work, but also for more general cost functions. For databases with relations of different cardinalities, we also find a lower bound, and give matching upper bounds for specific queries like the cartesian product, the join, the star query, and the triangle query. Our approach is inspired by the HyperCube algorithm, but there are additional challenges involved when machines have heterogeneous cost functions.","sentences":["We study the problem of computing a full Conjunctive Query in parallel using $p$ heterogeneous machines.","Our computational model is similar to the MPC model, but each machine has its own cost function mapping from the number of bits it receives to a cost.","An optimal algorithm should minimize the maximum cost across all machines.","We consider algorithms over a single communication round and give a lower bound and matching upper bound for databases where each relation has the same cardinality.","We do this for both linear cost functions like in previous work, but also for more general cost functions.","For databases with relations of different cardinalities, we also find a lower bound, and give matching upper bounds for specific queries like the cartesian product, the join, the star query, and the triangle query.","Our approach is inspired by the HyperCube algorithm, but there are additional challenges involved when machines have heterogeneous cost functions."],"url":"http://arxiv.org/abs/2501.08896v1"}
{"created":"2025-01-15 16:05:03","title":"Profile and neighbourhood complexity of graphs with excluded minors and tree-structured graphs","abstract":"The $r$-neighbourhood complexity of a graph $G$ is the function counting, for a given integer $k$, the largest possible number, over all vertex-subsets $A$ of size $k$, of subsets of $A$ realized as the intersection between the $r$-neighbourhood of some vertex and $A$. A refinement of this notion is the $r$-profile complexity, that counts the maximum number of distinct distance-vectors from any vertex to the vertices of $A$, ignoring distances larger than $r$. Typically, in structured graph classes such as graphs of bounded VC-dimension or chordal graphs, these functions are bounded, leading to insights into their structural properties and efficient algorithms.   We improve existing bounds on the $r$-profile complexity (and thus on the $r$-neighbourhood complexity) for graphs in several structured graph classes. We show that the $r$-profile complexity of graphs excluding $K_h$ as a minor is in $O_h(r^{3h-3}k)$. For graphs of treewidth at most $t$ we give a bound in $O_t(r^{t+1}k)$, which is tight up to a function of $t$ as a factor. These bounds improve results and answer a question of Joret and Rambaud [Combinatorica, 2024].   For outerplanar graphs, we can improve our treewidth bound by a factor of $r$ and conjecture that a similar improvement holds for graphs with bounded simple treewidth. For graphs of treelength at most $\\ell$, we give the upper bound in $O(k(r^2(\\ell+1)^k))$.   Our bounds also imply relations between the order, diameter and metric dimension of graphs in these classes, improving results from [Beaudou et al., SIDMA 2017].","sentences":["The $r$-neighbourhood complexity of a graph $G$ is the function counting, for a given integer $k$, the largest possible number, over all vertex-subsets $A$ of size $k$, of subsets of $A$ realized as the intersection between the $r$-neighbourhood of some vertex and $A$.","A refinement of this notion is the $r$-profile complexity, that counts the maximum number of distinct distance-vectors from any vertex to the vertices of $A$, ignoring distances larger than $r$. Typically, in structured graph classes such as graphs of bounded VC-dimension or chordal graphs, these functions are bounded, leading to insights into their structural properties and efficient algorithms.   ","We improve existing bounds on the $r$-profile complexity (and thus on the $r$-neighbourhood complexity) for graphs in several structured graph classes.","We show that the $r$-profile complexity of graphs excluding $K_h$ as a minor is in $O_h(r^{3h-3}k)$. For graphs of treewidth at most $t$ we give a bound in $O_t(r^{t+1}k)$, which is tight up to a function of $t$ as a factor.","These bounds improve results and answer a question of Joret and Rambaud","[Combinatorica, 2024].   ","For outerplanar graphs, we can improve our treewidth bound by a factor of $r$ and conjecture that a similar improvement holds for graphs with bounded simple treewidth.","For graphs of treelength at most $\\ell$, we give the upper bound in $O(k(r^2(\\ell+1)^k))$.   Our bounds also imply relations between the order, diameter and metric dimension of graphs in these classes, improving results from [Beaudou et al., SIDMA 2017]."],"url":"http://arxiv.org/abs/2501.08895v1"}
{"created":"2025-01-15 16:00:43","title":"Karatsuba Matrix Multiplication and its Efficient Custom Hardware Implementations","abstract":"While the Karatsuba algorithm reduces the complexity of large integer multiplication, the extra additions required minimize its benefits for smaller integers of more commonly-used bitwidths. In this work, we propose the extension of the scalar Karatsuba multiplication algorithm to matrix multiplication, showing how this maintains the reduction in multiplication complexity of the original Karatsuba algorithm while reducing the complexity of the extra additions. Furthermore, we propose new matrix multiplication hardware architectures for efficiently exploiting this extension of the Karatsuba algorithm in custom hardware. We show that the proposed algorithm and hardware architectures can provide real area or execution time improvements for integer matrix multiplication compared to scalar Karatsuba or conventional matrix multiplication algorithms, while also supporting implementation through proven systolic array and conventional multiplier architectures at the core. We provide a complexity analysis of the algorithm and architectures and evaluate the proposed designs both in isolation and in an end-to-end deep learning accelerator system compared to baseline designs and prior state-of-the-art works implemented on the same type of compute platform, demonstrating their ability to increase the performance-per-area of matrix multiplication hardware.","sentences":["While the Karatsuba algorithm reduces the complexity of large integer multiplication, the extra additions required minimize its benefits for smaller integers of more commonly-used bitwidths.","In this work, we propose the extension of the scalar Karatsuba multiplication algorithm to matrix multiplication, showing how this maintains the reduction in multiplication complexity of the original Karatsuba algorithm while reducing the complexity of the extra additions.","Furthermore, we propose new matrix multiplication hardware architectures for efficiently exploiting this extension of the Karatsuba algorithm in custom hardware.","We show that the proposed algorithm and hardware architectures can provide real area or execution time improvements for integer matrix multiplication compared to scalar Karatsuba or conventional matrix multiplication algorithms, while also supporting implementation through proven systolic array and conventional multiplier architectures at the core.","We provide a complexity analysis of the algorithm and architectures and evaluate the proposed designs both in isolation and in an end-to-end deep learning accelerator system compared to baseline designs and prior state-of-the-art works implemented on the same type of compute platform, demonstrating their ability to increase the performance-per-area of matrix multiplication hardware."],"url":"http://arxiv.org/abs/2501.08889v1"}
{"created":"2025-01-15 15:58:16","title":"A Two-Stage Pretraining-Finetuning Framework for Treatment Effect Estimation with Unmeasured Confounding","abstract":"Estimating the conditional average treatment effect (CATE) from observational data plays a crucial role in areas such as e-commerce, healthcare, and economics. Existing studies mainly rely on the strong ignorability assumption that there are no unmeasured confounders, whose presence cannot be tested from observational data and can invalidate any causal conclusion. In contrast, data collected from randomized controlled trials (RCT) do not suffer from confounding, but are usually limited by a small sample size. In this paper, we propose a two-stage pretraining-finetuning (TSPF) framework using both large-scale observational data and small-scale RCT data to estimate the CATE in the presence of unmeasured confounding. In the first stage, a foundational representation of covariates is trained to estimate counterfactual outcomes through large-scale observational data. In the second stage, we propose to train an augmented representation of the covariates, which is concatenated to the foundational representation obtained in the first stage to adjust for the unmeasured confounding. To avoid overfitting caused by the small-scale RCT data in the second stage, we further propose a partial parameter initialization approach, rather than training a separate network. The superiority of our approach is validated on two public datasets with extensive experiments. The code is available at https://github.com/zhouchuanCN/KDD25-TSPF.","sentences":["Estimating the conditional average treatment effect (CATE) from observational data plays a crucial role in areas such as e-commerce, healthcare, and economics.","Existing studies mainly rely on the strong ignorability assumption that there are no unmeasured confounders, whose presence cannot be tested from observational data and can invalidate any causal conclusion.","In contrast, data collected from randomized controlled trials (RCT) do not suffer from confounding, but are usually limited by a small sample size.","In this paper, we propose a two-stage pretraining-finetuning (TSPF) framework using both large-scale observational data and small-scale RCT data to estimate the CATE in the presence of unmeasured confounding.","In the first stage, a foundational representation of covariates is trained to estimate counterfactual outcomes through large-scale observational data.","In the second stage, we propose to train an augmented representation of the covariates, which is concatenated to the foundational representation obtained in the first stage to adjust for the unmeasured confounding.","To avoid overfitting caused by the small-scale RCT data in the second stage, we further propose a partial parameter initialization approach, rather than training a separate network.","The superiority of our approach is validated on two public datasets with extensive experiments.","The code is available at https://github.com/zhouchuanCN/KDD25-TSPF."],"url":"http://arxiv.org/abs/2501.08888v1"}
{"created":"2025-01-15 15:57:13","title":"PAC Learnability of Scenario Decision-Making Algorithms: Necessary and Sufficient Conditions","abstract":"We study the PAC property of scenario decision-making algorithms, that is, the ability to make a decision that has an arbitrarily low risk of violating an unknown safety constraint, provided sufficiently many realizations (called scenarios) of the safety constraint are sampled. Sufficient conditions for scenario decision-making algorithms to be PAC are available in the literature, such as finiteness of the VC dimension of its associated classifier and existence of a compression scheme. We study the question of whether these sufficient conditions are also necessary. We show with counterexamples that this is not the case in general. This contrasts with binary classification learning, for which the analogous conditions are sufficient and necessary. Popular scenario decision-making algorithms, such as scenario optimization, enjoy additional properties, such as stability and consistency. We show that even under these additional assumptions the above conclusions hold. Finally, we derive a necessary condition for scenario decision-making algorithms to be PAC, inspired by the VC dimension and the so-called no-free-lunch theorem.","sentences":["We study the PAC property of scenario decision-making algorithms, that is, the ability to make a decision that has an arbitrarily low risk of violating an unknown safety constraint, provided sufficiently many realizations (called scenarios) of the safety constraint are sampled.","Sufficient conditions for scenario decision-making algorithms to be PAC are available in the literature, such as finiteness of the VC dimension of its associated classifier and existence of a compression scheme.","We study the question of whether these sufficient conditions are also necessary.","We show with counterexamples that this is not the case in general.","This contrasts with binary classification learning, for which the analogous conditions are sufficient and necessary.","Popular scenario decision-making algorithms, such as scenario optimization, enjoy additional properties, such as stability and consistency.","We show that even under these additional assumptions the above conclusions hold.","Finally, we derive a necessary condition for scenario decision-making algorithms to be PAC, inspired by the VC dimension and the so-called no-free-lunch theorem."],"url":"http://arxiv.org/abs/2501.08887v1"}
{"created":"2025-01-15 15:56:06","title":"Feature-based One-For-All: A Universal Framework for Heterogeneous Knowledge Distillation","abstract":"Knowledge distillation (KD) involves transferring knowledge from a pre-trained heavy teacher model to a lighter student model, thereby reducing the inference cost while maintaining comparable effectiveness. Prior KD techniques typically assume homogeneity between the teacher and student models. However, as technology advances, a wide variety of architectures have emerged, ranging from initial Convolutional Neural Networks (CNNs) to Vision Transformers (ViTs), and Multi-Level Perceptrons (MLPs). Consequently, developing a universal KD framework compatible with any architecture has become an important research topic. In this paper, we introduce a feature-based one-for-all (FOFA) KD framework to enable feature distillation across diverse architecture. Our framework comprises two key components. First, we design prompt tuning blocks that incorporate student feedback, allowing teacher features to adapt to the student model's learning process. Second, we propose region-aware attention to mitigate the view mismatch problem between heterogeneous architecture. By leveraging these two modules, effective distillation of intermediate features can be achieved across heterogeneous architectures. Extensive experiments on CIFAR, ImageNet, and COCO demonstrate the superiority of the proposed method.","sentences":["Knowledge distillation (KD) involves transferring knowledge from a pre-trained heavy teacher model to a lighter student model, thereby reducing the inference cost while maintaining comparable effectiveness.","Prior KD techniques typically assume homogeneity between the teacher and student models.","However, as technology advances, a wide variety of architectures have emerged, ranging from initial Convolutional Neural Networks (CNNs) to Vision Transformers (ViTs), and Multi-Level Perceptrons (MLPs).","Consequently, developing a universal KD framework compatible with any architecture has become an important research topic.","In this paper, we introduce a feature-based one-for-all (FOFA) KD framework to enable feature distillation across diverse architecture.","Our framework comprises two key components.","First, we design prompt tuning blocks that incorporate student feedback, allowing teacher features to adapt to the student model's learning process.","Second, we propose region-aware attention to mitigate the view mismatch problem between heterogeneous architecture.","By leveraging these two modules, effective distillation of intermediate features can be achieved across heterogeneous architectures.","Extensive experiments on CIFAR, ImageNet, and COCO demonstrate the superiority of the proposed method."],"url":"http://arxiv.org/abs/2501.08885v1"}
{"created":"2025-01-15 15:53:27","title":"Increasing Batch Size Improves Convergence of Stochastic Gradient Descent with Momentum","abstract":"Stochastic gradient descent with momentum (SGDM), which is defined by adding a momentum term to SGD, has been well studied in both theory and practice. Theoretically investigated results showed that the settings of the learning rate and momentum weight affect the convergence of SGDM. Meanwhile, practical results showed that the setting of batch size strongly depends on the performance of SGDM. In this paper, we focus on mini-batch SGDM with constant learning rate and constant momentum weight, which is frequently used to train deep neural networks in practice. The contribution of this paper is showing theoretically that using a constant batch size does not always minimize the expectation of the full gradient norm of the empirical loss in training a deep neural network, whereas using an increasing batch size definitely minimizes it, that is, increasing batch size improves convergence of mini-batch SGDM. We also provide numerical results supporting our analyses, indicating specifically that mini-batch SGDM with an increasing batch size converges to stationary points faster than with a constant batch size. Python implementations of the optimizers used in the numerical experiments are available at https://anonymous.4open.science/r/momentum-increasing-batch-size-888C/.","sentences":["Stochastic gradient descent with momentum (SGDM), which is defined by adding a momentum term to SGD, has been well studied in both theory and practice.","Theoretically investigated results showed that the settings of the learning rate and momentum weight affect the convergence of SGDM.","Meanwhile, practical results showed that the setting of batch size strongly depends on the performance of SGDM.","In this paper, we focus on mini-batch SGDM with constant learning rate and constant momentum weight, which is frequently used to train deep neural networks in practice.","The contribution of this paper is showing theoretically that using a constant batch size does not always minimize the expectation of the full gradient norm of the empirical loss in training a deep neural network, whereas using an increasing batch size definitely minimizes it, that is, increasing batch size improves convergence of mini-batch SGDM.","We also provide numerical results supporting our analyses, indicating specifically that mini-batch SGDM with an increasing batch size converges to stationary points faster than with a constant batch size.","Python implementations of the optimizers used in the numerical experiments are available at https://anonymous.4open.science/r/momentum-increasing-batch-size-888C/."],"url":"http://arxiv.org/abs/2501.08883v1"}
{"created":"2025-01-15 15:51:06","title":"SLC$^2$-SLAM: Semantic-guided Loop Closure with Shared Latent Code for NeRF SLAM","abstract":"Targeting the notorious cumulative drift errors in NeRF SLAM, we propose a Semantic-guided Loop Closure with Shared Latent Code, dubbed SLC$^2$-SLAM. Especially, we argue that latent codes stored in many NeRF SLAM systems are not fully exploited, as they are only used for better reconstruction. In this paper, we propose a simple yet effective way to detect potential loops using the same latent codes as local features. To further improve the loop detection performance, we use the semantic information, which are also decoded from the same latent codes to guide the aggregation of local features. Finally, with the potential loops detected, we close them with a graph optimization followed by bundle adjustment to refine both the estimated poses and the reconstructed scene. To evaluate the performance of our SLC$^2$-SLAM, we conduct extensive experiments on Replica and ScanNet datasets. Our proposed semantic-guided loop closure significantly outperforms the pre-trained NetVLAD and ORB combined with Bag-of-Words, which are used in all the other NeRF SLAM with loop closure. As a result, our SLC$^2$-SLAM also demonstrated better tracking and reconstruction performance, especially in larger scenes with more loops, like ScanNet.","sentences":["Targeting the notorious cumulative drift errors in NeRF SLAM, we propose a Semantic-guided Loop Closure with Shared Latent Code, dubbed SLC$^2$-SLAM.","Especially, we argue that latent codes stored in many NeRF SLAM systems are not fully exploited, as they are only used for better reconstruction.","In this paper, we propose a simple yet effective way to detect potential loops using the same latent codes as local features.","To further improve the loop detection performance, we use the semantic information, which are also decoded from the same latent codes to guide the aggregation of local features.","Finally, with the potential loops detected, we close them with a graph optimization followed by bundle adjustment to refine both the estimated poses and the reconstructed scene.","To evaluate the performance of our SLC$^2$-SLAM, we conduct extensive experiments on Replica and ScanNet datasets.","Our proposed semantic-guided loop closure significantly outperforms the pre-trained NetVLAD and ORB combined with Bag-of-Words, which are used in all the other NeRF SLAM with loop closure.","As a result, our SLC$^2$-SLAM also demonstrated better tracking and reconstruction performance, especially in larger scenes with more loops, like ScanNet."],"url":"http://arxiv.org/abs/2501.08880v1"}
{"created":"2025-01-15 15:49:46","title":"Incrementally Learning Multiple Diverse Data Domains via Multi-Source Dynamic Expansion Model","abstract":"Continual Learning seeks to develop a model capable of incrementally assimilating new information while retaining prior knowledge. However, current research predominantly addresses a straightforward learning context, wherein all data samples originate from a singular data domain. This paper shifts focus to a more complex and realistic learning environment, characterized by data samples sourced from multiple distinct domains. We tackle this intricate learning challenge by introducing a novel methodology, termed the Multi-Source Dynamic Expansion Model (MSDEM), which leverages various pre-trained models as backbones and progressively establishes new experts based on them to adapt to emerging tasks. Additionally, we propose an innovative dynamic expandable attention mechanism designed to selectively harness knowledge from multiple backbones, thereby accelerating the new task learning. Moreover, we introduce a dynamic graph weight router that strategically reuses all previously acquired parameters and representations for new task learning, maximizing the positive knowledge transfer effect, which further improves generalization performance. We conduct a comprehensive series of experiments, and the empirical findings indicate that our proposed approach achieves state-of-the-art performance.","sentences":["Continual Learning seeks to develop a model capable of incrementally assimilating new information while retaining prior knowledge.","However, current research predominantly addresses a straightforward learning context, wherein all data samples originate from a singular data domain.","This paper shifts focus to a more complex and realistic learning environment, characterized by data samples sourced from multiple distinct domains.","We tackle this intricate learning challenge by introducing a novel methodology, termed the Multi-Source Dynamic Expansion Model (MSDEM), which leverages various pre-trained models as backbones and progressively establishes new experts based on them to adapt to emerging tasks.","Additionally, we propose an innovative dynamic expandable attention mechanism designed to selectively harness knowledge from multiple backbones, thereby accelerating the new task learning.","Moreover, we introduce a dynamic graph weight router that strategically reuses all previously acquired parameters and representations for new task learning, maximizing the positive knowledge transfer effect, which further improves generalization performance.","We conduct a comprehensive series of experiments, and the empirical findings indicate that our proposed approach achieves state-of-the-art performance."],"url":"http://arxiv.org/abs/2501.08878v1"}
{"created":"2025-01-15 15:40:42","title":"Joint Detection and Decoding: A Graph Neural Network Approach","abstract":"Narrowing the performance gap between optimal and feasible detection in inter-symbol interference (ISI) channels, this paper proposes to use graph neural networks (GNNs) for detection that can also be used to perform joint detection and decoding (JDD). For detection, the GNN is build upon the factor graph representations of the channel, while for JDD, the factor graph is expanded by the Tanner graph of the parity-check matrix (PCM) of the channel code, sharing the variable nodes (VNs). A particularly advantageous property of the GNN is a) the robustness against cycles in the factor graphs which is the main problem for sum-product algorithm (SPA)-based detection, and b) the robustness against channel state information (CSI) uncertainty at the receiver. Additionally, we propose using an input embedding resulting in a GNN independent of the channel impulse response (CIR). Consequently, a fully deep learning-based receiver enables joint optimization instead of individual optimization of the components, so-called end-to-end learning. Furthermore, we propose a parallel flooding schedule that also reduces the latency, which turns out to improve the error correcting performance. The proposed approach is analyzed and compared to state-of-the-art baselines for different modulations and codes in terms of error correcting capability and latency. The gain compared to SPA-based detection might be explained with improved messages between nodes and adaptive damping of messages. For a higher order modulation in a high-rate turbo detection and decoding (TDD) scenario the GNN shows a, at first glance, surprisingly high gain of 6.25 dB compared to the best, feasible non-neural baseline.","sentences":["Narrowing the performance gap between optimal and feasible detection in inter-symbol interference (ISI) channels, this paper proposes to use graph neural networks (GNNs) for detection that can also be used to perform joint detection and decoding (JDD).","For detection, the GNN is build upon the factor graph representations of the channel, while for JDD, the factor graph is expanded by the Tanner graph of the parity-check matrix (PCM) of the channel code, sharing the variable nodes (VNs).","A particularly advantageous property of the GNN is a) the robustness against cycles in the factor graphs which is the main problem for sum-product algorithm (SPA)-based detection, and b) the robustness against channel state information (CSI) uncertainty at the receiver.","Additionally, we propose using an input embedding resulting in a GNN independent of the channel impulse response (CIR).","Consequently, a fully deep learning-based receiver enables joint optimization instead of individual optimization of the components, so-called end-to-end learning.","Furthermore, we propose a parallel flooding schedule that also reduces the latency, which turns out to improve the error correcting performance.","The proposed approach is analyzed and compared to state-of-the-art baselines for different modulations and codes in terms of error correcting capability and latency.","The gain compared to SPA-based detection might be explained with improved messages between nodes and adaptive damping of messages.","For a higher order modulation in a high-rate turbo detection and decoding (TDD) scenario the GNN shows a, at first glance, surprisingly high gain of 6.25 dB compared to the best, feasible non-neural baseline."],"url":"http://arxiv.org/abs/2501.08871v1"}
{"created":"2025-01-15 15:38:56","title":"Silent Abandonment in Text-Based Contact Centers: Identifying, Quantifying, and Mitigating its Operational Impacts","abstract":"In the quest to improve services, companies offer customers the option to interact with agents via texting. Such contact centers face unique challenges compared to traditional call centers, as measuring customer experience proxies like abandonment and patience involves uncertainty. A key source of this uncertainty is silent abandonment, where customers leave without notifying the system, wasting agent time and leaving their status unclear. Silent abandonment also obscures whether a customer was served or left. Our goals are to measure the magnitude of silent abandonment and mitigate its effects. Classification models show that 3%-70% of customers across 17 companies abandon silently. In one study, 71.3% of abandoning customers did so silently, reducing agent efficiency by 3.2% and system capacity by 15.3%, incurring $5,457 in annual costs per agent. We develop an expectation-maximization (EM) algorithm to estimate customer patience under uncertainty and identify influencing covariates. We find that companies should use classification models to estimate abandonment scope and our EM algorithm to assess patience. We suggest strategies to operationally mitigate the impact of silent abandonment by predicting suspected silent-abandonment behavior or changing service design. Specifically, we show that while allowing customers to write while waiting in the queue creates a missing data challenge, it also significantly increases patience and reduces service time, leading to reduced abandonment and lower staffing requirements.","sentences":["In the quest to improve services, companies offer customers the option to interact with agents via texting.","Such contact centers face unique challenges compared to traditional call centers, as measuring customer experience proxies like abandonment and patience involves uncertainty.","A key source of this uncertainty is silent abandonment, where customers leave without notifying the system, wasting agent time and leaving their status unclear.","Silent abandonment also obscures whether a customer was served or left.","Our goals are to measure the magnitude of silent abandonment and mitigate its effects.","Classification models show that 3%-70% of customers across 17 companies abandon silently.","In one study, 71.3% of abandoning customers did so silently, reducing agent efficiency by 3.2% and system capacity by 15.3%, incurring $5,457 in annual costs per agent.","We develop an expectation-maximization (EM) algorithm to estimate customer patience under uncertainty and identify influencing covariates.","We find that companies should use classification models to estimate abandonment scope and our EM algorithm to assess patience.","We suggest strategies to operationally mitigate the impact of silent abandonment by predicting suspected silent-abandonment behavior or changing service design.","Specifically, we show that while allowing customers to write while waiting in the queue creates a missing data challenge, it also significantly increases patience and reduces service time, leading to reduced abandonment and lower staffing requirements."],"url":"http://arxiv.org/abs/2501.08869v1"}
{"created":"2025-01-15 15:30:31","title":"The geometry of moral decision making","abstract":"We show how (resource) bounded rationality can be understood as the interplay of two fundamental moral principles: deontology and utilitarianism. In particular, we interpret deontology as a regularisation function in an optimal control problem, coupled with a free parameter, the inverse temperature, to shield the individual from expected utility. We discuss the information geometry of bounded rationality and aspects of its relation to rate distortion theory. A central role is played by Markov kernels and regular conditional probability, which are also studied geometrically. A gradient equation is used to determine the utility expansion path. Finally, the framework is applied to the analysis of a disutility model of the restriction of constitutional rights that we derive from legal doctrine. The methods discussed here are also relevant to the theory of autonomous agents.","sentences":["We show how (resource) bounded rationality can be understood as the interplay of two fundamental moral principles: deontology and utilitarianism.","In particular, we interpret deontology as a regularisation function in an optimal control problem, coupled with a free parameter, the inverse temperature, to shield the individual from expected utility.","We discuss the information geometry of bounded rationality and aspects of its relation to rate distortion theory.","A central role is played by Markov kernels and regular conditional probability, which are also studied geometrically.","A gradient equation is used to determine the utility expansion path.","Finally, the framework is applied to the analysis of a disutility model of the restriction of constitutional rights that we derive from legal doctrine.","The methods discussed here are also relevant to the theory of autonomous agents."],"url":"http://arxiv.org/abs/2501.08865v1"}
{"created":"2025-01-15 15:27:03","title":"The New Calculator? Practices, Norms, and Implications of Generative AI in Higher Education","abstract":"Generative AI (GenAI) has introduced myriad opportunities and challenges for higher education. Anticipating this potential transformation requires understanding students' contextualised practices and norms around GenAI. We conducted semi-structured interviews with 26 students and 11 educators from diverse departments across two universities. Grounded in Strong Structuration Theory, we find diversity in students' uses and motivations for GenAI. Occurring in the context of unclear university guidelines, institutional fixation on plagiarism, and inconsistent educator communication, students' practices are informed by unspoken rules around appropriate use, GenAI limitations and reliance strategies, and consideration of agency and skills. Perceived impacts include changes in confidence, and concerns about skill development, relationships with educators, and plagiarism. Both groups envision changes in universities' attitude to GenAI, responsible use training, assessments, and integration of GenAI into education. We discuss socio-technical implications in terms of current and anticipated changes in the external and internal structures that contextualise students' GenAI use.","sentences":["Generative AI (GenAI) has introduced myriad opportunities and challenges for higher education.","Anticipating this potential transformation requires understanding students' contextualised practices and norms around GenAI.","We conducted semi-structured interviews with 26 students and 11 educators from diverse departments across two universities.","Grounded in Strong Structuration Theory, we find diversity in students' uses and motivations for GenAI.","Occurring in the context of unclear university guidelines, institutional fixation on plagiarism, and inconsistent educator communication, students' practices are informed by unspoken rules around appropriate use, GenAI limitations and reliance strategies, and consideration of agency and skills.","Perceived impacts include changes in confidence, and concerns about skill development, relationships with educators, and plagiarism.","Both groups envision changes in universities' attitude to GenAI, responsible use training, assessments, and integration of GenAI into education.","We discuss socio-technical implications in terms of current and anticipated changes in the external and internal structures that contextualise students' GenAI use."],"url":"http://arxiv.org/abs/2501.08864v1"}
{"created":"2025-01-15 15:22:57","title":"ARMOR: Shielding Unlearnable Examples against Data Augmentation","abstract":"Private data, when published online, may be collected by unauthorized parties to train deep neural networks (DNNs). To protect privacy, defensive noises can be added to original samples to degrade their learnability by DNNs. Recently, unlearnable examples are proposed to minimize the training loss such that the model learns almost nothing. However, raw data are often pre-processed before being used for training, which may restore the private information of protected data. In this paper, we reveal the data privacy violation induced by data augmentation, a commonly used data pre-processing technique to improve model generalization capability, which is the first of its kind as far as we are concerned. We demonstrate that data augmentation can significantly raise the accuracy of the model trained on unlearnable examples from 21.3% to 66.1%. To address this issue, we propose a defense framework, dubbed ARMOR, to protect data privacy from potential breaches of data augmentation. To overcome the difficulty of having no access to the model training process, we design a non-local module-assisted surrogate model that better captures the effect of data augmentation. In addition, we design a surrogate augmentation selection strategy that maximizes distribution alignment between augmented and non-augmented samples, to choose the optimal augmentation strategy for each class. We also use a dynamic step size adjustment algorithm to enhance the defensive noise generation process. Extensive experiments are conducted on 4 datasets and 5 data augmentation methods to verify the performance of ARMOR. Comparisons with 6 state-of-the-art defense methods have demonstrated that ARMOR can preserve the unlearnability of protected private data under data augmentation. ARMOR reduces the test accuracy of the model trained on augmented protected samples by as much as 60% more than baselines.","sentences":["Private data, when published online, may be collected by unauthorized parties to train deep neural networks (DNNs).","To protect privacy, defensive noises can be added to original samples to degrade their learnability by DNNs.","Recently, unlearnable examples are proposed to minimize the training loss such that the model learns almost nothing.","However, raw data are often pre-processed before being used for training, which may restore the private information of protected data.","In this paper, we reveal the data privacy violation induced by data augmentation, a commonly used data pre-processing technique to improve model generalization capability, which is the first of its kind as far as we are concerned.","We demonstrate that data augmentation can significantly raise the accuracy of the model trained on unlearnable examples from 21.3% to 66.1%.","To address this issue, we propose a defense framework, dubbed ARMOR, to protect data privacy from potential breaches of data augmentation.","To overcome the difficulty of having no access to the model training process, we design a non-local module-assisted surrogate model that better captures the effect of data augmentation.","In addition, we design a surrogate augmentation selection strategy that maximizes distribution alignment between augmented and non-augmented samples, to choose the optimal augmentation strategy for each class.","We also use a dynamic step size adjustment algorithm to enhance the defensive noise generation process.","Extensive experiments are conducted on 4 datasets and 5 data augmentation methods to verify the performance of ARMOR.","Comparisons with 6 state-of-the-art defense methods have demonstrated that ARMOR can preserve the unlearnability of protected private data under data augmentation.","ARMOR reduces the test accuracy of the model trained on augmented protected samples by as much as 60% more than baselines."],"url":"http://arxiv.org/abs/2501.08862v1"}
{"created":"2025-01-15 15:20:46","title":"Generative Planning with 3D-vision Language Pre-training for End-to-End Autonomous Driving","abstract":"Autonomous driving is a challenging task that requires perceiving and understanding the surrounding environment for safe trajectory planning. While existing vision-based end-to-end models have achieved promising results, these methods are still facing the challenges of vision understanding, decision reasoning and scene generalization. To solve these issues, a generative planning with 3D-vision language pre-training model named GPVL is proposed for end-to-end autonomous driving. The proposed paradigm has two significant aspects. On one hand, a 3D-vision language pre-training module is designed to bridge the gap between visual perception and linguistic understanding in the bird's eye view. On the other hand, a cross-modal language model is introduced to generate holistic driving decisions and fine-grained trajectories with perception and navigation information in an auto-regressive manner. Experiments on the challenging nuScenes dataset demonstrate that the proposed scheme achieves excellent performances compared with state-of-the-art methods. Besides, the proposed GPVL presents strong generalization ability and real-time potential when handling high-level commands in various scenarios. It is believed that the effective, robust and efficient performance of GPVL is crucial for the practical application of future autonomous driving systems. Code is available at https://github.com/ltp1995/GPVL","sentences":["Autonomous driving is a challenging task that requires perceiving and understanding the surrounding environment for safe trajectory planning.","While existing vision-based end-to-end models have achieved promising results, these methods are still facing the challenges of vision understanding, decision reasoning and scene generalization.","To solve these issues, a generative planning with 3D-vision language pre-training model named GPVL is proposed for end-to-end autonomous driving.","The proposed paradigm has two significant aspects.","On one hand, a 3D-vision language pre-training module is designed to bridge the gap between visual perception and linguistic understanding in the bird's eye view.","On the other hand, a cross-modal language model is introduced to generate holistic driving decisions and fine-grained trajectories with perception and navigation information in an auto-regressive manner.","Experiments on the challenging nuScenes dataset demonstrate that the proposed scheme achieves excellent performances compared with state-of-the-art methods.","Besides, the proposed GPVL presents strong generalization ability and real-time potential when handling high-level commands in various scenarios.","It is believed that the effective, robust and efficient performance of GPVL is crucial for the practical application of future autonomous driving systems.","Code is available at https://github.com/ltp1995/GPVL"],"url":"http://arxiv.org/abs/2501.08861v1"}
{"created":"2025-01-15 15:05:49","title":"Digital Phenotyping for Adolescent Mental Health: A Feasibility Study Employing Machine Learning to Predict Mental Health Risk From Active and Passive Smartphone Data","abstract":"Background: Adolescents are particularly vulnerable to mental disorders, with over 75% of cases manifesting before the age of 25. Research indicates that only 18 to 34% of young people experiencing high levels of depression or anxiety symptoms seek support. Digital tools leveraging smartphones offer scalable and early intervention opportunities. Objective: Using a novel machine learning framework, this study evaluated the feasibility of integrating active and passive smartphone data to predict mental disorders in non-clinical adolescents. Specifically, we investigated the utility of the Mindcraft app in predicting risks for internalising and externalising disorders, eating disorders, insomnia and suicidal ideation. Methods: Participants (N=103; mean age 16.1 years) were recruited from three London schools. Participants completed the Strengths and Difficulties Questionnaire, the Eating Disorders-15 Questionnaire, Sleep Condition Indicator Questionnaire and indicated the presence/absence of suicidal ideation. They used the Mindcraft app for 14 days, contributing active data via self-reports and passive data from smartphone sensors. A contrastive pretraining phase was applied to enhance user-specific feature stability, followed by supervised fine-tuning. The model evaluation employed leave-one-subject-out cross-validation using balanced accuracy as the primary metric. Results: The integration of active and passive data achieved superior performance compared to individual data sources, with mean balanced accuracies of 0.71 for SDQ-High risk, 0.67 for insomnia, 0.77 for suicidal ideation and 0.70 for eating disorders. The contrastive learning framework stabilised daily behavioural representations, enhancing predictive robustness. This study demonstrates the potential of integrating active and passive smartphone data with advanced machine-learning techniques for predicting mental health risks.","sentences":["Background: Adolescents are particularly vulnerable to mental disorders, with over 75% of cases manifesting before the age of 25.","Research indicates that only 18 to 34% of young people experiencing high levels of depression or anxiety symptoms seek support.","Digital tools leveraging smartphones offer scalable and early intervention opportunities.","Objective: Using a novel machine learning framework, this study evaluated the feasibility of integrating active and passive smartphone data to predict mental disorders in non-clinical adolescents.","Specifically, we investigated the utility of the Mindcraft app in predicting risks for internalising and externalising disorders, eating disorders, insomnia and suicidal ideation.","Methods: Participants (N=103; mean age 16.1 years) were recruited from three London schools.","Participants completed the Strengths and Difficulties Questionnaire, the Eating Disorders-15 Questionnaire, Sleep Condition Indicator Questionnaire and indicated the presence/absence of suicidal ideation.","They used the Mindcraft app for 14 days, contributing active data via self-reports and passive data from smartphone sensors.","A contrastive pretraining phase was applied to enhance user-specific feature stability, followed by supervised fine-tuning.","The model evaluation employed leave-one-subject-out cross-validation using balanced accuracy as the primary metric.","Results: The integration of active and passive data achieved superior performance compared to individual data sources, with mean balanced accuracies of 0.71 for SDQ-High risk, 0.67 for insomnia, 0.77 for suicidal ideation and 0.70 for eating disorders.","The contrastive learning framework stabilised daily behavioural representations, enhancing predictive robustness.","This study demonstrates the potential of integrating active and passive smartphone data with advanced machine-learning techniques for predicting mental health risks."],"url":"http://arxiv.org/abs/2501.08851v1"}
{"created":"2025-01-15 15:04:10","title":"Graph Counterfactual Explainable AI via Latent Space Traversal","abstract":"Explaining the predictions of a deep neural network is a nontrivial task, yet high-quality explanations for predictions are often a prerequisite for practitioners to trust these models. Counterfactual explanations aim to explain predictions by finding the ''nearest'' in-distribution alternative input whose prediction changes in a pre-specified way. However, it remains an open question how to define this nearest alternative input, whose solution depends on both the domain (e.g. images, graphs, tabular data, etc.) and the specific application considered. For graphs, this problem is complicated i) by their discrete nature, as opposed to the continuous nature of state-of-the-art graph classifiers; and ii) by the node permutation group acting on the graphs. We propose a method to generate counterfactual explanations for any differentiable black-box graph classifier, utilizing a case-specific permutation equivariant graph variational autoencoder. We generate counterfactual explanations in a continuous fashion by traversing the latent space of the autoencoder across the classification boundary of the classifier, allowing for seamless integration of discrete graph structure and continuous graph attributes. We empirically validate the approach on three graph datasets, showing that our model is consistently high-performing and more robust than the baselines.","sentences":["Explaining the predictions of a deep neural network is a nontrivial task, yet high-quality explanations for predictions are often a prerequisite for practitioners to trust these models.","Counterfactual explanations aim to explain predictions by finding the ''nearest'' in-distribution alternative input whose prediction changes in a pre-specified way.","However, it remains an open question how to define this nearest alternative input, whose solution depends on both the domain (e.g. images, graphs, tabular data, etc.) and the specific application considered.","For graphs, this problem is complicated i) by their discrete nature, as opposed to the continuous nature of state-of-the-art graph classifiers; and ii) by the node permutation group acting on the graphs.","We propose a method to generate counterfactual explanations for any differentiable black-box graph classifier, utilizing a case-specific permutation equivariant graph variational autoencoder.","We generate counterfactual explanations in a continuous fashion by traversing the latent space of the autoencoder across the classification boundary of the classifier, allowing for seamless integration of discrete graph structure and continuous graph attributes.","We empirically validate the approach on three graph datasets, showing that our model is consistently high-performing and more robust than the baselines."],"url":"http://arxiv.org/abs/2501.08850v1"}
{"created":"2025-01-15 15:00:11","title":"RouteNet-Gauss: Hardware-Enhanced Network Modeling with Machine Learning","abstract":"Network simulation is pivotal in network modeling, assisting with tasks ranging from capacity planning to performance estimation. Traditional approaches such as Discrete Event Simulation (DES) face limitations in terms of computational cost and accuracy. This paper introduces RouteNet-Gauss, a novel integration of a testbed network with a Machine Learning (ML) model to address these challenges. By using the testbed as a hardware accelerator, RouteNet-Gauss generates training datasets rapidly and simulates network scenarios with high fidelity to real-world conditions. Experimental results show that RouteNet-Gauss significantly reduces prediction errors by up to 95% and achieves a 488x speedup in inference time compared to state-of-the-art DES-based methods. RouteNet-Gauss's modular architecture is dynamically constructed based on the specific characteristics of the network scenario, such as topology and routing. This enables it to understand and generalize to different network configurations beyond those seen during training, including networks up to 10x larger. Additionally, it supports Temporal Aggregated Performance Estimation (TAPE), providing configurable temporal granularity and maintaining high accuracy in flow performance metrics. This approach shows promise in improving both simulation efficiency and accuracy, offering a valuable tool for network operators.","sentences":["Network simulation is pivotal in network modeling, assisting with tasks ranging from capacity planning to performance estimation.","Traditional approaches such as Discrete Event Simulation (DES) face limitations in terms of computational cost and accuracy.","This paper introduces RouteNet-Gauss, a novel integration of a testbed network with a Machine Learning (ML) model to address these challenges.","By using the testbed as a hardware accelerator, RouteNet-Gauss generates training datasets rapidly and simulates network scenarios with high fidelity to real-world conditions.","Experimental results show that RouteNet-Gauss significantly reduces prediction errors by up to 95% and achieves a 488x speedup in inference time compared to state-of-the-art DES-based methods.","RouteNet-Gauss's modular architecture is dynamically constructed based on the specific characteristics of the network scenario, such as topology and routing.","This enables it to understand and generalize to different network configurations beyond those seen during training, including networks up to 10x larger.","Additionally, it supports Temporal Aggregated Performance Estimation (TAPE), providing configurable temporal granularity and maintaining high accuracy in flow performance metrics.","This approach shows promise in improving both simulation efficiency and accuracy, offering a valuable tool for network operators."],"url":"http://arxiv.org/abs/2501.08848v1"}
{"created":"2025-01-15 14:59:00","title":"Automatic tuning of communication protocols for vehicular ad hoc networks using metaheuristics","abstract":"The emerging field of vehicular ad hoc networks (VANETs) deals with a set of communicating vehicles which are able to spontaneously interconnect without any pre-existing infrastructure. In such kind of networks, it is crucial to make an optimal configuration of the communication protocols previously to the final network deployment. This way, a human designer can obtain an optimal QoS of the network beforehand. The problem we consider in this work lies in configuring the File Transfer protocol Configuration (FTC) with the aim of optimizing the transmission time, the number of lost packets, and the amount of data transferred in realistic VANET scenarios. We face the FTC with five representative state-of-the-art optimization techniques and compare their performance. These algorithms are: Particle Swarm Optimization (PSO), Differential Evolution (DE), Genetic Algorithm (GA), Evolutionary Strategy (ES), and Simulated Annealing (SA). For our tests, two typical environment instances of VANETs for Urban and Highway scenarios have been defined. The experiments using ns- 2 (a well-known realistic VANET simulator) reveal that PSO outperforms all the compared algorithms for both studied VANET instances.","sentences":["The emerging field of vehicular ad hoc networks (VANETs) deals with a set of communicating vehicles which are able to spontaneously interconnect without any pre-existing infrastructure.","In such kind of networks, it is crucial to make an optimal configuration of the communication protocols previously to the final network deployment.","This way, a human designer can obtain an optimal QoS of the network beforehand.","The problem we consider in this work lies in configuring the File Transfer protocol Configuration (FTC) with the aim of optimizing the transmission time, the number of lost packets, and the amount of data transferred in realistic VANET scenarios.","We face the FTC with five representative state-of-the-art optimization techniques and compare their performance.","These algorithms are: Particle Swarm Optimization (PSO), Differential Evolution (DE), Genetic Algorithm (GA), Evolutionary Strategy (ES), and Simulated Annealing (SA).","For our tests, two typical environment instances of VANETs for Urban and Highway scenarios have been defined.","The experiments using ns- 2 (a well-known realistic VANET simulator) reveal that PSO outperforms all the compared algorithms for both studied VANET instances."],"url":"http://arxiv.org/abs/2501.08847v1"}
{"created":"2025-01-15 14:58:41","title":"Beating Competitive Ratio 4 for Graphic Matroid Secretary","abstract":"One of the classic problems in online decision-making is the *secretary problem* where to goal is to maximize the probability of choosing the largest number from a randomly ordered sequence. A natural extension allows selecting multiple values under a combinatorial constraint. Babaioff, Immorlica, Kempe, and Kleinberg (SODA'07, JACM'18) introduced the *matroid secretary conjecture*, suggesting an $O(1)$-competitive algorithm exists for matroids. Many works since have attempted to obtain algorithms for both general matroids and specific classes of matroids. The ultimate goal is to obtain an $e$-competitive algorithm, and the *strong matroid secretary conjecture* states that this is possible for general matroids.   A key class of matroids is the *graphic matroid*, where a set of graph edges is independent if it contains no cycle. The rich combinatorial structure of graphs makes them a natural first step towards solving a problem for general matroids. Babaioff et al. (SODA'07, JACM'18) first studied the graphic matroid setting, achieving a $16$-competitive algorithm. Subsequent works have improved the competitive ratio, most recently to 4 by Soto, Turkieltaub, and Verdugo (SODA'18).   We break this $4$-competitive barrier, presenting a new algorithm with a competitive ratio of $3.95$. For simple graphs, we further improve this to $3.77$. Intuitively, solving the problem for simple graphs is easier since they lack length-two cycles. A natural question is whether a ratio arbitrarily close to $e$ can be achieved by assuming sufficiently large girth.   We answer this affirmatively, showing a competitive ratio arbitrarily close to $e$ even for constant girth values, supporting the strong matroid secretary conjecture. We also prove this bound is tight: for any constant $g$, no algorithm can achieve a ratio better than $e$ even when the graph has girth at least $g$.","sentences":["One of the classic problems in online decision-making is the *secretary problem* where to goal is to maximize the probability of choosing the largest number from a randomly ordered sequence.","A natural extension allows selecting multiple values under a combinatorial constraint.","Babaioff, Immorlica, Kempe, and Kleinberg (SODA'07, JACM'18) introduced the *matroid secretary conjecture*, suggesting an $O(1)$-competitive algorithm exists for matroids.","Many works since have attempted to obtain algorithms for both general matroids and specific classes of matroids.","The ultimate goal is to obtain an $e$-competitive algorithm, and the *strong matroid secretary conjecture* states that this is possible for general matroids.   ","A key class of matroids is the *graphic matroid*, where a set of graph edges is independent if it contains no cycle.","The rich combinatorial structure of graphs makes them a natural first step towards solving a problem for general matroids.","Babaioff et al.","(SODA'07, JACM'18) first studied the graphic matroid setting, achieving a $16$-competitive algorithm.","Subsequent works have improved the competitive ratio, most recently to 4 by Soto, Turkieltaub, and Verdugo (SODA'18).   ","We break this $4$-competitive barrier, presenting a new algorithm with a competitive ratio of $3.95$. For simple graphs, we further improve this to $3.77$. Intuitively, solving the problem for simple graphs is easier since they lack length-two cycles.","A natural question is whether a ratio arbitrarily close to $e$ can be achieved by assuming sufficiently large girth.   ","We answer this affirmatively, showing a competitive ratio arbitrarily close to $e$ even for constant girth values, supporting the strong matroid secretary conjecture.","We also prove this bound is tight: for any constant $g$, no algorithm can achieve a ratio better than $e$ even when the graph has girth at least $g$."],"url":"http://arxiv.org/abs/2501.08846v1"}
{"created":"2025-01-15 14:52:20","title":"Exploring Task-Level Optimal Prompts for Visual In-Context Learning","abstract":"With the development of Vision Foundation Models (VFMs) in recent years, Visual In-Context Learning (VICL) has become a better choice compared to modifying models in most scenarios. Different from retraining or fine-tuning model, VICL does not require modifications to the model's weights or architecture, and only needs a prompt with demonstrations to teach VFM how to solve tasks. Currently, significant computational cost for finding optimal prompts for every test sample hinders the deployment of VICL, as determining which demonstrations to use for constructing prompts is very costly. In this paper, however, we find a counterintuitive phenomenon that most test samples actually achieve optimal performance under the same prompts, and searching for sample-level prompts only costs more time but results in completely identical prompts. Therefore, we propose task-level prompting to reduce the cost of searching for prompts during the inference stage and introduce two time-saving yet effective task-level prompt search strategies. Extensive experimental results show that our proposed method can identify near-optimal prompts and reach the best VICL performance with a minimal cost that prior work has never achieved.","sentences":["With the development of Vision Foundation Models (VFMs) in recent years, Visual In-Context Learning (VICL) has become a better choice compared to modifying models in most scenarios.","Different from retraining or fine-tuning model, VICL does not require modifications to the model's weights or architecture, and only needs a prompt with demonstrations to teach VFM how to solve tasks.","Currently, significant computational cost for finding optimal prompts for every test sample hinders the deployment of VICL, as determining which demonstrations to use for constructing prompts is very costly.","In this paper, however, we find a counterintuitive phenomenon that most test samples actually achieve optimal performance under the same prompts, and searching for sample-level prompts only costs more time but results in completely identical prompts.","Therefore, we propose task-level prompting to reduce the cost of searching for prompts during the inference stage and introduce two time-saving yet effective task-level prompt search strategies.","Extensive experimental results show that our proposed method can identify near-optimal prompts and reach the best VICL performance with a minimal cost that prior work has never achieved."],"url":"http://arxiv.org/abs/2501.08841v1"}
{"created":"2025-01-15 14:50:46","title":"CveBinarySheet: A Comprehensive Pre-built Binaries Database for IoT Vulnerability Analysis","abstract":"Binary Static Code Analysis (BSCA) is a pivotal area in software vulnerability research, focusing on the precise localization of vulnerabilities within binary executables. Despite advancements in BSCA techniques, there is a notable scarcity of comprehensive and readily usable vulnerability datasets tailored for diverse environments such as IoT, UEFI, and MCU firmware. To address this gap, we present CveBinarySheet, a meticulously curated database containing 1033 CVE entries spanning from 1999 to 2024. Our dataset encompasses 16 essential third-party components, including busybox and curl, and supports five CPU architectures: x86-64, i386, MIPS, ARMv7, and RISC-V64. Each precompiled binary is available at two compiler optimization levels (O0 and O3), facilitating comprehensive vulnerability analysis under different compilation scenarios. By providing detailed metadata and diverse binary samples, CveBinarySheet aims to accelerate the development of state-of-the-art BSCA tools, binary similarity analysis, and vulnerability matching applications.","sentences":["Binary Static Code Analysis (BSCA) is a pivotal area in software vulnerability research, focusing on the precise localization of vulnerabilities within binary executables.","Despite advancements in BSCA techniques, there is a notable scarcity of comprehensive and readily usable vulnerability datasets tailored for diverse environments such as IoT, UEFI, and MCU firmware.","To address this gap, we present CveBinarySheet, a meticulously curated database containing 1033 CVE entries spanning from 1999 to 2024.","Our dataset encompasses 16 essential third-party components, including busybox and curl, and supports five CPU architectures: x86-64, i386, MIPS, ARMv7, and RISC-V64.","Each precompiled binary is available at two compiler optimization levels (O0 and O3), facilitating comprehensive vulnerability analysis under different compilation scenarios.","By providing detailed metadata and diverse binary samples, CveBinarySheet aims to accelerate the development of state-of-the-art BSCA tools, binary similarity analysis, and vulnerability matching applications."],"url":"http://arxiv.org/abs/2501.08840v1"}
{"created":"2025-01-15 14:47:02","title":"ToMATO: Verbalizing the Mental States of Role-Playing LLMs for Benchmarking Theory of Mind","abstract":"Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in three aspects: 1) they assess a limited range of mental states such as beliefs, 2) false beliefs are not comprehensively explored, and 3) the diverse personality traits of characters are overlooked. To address these challenges, we introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over conversations. ToMATO is generated via LLM-LLM conversations featuring information asymmetry. By employing a prompting method that requires role-playing LLMs to verbalize their thoughts before each utterance, we capture both first- and second-order mental states across five categories: belief, intention, desire, emotion, and knowledge. These verbalized thoughts serve as answers to questions designed to assess the mental states of characters within conversations. Furthermore, the information asymmetry introduced by hiding thoughts from others induces the generation of false beliefs about various mental states. Assigning distinct personality traits to LLMs further diversifies both utterances and thoughts. ToMATO consists of 5.4k questions, 753 conversations, and 15 personality trait patterns. Our analysis shows that this dataset construction approach frequently generates false beliefs due to the information asymmetry between role-playing LLMs, and effectively reflects diverse personalities. We evaluate nine LLMs on ToMATO and find that even GPT-4o mini lags behind human performance, especially in understanding false beliefs, and lacks robustness to various personality traits.","sentences":["Existing Theory of Mind (ToM) benchmarks diverge from real-world scenarios in three aspects: 1) they assess a limited range of mental states such as beliefs, 2) false beliefs are not comprehensively explored, and 3) the diverse personality traits of characters are overlooked.","To address these challenges, we introduce ToMATO, a new ToM benchmark formulated as multiple-choice QA over conversations.","ToMATO is generated via LLM-LLM conversations featuring information asymmetry.","By employing a prompting method that requires role-playing LLMs to verbalize their thoughts before each utterance, we capture both first- and second-order mental states across five categories: belief, intention, desire, emotion, and knowledge.","These verbalized thoughts serve as answers to questions designed to assess the mental states of characters within conversations.","Furthermore, the information asymmetry introduced by hiding thoughts from others induces the generation of false beliefs about various mental states.","Assigning distinct personality traits to LLMs further diversifies both utterances and thoughts.","ToMATO consists of 5.4k questions, 753 conversations, and 15 personality trait patterns.","Our analysis shows that this dataset construction approach frequently generates false beliefs due to the information asymmetry between role-playing LLMs, and effectively reflects diverse personalities.","We evaluate nine LLMs on ToMATO and find that even GPT-4o mini lags behind human performance, especially in understanding false beliefs, and lacks robustness to various personality traits."],"url":"http://arxiv.org/abs/2501.08838v1"}
{"created":"2025-01-15 14:46:44","title":"MANTA: Diffusion Mamba for Efficient and Effective Stochastic Long-Term Dense Anticipation","abstract":"Our work addresses the problem of stochastic long-term dense anticipation. The goal of this task is to predict actions and their durations several minutes into the future based on provided video observations. Anticipation over extended horizons introduces high uncertainty, as a single observation can lead to multiple plausible future outcomes. To address this uncertainty, stochastic models are designed to predict several potential future action sequences. Recent work has further proposed to incorporate uncertainty modelling for observed frames by simultaneously predicting per-frame past and future actions in a unified manner. While such joint modelling of actions is beneficial, it requires long-range temporal capabilities to connect events across distant past and future time points. However, the previous work struggles to achieve such a long-range understanding due to its limited and/or sparse receptive field. To alleviate this issue, we propose a novel MANTA (MAmba for ANTicipation) network. Our model enables effective long-term temporal modelling even for very long sequences while maintaining linear complexity in sequence length. We demonstrate that our approach achieves state-of-the-art results on three datasets - Breakfast, 50Salads, and Assembly101 - while also significantly improving computational and memory efficiency.","sentences":["Our work addresses the problem of stochastic long-term dense anticipation.","The goal of this task is to predict actions and their durations several minutes into the future based on provided video observations.","Anticipation over extended horizons introduces high uncertainty, as a single observation can lead to multiple plausible future outcomes.","To address this uncertainty, stochastic models are designed to predict several potential future action sequences.","Recent work has further proposed to incorporate uncertainty modelling for observed frames by simultaneously predicting per-frame past and future actions in a unified manner.","While such joint modelling of actions is beneficial, it requires long-range temporal capabilities to connect events across distant past and future time points.","However, the previous work struggles to achieve such a long-range understanding due to its limited and/or sparse receptive field.","To alleviate this issue, we propose a novel MANTA (MAmba for ANTicipation) network.","Our model enables effective long-term temporal modelling even for very long sequences while maintaining linear complexity in sequence length.","We demonstrate that our approach achieves state-of-the-art results on three datasets - Breakfast, 50Salads, and Assembly101 - while also significantly improving computational and memory efficiency."],"url":"http://arxiv.org/abs/2501.08837v1"}
{"created":"2025-01-15 14:38:18","title":"Smart Contract Fuzzing Towards Profitable Vulnerabilities","abstract":"Billions of dollars are transacted through smart contracts, making vulnerabilities a major financial risk. One focus in the security arms race is on profitable vulnerabilities that attackers can exploit. Fuzzing is a key method for identifying these vulnerabilities. However, current solutions face two main limitations: a lack of profit-centric techniques for expediting detection, and insufficient automation in maximizing the profitability of discovered vulnerabilities, leaving the analysis to human experts. To address these gaps, we have developed VERITE, a profit-centric smart contract fuzzing framework that not only effectively detects those profitable vulnerabilities but also maximizes the exploited profits.   VERITE has three key features: 1) DeFi action-based mutators for boosting the exploration of transactions with different fund flows; 2) potentially profitable candidates identification criteria, which checks whether the input has caused abnormal fund flow properties during testing; 3) a gradient descent-based profit maximization strategy for these identified candidates.   VERITE is fully developed from scratch and evaluated on a dataset consisting of 61 exploited real-world DeFi projects with an average of over 1.1 million dollars loss. The results show that VERITE can automatically extract more than 18 million dollars in total and is significantly better than state-of-the-art fuzzer ITYFUZZ in both detection (29/9) and exploitation (58 times more profits gained on average). Remarkbly, in 12 targets, it gains more profits than real-world attacking exploits (1.01 to 11.45 times more). VERITE is also applied by auditors in contract auditing, where 6 (5 high severity) zero-day vulnerabilities are found with over $2,500 bounty rewards.","sentences":["Billions of dollars are transacted through smart contracts, making vulnerabilities a major financial risk.","One focus in the security arms race is on profitable vulnerabilities that attackers can exploit.","Fuzzing is a key method for identifying these vulnerabilities.","However, current solutions face two main limitations: a lack of profit-centric techniques for expediting detection, and insufficient automation in maximizing the profitability of discovered vulnerabilities, leaving the analysis to human experts.","To address these gaps, we have developed VERITE, a profit-centric smart contract fuzzing framework that not only effectively detects those profitable vulnerabilities but also maximizes the exploited profits.   ","VERITE has three key features: 1) DeFi action-based mutators for boosting the exploration of transactions with different fund flows; 2) potentially profitable candidates identification criteria, which checks whether the input has caused abnormal fund flow properties during testing; 3) a gradient descent-based profit maximization strategy for these identified candidates.   ","VERITE is fully developed from scratch and evaluated on a dataset consisting of 61 exploited real-world DeFi projects with an average of over 1.1 million dollars loss.","The results show that VERITE can automatically extract more than 18 million dollars in total and is significantly better than state-of-the-art fuzzer ITYFUZZ in both detection (29/9) and exploitation (58 times more profits gained on average).","Remarkbly, in 12 targets, it gains more profits than real-world attacking exploits (1.01 to 11.45 times more).","VERITE is also applied by auditors in contract auditing, where 6 (5 high severity) zero-day vulnerabilities are found with over $2,500 bounty rewards."],"url":"http://arxiv.org/abs/2501.08834v1"}
{"created":"2025-01-15 14:30:13","title":"MMDocIR: Benchmarking Multi-Modal Retrieval for Long Documents","abstract":"Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents. Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance of systems in multi-modal document retrieval. To address this gap, this work introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks: page-level and layout-level retrieval. The former focuses on localizing the most relevant pages within a long document, while the latter targets the detection of specific layouts, offering a more fine-grained granularity than whole-page analysis. A layout can refer to a variety of elements such as textual paragraphs, equations, figures, tables, or charts. The MMDocIR benchmark comprises a rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it a pivotal resource for advancing multi-modal document retrieval for both training and evaluation. Through rigorous experiments, we reveal that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR train set can effectively benefit the training process of multi-modal document retrieval and (iii) text retrievers leveraging on VLM-text perform much better than those using OCR-text. These findings underscores the potential advantages of integrating visual elements for multi-modal document retrieval.","sentences":["Multi-modal document retrieval is designed to identify and retrieve various forms of multi-modal content, such as figures, tables, charts, and layout information from extensive documents.","Despite its significance, there is a notable lack of a robust benchmark to effectively evaluate the performance of systems in multi-modal document retrieval.","To address this gap, this work introduces a new benchmark, named as MMDocIR, encompassing two distinct tasks: page-level and layout-level retrieval.","The former focuses on localizing the most relevant pages within a long document, while the latter targets the detection of specific layouts, offering a more fine-grained granularity than whole-page analysis.","A layout can refer to a variety of elements such as textual paragraphs, equations, figures, tables, or charts.","The MMDocIR benchmark comprises a rich dataset featuring expertly annotated labels for 1,685 questions and bootstrapped labels for 173,843 questions, making it a pivotal resource for advancing multi-modal document retrieval for both training and evaluation.","Through rigorous experiments, we reveal that (i) visual retrievers significantly outperform their text counterparts, (ii) MMDocIR train set can effectively benefit the training process of multi-modal document retrieval and (iii) text retrievers leveraging on VLM-text perform much better than those using OCR-text.","These findings underscores the potential advantages of integrating visual elements for multi-modal document retrieval."],"url":"http://arxiv.org/abs/2501.08828v1"}
{"created":"2025-01-15 14:19:03","title":"A Closer Look at the Learnability of Out-of-Distribution (OOD) Detection","abstract":"Machine learning algorithms often encounter different or \"out-of-distribution\" (OOD) data at deployment time, and OOD detection is frequently employed to detect these examples. While it works reasonably well in practice, existing theoretical results on OOD detection are highly pessimistic. In this work, we take a closer look at this problem, and make a distinction between uniform and non-uniform learnability, following PAC learning theory. We characterize under what conditions OOD detection is uniformly and non-uniformly learnable, and we show that in several cases, non-uniform learnability turns a number of negative results into positive. In all cases where OOD detection is learnable, we provide concrete learning algorithms and a sample-complexity analysis.","sentences":["Machine learning algorithms often encounter different or \"out-of-distribution\" (OOD) data at deployment time, and OOD detection is frequently employed to detect these examples.","While it works reasonably well in practice, existing theoretical results on OOD detection are highly pessimistic.","In this work, we take a closer look at this problem, and make a distinction between uniform and non-uniform learnability, following PAC learning theory.","We characterize under what conditions OOD detection is uniformly and non-uniformly learnable, and we show that in several cases, non-uniform learnability turns a number of negative results into positive.","In all cases where OOD detection is learnable, we provide concrete learning algorithms and a sample-complexity analysis."],"url":"http://arxiv.org/abs/2501.08821v1"}
