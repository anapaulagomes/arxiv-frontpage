{"created":"2024-08-28 17:59:31","title":"Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders","abstract":"The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs). Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis. A number of recent MLLMs achieve this goal using a mixture of vision encoders. Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts. This study provides an extensive exploration of the design space for MLLMs using a mixture of vision encoders and resolutions. Our findings reveal several underlying principles common to various existing strategies, leading to a streamlined yet effective design approach. We discover that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies. We additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens, enhancing model coherence. The resulting family of MLLMs, Eagle, surpasses other leading open-source models on major MLLM benchmarks. Models and code: https://github.com/NVlabs/Eagle","sentences":["The ability to accurately interpret complex visual information is a crucial topic of multimodal large language models (MLLMs).","Recent work indicates that enhanced visual perception significantly reduces hallucinations and improves performance on resolution-sensitive tasks, such as optical character recognition and document analysis.","A number of recent MLLMs achieve this goal using a mixture of vision encoders.","Despite their success, there is a lack of systematic comparisons and detailed ablation studies addressing critical aspects, such as expert selection and the integration of multiple vision experts.","This study provides an extensive exploration of the design space for MLLMs using a mixture of vision encoders and resolutions.","Our findings reveal several underlying principles common to various existing strategies, leading to a streamlined yet effective design approach.","We discover that simply concatenating visual tokens from a set of complementary vision encoders is as effective as more complex mixing architectures or strategies.","We additionally introduce Pre-Alignment to bridge the gap between vision-focused encoders and language tokens, enhancing model coherence.","The resulting family of MLLMs, Eagle, surpasses other leading open-source models on major MLLM benchmarks.","Models and code: https://github.com/NVlabs/Eagle"],"url":"http://arxiv.org/abs/2408.15998v1"}
{"created":"2024-08-28 17:59:27","title":"Mamba or Transformer for Time Series Forecasting? Mixture of Universals (MoU) Is All You Need","abstract":"Time series forecasting requires balancing short-term and long-term dependencies for accurate predictions. Existing methods mainly focus on long-term dependency modeling, neglecting the complexities of short-term dynamics, which may hinder performance. Transformers are superior in modeling long-term dependencies but are criticized for their quadratic computational cost. Mamba provides a near-linear alternative but is reported less effective in time series longterm forecasting due to potential information loss. Current architectures fall short in offering both high efficiency and strong performance for long-term dependency modeling. To address these challenges, we introduce Mixture of Universals (MoU), a versatile model to capture both short-term and long-term dependencies for enhancing performance in time series forecasting. MoU is composed of two novel designs: Mixture of Feature Extractors (MoF), an adaptive method designed to improve time series patch representations for short-term dependency, and Mixture of Architectures (MoA), which hierarchically integrates Mamba, FeedForward, Convolution, and Self-Attention architectures in a specialized order to model long-term dependency from a hybrid perspective. The proposed approach achieves state-of-the-art performance while maintaining relatively low computational costs. Extensive experiments on seven real-world datasets demonstrate the superiority of MoU. Code is available at https://github.com/lunaaa95/mou/.","sentences":["Time series forecasting requires balancing short-term and long-term dependencies for accurate predictions.","Existing methods mainly focus on long-term dependency modeling, neglecting the complexities of short-term dynamics, which may hinder performance.","Transformers are superior in modeling long-term dependencies but are criticized for their quadratic computational cost.","Mamba provides a near-linear alternative but is reported less effective in time series longterm forecasting due to potential information loss.","Current architectures fall short in offering both high efficiency and strong performance for long-term dependency modeling.","To address these challenges, we introduce Mixture of Universals (MoU), a versatile model to capture both short-term and long-term dependencies for enhancing performance in time series forecasting.","MoU is composed of two novel designs: Mixture of Feature Extractors (MoF), an adaptive method designed to improve time series patch representations for short-term dependency, and Mixture of Architectures (MoA), which hierarchically integrates Mamba, FeedForward, Convolution, and Self-Attention architectures in a specialized order to model long-term dependency from a hybrid perspective.","The proposed approach achieves state-of-the-art performance while maintaining relatively low computational costs.","Extensive experiments on seven real-world datasets demonstrate the superiority of MoU. Code is available at https://github.com/lunaaa95/mou/."],"url":"http://arxiv.org/abs/2408.15997v1"}
{"created":"2024-08-28 17:59:05","title":"Spatio-Temporal Context Prompting for Zero-Shot Action Detection","abstract":"Spatio-temporal action detection encompasses the tasks of localizing and classifying individual actions within a video. Recent works aim to enhance this process by incorporating interaction modeling, which captures the relationship between people and their surrounding context. However, these approaches have primarily focused on fully-supervised learning, and the current limitation lies in the lack of generalization capability to recognize unseen action categories. In this paper, we aim to adapt the pretrained image-language models to detect unseen actions. To this end, we propose a method which can effectively leverage the rich knowledge of visual-language models to perform Person-Context Interaction. Meanwhile, our Context Prompting module will utilize contextual information to prompt labels, thereby enhancing the generation of more representative text features. Moreover, to address the challenge of recognizing distinct actions by multiple people at the same timestamp, we design the Interest Token Spotting mechanism which employs pretrained visual knowledge to find each person's interest context tokens, and then these tokens will be used for prompting to generate text features tailored to each individual. To evaluate the ability to detect unseen actions, we propose a comprehensive benchmark on J-HMDB, UCF101-24, and AVA datasets. The experiments show that our method achieves superior results compared to previous approaches and can be further extended to multi-action videos, bringing it closer to real-world applications. The code and data can be found in https://webber2933.github.io/ST-CLIP-project-page.","sentences":["Spatio-temporal action detection encompasses the tasks of localizing and classifying individual actions within a video.","Recent works aim to enhance this process by incorporating interaction modeling, which captures the relationship between people and their surrounding context.","However, these approaches have primarily focused on fully-supervised learning, and the current limitation lies in the lack of generalization capability to recognize unseen action categories.","In this paper, we aim to adapt the pretrained image-language models to detect unseen actions.","To this end, we propose a method which can effectively leverage the rich knowledge of visual-language models to perform Person-Context Interaction.","Meanwhile, our Context Prompting module will utilize contextual information to prompt labels, thereby enhancing the generation of more representative text features.","Moreover, to address the challenge of recognizing distinct actions by multiple people at the same timestamp, we design the Interest Token Spotting mechanism which employs pretrained visual knowledge to find each person's interest context tokens, and then these tokens will be used for prompting to generate text features tailored to each individual.","To evaluate the ability to detect unseen actions, we propose a comprehensive benchmark on J-HMDB, UCF101-24, and AVA datasets.","The experiments show that our method achieves superior results compared to previous approaches and can be further extended to multi-action videos, bringing it closer to real-world applications.","The code and data can be found in https://webber2933.github.io/ST-CLIP-project-page."],"url":"http://arxiv.org/abs/2408.15996v1"}
{"created":"2024-08-28 17:59:02","title":"TEDRA: Text-based Editing of Dynamic and Photoreal Actors","abstract":"Over the past years, significant progress has been made in creating photorealistic and drivable 3D avatars solely from videos of real humans. However, a core remaining challenge is the fine-grained and user-friendly editing of clothing styles by means of textual descriptions. To this end, we present TEDRA, the first method allowing text-based edits of an avatar, which maintains the avatar's high fidelity, space-time coherency, as well as dynamics, and enables skeletal pose and view control. We begin by training a model to create a controllable and high-fidelity digital replica of the real actor. Next, we personalize a pretrained generative diffusion model by fine-tuning it on various frames of the real character captured from different camera angles, ensuring the digital representation faithfully captures the dynamics and movements of the real person. This two-stage process lays the foundation for our approach to dynamic human avatar editing. Utilizing this personalized diffusion model, we modify the dynamic avatar based on a provided text prompt using our Personalized Normal Aligned Score Distillation Sampling (PNA-SDS) within a model-based guidance framework. Additionally, we propose a time step annealing strategy to ensure high-quality edits. Our results demonstrate a clear improvement over prior work in functionality and visual quality.","sentences":["Over the past years, significant progress has been made in creating photorealistic and drivable 3D avatars solely from videos of real humans.","However, a core remaining challenge is the fine-grained and user-friendly editing of clothing styles by means of textual descriptions.","To this end, we present TEDRA, the first method allowing text-based edits of an avatar, which maintains the avatar's high fidelity, space-time coherency, as well as dynamics, and enables skeletal pose and view control.","We begin by training a model to create a controllable and high-fidelity digital replica of the real actor.","Next, we personalize a pretrained generative diffusion model by fine-tuning it on various frames of the real character captured from different camera angles, ensuring the digital representation faithfully captures the dynamics and movements of the real person.","This two-stage process lays the foundation for our approach to dynamic human avatar editing.","Utilizing this personalized diffusion model, we modify the dynamic avatar based on a provided text prompt using our Personalized Normal Aligned Score Distillation Sampling (PNA-SDS) within a model-based guidance framework.","Additionally, we propose a time step annealing strategy to ensure high-quality edits.","Our results demonstrate a clear improvement over prior work in functionality and visual quality."],"url":"http://arxiv.org/abs/2408.15995v1"}
{"created":"2024-08-28 17:58:54","title":"Perceive-IR: Learning to Perceive Degradation Better for All-in-One Image Restoration","abstract":"The limitations of task-specific and general image restoration methods for specific degradation have prompted the development of all-in-one image restoration techniques. However, the diversity of patterns among multiple degradation, along with the significant uncertainties in mapping between degraded images of different severities and their corresponding undistorted versions, pose significant challenges to the all-in-one restoration tasks. To address these challenges, we propose Perceive-IR, an all-in-one image restorer designed to achieve fine-grained quality control that enables restored images to more closely resemble their undistorted counterparts, regardless of the type or severity of degradation. Specifically, Perceive-IR contains two stages: (1) prompt learning stage and (2) restoration stage. In the prompt learning stage, we leverage prompt learning to acquire a fine-grained quality perceiver capable of distinguishing three-tier quality levels by constraining the prompt-image similarity in the CLIP perception space. Subsequently, this quality perceiver and difficulty-adaptive perceptual loss are integrated as a quality-aware learning strategy to realize fine-grained quality control in restoration stage. For the restoration stage, a semantic guidance module (SGM) and compact feature extraction (CFE) are proposed to further promote the restoration process by utilizing the robust semantic information from the pre-trained large scale vision models and distinguishing degradation-specific features. Extensive experiments demonstrate that our Perceive-IR outperforms state-of-the-art methods in all-in-one image restoration tasks and exhibit superior generalization ability when dealing with unseen tasks.","sentences":["The limitations of task-specific and general image restoration methods for specific degradation have prompted the development of all-in-one image restoration techniques.","However, the diversity of patterns among multiple degradation, along with the significant uncertainties in mapping between degraded images of different severities and their corresponding undistorted versions, pose significant challenges to the all-in-one restoration tasks.","To address these challenges, we propose Perceive-IR, an all-in-one image restorer designed to achieve fine-grained quality control that enables restored images to more closely resemble their undistorted counterparts, regardless of the type or severity of degradation.","Specifically, Perceive-IR contains two stages: (1) prompt learning stage and (2) restoration stage.","In the prompt learning stage, we leverage prompt learning to acquire a fine-grained quality perceiver capable of distinguishing three-tier quality levels by constraining the prompt-image similarity in the CLIP perception space.","Subsequently, this quality perceiver and difficulty-adaptive perceptual loss are integrated as a quality-aware learning strategy to realize fine-grained quality control in restoration stage.","For the restoration stage, a semantic guidance module (SGM) and compact feature extraction (CFE) are proposed to further promote the restoration process by utilizing the robust semantic information from the pre-trained large scale vision models and distinguishing degradation-specific features.","Extensive experiments demonstrate that our Perceive-IR outperforms state-of-the-art methods in all-in-one image restoration tasks and exhibit superior generalization ability when dealing with unseen tasks."],"url":"http://arxiv.org/abs/2408.15994v1"}
{"created":"2024-08-28 17:58:53","title":"ClimDetect: A Benchmark Dataset for Climate Change Detection and Attribution","abstract":"Detecting and attributing temperature increases due to climate change is crucial for understanding global warming and guiding adaptation strategies. The complexity of distinguishing human-induced climate signals from natural variability has challenged traditional detection and attribution (D&A) approaches, which seek to identify specific \"fingerprints\" in climate response variables. Deep learning offers potential for discerning these complex patterns in expansive spatial datasets. However, lack of standard protocols has hindered consistent comparisons across studies. We introduce ClimDetect, a standardized dataset of over 816k daily climate snapshots, designed to enhance model accuracy in identifying climate change signals. ClimDetect integrates various input and target variables used in past research, ensuring comparability and consistency. We also explore the application of vision transformers (ViT) to climate data, a novel and modernizing approach in this context. Our open-access data and code serve as a benchmark for advancing climate science through improved model evaluations. ClimDetect is publicly accessible via Huggingface dataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect.","sentences":["Detecting and attributing temperature increases due to climate change is crucial for understanding global warming and guiding adaptation strategies.","The complexity of distinguishing human-induced climate signals from natural variability has challenged traditional detection and attribution (D&A) approaches, which seek to identify specific \"fingerprints\" in climate response variables.","Deep learning offers potential for discerning these complex patterns in expansive spatial datasets.","However, lack of standard protocols has hindered consistent comparisons across studies.","We introduce ClimDetect, a standardized dataset of over 816k daily climate snapshots, designed to enhance model accuracy in identifying climate change signals.","ClimDetect integrates various input and target variables used in past research, ensuring comparability and consistency.","We also explore the application of vision transformers (ViT) to climate data, a novel and modernizing approach in this context.","Our open-access data and code serve as a benchmark for advancing climate science through improved model evaluations.","ClimDetect is publicly accessible via Huggingface dataet respository at: https://huggingface.co/datasets/ClimDetect/ClimDetect."],"url":"http://arxiv.org/abs/2408.15993v1"}
{"created":"2024-08-28 17:58:39","title":"CoGen: Learning from Feedback with Coupled Comprehension and Generation","abstract":"Systems with both language comprehension and generation capabilities can benefit from the tight connection between the two. This work studies coupling comprehension and generation with focus on continually learning from interaction with users. We propose techniques to tightly integrate the two capabilities for both learning and inference. We situate our studies in two-player reference games, and deploy various models for thousands of interactions with human users, while learning from interaction feedback signals. We show dramatic improvements in performance over time, with comprehension-generation coupling leading to performance improvements up to 26% in absolute terms and up to 17% higher accuracies compared to a non-coupled system. Our analysis also shows coupling has substantial qualitative impact on the system's language, making it significantly more human-like.","sentences":["Systems with both language comprehension and generation capabilities can benefit from the tight connection between the two.","This work studies coupling comprehension and generation with focus on continually learning from interaction with users.","We propose techniques to tightly integrate the two capabilities for both learning and inference.","We situate our studies in two-player reference games, and deploy various models for thousands of interactions with human users, while learning from interaction feedback signals.","We show dramatic improvements in performance over time, with comprehension-generation coupling leading to performance improvements up to 26% in absolute terms and up to 17% higher accuracies compared to a non-coupled system.","Our analysis also shows coupling has substantial qualitative impact on the system's language, making it significantly more human-like."],"url":"http://arxiv.org/abs/2408.15992v1"}
{"created":"2024-08-28 17:58:17","title":"Distribution Backtracking Builds A Faster Convergence Trajectory for One-step Diffusion Distillation","abstract":"Accelerating the sampling speed of diffusion models remains a significant challenge. Recent score distillation methods distill a heavy teacher model into an one-step student generator, which is optimized by calculating the difference between the two score functions on the samples generated by the student model. However, there is a score mismatch issue in the early stage of the distillation process, because existing methods mainly focus on using the endpoint of pre-trained diffusion models as teacher models, overlooking the importance of the convergence trajectory between the student generator and the teacher model. To address this issue, we extend the score distillation process by introducing the entire convergence trajectory of teacher models and propose Distribution Backtracking Distillation (DisBack) for distilling student generators. DisBask is composed of two stages: Degradation Recording and Distribution Backtracking. Degradation Recording is designed to obtain the convergence trajectory of teacher models, which records the degradation path from the trained teacher model to the untrained initial student generator. The degradation path implicitly represents the intermediate distributions of teacher models. Then Distribution Backtracking trains a student generator to backtrack the intermediate distributions for approximating the convergence trajectory of teacher models. Extensive experiments show that DisBack achieves faster and better convergence than the existing distillation method and accomplishes comparable generation performance. Notably, DisBack is easy to implement and can be generalized to existing distillation methods to boost performance. Our code is publicly available on https://github.com/SYZhang0805/DisBack.","sentences":["Accelerating the sampling speed of diffusion models remains a significant challenge.","Recent score distillation methods distill a heavy teacher model into an one-step student generator, which is optimized by calculating the difference between the two score functions on the samples generated by the student model.","However, there is a score mismatch issue in the early stage of the distillation process, because existing methods mainly focus on using the endpoint of pre-trained diffusion models as teacher models, overlooking the importance of the convergence trajectory between the student generator and the teacher model.","To address this issue, we extend the score distillation process by introducing the entire convergence trajectory of teacher models and propose Distribution Backtracking Distillation (DisBack) for distilling student generators.","DisBask is composed of two stages: Degradation Recording and Distribution Backtracking.","Degradation Recording is designed to obtain the convergence trajectory of teacher models, which records the degradation path from the trained teacher model to the untrained initial student generator.","The degradation path implicitly represents the intermediate distributions of teacher models.","Then Distribution Backtracking trains a student generator to backtrack the intermediate distributions for approximating the convergence trajectory of teacher models.","Extensive experiments show that DisBack achieves faster and better convergence than the existing distillation method and accomplishes comparable generation performance.","Notably, DisBack is easy to implement and can be generalized to existing distillation methods to boost performance.","Our code is publicly available on https://github.com/SYZhang0805/DisBack."],"url":"http://arxiv.org/abs/2408.15991v1"}
{"created":"2024-08-28 17:55:17","title":"Software Solutions for Newcomers' Onboarding in Software Projects: A Systematic Literature Review","abstract":"[Context] Newcomers joining an unfamiliar software project face numerous barriers; therefore, effective onboarding is essential to help them engage with the team and develop the behaviors, attitudes, and skills needed to excel in their roles. However, onboarding can be a lengthy, costly, and error-prone process. Software solutions can help mitigate these barriers and streamline the process without overloading senior members. [Objective] This study aims to identify the state-of-the-art software solutions for onboarding newcomers. [Method] We conducted a systematic literature review (SLR) to answer six research questions. [Results] We analyzed 32 studies about software solutions for onboarding newcomers and yielded several key findings: (1) a range of strategies exists, with recommendation systems being the most prevalent; (2) most solutions are web-based; (3) solutions target a variety of onboarding aspects, with a focus on process; (4) many onboarding barriers remain unaddressed by existing solutions; (5) laboratory experiments are the most commonly used method for evaluating these solutions; and (6) diversity and inclusion aspects primarily address experience level. [Conclusion] We shed light on current technological support and identify research opportunities to develop more inclusive software solutions for onboarding. These insights may also guide practitioners in refining existing platforms and onboarding programs to promote smoother integration of newcomers into software projects.","sentences":["[Context] Newcomers joining an unfamiliar software project face numerous barriers; therefore, effective onboarding is essential to help them engage with the team and develop the behaviors, attitudes, and skills needed to excel in their roles.","However, onboarding can be a lengthy, costly, and error-prone process.","Software solutions can help mitigate these barriers and streamline the process without overloading senior members.","[Objective] This study aims to identify the state-of-the-art software solutions for onboarding newcomers.","[Method] We conducted a systematic literature review (SLR) to answer six research questions.","[Results] We analyzed 32 studies about software solutions for onboarding newcomers and yielded several key findings: (1) a range of strategies exists, with recommendation systems being the most prevalent; (2) most solutions are web-based; (3) solutions target a variety of onboarding aspects, with a focus on process; (4) many onboarding barriers remain unaddressed by existing solutions; (5) laboratory experiments are the most commonly used method for evaluating these solutions; and (6) diversity and inclusion aspects primarily address experience level.","[Conclusion] We shed light on current technological support and identify research opportunities to develop more inclusive software solutions for onboarding.","These insights may also guide practitioners in refining existing platforms and onboarding programs to promote smoother integration of newcomers into software projects."],"url":"http://arxiv.org/abs/2408.15989v1"}
{"created":"2024-08-28 17:50:19","title":"In-Context Imitation Learning via Next-Token Prediction","abstract":"We explore how to enhance next-token prediction models to perform in-context imitation learning on a real robot, where the robot executes new tasks by interpreting contextual information provided during the input phase, without updating its underlying policy parameters. We propose In-Context Robot Transformer (ICRT), a causal transformer that performs autoregressive prediction on sensorimotor trajectories without relying on any linguistic data or reward function. This formulation enables flexible and training-free execution of new tasks at test time, achieved by prompting the model with sensorimotor trajectories of the new task composing of image observations, actions and states tuples, collected through human teleoperation. Experiments with a Franka Emika robot demonstrate that the ICRT can adapt to new tasks specified by prompts, even in environment configurations that differ from both the prompt and the training data. In a multitask environment setup, ICRT significantly outperforms current state-of-the-art next-token prediction models in robotics on generalizing to unseen tasks. Code, checkpoints and data are available on https://icrt.dev/","sentences":["We explore how to enhance next-token prediction models to perform in-context imitation learning on a real robot, where the robot executes new tasks by interpreting contextual information provided during the input phase, without updating its underlying policy parameters.","We propose In-Context Robot Transformer (ICRT), a causal transformer that performs autoregressive prediction on sensorimotor trajectories without relying on any linguistic data or reward function.","This formulation enables flexible and training-free execution of new tasks at test time, achieved by prompting the model with sensorimotor trajectories of the new task composing of image observations, actions and states tuples, collected through human teleoperation.","Experiments with a Franka Emika robot demonstrate that the ICRT can adapt to new tasks specified by prompts, even in environment configurations that differ from both the prompt and the training data.","In a multitask environment setup, ICRT significantly outperforms current state-of-the-art next-token prediction models in robotics on generalizing to unseen tasks.","Code, checkpoints and data are available on https://icrt.dev/"],"url":"http://arxiv.org/abs/2408.15980v1"}
{"created":"2024-08-28 17:49:29","title":"WebPilot: A Versatile and Autonomous Multi-Agent System for Web Task Execution with Strategic Exploration","abstract":"LLM-based autonomous agents often fail to execute complex web tasks that require dynamic interaction due to the inherent uncertainty and complexity of these environments. Existing LLM-based web agents typically rely on rigid, expert-designed policies specific to certain states and actions, which lack the flexibility and generalizability needed to adapt to unseen tasks. In contrast, humans excel by exploring unknowns, continuously adapting strategies, and resolving ambiguities through exploration. To emulate human-like adaptability, web agents need strategic exploration and complex decision-making. Monte Carlo Tree Search (MCTS) is well-suited for this, but classical MCTS struggles with vast action spaces, unpredictable state transitions, and incomplete information in web tasks. In light of this, we develop WebPilot, a multi-agent system with a dual optimization strategy that improves MCTS to better handle complex web environments. Specifically, the Global Optimization phase involves generating a high-level plan by breaking down tasks into manageable subtasks and continuously refining this plan, thereby focusing the search process and mitigating the challenges posed by vast action spaces in classical MCTS. Subsequently, the Local Optimization phase executes each subtask using a tailored MCTS designed for complex environments, effectively addressing uncertainties and managing incomplete information. Experimental results on WebArena and MiniWoB++ demonstrate the effectiveness of WebPilot. Notably, on WebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93% relative increase in success rate over the concurrent tree search-based method. WebPilot marks a significant advancement in general autonomous agent capabilities, paving the way for more advanced and reliable decision-making in practical environments.","sentences":["LLM-based autonomous agents often fail to execute complex web tasks that require dynamic interaction due to the inherent uncertainty and complexity of these environments.","Existing LLM-based web agents typically rely on rigid, expert-designed policies specific to certain states and actions, which lack the flexibility and generalizability needed to adapt to unseen tasks.","In contrast, humans excel by exploring unknowns, continuously adapting strategies, and resolving ambiguities through exploration.","To emulate human-like adaptability, web agents need strategic exploration and complex decision-making.","Monte Carlo Tree Search (MCTS) is well-suited for this, but classical MCTS struggles with vast action spaces, unpredictable state transitions, and incomplete information in web tasks.","In light of this, we develop WebPilot, a multi-agent system with a dual optimization strategy that improves MCTS to better handle complex web environments.","Specifically, the Global Optimization phase involves generating a high-level plan by breaking down tasks into manageable subtasks and continuously refining this plan, thereby focusing the search process and mitigating the challenges posed by vast action spaces in classical MCTS.","Subsequently, the Local Optimization phase executes each subtask using a tailored MCTS designed for complex environments, effectively addressing uncertainties and managing incomplete information.","Experimental results on WebArena and MiniWoB++ demonstrate the effectiveness of WebPilot.","Notably, on WebArena, WebPilot achieves SOTA performance with GPT-4, achieving a 93% relative increase in success rate over the concurrent tree search-based method.","WebPilot marks a significant advancement in general autonomous agent capabilities, paving the way for more advanced and reliable decision-making in practical environments."],"url":"http://arxiv.org/abs/2408.15978v1"}
{"created":"2024-08-28 17:43:55","title":"BattleAgentBench: A Benchmark for Evaluating Cooperation and Competition Capabilities of Language Models in Multi-Agent Systems","abstract":"Large Language Models (LLMs) are becoming increasingly powerful and capable of handling complex tasks, e.g., building single agents and multi-agent systems. Compared to single agents, multi-agent systems have higher requirements for the collaboration capabilities of language models. Many benchmarks are proposed to evaluate their collaborative abilities. However, these benchmarks lack fine-grained evaluations of LLM collaborative capabilities. Additionally, multi-agent collaborative and competitive scenarios are ignored in existing works. To address these two problems, we propose a benchmark, called BattleAgentBench, which defines seven sub-stages of three varying difficulty levels and conducts a fine-grained evaluation of language models in terms of single-agent scenario navigation capabilities, paired-agent task execution abilities, and multi-agent collaboration and competition capabilities. We conducted extensive evaluations on leading four closed-source and seven open-source models. Experimental results indicate that API-based models perform excellently on simple tasks but open-source small models struggle with simple tasks. Regarding difficult tasks that require collaborative and competitive abilities, although API-based models have demonstrated some collaborative capabilities, there is still enormous room for improvement.","sentences":["Large Language Models (LLMs) are becoming increasingly powerful and capable of handling complex tasks, e.g., building single agents and multi-agent systems.","Compared to single agents, multi-agent systems have higher requirements for the collaboration capabilities of language models.","Many benchmarks are proposed to evaluate their collaborative abilities.","However, these benchmarks lack fine-grained evaluations of LLM collaborative capabilities.","Additionally, multi-agent collaborative and competitive scenarios are ignored in existing works.","To address these two problems, we propose a benchmark, called BattleAgentBench, which defines seven sub-stages of three varying difficulty levels and conducts a fine-grained evaluation of language models in terms of single-agent scenario navigation capabilities, paired-agent task execution abilities, and multi-agent collaboration and competition capabilities.","We conducted extensive evaluations on leading four closed-source and seven open-source models.","Experimental results indicate that API-based models perform excellently on simple tasks but open-source small models struggle with simple tasks.","Regarding difficult tasks that require collaborative and competitive abilities, although API-based models have demonstrated some collaborative capabilities, there is still enormous room for improvement."],"url":"http://arxiv.org/abs/2408.15971v1"}
{"created":"2024-08-28 17:43:21","title":"Ain't How You Deploy: An Analysis of BGP Security Policies Performance Against Various Attack Scenarios with Differing Deployment Strategies","abstract":"This paper investigates the performance of various Border Gateway Protocol (BGP) security policies against multiple attack scenarios using different deployment strategies. Through extensive simulations, we evaluate the effectiveness of defensive mechanisms such as Root Origin Validation (ROV), Autonomous System Provider Authorization (ASPA), and PeerROV across distinct AS deployment types. Our findings reveal critical insights into the strengths and limitations of current BGP security measures, providing guidance for future policy development and implementation.","sentences":["This paper investigates the performance of various Border Gateway Protocol (BGP) security policies against multiple attack scenarios using different deployment strategies.","Through extensive simulations, we evaluate the effectiveness of defensive mechanisms such as Root Origin Validation (ROV), Autonomous System Provider Authorization (ASPA), and PeerROV across distinct AS deployment types.","Our findings reveal critical insights into the strengths and limitations of current BGP security measures, providing guidance for future policy development and implementation."],"url":"http://arxiv.org/abs/2408.15970v1"}
{"created":"2024-08-28 17:38:44","title":"More Text, Less Point: Towards 3D Data-Efficient Point-Language Understanding","abstract":"Enabling Large Language Models (LLMs) to comprehend the 3D physical world remains a significant challenge. Due to the lack of large-scale 3D-text pair datasets, the success of LLMs has yet to be replicated in 3D understanding. In this paper, we rethink this issue and propose a new task: 3D Data-Efficient Point-Language Understanding. The goal is to enable LLMs to achieve robust 3D object understanding with minimal 3D point cloud and text data pairs. To address this task, we introduce GreenPLM, which leverages more text data to compensate for the lack of 3D data. First, inspired by using CLIP to align images and text, we utilize a pre-trained point cloud-text encoder to map the 3D point cloud space to the text space. This mapping leaves us to seamlessly connect the text space with LLMs. Once the point-text-LLM connection is established, we further enhance text-LLM alignment by expanding the intermediate text space, thereby reducing the reliance on 3D point cloud data. Specifically, we generate 6M free-text descriptions of 3D objects, and design a three-stage training strategy to help LLMs better explore the intrinsic connections between different modalities. To achieve efficient modality alignment, we design a zero-parameter cross-attention module for token pooling. Extensive experimental results show that GreenPLM requires only 12% of the 3D training data used by existing state-of-the-art models to achieve superior 3D understanding. Remarkably, GreenPLM also achieves competitive performance using text-only data. The code and weights are available at: https://github.com/TangYuan96/GreenPLM.","sentences":["Enabling Large Language Models (LLMs) to comprehend the 3D physical world remains a significant challenge.","Due to the lack of large-scale 3D-text pair datasets, the success of LLMs has yet to be replicated in 3D understanding.","In this paper, we rethink this issue and propose a new task: 3D Data-Efficient Point-Language Understanding.","The goal is to enable LLMs to achieve robust 3D object understanding with minimal 3D point cloud and text data pairs.","To address this task, we introduce GreenPLM, which leverages more text data to compensate for the lack of 3D data.","First, inspired by using CLIP to align images and text, we utilize a pre-trained point cloud-text encoder to map the 3D point cloud space to the text space.","This mapping leaves us to seamlessly connect the text space with LLMs.","Once the point-text-LLM connection is established, we further enhance text-LLM alignment by expanding the intermediate text space, thereby reducing the reliance on 3D point cloud data.","Specifically, we generate 6M free-text descriptions of 3D objects, and design a three-stage training strategy to help LLMs better explore the intrinsic connections between different modalities.","To achieve efficient modality alignment, we design a zero-parameter cross-attention module for token pooling.","Extensive experimental results show that GreenPLM requires only 12% of the 3D training data used by existing state-of-the-art models to achieve superior 3D understanding.","Remarkably, GreenPLM also achieves competitive performance using text-only data.","The code and weights are available at: https://github.com/TangYuan96/GreenPLM."],"url":"http://arxiv.org/abs/2408.15966v1"}
{"created":"2024-08-28 17:20:56","title":"Efficient Slice Anomaly Detection Network for 3D Brain MRI Volume","abstract":"Current anomaly detection methods excel with benchmark industrial data but struggle with natural images and medical data due to varying definitions of 'normal' and 'abnormal.' This makes accurate identification of deviations in these fields particularly challenging. Especially for 3D brain MRI data, all the state-of-the-art models are reconstruction-based with 3D convolutional neural networks which are memory-intensive, time-consuming and producing noisy outputs that require further post-processing. We propose a framework called Simple Slice-based Network (SimpleSliceNet), which utilizes a model pre-trained on ImageNet and fine-tuned on a separate MRI dataset as a 2D slice feature extractor to reduce computational cost. We aggregate the extracted features to perform anomaly detection tasks on 3D brain MRI volumes. Our model integrates a conditional normalizing flow to calculate log likelihood of features and employs the Semi-Push-Pull Mechanism to enhance anomaly detection accuracy. The results indicate improved performance, showcasing our model's remarkable adaptability and effectiveness when addressing the challenges exists in brain MRI data. In addition, for the large-scale 3D brain volumes, our model SimpleSliceNet outperforms the state-of-the-art 2D and 3D models in terms of accuracy, memory usage and time consumption. Code is available at: https://anonymous.4open.science/r/SimpleSliceNet-8EA3.","sentences":["Current anomaly detection methods excel with benchmark industrial data but struggle with natural images and medical data due to varying definitions of 'normal' and 'abnormal.'","This makes accurate identification of deviations in these fields particularly challenging.","Especially for 3D brain MRI data, all the state-of-the-art models are reconstruction-based with 3D convolutional neural networks which are memory-intensive, time-consuming and producing noisy outputs that require further post-processing.","We propose a framework called Simple Slice-based Network (SimpleSliceNet), which utilizes a model pre-trained on ImageNet and fine-tuned on a separate MRI dataset as a 2D slice feature extractor to reduce computational cost.","We aggregate the extracted features to perform anomaly detection tasks on 3D brain MRI volumes.","Our model integrates a conditional normalizing flow to calculate log likelihood of features and employs the Semi-Push-Pull Mechanism to enhance anomaly detection accuracy.","The results indicate improved performance, showcasing our model's remarkable adaptability and effectiveness when addressing the challenges exists in brain MRI data.","In addition, for the large-scale 3D brain volumes, our model SimpleSliceNet outperforms the state-of-the-art 2D and 3D models in terms of accuracy, memory usage and time consumption.","Code is available at: https://anonymous.4open.science/r/SimpleSliceNet-8EA3."],"url":"http://arxiv.org/abs/2408.15958v1"}
{"created":"2024-08-28 17:14:51","title":"Fall Detection for Smart Living using YOLOv5","abstract":"This work introduces a fall detection system using the YOLOv5mu model, which achieved a mean average precision (mAP) of 0.995, demonstrating exceptional accuracy in identifying fall events within smart home environments. Enhanced by advanced data augmentation techniques, the model demonstrates significant robustness and adaptability across various conditions. The integration of YOLOv5mu offers precise, real-time fall detection, which is crucial for improving safety and emergency response for residents. Future research will focus on refining the system by incorporating contextual data and exploring multi-sensor approaches to enhance its performance and practical applicability in diverse environments.","sentences":["This work introduces a fall detection system using the YOLOv5mu model, which achieved a mean average precision (mAP) of 0.995, demonstrating exceptional accuracy in identifying fall events within smart home environments.","Enhanced by advanced data augmentation techniques, the model demonstrates significant robustness and adaptability across various conditions.","The integration of YOLOv5mu offers precise, real-time fall detection, which is crucial for improving safety and emergency response for residents.","Future research will focus on refining the system by incorporating contextual data and exploring multi-sensor approaches to enhance its performance and practical applicability in diverse environments."],"url":"http://arxiv.org/abs/2408.15955v1"}
{"created":"2024-08-28 17:14:21","title":"InstanSeg: an embedding-based instance segmentation algorithm optimized for accurate, efficient and portable cell segmentation","abstract":"Cell and nucleus segmentation are fundamental tasks for quantitative bioimage analysis. Despite progress in recent years, biologists and other domain experts still require novel algorithms to handle increasingly large and complex real-world datasets. These algorithms must not only achieve state-of-the-art accuracy, but also be optimized for efficiency, portability and user-friendliness. Here, we introduce InstanSeg: a novel embedding-based instance segmentation pipeline designed to identify cells and nuclei in microscopy images. Using six public cell segmentation datasets, we demonstrate that InstanSeg can significantly improve accuracy when compared to the most widely used alternative methods, while reducing the processing time by at least 60%. Furthermore, InstanSeg is designed to be fully serializable as TorchScript and supports GPU acceleration on a range of hardware. We provide an open-source implementation of InstanSeg in Python, in addition to a user-friendly, interactive QuPath extension for inference written in Java. Our code and pre-trained models are available at https://github.com/instanseg/instanseg .","sentences":["Cell and nucleus segmentation are fundamental tasks for quantitative bioimage analysis.","Despite progress in recent years, biologists and other domain experts still require novel algorithms to handle increasingly large and complex real-world datasets.","These algorithms must not only achieve state-of-the-art accuracy, but also be optimized for efficiency, portability and user-friendliness.","Here, we introduce InstanSeg: a novel embedding-based instance segmentation pipeline designed to identify cells and nuclei in microscopy images.","Using six public cell segmentation datasets, we demonstrate that InstanSeg can significantly improve accuracy when compared to the most widely used alternative methods, while reducing the processing time by at least 60%.","Furthermore, InstanSeg is designed to be fully serializable as TorchScript and supports GPU acceleration on a range of hardware.","We provide an open-source implementation of InstanSeg in Python, in addition to a user-friendly, interactive QuPath extension for inference written in Java.","Our code and pre-trained models are available at https://github.com/instanseg/instanseg ."],"url":"http://arxiv.org/abs/2408.15954v1"}
{"created":"2024-08-28 17:12:01","title":"Modeling and Analyzing the Influence of Non-Item Pages on Sequential Next-Item Prediction","abstract":"Analyzing the sequence of historical interactions between users and items, sequential recommendation models learn user intent and make predictions about the next item of interest. Next to these item interactions, most systems also have interactions with pages not related to specific items, for example navigation pages, account pages, and pages for a specific category, which may provide additional insights into the user's interests. However, while there are several approaches to integrate additional information about items and users, the topic of integrating non-item pages has been less explored. We use the hypotheses testing framework HypTrails to show that there is indeed a relationship between these non-item pages and the items of interest and fill this gap by proposing various approaches of representing non-item pages (e.g, based on their content) to use them as an additional information source for the task of sequential next-item prediction.   We create a synthetic dataset with non-item pages highly related to the subsequent item to show that the models are generally capable of learning from these interactions, and subsequently evaluate the improvements gained by including non-item pages in two real-world datasets.   We adapt eight popular sequential recommender models, covering CNN-, RNN- and transformer-based architectures, to integrate non-item pages and investigate the capabilities of these models to leverage their information for next item prediction. We also analyze their behavior on noisy data and compare different item representation strategies.   Our results show that non-item pages are a valuable source of information, but representing such a page well is the key to successfully leverage them. The inclusion of non-item pages can increase the performance for next-item prediction in all examined model architectures with a varying degree.","sentences":["Analyzing the sequence of historical interactions between users and items, sequential recommendation models learn user intent and make predictions about the next item of interest.","Next to these item interactions, most systems also have interactions with pages not related to specific items, for example navigation pages, account pages, and pages for a specific category, which may provide additional insights into the user's interests.","However, while there are several approaches to integrate additional information about items and users, the topic of integrating non-item pages has been less explored.","We use the hypotheses testing framework HypTrails to show that there is indeed a relationship between these non-item pages and the items of interest and fill this gap by proposing various approaches of representing non-item pages (e.g, based on their content) to use them as an additional information source for the task of sequential next-item prediction.   ","We create a synthetic dataset with non-item pages highly related to the subsequent item to show that the models are generally capable of learning from these interactions, and subsequently evaluate the improvements gained by including non-item pages in two real-world datasets.   ","We adapt eight popular sequential recommender models, covering CNN-, RNN- and transformer-based architectures, to integrate non-item pages and investigate the capabilities of these models to leverage their information for next item prediction.","We also analyze their behavior on noisy data and compare different item representation strategies.   ","Our results show that non-item pages are a valuable source of information, but representing such a page well is the key to successfully leverage them.","The inclusion of non-item pages can increase the performance for next-item prediction in all examined model architectures with a varying degree."],"url":"http://arxiv.org/abs/2408.15953v1"}
{"created":"2024-08-28 17:08:56","title":"Atari-GPT: Investigating the Capabilities of Multimodal Large Language Models as Low-Level Policies for Atari Games","abstract":"Recent advancements in large language models (LLMs) have expanded their capabilities beyond traditional text-based tasks to multimodal domains, integrating visual, auditory, and textual data. While multimodal LLMs have been extensively explored for high-level planning in domains like robotics and games, their potential as low-level controllers remains largely untapped. This paper explores the application of multimodal LLMs as low-level controllers in the domain of Atari video games, introducing Atari game performance as a new benchmark for evaluating the ability of multimodal LLMs to perform low-level control tasks. Unlike traditional reinforcement learning (RL) and imitation learning (IL) methods that require extensive computational resources as well as reward function specification, these LLMs utilize pre-existing multimodal knowledge to directly engage with game environments. Our study assesses multiple multimodal LLMs performance against traditional RL agents, human players, and random agents, focusing on their ability to understand and interact with complex visual scenes and formulate strategic responses. Additionally, we examine the impact of In-Context Learning (ICL) by incorporating human-demonstrated game-play trajectories to enhance the models contextual understanding. Through this investigation, we aim to determine the extent to which multimodal LLMs can leverage their extensive training to effectively function as low-level controllers, thereby redefining potential applications in dynamic and visually complex environments. Additional results and videos are available at our project webpage: https://sites.google.com/view/atari-gpt/.","sentences":["Recent advancements in large language models (LLMs) have expanded their capabilities beyond traditional text-based tasks to multimodal domains, integrating visual, auditory, and textual data.","While multimodal LLMs have been extensively explored for high-level planning in domains like robotics and games, their potential as low-level controllers remains largely untapped.","This paper explores the application of multimodal LLMs as low-level controllers in the domain of Atari video games, introducing Atari game performance as a new benchmark for evaluating the ability of multimodal LLMs to perform low-level control tasks.","Unlike traditional reinforcement learning (RL) and imitation learning (IL) methods that require extensive computational resources as well as reward function specification, these LLMs utilize pre-existing multimodal knowledge to directly engage with game environments.","Our study assesses multiple multimodal LLMs performance against traditional RL agents, human players, and random agents, focusing on their ability to understand and interact with complex visual scenes and formulate strategic responses.","Additionally, we examine the impact of In-Context Learning (ICL) by incorporating human-demonstrated game-play trajectories to enhance the models contextual understanding.","Through this investigation, we aim to determine the extent to which multimodal LLMs can leverage their extensive training to effectively function as low-level controllers, thereby redefining potential applications in dynamic and visually complex environments.","Additional results and videos are available at our project webpage: https://sites.google.com/view/atari-gpt/."],"url":"http://arxiv.org/abs/2408.15950v1"}
{"created":"2024-08-28 17:05:40","title":"SLAM2REF: Advancing Long-Term Mapping with 3D LiDAR and Reference Map Integration for Precise 6-DoF Trajectory Estimation and Map Extension","abstract":"This paper presents a pioneering solution to the task of integrating mobile 3D LiDAR and inertial measurement unit (IMU) data with existing building information models or point clouds, which is crucial for achieving precise long-term localization and mapping in indoor, GPS-denied environments. Our proposed framework, SLAM2REF, introduces a novel approach for automatic alignment and map extension utilizing reference 3D maps. The methodology is supported by a sophisticated multi-session anchoring technique, which integrates novel descriptors and registration methodologies. Real-world experiments reveal the framework's remarkable robustness and accuracy, surpassing current state-of-the-art methods. Our open-source framework's significance lies in its contribution to resilient map data management, enhancing processes across diverse sectors such as construction site monitoring, emergency response, disaster management, and others, where fast-updated digital 3D maps contribute to better decision-making and productivity. Moreover, it offers advancements in localization and mapping research. Link to the repository: https://github.com/MigVega/SLAM2REF, Data: https://doi.org/10.14459/2024mp1743877.","sentences":["This paper presents a pioneering solution to the task of integrating mobile 3D LiDAR and inertial measurement unit (IMU) data with existing building information models or point clouds, which is crucial for achieving precise long-term localization and mapping in indoor, GPS-denied environments.","Our proposed framework, SLAM2REF, introduces a novel approach for automatic alignment and map extension utilizing reference 3D maps.","The methodology is supported by a sophisticated multi-session anchoring technique, which integrates novel descriptors and registration methodologies.","Real-world experiments reveal the framework's remarkable robustness and accuracy, surpassing current state-of-the-art methods.","Our open-source framework's significance lies in its contribution to resilient map data management, enhancing processes across diverse sectors such as construction site monitoring, emergency response, disaster management, and others, where fast-updated digital 3D maps contribute to better decision-making and productivity.","Moreover, it offers advancements in localization and mapping research.","Link to the repository: https://github.com/MigVega/SLAM2REF, Data: https://doi.org/10.14459/2024mp1743877."],"url":"http://arxiv.org/abs/2408.15948v1"}
{"created":"2024-08-28 16:39:58","title":"Explicit Folded Reed-Solomon and Multiplicity Codes Achieve Relaxed Generalized Singleton Bound","abstract":"In this paper, we prove that any `appropriate' folded Reed-Solomon and univariate multiplicity codes achieve relaxed generalized Singleton bound for list size $L\\ge1.$ More concretely, we show the following: (1) Any $(s,\\gamma)$-folded RS code over the alphabet $\\mathbb{F}_q^s$ of block length $n$ and rate $R$ with pair-wise distinct evaluation points $\\{\\gamma^i\\alpha_j\\}_{(i,j)\\in\\left(\\{0\\}\\sqcup[s-1],[n]\\right)}\\subset\\mathbb{F}_q$ are $\\left(\\frac{L}{L+1}\\left(1-\\frac{sR}{s-L+1}\\right),L\\right)$ (average-radius) list-decodable for list size $L\\in[s]$. (2) Any $s$-order univariate multiplicity code over the alphabet $\\mathbb{F}_p^s$ ($p$ is a prime) of block length $n$ and rate $R$ with pair-wise distinct evaluation points $\\{\\alpha_i\\}_{i\\in[n]}\\subset\\mathbb{F}_p$ are $\\left(\\frac{L}{L+1}\\left(1-\\frac{sR}{s-L+1}\\right),L\\right)$ (average-radius) list-decodable for list size $L\\in[s]$.   Choose $s=\\Theta(1/\\epsilon^2)$ and $L=O(1/\\epsilon)$, our results imply that both explicit folded RS codes and explicit univariate multiplicity codes achieve list decoding capacity $1-R-\\epsilon$ with evidently optimal list size $O(1/\\epsilon)$, which exponentially improves the previous state-of-the-art $(1/\\epsilon)^{O(1/\\epsilon)}$ established by Kopparty, Ron-Zewi, Saraf, and Wootters (FOCS 2018 or SICOMP, 2023) and Tamo (IEEE TIT, 2024). In particular, our results on folded Reed--Solomon codes fully resolve a long-standing open problem originally proposed by Guruswami and Rudra (STOC 2006 or IEEE TIT, 2008). Furthermore, our results imply the first explicit constructions of $(1-R-\\epsilon,O(1/\\epsilon))$ (average-radius) list-decodable codes of rate $R$ with polynomial-sized alphabets in the literature.","sentences":["In this paper, we prove that any `appropriate' folded Reed-Solomon and univariate multiplicity codes achieve relaxed generalized Singleton bound for list size $L\\ge1.$ More concretely, we show the following: (1) Any $(s,\\gamma)$-folded RS code over the alphabet $\\mathbb{F}_q^s$ of block length $n$ and rate $R$ with pair-wise distinct evaluation points $\\{\\gamma^i\\alpha_j\\}_{(i,j)\\in\\left(\\{0\\}\\sqcup[s-1],[n]\\right)}\\subset\\mathbb{F}_q$ are $\\left(\\frac{L}{L+1}\\left(1-\\frac{sR}{s-L+1}\\right),L\\right)$ (average-radius) list-decodable for list size $L\\in[s]$. (2) Any $s$-order univariate multiplicity code over the alphabet $\\mathbb{F}_p^s$ ($p$ is a prime) of block length $n$ and rate $R$ with pair-wise distinct evaluation points $\\{\\alpha_i\\}_{i\\in[n]}\\subset\\mathbb{F}_p$ are $\\left(\\frac{L}{L+1}\\left(1-\\frac{sR}{s-L+1}\\right),L\\right)$ (average-radius) list-decodable for list size $L\\in[s]$.   Choose $s=\\Theta(1/\\epsilon^2)$ and $L=O(1/\\epsilon)$, our results imply that both explicit folded RS codes and explicit univariate multiplicity codes achieve list decoding capacity $1-R-\\epsilon$ with evidently optimal list size $O(1/\\epsilon)$, which exponentially improves the previous state-of-the-art $(1/\\epsilon)^{O(1/\\epsilon)}$ established by Kopparty, Ron-Zewi, Saraf, and Wootters (FOCS 2018 or SICOMP, 2023) and Tamo (IEEE TIT, 2024).","In particular, our results on folded Reed--Solomon codes fully resolve a long-standing open problem originally proposed by Guruswami and Rudra (STOC 2006 or IEEE TIT, 2008).","Furthermore, our results imply the first explicit constructions of $(1-R-\\epsilon,O(1/\\epsilon))$ (average-radius) list-decodable codes of rate $R$ with polynomial-sized alphabets in the literature."],"url":"http://arxiv.org/abs/2408.15925v1"}
{"created":"2024-08-28 16:36:23","title":"Local Descriptors Weighted Adaptive Threshold Filtering For Few-Shot Learning","abstract":"Few-shot image classification is a challenging task in the field of machine learning, involving the identification of new categories using a limited number of labeled samples. In recent years, methods based on local descriptors have made significant progress in this area. However, the key to improving classification accuracy lies in effectively filtering background noise and accurately selecting critical local descriptors highly relevant to image category information.   To address this challenge, we propose an innovative weighted adaptive threshold filtering (WATF) strategy for local descriptors. This strategy can dynamically adjust based on the current task and image context, thereby selecting local descriptors most relevant to the image category. This enables the model to better focus on category-related information while effectively mitigating interference from irrelevant background regions.   To evaluate the effectiveness of our method, we adopted the N-way K-shot experimental framework. Experimental results show that our method not only improves the clustering effect of selected local descriptors but also significantly enhances the discriminative ability between image categories. Notably, our method maintains a simple and lightweight design philosophy without introducing additional learnable parameters. This feature ensures consistency in filtering capability during both training and testing phases, further enhancing the reliability and practicality of the method.","sentences":["Few-shot image classification is a challenging task in the field of machine learning, involving the identification of new categories using a limited number of labeled samples.","In recent years, methods based on local descriptors have made significant progress in this area.","However, the key to improving classification accuracy lies in effectively filtering background noise and accurately selecting critical local descriptors highly relevant to image category information.   ","To address this challenge, we propose an innovative weighted adaptive threshold filtering (WATF) strategy for local descriptors.","This strategy can dynamically adjust based on the current task and image context, thereby selecting local descriptors most relevant to the image category.","This enables the model to better focus on category-related information while effectively mitigating interference from irrelevant background regions.   ","To evaluate the effectiveness of our method, we adopted the N-way K-shot experimental framework.","Experimental results show that our method not only improves the clustering effect of selected local descriptors but also significantly enhances the discriminative ability between image categories.","Notably, our method maintains a simple and lightweight design philosophy without introducing additional learnable parameters.","This feature ensures consistency in filtering capability during both training and testing phases, further enhancing the reliability and practicality of the method."],"url":"http://arxiv.org/abs/2408.15924v1"}
{"created":"2024-08-28 16:36:09","title":"DiffAge3D: Diffusion-based 3D-aware Face Aging","abstract":"Face aging is the process of converting an individual's appearance to a younger or older version of themselves. Existing face aging techniques have been limited to 2D settings, which often weaken their applications as there is a growing demand for 3D face modeling. Moreover, existing aging methods struggle to perform faithful aging, maintain identity, and retain the fine details of the input images. Given these limitations and the need for a 3D-aware aging method, we propose DiffAge3D, the first 3D-aware aging framework that not only performs faithful aging and identity preservation but also operates in a 3D setting. Our aging framework allows to model the aging and camera pose separately by only taking a single image with a target age. Our framework includes a robust 3D-aware aging dataset generation pipeline by utilizing a pre-trained 3D GAN and the rich text embedding capabilities within CLIP model. Notably, we do not employ any inversion bottleneck in dataset generation. Instead, we randomly generate training samples from the latent space of 3D GAN, allowing us to manipulate the rich latent space of GAN to generate ages even with large gaps. With the generated dataset, we train a viewpoint-aware diffusion-based aging model to control the camera pose and facial age. Through quantitative and qualitative evaluations, we demonstrate that DiffAge3D outperforms existing methods, particularly in multiview-consistent aging and fine details preservation.","sentences":["Face aging is the process of converting an individual's appearance to a younger or older version of themselves.","Existing face aging techniques have been limited to 2D settings, which often weaken their applications as there is a growing demand for 3D face modeling.","Moreover, existing aging methods struggle to perform faithful aging, maintain identity, and retain the fine details of the input images.","Given these limitations and the need for a 3D-aware aging method, we propose DiffAge3D, the first 3D-aware aging framework that not only performs faithful aging and identity preservation but also operates in a 3D setting.","Our aging framework allows to model the aging and camera pose separately by only taking a single image with a target age.","Our framework includes a robust 3D-aware aging dataset generation pipeline by utilizing a pre-trained 3D GAN and the rich text embedding capabilities within CLIP model.","Notably, we do not employ any inversion bottleneck in dataset generation.","Instead, we randomly generate training samples from the latent space of 3D GAN, allowing us to manipulate the rich latent space of GAN to generate ages even with large gaps.","With the generated dataset, we train a viewpoint-aware diffusion-based aging model to control the camera pose and facial age.","Through quantitative and qualitative evaluations, we demonstrate that DiffAge3D outperforms existing methods, particularly in multiview-consistent aging and fine details preservation."],"url":"http://arxiv.org/abs/2408.15922v1"}
{"created":"2024-08-28 16:33:21","title":"DeMoBot: Deformable Mobile Manipulation with Vision-based Sub-goal Retrieval","abstract":"Imitation learning (IL) algorithms typically distill experience into parametric behavior policies to mimic expert demonstrations. Despite their effectiveness, previous methods often struggle with data efficiency and accurately aligning the current state with expert demonstrations, especially in deformable mobile manipulation tasks characterized by partial observations and dynamic object deformations. In this paper, we introduce \\textbf{DeMoBot}, a novel IL approach that directly retrieves observations from demonstrations to guide robots in \\textbf{De}formable \\textbf{Mo}bile manipulation tasks. DeMoBot utilizes vision foundation models to identify relevant expert data based on visual similarity and matches the current trajectory with demonstrated trajectories using trajectory similarity and forward reachability constraints to select suitable sub-goals. Once a goal is determined, a motion generation policy will guide the robot to the next state until the task is completed. We evaluated DeMoBot using a Spot robot in several simulated and real-world settings, demonstrating its effectiveness and generalizability. With only 20 demonstrations, DeMoBot significantly outperforms the baselines, reaching a 50\\% success rate in curtain opening and 85\\% in gap covering in simulation.","sentences":["Imitation learning (IL) algorithms typically distill experience into parametric behavior policies to mimic expert demonstrations.","Despite their effectiveness, previous methods often struggle with data efficiency and accurately aligning the current state with expert demonstrations, especially in deformable mobile manipulation tasks characterized by partial observations and dynamic object deformations.","In this paper, we introduce \\textbf{DeMoBot}, a novel IL approach that directly retrieves observations from demonstrations to guide robots in \\textbf{De}formable \\textbf{Mo}bile manipulation tasks.","DeMoBot utilizes vision foundation models to identify relevant expert data based on visual similarity and matches the current trajectory with demonstrated trajectories using trajectory similarity and forward reachability constraints to select suitable sub-goals.","Once a goal is determined, a motion generation policy will guide the robot to the next state until the task is completed.","We evaluated DeMoBot using a Spot robot in several simulated and real-world settings, demonstrating its effectiveness and generalizability.","With only 20 demonstrations, DeMoBot significantly outperforms the baselines, reaching a 50\\% success rate in curtain opening and 85\\% in gap covering in simulation."],"url":"http://arxiv.org/abs/2408.15919v1"}
{"created":"2024-08-28 16:31:26","title":"Comprehensive Systems for Primary Decompositions of Parametric Ideals","abstract":"We present an effective method for computing parametric primary decomposition via comprehensive Gr\\\"obner systems. In general, it is very difficult to compute a parametric primary decomposition of a given ideal in the polynomial ring with rational coefficients $\\mathbb{Q}[A,X]$ where $A$ is the set of parameters and $X$ is the set of ordinary variables. One cause of the difficulty is related to the irreducibility of the specialized polynomial. Thus, we introduce a new notion of ``feasibility'' on the stability of the structure of the ideal in terms of its primary decomposition, and we give a new algorithm for computing a so-called comprehensive system consisting of pairs $(C, \\mathcal{Q})$, where for each parameter value in $C$, the ideal has the stable decomposition $\\mathcal{Q}$. We may call this comprehensive system a parametric primary decomposition of the ideal. Also, one can also compute a dense set $\\mathcal{O}$ such that $\\varphi_\\alpha(\\mathcal{Q})$ is a primary decomposition for any $\\alpha\\in C\\cap \\mathcal{O}$ via irreducible polynomials. In addition, we give several computational examples to examine the effectiveness of our new decomposition.","sentences":["We present an effective method for computing parametric primary decomposition via comprehensive Gr\\\"obner systems.","In general, it is very difficult to compute a parametric primary decomposition of a given ideal in the polynomial ring with rational coefficients $\\mathbb{Q}[A,X]$ where $A$ is the set of parameters and $X$ is the set of ordinary variables.","One cause of the difficulty is related to the irreducibility of the specialized polynomial.","Thus, we introduce a new notion of ``feasibility'' on the stability of the structure of the ideal in terms of its primary decomposition, and we give a new algorithm for computing a so-called comprehensive system consisting of pairs $(C, \\mathcal{Q})$, where for each parameter value in $C$, the ideal has the stable decomposition $\\mathcal{Q}$. We may call this comprehensive system a parametric primary decomposition of the ideal.","Also, one can also compute a dense set $\\mathcal{O}$ such that $\\varphi_\\alpha(\\mathcal{Q})$ is a primary decomposition for any $\\alpha\\in C\\cap \\mathcal{O}$ via irreducible polynomials.","In addition, we give several computational examples to examine the effectiveness of our new decomposition."],"url":"http://arxiv.org/abs/2408.15917v1"}
{"created":"2024-08-28 16:28:07","title":"Leveraging Open Knowledge for Advancing Task Expertise in Large Language Models","abstract":"The cultivation of expertise for large language models (LLMs) to solve tasks of specific areas often requires special-purpose tuning with calibrated behaviors on the expected stable outputs. To avoid huge cost brought by manual preparation of instruction datasets and training resources up to hundreds of hours, the exploitation of open knowledge including a wealth of low rank adaptation (LoRA) models and instruction datasets serves as a good starting point. However, existing methods on model and data selection focus on the performance of general-purpose capabilities while neglecting the knowledge gap exposed in domain-specific deployment. In the present study, we propose to bridge such gap by introducing few human-annotated samples (i.e., K-shot) for advancing task expertise of LLMs with open knowledge. Specifically, we develop an efficient and scalable pipeline to cost-efficiently produce task experts where K-shot data intervene in selecting the most promising expert candidates and the task-relevant instructions. A mixture-of-expert (MoE) system is built to make the best use of individual-yet-complementary knowledge between multiple experts. We unveil the two keys to the success of a MoE system, 1) the abidance by K-shot, and 2) the insistence on diversity. For the former, we ensure that models that truly possess problem-solving abilities on K-shot are selected rather than those blind guessers. Besides, during data selection, instructions that share task-relevant contexts with K-shot are prioritized. For the latter, we highlight the diversity of constituting experts and that of the fine-tuning instructions throughout the model and data selection process. Extensive experimental results confirm the superiority of our approach over existing methods on utilization of open knowledge across various tasks. Codes and models will be released later.","sentences":["The cultivation of expertise for large language models (LLMs) to solve tasks of specific areas often requires special-purpose tuning with calibrated behaviors on the expected stable outputs.","To avoid huge cost brought by manual preparation of instruction datasets and training resources up to hundreds of hours, the exploitation of open knowledge including a wealth of low rank adaptation (LoRA) models and instruction datasets serves as a good starting point.","However, existing methods on model and data selection focus on the performance of general-purpose capabilities while neglecting the knowledge gap exposed in domain-specific deployment.","In the present study, we propose to bridge such gap by introducing few human-annotated samples (i.e., K-shot) for advancing task expertise of LLMs with open knowledge.","Specifically, we develop an efficient and scalable pipeline to cost-efficiently produce task experts where K-shot data intervene in selecting the most promising expert candidates and the task-relevant instructions.","A mixture-of-expert (MoE) system is built to make the best use of individual-yet-complementary knowledge between multiple experts.","We unveil the two keys to the success of a MoE system, 1) the abidance by K-shot, and 2) the insistence on diversity.","For the former, we ensure that models that truly possess problem-solving abilities on K-shot are selected rather than those blind guessers.","Besides, during data selection, instructions that share task-relevant contexts with K-shot are prioritized.","For the latter, we highlight the diversity of constituting experts and that of the fine-tuning instructions throughout the model and data selection process.","Extensive experimental results confirm the superiority of our approach over existing methods on utilization of open knowledge across various tasks.","Codes and models will be released later."],"url":"http://arxiv.org/abs/2408.15915v1"}
{"created":"2024-08-28 16:27:58","title":"CoRe: Context-Regularized Text Embedding Learning for Text-to-Image Personalization","abstract":"Recent advances in text-to-image personalization have enabled high-quality and controllable image synthesis for user-provided concepts. However, existing methods still struggle to balance identity preservation with text alignment. Our approach is based on the fact that generating prompt-aligned images requires a precise semantic understanding of the prompt, which involves accurately processing the interactions between the new concept and its surrounding context tokens within the CLIP text encoder. To address this, we aim to embed the new concept properly into the input embedding space of the text encoder, allowing for seamless integration with existing tokens. We introduce Context Regularization (CoRe), which enhances the learning of the new concept's text embedding by regularizing its context tokens in the prompt. This is based on the insight that appropriate output vectors of the text encoder for the context tokens can only be achieved if the new concept's text embedding is correctly learned. CoRe can be applied to arbitrary prompts without requiring the generation of corresponding images, thus improving the generalization of the learned text embedding. Additionally, CoRe can serve as a test-time optimization technique to further enhance the generations for specific prompts. Comprehensive experiments demonstrate that our method outperforms several baseline methods in both identity preservation and text alignment. Code will be made publicly available.","sentences":["Recent advances in text-to-image personalization have enabled high-quality and controllable image synthesis for user-provided concepts.","However, existing methods still struggle to balance identity preservation with text alignment.","Our approach is based on the fact that generating prompt-aligned images requires a precise semantic understanding of the prompt, which involves accurately processing the interactions between the new concept and its surrounding context tokens within the CLIP text encoder.","To address this, we aim to embed the new concept properly into the input embedding space of the text encoder, allowing for seamless integration with existing tokens.","We introduce Context Regularization (CoRe), which enhances the learning of the new concept's text embedding by regularizing its context tokens in the prompt.","This is based on the insight that appropriate output vectors of the text encoder for the context tokens can only be achieved if the new concept's text embedding is correctly learned.","CoRe can be applied to arbitrary prompts without requiring the generation of corresponding images, thus improving the generalization of the learned text embedding.","Additionally, CoRe can serve as a test-time optimization technique to further enhance the generations for specific prompts.","Comprehensive experiments demonstrate that our method outperforms several baseline methods in both identity preservation and text alignment.","Code will be made publicly available."],"url":"http://arxiv.org/abs/2408.15914v1"}
{"created":"2024-08-28 16:20:45","title":"Decentralized LLM Inference over Edge Networks with Energy Harvesting","abstract":"Large language models have significantly transformed multiple fields with their exceptional performance in natural language tasks, but their deployment in resource-constrained environments like edge networks presents an ongoing challenge. Decentralized techniques for inference have emerged, distributing the model blocks among multiple devices to improve flexibility and cost effectiveness. However, energy limitations remain a significant concern for edge devices. We propose a sustainable model for collaborative inference on interconnected, battery-powered edge devices with energy harvesting. A semi-Markov model is developed to describe the states of the devices, considering processing parameters and average green energy arrivals. This informs the design of scheduling algorithms that aim to minimize device downtimes and maximize network throughput. Through empirical evaluations and simulated runs, we validate the effectiveness of our approach, paving the way for energy-efficient decentralized inference over edge networks.","sentences":["Large language models have significantly transformed multiple fields with their exceptional performance in natural language tasks, but their deployment in resource-constrained environments like edge networks presents an ongoing challenge.","Decentralized techniques for inference have emerged, distributing the model blocks among multiple devices to improve flexibility and cost effectiveness.","However, energy limitations remain a significant concern for edge devices.","We propose a sustainable model for collaborative inference on interconnected, battery-powered edge devices with energy harvesting.","A semi-Markov model is developed to describe the states of the devices, considering processing parameters and average green energy arrivals.","This informs the design of scheduling algorithms that aim to minimize device downtimes and maximize network throughput.","Through empirical evaluations and simulated runs, we validate the effectiveness of our approach, paving the way for energy-efficient decentralized inference over edge networks."],"url":"http://arxiv.org/abs/2408.15907v1"}
{"created":"2024-08-28 16:19:55","title":"Exploring the potential of AI in nurturing learner empathy, prosocial values and environmental stewardship","abstract":"With Artificial Intelligence (AI) becoming a powerful tool for education (Zawacki-Richter et al., 2019), this chapter describes the concept of combining generative and traditional AI, citizen-science physiological, neuroergonomic wearables and environmental sensors into activities for learners to understand their own well-being and emotional states better with a view to developing empathy and environmental stewardship. Alongside bespoke and affordable wearables (DIY EEG headsets and biometric wristbands), interpretable AI and data science are used for learners to explore how the environment affects them physiologically and mentally in authentic environments. For example, relationships between environmental changes (e.g. poorer air quality) and their well-being (e.g. cognitive functioning) can be discovered. This is particularly crucial, as relevant knowledge can influence the way people treat the environment, as suggested by the disciplines of environmental neuroscience and environmental psychology (Doell et al., 2023). Yet, according to Palme and Salvati, there have been relatively few studies on the relationships between microclimates and human health and emotions (Palme and Salvati, 2021). As anthropogenic environmental pollution is becoming a prevalent problem, our research also aims to leverage on generative AI to introduce hypothetical scenarios of the environment as emotionally strong stimuli of relevance to the learners. This would provoke an emotional response for them to learn about their own physiological and neurological responses (using neuro-physiological data). Ultimately, we hope to establish a bidirectional understanding of how the environment affects humans physiologically and mentally; after which, to gain insights as to how AI can be used to effectively foster empathy, pro-environmental attitudes and stewardship.","sentences":["With Artificial Intelligence (AI) becoming a powerful tool for education (Zawacki-Richter et al., 2019), this chapter describes the concept of combining generative and traditional AI, citizen-science physiological, neuroergonomic wearables and environmental sensors into activities for learners to understand their own well-being and emotional states better with a view to developing empathy and environmental stewardship.","Alongside bespoke and affordable wearables (DIY EEG headsets and biometric wristbands), interpretable AI and data science are used for learners to explore how the environment affects them physiologically and mentally in authentic environments.","For example, relationships between environmental changes (e.g. poorer air quality) and their well-being (e.g. cognitive functioning) can be discovered.","This is particularly crucial, as relevant knowledge can influence the way people treat the environment, as suggested by the disciplines of environmental neuroscience and environmental psychology (Doell et al., 2023).","Yet, according to Palme and Salvati, there have been relatively few studies on the relationships between microclimates and human health and emotions (Palme and Salvati, 2021).","As anthropogenic environmental pollution is becoming a prevalent problem, our research also aims to leverage on generative AI to introduce hypothetical scenarios of the environment as emotionally strong stimuli of relevance to the learners.","This would provoke an emotional response for them to learn about their own physiological and neurological responses (using neuro-physiological data).","Ultimately, we hope to establish a bidirectional understanding of how the environment affects humans physiologically and mentally; after which, to gain insights as to how AI can be used to effectively foster empathy, pro-environmental attitudes and stewardship."],"url":"http://arxiv.org/abs/2408.15906v1"}
{"created":"2024-08-28 16:19:35","title":"MetaGFN: Exploring Distant Modes with Adapted Metadynamics for Continuous GFlowNets","abstract":"Generative Flow Networks (GFlowNets) are a class of generative models that sample objects in proportion to a specified reward function through a learned policy. They can be trained either on-policy or off-policy, needing a balance between exploration and exploitation for fast convergence to a target distribution. While exploration strategies for discrete GFlowNets have been studied, exploration in the continuous case remains to be investigated, despite the potential for novel exploration algorithms due to the local connectedness of continuous domains. Here, we introduce Adapted Metadynamics, a variant of metadynamics that can be applied to arbitrary black-box reward functions on continuous domains. We use Adapted Metadynamics as an exploration strategy for continuous GFlowNets. We show three continuous domains where the resulting algorithm, MetaGFN, accelerates convergence to the target distribution and discovers more distant reward modes than previous off-policy exploration strategies used for GFlowNets.","sentences":["Generative Flow Networks (GFlowNets) are a class of generative models that sample objects in proportion to a specified reward function through a learned policy.","They can be trained either on-policy or off-policy, needing a balance between exploration and exploitation for fast convergence to a target distribution.","While exploration strategies for discrete GFlowNets have been studied, exploration in the continuous case remains to be investigated, despite the potential for novel exploration algorithms due to the local connectedness of continuous domains.","Here, we introduce Adapted Metadynamics, a variant of metadynamics that can be applied to arbitrary black-box reward functions on continuous domains.","We use Adapted Metadynamics as an exploration strategy for continuous GFlowNets.","We show three continuous domains where the resulting algorithm, MetaGFN, accelerates convergence to the target distribution and discovers more distant reward modes than previous off-policy exploration strategies used for GFlowNets."],"url":"http://arxiv.org/abs/2408.15905v1"}
{"created":"2024-08-28 16:15:45","title":"LLM-Based Multi-Hop Question Answering with Knowledge Graph Integration in Evolving Environments","abstract":"The rapid obsolescence of information in Large Language Models (LLMs) has driven the development of various techniques to incorporate new facts. However, existing methods for knowledge editing still face difficulties with multi-hop questions that require accurate fact identification and sequential logical reasoning, particularly among numerous fact updates. To tackle these challenges, this paper introduces Graph Memory-based Editing for Large Language Models (GMeLLo), a straitforward and effective method that merges the explicit knowledge representation of Knowledge Graphs (KGs) with the linguistic flexibility of LLMs. Beyond merely leveraging LLMs for question answering, GMeLLo employs these models to convert free-form language into structured queries and fact triples, facilitating seamless interaction with KGs for rapid updates and precise multi-hop reasoning. Our results show that GMeLLo significantly surpasses current state-of-the-art knowledge editing methods in the multi-hop question answering benchmark, MQuAKE, especially in scenarios with extensive knowledge edits.","sentences":["The rapid obsolescence of information in Large Language Models (LLMs) has driven the development of various techniques to incorporate new facts.","However, existing methods for knowledge editing still face difficulties with multi-hop questions that require accurate fact identification and sequential logical reasoning, particularly among numerous fact updates.","To tackle these challenges, this paper introduces Graph Memory-based Editing for Large Language Models (GMeLLo), a straitforward and effective method that merges the explicit knowledge representation of Knowledge Graphs (KGs) with the linguistic flexibility of LLMs.","Beyond merely leveraging LLMs for question answering, GMeLLo employs these models to convert free-form language into structured queries and fact triples, facilitating seamless interaction with KGs for rapid updates and precise multi-hop reasoning.","Our results show that GMeLLo significantly surpasses current state-of-the-art knowledge editing methods in the multi-hop question answering benchmark, MQuAKE, especially in scenarios with extensive knowledge edits."],"url":"http://arxiv.org/abs/2408.15903v1"}
{"created":"2024-08-28 16:12:55","title":"Nexus: Specialization meets Adaptability for Efficiently Training Mixture of Experts","abstract":"Efficiency, specialization, and adaptability to new data distributions are qualities that are hard to combine in current Large Language Models. The Mixture of Experts (MoE) architecture has been the focus of significant research because its inherent conditional computation enables such desirable properties. In this work, we focus on \"upcycling\" dense expert models into an MoE, aiming to improve specialization while also adding the ability to adapt to new tasks easily. We introduce Nexus, an enhanced MoE architecture with adaptive routing where the model learns to project expert embeddings from domain representations. This approach allows Nexus to flexibly add new experts after the initial upcycling through separately trained dense models, without requiring large-scale MoE training for unseen data domains. Our experiments show that Nexus achieves a relative gain of up to 2.1% over the baseline for initial upcycling, and a 18.8% relative gain for extending the MoE with a new expert by using limited finetuning data. This flexibility of Nexus is crucial to enable an open-source ecosystem where every user continuously assembles their own MoE-mix according to their needs.","sentences":["Efficiency, specialization, and adaptability to new data distributions are qualities that are hard to combine in current Large Language Models.","The Mixture of Experts (MoE) architecture has been the focus of significant research because its inherent conditional computation enables such desirable properties.","In this work, we focus on \"upcycling\" dense expert models into an MoE, aiming to improve specialization while also adding the ability to adapt to new tasks easily.","We introduce Nexus, an enhanced MoE architecture with adaptive routing where the model learns to project expert embeddings from domain representations.","This approach allows Nexus to flexibly add new experts after the initial upcycling through separately trained dense models, without requiring large-scale MoE training for unseen data domains.","Our experiments show that Nexus achieves a relative gain of up to 2.1% over the baseline for initial upcycling, and a 18.8% relative gain for extending the MoE with a new expert by using limited finetuning data.","This flexibility of Nexus is crucial to enable an open-source ecosystem where every user continuously assembles their own MoE-mix according to their needs."],"url":"http://arxiv.org/abs/2408.15901v1"}
{"created":"2024-08-28 16:12:28","title":"Gen-Swarms: Adapting Deep Generative Models to Swarms of Drones","abstract":"Gen-Swarms is an innovative method that leverages and combines the capabilities of deep generative models with reactive navigation algorithms to automate the creation of drone shows. Advancements in deep generative models, particularly diffusion models, have demonstrated remarkable effectiveness in generating high-quality 2D images. Building on this success, various works have extended diffusion models to 3D point cloud generation. In contrast, alternative generative models such as flow matching have been proposed, offering a simple and intuitive transition from noise to meaningful outputs. However, the application of flow matching models to 3D point cloud generation remains largely unexplored. Gen-Swarms adapts these models to automatically generate drone shows. Existing 3D point cloud generative models create point trajectories which are impractical for drone swarms. In contrast, our method not only generates accurate 3D shapes but also guides the swarm motion, producing smooth trajectories and accounting for potential collisions through a reactive navigation algorithm incorporated into the sampling process. For example, when given a text category like Airplane, Gen-Swarms can rapidly and continuously generate numerous variations of 3D airplane shapes. Our experiments demonstrate that this approach is particularly well-suited for drone shows, providing feasible trajectories, creating representative final shapes, and significantly enhancing the overall performance of drone show generation.","sentences":["Gen-Swarms is an innovative method that leverages and combines the capabilities of deep generative models with reactive navigation algorithms to automate the creation of drone shows.","Advancements in deep generative models, particularly diffusion models, have demonstrated remarkable effectiveness in generating high-quality 2D images.","Building on this success, various works have extended diffusion models to 3D point cloud generation.","In contrast, alternative generative models such as flow matching have been proposed, offering a simple and intuitive transition from noise to meaningful outputs.","However, the application of flow matching models to 3D point cloud generation remains largely unexplored.","Gen-Swarms adapts these models to automatically generate drone shows.","Existing 3D point cloud generative models create point trajectories which are impractical for drone swarms.","In contrast, our method not only generates accurate 3D shapes but also guides the swarm motion, producing smooth trajectories and accounting for potential collisions through a reactive navigation algorithm incorporated into the sampling process.","For example, when given a text category like Airplane, Gen-Swarms can rapidly and continuously generate numerous variations of 3D airplane shapes.","Our experiments demonstrate that this approach is particularly well-suited for drone shows, providing feasible trajectories, creating representative final shapes, and significantly enhancing the overall performance of drone show generation."],"url":"http://arxiv.org/abs/2408.15899v1"}
{"created":"2024-08-28 16:12:16","title":"Airfoil Diffusion: Denoising Diffusion Model For Conditional Airfoil Generation","abstract":"The design of aerodynamic shapes, such as airfoils, has traditionally required significant computational resources and relied on predefined design parameters, which limit the potential for novel shape synthesis. In this work, we introduce a data-driven methodology for airfoil generation using a diffusion model. Trained on a dataset of preexisting airfoils, our model can generate an arbitrary number of new airfoils from random vectors, which can be conditioned on specific aerodynamic performance metrics such as lift and drag, or geometric criteria. Our results demonstrate that the diffusion model effectively produces airfoil shapes with realistic aerodynamic properties, offering substantial improvements in efficiency, flexibility, and the potential for discovering innovative airfoil designs. This approach significantly expands the design space, facilitating the synthesis of high-performance aerodynamic shapes that transcend the limitations of traditional methods.","sentences":["The design of aerodynamic shapes, such as airfoils, has traditionally required significant computational resources and relied on predefined design parameters, which limit the potential for novel shape synthesis.","In this work, we introduce a data-driven methodology for airfoil generation using a diffusion model.","Trained on a dataset of preexisting airfoils, our model can generate an arbitrary number of new airfoils from random vectors, which can be conditioned on specific aerodynamic performance metrics such as lift and drag, or geometric criteria.","Our results demonstrate that the diffusion model effectively produces airfoil shapes with realistic aerodynamic properties, offering substantial improvements in efficiency, flexibility, and the potential for discovering innovative airfoil designs.","This approach significantly expands the design space, facilitating the synthesis of high-performance aerodynamic shapes that transcend the limitations of traditional methods."],"url":"http://arxiv.org/abs/2408.15898v1"}
{"created":"2024-08-28 16:06:12","title":"A New Method for Cross-Lingual-based Semantic Role Labeling","abstract":"Semantic role labeling is a crucial task in natural language processing, enabling better comprehension of natural language. However, the lack of annotated data in multiple languages has posed a challenge for researchers. To address this, a deep learning algorithm based on model transfer has been proposed. The algorithm utilizes a dataset consisting of the English portion of CoNLL2009 and a corpus of semantic roles in Persian. To optimize the efficiency of training, only ten percent of the educational data from each language is used. The results of the proposed model demonstrate significant improvements compared to Niksirt et al.'s model. In monolingual mode, the proposed model achieved a 2.05 percent improvement on F1-score, while in cross-lingual mode, the improvement was even more substantial, reaching 6.23 percent. Worth noting is that the compared model only trained two of the four stages of semantic role labeling and employed golden data for the remaining two stages. This suggests that the actual superiority of the proposed model surpasses the reported numbers by a significant margin. The development of cross-lingual methods for semantic role labeling holds promise, particularly in addressing the scarcity of annotated data for various languages. These advancements pave the way for further research in understanding and processing natural language across different linguistic contexts.","sentences":["Semantic role labeling is a crucial task in natural language processing, enabling better comprehension of natural language.","However, the lack of annotated data in multiple languages has posed a challenge for researchers.","To address this, a deep learning algorithm based on model transfer has been proposed.","The algorithm utilizes a dataset consisting of the English portion of CoNLL2009 and a corpus of semantic roles in Persian.","To optimize the efficiency of training, only ten percent of the educational data from each language is used.","The results of the proposed model demonstrate significant improvements compared to Niksirt et al.'s model.","In monolingual mode, the proposed model achieved a 2.05 percent improvement on F1-score, while in cross-lingual mode, the improvement was even more substantial, reaching 6.23 percent.","Worth noting is that the compared model only trained two of the four stages of semantic role labeling and employed golden data for the remaining two stages.","This suggests that the actual superiority of the proposed model surpasses the reported numbers by a significant margin.","The development of cross-lingual methods for semantic role labeling holds promise, particularly in addressing the scarcity of annotated data for various languages.","These advancements pave the way for further research in understanding and processing natural language across different linguistic contexts."],"url":"http://arxiv.org/abs/2408.15896v1"}
{"created":"2024-08-28 16:05:20","title":"Bias in LLMs as Annotators: The Effect of Party Cues on Labelling Decision by Large Language Models","abstract":"Human coders are biased. We test similar biases in Large Language Models (LLMs) as annotators. By replicating an experiment run by Ennser-Jedenastik and Meyer (2018), we find evidence that LLMs use political information, and specifically party cues, to judge political statements. Not only do LLMs use relevant information to contextualize whether a statement is positive, negative, or neutral based on the party cue, they also reflect the biases of the human-generated data upon which they have been trained. We also find that unlike humans, who are only biased when faced with statements from extreme parties, LLMs exhibit significant bias even when prompted with statements from center-left and center-right parties. The implications of our findings are discussed in the conclusion.","sentences":["Human coders are biased.","We test similar biases in Large Language Models (LLMs) as annotators.","By replicating an experiment run by Ennser-Jedenastik and Meyer (2018), we find evidence that LLMs use political information, and specifically party cues, to judge political statements.","Not only do LLMs use relevant information to contextualize whether a statement is positive, negative, or neutral based on the party cue, they also reflect the biases of the human-generated data upon which they have been trained.","We also find that unlike humans, who are only biased when faced with statements from extreme parties, LLMs exhibit significant bias even when prompted with statements from center-left and center-right parties.","The implications of our findings are discussed in the conclusion."],"url":"http://arxiv.org/abs/2408.15895v1"}
{"created":"2024-08-28 16:04:40","title":"The Role of Fibration Symmetries in Geometric Deep Learning","abstract":"Geometric Deep Learning (GDL) unifies a broad class of machine learning techniques from the perspectives of symmetries, offering a framework for introducing problem-specific inductive biases like Graph Neural Networks (GNNs). However, the current formulation of GDL is limited to global symmetries that are not often found in real-world problems. We propose to relax GDL to allow for local symmetries, specifically fibration symmetries in graphs, to leverage regularities of realistic instances. We show that GNNs apply the inductive bias of fibration symmetries and derive a tighter upper bound for their expressive power. Additionally, by identifying symmetries in networks, we collapse network nodes, thereby increasing their computational efficiency during both inference and training of deep neural networks. The mathematical extension introduced here applies beyond graphs to manifolds, bundles, and grids for the development of models with inductive biases induced by local symmetries that can lead to better generalization.","sentences":["Geometric Deep Learning (GDL) unifies a broad class of machine learning techniques from the perspectives of symmetries, offering a framework for introducing problem-specific inductive biases like Graph Neural Networks (GNNs).","However, the current formulation of GDL is limited to global symmetries that are not often found in real-world problems.","We propose to relax GDL to allow for local symmetries, specifically fibration symmetries in graphs, to leverage regularities of realistic instances.","We show that GNNs apply the inductive bias of fibration symmetries and derive a tighter upper bound for their expressive power.","Additionally, by identifying symmetries in networks, we collapse network nodes, thereby increasing their computational efficiency during both inference and training of deep neural networks.","The mathematical extension introduced here applies beyond graphs to manifolds, bundles, and grids for the development of models with inductive biases induced by local symmetries that can lead to better generalization."],"url":"http://arxiv.org/abs/2408.15894v1"}
{"created":"2024-08-28 16:03:18","title":"Disentangled Diffusion Autoencoder for Harmonization of Multi-site Neuroimaging Data","abstract":"Combining neuroimaging datasets from multiple sites and scanners can help increase statistical power and thus provide greater insight into subtle neuroanatomical effects. However, site-specific effects pose a challenge by potentially obscuring the biological signal and introducing unwanted variance. Existing harmonization techniques, which use statistical models to remove such effects, have been shown to incompletely remove site effects while also failing to preserve biological variability. More recently, generative models using GANs or autoencoder-based approaches, have been proposed for site adjustment. However, such methods are known for instability during training or blurry image generation. In recent years, diffusion models have become increasingly popular for their ability to generate high-quality synthetic images. In this work, we introduce the disentangled diffusion autoencoder (DDAE), a novel diffusion model designed for controlling specific aspects of an image. We apply the DDAE to the task of harmonizing MR images by generating high-quality site-adjusted images that preserve biological variability. We use data from 7 different sites and demonstrate the DDAE's superiority in generating high-resolution, harmonized 2D MR images over previous approaches. As far as we are aware, this work marks the first diffusion-based model for site adjustment of neuroimaging data.","sentences":["Combining neuroimaging datasets from multiple sites and scanners can help increase statistical power and thus provide greater insight into subtle neuroanatomical effects.","However, site-specific effects pose a challenge by potentially obscuring the biological signal and introducing unwanted variance.","Existing harmonization techniques, which use statistical models to remove such effects, have been shown to incompletely remove site effects while also failing to preserve biological variability.","More recently, generative models using GANs or autoencoder-based approaches, have been proposed for site adjustment.","However, such methods are known for instability during training or blurry image generation.","In recent years, diffusion models have become increasingly popular for their ability to generate high-quality synthetic images.","In this work, we introduce the disentangled diffusion autoencoder (DDAE), a novel diffusion model designed for controlling specific aspects of an image.","We apply the DDAE to the task of harmonizing MR images by generating high-quality site-adjusted images that preserve biological variability.","We use data from 7 different sites and demonstrate the DDAE's superiority in generating high-resolution, harmonized 2D MR images over previous approaches.","As far as we are aware, this work marks the first diffusion-based model for site adjustment of neuroimaging data."],"url":"http://arxiv.org/abs/2408.15890v1"}
{"created":"2024-08-28 15:58:49","title":"Enhancing Intrusion Detection in IoT Environments: An Advanced Ensemble Approach Using Kolmogorov-Arnold Networks","abstract":"In recent years, the evolution of machine learning techniques has significantly impacted the field of intrusion detection, particularly within the context of the Internet of Things (IoT). As IoT networks expand, the need for robust security measures to counteract potential threats has become increasingly critical. This paper introduces a hybrid Intrusion Detection System (IDS) that synergistically combines Kolmogorov-Arnold Networks (KANs) with the XGBoost algorithm. Our proposed IDS leverages the unique capabilities of KANs, which utilize learnable activation functions to model complex relationships within data, alongside the powerful ensemble learning techniques of XGBoost, known for its high performance in classification tasks. This hybrid approach not only enhances the detection accuracy but also improves the interpretability of the model, making it suitable for dynamic and intricate IoT environments. Experimental evaluations demonstrate that our hybrid IDS achieves an impressive detection accuracy exceeding 99% in distinguishing between benign and malicious activities. Additionally, we were able to achieve F1 scores, precision, and recall that exceeded 98%. Furthermore, we conduct a comparative analysis against traditional Multi-Layer Perceptron (MLP) networks, assessing performance metrics such as Precision, Recall, and F1-score. The results underscore the efficacy of integrating KANs with XGBoost, highlighting the potential of this innovative approach to significantly strengthen the security framework of IoT networks.","sentences":["In recent years, the evolution of machine learning techniques has significantly impacted the field of intrusion detection, particularly within the context of the Internet of Things (IoT).","As IoT networks expand, the need for robust security measures to counteract potential threats has become increasingly critical.","This paper introduces a hybrid Intrusion Detection System (IDS) that synergistically combines Kolmogorov-Arnold Networks (KANs) with the XGBoost algorithm.","Our proposed IDS leverages the unique capabilities of KANs, which utilize learnable activation functions to model complex relationships within data, alongside the powerful ensemble learning techniques of XGBoost, known for its high performance in classification tasks.","This hybrid approach not only enhances the detection accuracy but also improves the interpretability of the model, making it suitable for dynamic and intricate IoT environments.","Experimental evaluations demonstrate that our hybrid IDS achieves an impressive detection accuracy exceeding 99% in distinguishing between benign and malicious activities.","Additionally, we were able to achieve F1 scores, precision, and recall that exceeded 98%.","Furthermore, we conduct a comparative analysis against traditional Multi-Layer Perceptron (MLP) networks, assessing performance metrics such as Precision, Recall, and F1-score.","The results underscore the efficacy of integrating KANs with XGBoost, highlighting the potential of this innovative approach to significantly strengthen the security framework of IoT networks."],"url":"http://arxiv.org/abs/2408.15886v1"}
{"created":"2024-08-28 15:52:23","title":"LLaVA-MoD: Making LLaVA Tiny via MoE Knowledge Distillation","abstract":"We introduce LLaVA-MoD, a novel framework designed to enable the efficient training of small-scale Multimodal Language Models (s-MLLM) by distilling knowledge from large-scale MLLM (l-MLLM). Our approach tackles two fundamental challenges in MLLM distillation. First, we optimize the network structure of s-MLLM by integrating a sparse Mixture of Experts (MoE) architecture into the language model, striking a balance between computational efficiency and model expressiveness. Second, we propose a progressive knowledge transfer strategy to ensure comprehensive knowledge migration. This strategy begins with mimic distillation, where we minimize the Kullback-Leibler (KL) divergence between output distributions to enable the student model to emulate the teacher network's understanding. Following this, we introduce preference distillation via Direct Preference Optimization (DPO), where the key lies in treating l-MLLM as the reference model. During this phase, the s-MLLM's ability to discriminate between superior and inferior examples is significantly enhanced beyond l-MLLM, leading to a better student that surpasses its teacher, particularly in hallucination benchmarks. Extensive experiments demonstrate that LLaVA-MoD outperforms existing models across various multimodal benchmarks while maintaining a minimal number of activated parameters and low computational costs. Remarkably, LLaVA-MoD, with only 2B activated parameters, surpasses Qwen-VL-Chat-7B by an average of 8.8% across benchmarks, using merely 0.3% of the training data and 23% trainable parameters. These results underscore LLaVA-MoD's ability to effectively distill comprehensive knowledge from its teacher model, paving the way for the development of more efficient MLLMs. The code will be available on: https://github.com/shufangxun/LLaVA-MoD.","sentences":["We introduce LLaVA-MoD, a novel framework designed to enable the efficient training of small-scale Multimodal Language Models (s-MLLM) by distilling knowledge from large-scale MLLM (l-MLLM).","Our approach tackles two fundamental challenges in MLLM distillation.","First, we optimize the network structure of s-MLLM by integrating a sparse Mixture of Experts (MoE) architecture into the language model, striking a balance between computational efficiency and model expressiveness.","Second, we propose a progressive knowledge transfer strategy to ensure comprehensive knowledge migration.","This strategy begins with mimic distillation, where we minimize the Kullback-Leibler (KL) divergence between output distributions to enable the student model to emulate the teacher network's understanding.","Following this, we introduce preference distillation via Direct Preference Optimization (DPO), where the key lies in treating l-MLLM as the reference model.","During this phase, the s-MLLM's ability to discriminate between superior and inferior examples is significantly enhanced beyond l-MLLM, leading to a better student that surpasses its teacher, particularly in hallucination benchmarks.","Extensive experiments demonstrate that LLaVA-MoD outperforms existing models across various multimodal benchmarks while maintaining a minimal number of activated parameters and low computational costs.","Remarkably, LLaVA-MoD, with only 2B activated parameters, surpasses Qwen-VL-Chat-7B by an average of 8.8% across benchmarks, using merely 0.3% of the training data and 23% trainable parameters.","These results underscore LLaVA-MoD's ability to effectively distill comprehensive knowledge from its teacher model, paving the way for the development of more efficient MLLMs.","The code will be available on: https://github.com/shufangxun/LLaVA-MoD."],"url":"http://arxiv.org/abs/2408.15881v1"}
{"created":"2024-08-28 15:50:41","title":"Persuasion Games using Large Language Models","abstract":"Large Language Models (LLMs) have emerged as formidable instruments capable of comprehending and producing human-like text. This paper explores the potential of LLMs, to shape human perspectives and subsequently influence their decisions on particular tasks. This capability finds applications in diverse domains such as Investment, Credit cards and Insurance, wherein they assist users in selecting appropriate insurance policies, investment plans, Credit cards, Retail, as well as in Behavioral Change Support Systems (BCSS).   We present a sophisticated multi-agent framework wherein a consortium of agents operate in collaborative manner. The primary agent engages directly with users through persuasive dialogue, while the auxiliary agents perform tasks such as information retrieval, response analysis, development of persuasion strategies, and validation of facts. Empirical evidence from our experiments demonstrates that this collaborative methodology significantly enhances the persuasive efficacy of the LLM. We analyze user resistance to persuasive efforts continuously and counteract it by employing a combination of rule-based and LLM-based resistance-persuasion mapping techniques.   We employ simulated personas and generate conversations in insurance, banking, and retail domains to evaluate the proficiency of large language models (LLMs) in recognizing, adjusting to, and influencing various personality types. Concurrently, we examine the resistance mechanisms employed by LLM simulated personas. Persuasion is quantified via measurable surveys before and after interaction, LLM-generated scores on conversation, and user decisions (purchase or non-purchase).","sentences":["Large Language Models (LLMs) have emerged as formidable instruments capable of comprehending and producing human-like text.","This paper explores the potential of LLMs, to shape human perspectives and subsequently influence their decisions on particular tasks.","This capability finds applications in diverse domains such as Investment, Credit cards and Insurance, wherein they assist users in selecting appropriate insurance policies, investment plans, Credit cards, Retail, as well as in Behavioral Change Support Systems (BCSS).   ","We present a sophisticated multi-agent framework wherein a consortium of agents operate in collaborative manner.","The primary agent engages directly with users through persuasive dialogue, while the auxiliary agents perform tasks such as information retrieval, response analysis, development of persuasion strategies, and validation of facts.","Empirical evidence from our experiments demonstrates that this collaborative methodology significantly enhances the persuasive efficacy of the LLM.","We analyze user resistance to persuasive efforts continuously and counteract it by employing a combination of rule-based and LLM-based resistance-persuasion mapping techniques.   ","We employ simulated personas and generate conversations in insurance, banking, and retail domains to evaluate the proficiency of large language models (LLMs) in recognizing, adjusting to, and influencing various personality types.","Concurrently, we examine the resistance mechanisms employed by LLM simulated personas.","Persuasion is quantified via measurable surveys before and after interaction, LLM-generated scores on conversation, and user decisions (purchase or non-purchase)."],"url":"http://arxiv.org/abs/2408.15879v1"}
{"created":"2024-08-28 15:48:39","title":"A Non-Traditional Approach to Assisting Data Address Translation","abstract":"This paper proposes a novel way to assist conventional data address translation. The approach, PC-Indexed Data Address Translation (PCAX), uses the PC of a load instruction, and not a data virtual address, to obtain the page table entry (PTE) for the data accessed by a load instruction. PCAX is intended to be used for a small subset of the static loads in a program. We observe that: (i) a small subset of static loads is responsible for most of the misses in a data translation lookaside buffer (DTLB), and (ii) often a dynamic instance of a static load instruction accesses the same PTE as the last dynamic instance, and consider PCAX for this subset. With PCAX the effective miss rate of a conventional DTLB can be cut down by a factor of 2-3X in many cases, and even more in some cases. PCAX is also beneficial in reducing the number of secondary TLB (STLB) misses. Since the tables used for PCAX can be accessed alongside instruction fetch, they can be slow, yet frequently provide a valid PTE even before the data address calculation. This results in a performance improvement, and reduced data address translation energy, in most cases.","sentences":["This paper proposes a novel way to assist conventional data address translation.","The approach, PC-Indexed Data Address Translation (PCAX), uses the PC of a load instruction, and not a data virtual address, to obtain the page table entry (PTE) for the data accessed by a load instruction.","PCAX is intended to be used for a small subset of the static loads in a program.","We observe that: (i) a small subset of static loads is responsible for most of the misses in a data translation lookaside buffer (DTLB), and (ii) often a dynamic instance of a static load instruction accesses the same PTE as the last dynamic instance, and consider PCAX for this subset.","With PCAX the effective miss rate of a conventional DTLB can be cut down by a factor of 2-3X in many cases, and even more in some cases.","PCAX is also beneficial in reducing the number of secondary TLB (STLB) misses.","Since the tables used for PCAX can be accessed alongside instruction fetch, they can be slow, yet frequently provide a valid PTE even before the data address calculation.","This results in a performance improvement, and reduced data address translation energy, in most cases."],"url":"http://arxiv.org/abs/2408.15878v1"}
{"created":"2024-08-28 15:47:32","title":"Unleashing the Temporal-Spatial Reasoning Capacity of GPT for Training-Free Audio and Language Referenced Video Object Segmentation","abstract":"In this paper, we propose an Audio-Language-Referenced SAM 2 (AL-Ref-SAM 2) pipeline to explore the training-free paradigm for audio and language-referenced video object segmentation, namely AVS and RVOS tasks. The intuitive solution leverages GroundingDINO to identify the target object from a single frame and SAM 2 to segment the identified object throughout the video, which is less robust to spatiotemporal variations due to a lack of video context exploration. Thus, in our AL-Ref-SAM 2 pipeline, we propose a novel GPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to perform two-step temporal-spatial reasoning for sequentially selecting pivot frames and pivot boxes, thereby providing SAM 2 with a high-quality initial object prompt. Within GPT-PS, two task-specific Chain-of-Thought prompts are designed to unleash GPT's temporal-spatial reasoning capacity by guiding GPT to make selections based on a comprehensive understanding of video and reference information. Furthermore, we propose a Language-Binded Reference Unification (LBRU) module to convert audio signals into language-formatted references, thereby unifying the formats of AVS and RVOS tasks in the same pipeline. Extensive experiments on both tasks show that our training-free AL-Ref-SAM 2 pipeline achieves performances comparable to or even better than fully-supervised fine-tuning methods. The code is available at: https://github.com/appletea233/AL-Ref-SAM2.","sentences":["In this paper, we propose an Audio-Language-Referenced SAM 2 (AL-Ref-SAM 2) pipeline to explore the training-free paradigm for audio and language-referenced video object segmentation, namely AVS and RVOS tasks.","The intuitive solution leverages GroundingDINO to identify the target object from a single frame and SAM 2 to segment the identified object throughout the video, which is less robust to spatiotemporal variations due to a lack of video context exploration.","Thus, in our AL-Ref-SAM 2 pipeline, we propose a novel GPT-assisted Pivot Selection (GPT-PS) module to instruct GPT-4 to perform two-step temporal-spatial reasoning for sequentially selecting pivot frames and pivot boxes, thereby providing SAM 2 with a high-quality initial object prompt.","Within GPT-PS, two task-specific Chain-of-Thought prompts are designed to unleash GPT's temporal-spatial reasoning capacity by guiding GPT to make selections based on a comprehensive understanding of video and reference information.","Furthermore, we propose a Language-Binded Reference Unification (LBRU) module to convert audio signals into language-formatted references, thereby unifying the formats of AVS and RVOS tasks in the same pipeline.","Extensive experiments on both tasks show that our training-free AL-Ref-SAM 2 pipeline achieves performances comparable to or even better than fully-supervised fine-tuning methods.","The code is available at: https://github.com/appletea233/AL-Ref-SAM2."],"url":"http://arxiv.org/abs/2408.15876v1"}
{"created":"2024-08-28 15:44:34","title":"Robust Statistical Scaling of Outlier Scores: Improving the Quality of Outlier Probabilities for Outliers (Extended Version)","abstract":"Outlier detection algorithms typically assign an outlier score to each observation in a dataset, indicating the degree to which an observation is an outlier. However, these scores are often not comparable across algorithms and can be difficult for humans to interpret. Statistical scaling addresses this problem by transforming outlier scores into outlier probabilities without using ground-truth labels, thereby improving interpretability and comparability across algorithms. However, the quality of this transformation can be different for outliers and inliers. Missing outliers in scenarios where they are of particular interest - such as healthcare, finance, or engineering - can be costly or dangerous. Thus, ensuring good probabilities for outliers is essential. This paper argues that statistical scaling, as commonly used in the literature, does not produce equally good probabilities for outliers as for inliers. Therefore, we propose robust statistical scaling, which uses robust estimators to improve the probabilities for outliers. We evaluate several variants of our method against other outlier score transformations for real-world datasets and outlier detection algorithms, where it can improve the probabilities for outliers.","sentences":["Outlier detection algorithms typically assign an outlier score to each observation in a dataset, indicating the degree to which an observation is an outlier.","However, these scores are often not comparable across algorithms and can be difficult for humans to interpret.","Statistical scaling addresses this problem by transforming outlier scores into outlier probabilities without using ground-truth labels, thereby improving interpretability and comparability across algorithms.","However, the quality of this transformation can be different for outliers and inliers.","Missing outliers in scenarios where they are of particular interest - such as healthcare, finance, or engineering - can be costly or dangerous.","Thus, ensuring good probabilities for outliers is essential.","This paper argues that statistical scaling, as commonly used in the literature, does not produce equally good probabilities for outliers as for inliers.","Therefore, we propose robust statistical scaling, which uses robust estimators to improve the probabilities for outliers.","We evaluate several variants of our method against other outlier score transformations for real-world datasets and outlier detection algorithms, where it can improve the probabilities for outliers."],"url":"http://arxiv.org/abs/2408.15874v1"}
{"created":"2024-08-28 15:44:06","title":"Constructing a Common Ground: Analyzing the quality and usage of International Auxiliary Languages in Wikipedia","abstract":"International Auxiliary Languages (IALs) are constructed languages designed to facilitate communication among speakers of different native languages while fostering equality, efficiency, and cross-cultural understanding. This study focuses on analyzing the editions of IALs on Wikipedia, including Simple English, Esperanto, Ido, Interlingua, Volapuk, Interlingue, and Novial. We compare them with three natural languages: English, Spanish, and Catalan. Our aim is to establish a basis for the use of IALs in Wikipedia as well as showcase a new methodology for categorizing wikis. We found in total there are 1.3 million articles written in these languages and they gather 15.6 million monthly views. Although this is not a negligible amount of content, in comparison with large natural language projects there is still a big room for improvement. We concluded that IAL editions on Wikipedia are similar to other projects, behaving proportionally to their communities' size. Therefore, the key to their growth is augmenting the amount and quality of the content offered in these languages. To that end, we offer a set of statistics to understand and improve these projects, and we developed a webpage that displays our findings to foster knowledge sharing and facilitate the expansion of the IAL communities.","sentences":["International Auxiliary Languages (IALs) are constructed languages designed to facilitate communication among speakers of different native languages while fostering equality, efficiency, and cross-cultural understanding.","This study focuses on analyzing the editions of IALs on Wikipedia, including Simple English, Esperanto, Ido, Interlingua, Volapuk, Interlingue, and Novial.","We compare them with three natural languages: English, Spanish, and Catalan.","Our aim is to establish a basis for the use of IALs in Wikipedia as well as showcase a new methodology for categorizing wikis.","We found in total there are 1.3 million articles written in these languages and they gather 15.6 million monthly views.","Although this is not a negligible amount of content, in comparison with large natural language projects there is still a big room for improvement.","We concluded that IAL editions on Wikipedia are similar to other projects, behaving proportionally to their communities' size.","Therefore, the key to their growth is augmenting the amount and quality of the content offered in these languages.","To that end, we offer a set of statistics to understand and improve these projects, and we developed a webpage that displays our findings to foster knowledge sharing and facilitate the expansion of the IAL communities."],"url":"http://arxiv.org/abs/2408.15873v1"}
{"created":"2024-08-28 15:40:06","title":"BIM-SLAM: Integrating BIM Models in Multi-session SLAM for Lifelong Mapping using 3D LiDAR","abstract":"While 3D LiDAR sensor technology is becoming more advanced and cheaper every day, the growth of digitalization in the AEC industry contributes to the fact that 3D building information models (BIM models) are now available for a large part of the built environment. These two facts open the question of how 3D models can support 3D LiDAR long-term SLAM in indoor, GPS-denied environments. This paper proposes a methodology that leverages BIM models to create an updated map of indoor environments with sequential LiDAR measurements. Session data (pose graph-based map and descriptors) are initially generated from BIM models. Then, real-world data is aligned with the session data from the model using multi-session anchoring while minimizing the drift on the real-world data. Finally, the new elements not present in the BIM model are identified, grouped, and reconstructed in a surface representation, allowing a better visualization next to the BIM model. The framework enables the creation of a coherent map aligned with the BIM model that does not require prior knowledge of the initial pose of the robot, and it does not need to be inside the map.","sentences":["While 3D LiDAR sensor technology is becoming more advanced and cheaper every day, the growth of digitalization in the AEC industry contributes to the fact that 3D building information models (BIM models) are now available for a large part of the built environment.","These two facts open the question of how 3D models can support 3D LiDAR long-term SLAM in indoor, GPS-denied environments.","This paper proposes a methodology that leverages BIM models to create an updated map of indoor environments with sequential LiDAR measurements.","Session data (pose graph-based map and descriptors) are initially generated from BIM models.","Then, real-world data is aligned with the session data from the model using multi-session anchoring while minimizing the drift on the real-world data.","Finally, the new elements not present in the BIM model are identified, grouped, and reconstructed in a surface representation, allowing a better visualization next to the BIM model.","The framework enables the creation of a coherent map aligned with the BIM model that does not require prior knowledge of the initial pose of the robot, and it does not need to be inside the map."],"url":"http://arxiv.org/abs/2408.15870v1"}
{"created":"2024-08-28 15:37:44","title":"GenDDS: Generating Diverse Driving Video Scenarios with Prompt-to-Video Generative Model","abstract":"Autonomous driving training requires a diverse range of datasets encompassing various traffic conditions, weather scenarios, and road types. Traditional data augmentation methods often struggle to generate datasets that represent rare occurrences. To address this challenge, we propose GenDDS, a novel approach for generating driving scenarios generation by leveraging the capabilities of Stable Diffusion XL (SDXL), an advanced latent diffusion model. Our methodology involves the use of descriptive prompts to guide the synthesis process, aimed at producing realistic and diverse driving scenarios. With the power of the latest computer vision techniques, such as ControlNet and Hotshot-XL, we have built a complete pipeline for video generation together with SDXL. We employ the KITTI dataset, which includes real-world driving videos, to train the model. Through a series of experiments, we demonstrate that our model can generate high-quality driving videos that closely replicate the complexity and variability of real-world driving scenarios. This research contributes to the development of sophisticated training data for autonomous driving systems and opens new avenues for creating virtual environments for simulation and validation purposes.","sentences":["Autonomous driving training requires a diverse range of datasets encompassing various traffic conditions, weather scenarios, and road types.","Traditional data augmentation methods often struggle to generate datasets that represent rare occurrences.","To address this challenge, we propose GenDDS, a novel approach for generating driving scenarios generation by leveraging the capabilities of Stable Diffusion XL (SDXL), an advanced latent diffusion model.","Our methodology involves the use of descriptive prompts to guide the synthesis process, aimed at producing realistic and diverse driving scenarios.","With the power of the latest computer vision techniques, such as ControlNet and Hotshot-XL, we have built a complete pipeline for video generation together with SDXL.","We employ the KITTI dataset, which includes real-world driving videos, to train the model.","Through a series of experiments, we demonstrate that our model can generate high-quality driving videos that closely replicate the complexity and variability of real-world driving scenarios.","This research contributes to the development of sophisticated training data for autonomous driving systems and opens new avenues for creating virtual environments for simulation and validation purposes."],"url":"http://arxiv.org/abs/2408.15868v1"}
{"created":"2024-08-28 15:33:47","title":"Retrieval-Augmented Instruction Tuning for Automated Process Engineering Calculations : A Tool-Chaining Problem-Solving Framework with Attributable Reflection","abstract":"The current technology landscape lacks a foundational AI model for solving process engineering calculations. In this work, we introduce a novel autonomous agent framework leveraging Retrieval-Augmented Instruction-Tuning (RAIT) to enhance open, customizable small code language models (SLMs) for these calculations. By combining instruction tuned code SLMs with Retrieval-Augmented Code Generation (RACG) using external tools, the agent generates, debugs, and optimizes code from natural language specifications. Our approach addresses the limitations of the current lack of a foundational AI model for specialized process engineering tasks and offers benefits of explainability, knowledge editing, and cost-effectiveness. Additionally, we curate custom datasets of chemical and process engineering problems and solutions to overcome data scarcity. Experimental results show that our framework matches the performance of large-scale proprietary models on benchmark datasets, proving its effectiveness and usability.","sentences":["The current technology landscape lacks a foundational AI model for solving process engineering calculations.","In this work, we introduce a novel autonomous agent framework leveraging Retrieval-Augmented Instruction-Tuning (RAIT) to enhance open, customizable small code language models (SLMs) for these calculations.","By combining instruction tuned code SLMs with Retrieval-Augmented Code Generation (RACG) using external tools, the agent generates, debugs, and optimizes code from natural language specifications.","Our approach addresses the limitations of the current lack of a foundational AI model for specialized process engineering tasks and offers benefits of explainability, knowledge editing, and cost-effectiveness.","Additionally, we curate custom datasets of chemical and process engineering problems and solutions to overcome data scarcity.","Experimental results show that our framework matches the performance of large-scale proprietary models on benchmark datasets, proving its effectiveness and usability."],"url":"http://arxiv.org/abs/2408.15866v1"}
{"created":"2024-08-28 15:29:27","title":"microYOLO: Towards Single-Shot Object Detection on Microcontrollers","abstract":"This work-in-progress paper presents results on the feasibility of single-shot object detection on microcontrollers using YOLO. Single-shot object detectors like YOLO are widely used, however due to their complexity mainly on larger GPU-based platforms. We present microYOLO, which can be used on Cortex-M based microcontrollers, such as the OpenMV H7 R2, achieving about 3.5 FPS when classifying 128x128 RGB images while using less than 800 KB Flash and less than 350 KB RAM. Furthermore, we share experimental results for three different object detection tasks, analyzing the accuracy of microYOLO on them.","sentences":["This work-in-progress paper presents results on the feasibility of single-shot object detection on microcontrollers using YOLO.","Single-shot object detectors like YOLO are widely used, however due to their complexity mainly on larger GPU-based platforms.","We present microYOLO, which can be used on Cortex-M based microcontrollers, such as the OpenMV H7 R2, achieving about 3.5 FPS when classifying 128x128 RGB images while using less than 800 KB Flash and less than 350 KB RAM.","Furthermore, we share experimental results for three different object detection tasks, analyzing the accuracy of microYOLO on them."],"url":"http://arxiv.org/abs/2408.15865v1"}
{"created":"2024-08-28 15:23:08","title":"FlowAct: A Proactive Multimodal Human-robot Interaction System with Continuous Flow of Perception and Modular Action Sub-systems","abstract":"The evolution of autonomous systems in the context of human-robot interaction systems necessitates a synergy between the continuous perception of the environment and the potential actions to navigate or interact within it. We present Flowact, a proactive multimodal human-robot interaction architecture, working as an asynchronous endless loop of robot sensors into actuators and organized by two controllers, the Environment State Tracking (EST) and the Action Planner. The EST continuously collects and publishes a representation of the operative environment, ensuring a steady flow of perceptual data. This persistent perceptual flow is pivotal for our advanced Action Planner which orchestrates a collection of modular action subsystems, such as movement and speaking modules, governing their initiation or cessation based on the evolving environmental narrative. The EST employs a fusion of diverse sensory modalities to build a rich, real-time representation of the environment that is distributed to the Action Planner. This planner uses a decision-making framework to dynamically coordinate action modules, allowing them to respond proactively and coherently to changes in the environment. Through a series of real-world experiments, we exhibit the efficacy of the system in maintaining a continuous perception-action loop, substantially enhancing the responsiveness and adaptability of autonomous pro-active agents. The modular architecture of the action subsystems facilitates easy extensibility and adaptability to a broad spectrum of tasks and scenarios.","sentences":["The evolution of autonomous systems in the context of human-robot interaction systems necessitates a synergy between the continuous perception of the environment and the potential actions to navigate or interact within it.","We present Flowact, a proactive multimodal human-robot interaction architecture, working as an asynchronous endless loop of robot sensors into actuators and organized by two controllers, the Environment State Tracking (EST) and the Action Planner.","The EST continuously collects and publishes a representation of the operative environment, ensuring a steady flow of perceptual data.","This persistent perceptual flow is pivotal for our advanced Action Planner which orchestrates a collection of modular action subsystems, such as movement and speaking modules, governing their initiation or cessation based on the evolving environmental narrative.","The EST employs a fusion of diverse sensory modalities to build a rich, real-time representation of the environment that is distributed to the Action Planner.","This planner uses a decision-making framework to dynamically coordinate action modules, allowing them to respond proactively and coherently to changes in the environment.","Through a series of real-world experiments, we exhibit the efficacy of the system in maintaining a continuous perception-action loop, substantially enhancing the responsiveness and adaptability of autonomous pro-active agents.","The modular architecture of the action subsystems facilitates easy extensibility and adaptability to a broad spectrum of tasks and scenarios."],"url":"http://arxiv.org/abs/2408.15864v1"}
{"created":"2024-08-28 15:21:10","title":"Fusing Pruned and Backdoored Models: Optimal Transport-based Data-free Backdoor Mitigation","abstract":"Backdoor attacks present a serious security threat to deep neuron networks (DNNs). Although numerous effective defense techniques have been proposed in recent years, they inevitably rely on the availability of either clean or poisoned data. In contrast, data-free defense techniques have evolved slowly and still lag significantly in performance. To address this issue, different from the traditional approach of pruning followed by fine-tuning, we propose a novel data-free defense method named Optimal Transport-based Backdoor Repairing (OTBR) in this work. This method, based on our findings on neuron weight changes (NWCs) of random unlearning, uses optimal transport (OT)-based model fusion to combine the advantages of both pruned and backdoored models. Specifically, we first demonstrate our findings that the NWCs of random unlearning are positively correlated with those of poison unlearning. Based on this observation, we propose a random-unlearning NWC pruning technique to eliminate the backdoor effect and obtain a backdoor-free pruned model. Then, motivated by the OT-based model fusion, we propose the pruned-to-backdoored OT-based fusion technique, which fuses pruned and backdoored models to combine the advantages of both, resulting in a model that demonstrates high clean accuracy and a low attack success rate. To our knowledge, this is the first work to apply OT and model fusion techniques to backdoor defense. Extensive experiments show that our method successfully defends against all seven backdoor attacks across three benchmark datasets, outperforming both state-of-the-art (SOTA) data-free and data-dependent methods. The code implementation and Appendix are provided in the Supplementary Material.","sentences":["Backdoor attacks present a serious security threat to deep neuron networks (DNNs).","Although numerous effective defense techniques have been proposed in recent years, they inevitably rely on the availability of either clean or poisoned data.","In contrast, data-free defense techniques have evolved slowly and still lag significantly in performance.","To address this issue, different from the traditional approach of pruning followed by fine-tuning, we propose a novel data-free defense method named Optimal Transport-based Backdoor Repairing (OTBR) in this work.","This method, based on our findings on neuron weight changes (NWCs) of random unlearning, uses optimal transport (OT)-based model fusion to combine the advantages of both pruned and backdoored models.","Specifically, we first demonstrate our findings that the NWCs of random unlearning are positively correlated with those of poison unlearning.","Based on this observation, we propose a random-unlearning NWC pruning technique to eliminate the backdoor effect and obtain a backdoor-free pruned model.","Then, motivated by the OT-based model fusion, we propose the pruned-to-backdoored OT-based fusion technique, which fuses pruned and backdoored models to combine the advantages of both, resulting in a model that demonstrates high clean accuracy and a low attack success rate.","To our knowledge, this is the first work to apply OT and model fusion techniques to backdoor defense.","Extensive experiments show that our method successfully defends against all seven backdoor attacks across three benchmark datasets, outperforming both state-of-the-art (SOTA) data-free and data-dependent methods.","The code implementation and Appendix are provided in the Supplementary Material."],"url":"http://arxiv.org/abs/2408.15861v1"}
{"created":"2024-08-28 15:18:46","title":"What is YOLOv8: An In-Depth Exploration of the Internal Features of the Next-Generation Object Detector","abstract":"This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5. Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined. The paper reviews YOLOv8's performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms. Additionally, the study explores YOLOv8's developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment. Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field.","sentences":["This study presents a detailed analysis of the YOLOv8 object detection model, focusing on its architecture, training techniques, and performance improvements over previous iterations like YOLOv5.","Key innovations, including the CSPNet backbone for enhanced feature extraction, the FPN+PAN neck for superior multi-scale object detection, and the transition to an anchor-free approach, are thoroughly examined.","The paper reviews YOLOv8's performance across benchmarks like Microsoft COCO and Roboflow 100, highlighting its high accuracy and real-time capabilities across diverse hardware platforms.","Additionally, the study explores YOLOv8's developer-friendly enhancements, such as its unified Python package and CLI, which streamline model training and deployment.","Overall, this research positions YOLOv8 as a state-of-the-art solution in the evolving object detection field."],"url":"http://arxiv.org/abs/2408.15857v1"}
{"created":"2024-08-28 15:15:52","title":"An Empirical Study of API Misuses of Data-Centric Libraries","abstract":"Developers rely on third-party library Application Programming Interfaces (APIs) when developing software. However, libraries typically come with assumptions and API usage constraints, whose violation results in API misuse. API misuses may result in crashes or incorrect behavior. Even though API misuse is a well-studied area, a recent study of API misuse of deep learning libraries showed that the nature of these misuses and their symptoms are different from misuses of traditional libraries, and as a result highlighted potential shortcomings of current misuse detection tools. We speculate that these observations may not be limited to deep learning API misuses but may stem from the data-centric nature of these APIs. Data-centric libraries often deal with diverse data structures, intricate processing workflows, and a multitude of parameters, which can make them inherently more challenging to use correctly. Therefore, understanding the potential misuses of these libraries is important to avoid unexpected application behavior. To this end, this paper contributes an empirical study of API misuses of five data-centric libraries that cover areas such as data processing, numerical computation, machine learning, and visualization. We identify misuses of these libraries by analyzing data from both Stack Overflow and GitHub. Our results show that many of the characteristics of API misuses observed for deep learning libraries extend to misuses of the data-centric library APIs we study. We also find that developers tend to misuse APIs from data-centric libraries, regardless of whether the API directive appears in the documentation. Overall, our work exposes the challenges of API misuse in data-centric libraries, rather than only focusing on deep learning libraries. Our collected misuses and their characterization lay groundwork for future research to help reduce misuses of these libraries.","sentences":["Developers rely on third-party library Application Programming Interfaces (APIs) when developing software.","However, libraries typically come with assumptions and API usage constraints, whose violation results in API misuse.","API misuses may result in crashes or incorrect behavior.","Even though API misuse is a well-studied area, a recent study of API misuse of deep learning libraries showed that the nature of these misuses and their symptoms are different from misuses of traditional libraries, and as a result highlighted potential shortcomings of current misuse detection tools.","We speculate that these observations may not be limited to deep learning API misuses but may stem from the data-centric nature of these APIs.","Data-centric libraries often deal with diverse data structures, intricate processing workflows, and a multitude of parameters, which can make them inherently more challenging to use correctly.","Therefore, understanding the potential misuses of these libraries is important to avoid unexpected application behavior.","To this end, this paper contributes an empirical study of API misuses of five data-centric libraries that cover areas such as data processing, numerical computation, machine learning, and visualization.","We identify misuses of these libraries by analyzing data from both Stack Overflow and GitHub.","Our results show that many of the characteristics of API misuses observed for deep learning libraries extend to misuses of the data-centric library APIs we study.","We also find that developers tend to misuse APIs from data-centric libraries, regardless of whether the API directive appears in the documentation.","Overall, our work exposes the challenges of API misuse in data-centric libraries, rather than only focusing on deep learning libraries.","Our collected misuses and their characterization lay groundwork for future research to help reduce misuses of these libraries."],"url":"http://arxiv.org/abs/2408.15853v1"}
{"created":"2024-08-28 15:04:52","title":"Shot Segmentation Based on Von Neumann Entropy for Key Frame Extraction","abstract":"Video key frame extraction is important in various fields, such as video summary, retrieval, and compression. Therefore, we suggest a video key frame extraction algorithm based on shot segmentation using Von Neumann entropy. The segmentation of shots is achieved through the computation of Von Neumann entropy of the similarity matrix among frames within the video sequence. The initial frame of each shot is selected as key frames, which combines the temporal sequence information of frames. The experimental results show the extracted key frames can fully and accurately represent the original video content while minimizing the number of repeated frames.","sentences":["Video key frame extraction is important in various fields, such as video summary, retrieval, and compression.","Therefore, we suggest a video key frame extraction algorithm based on shot segmentation using Von Neumann entropy.","The segmentation of shots is achieved through the computation of Von Neumann entropy of the similarity matrix among frames within the video sequence.","The initial frame of each shot is selected as key frames, which combines the temporal sequence information of frames.","The experimental results show the extracted key frames can fully and accurately represent the original video content while minimizing the number of repeated frames."],"url":"http://arxiv.org/abs/2408.15844v1"}
{"created":"2024-08-28 15:03:38","title":"On the (In)security of optimized Stern-like signature schemes","abstract":"Stern's signature scheme is a historically important code-based signature scheme. A crucial optimization of this scheme is to generate pseudo-random vectors and a permutation instead of random ones, and most proposals that are based on Stern's signature use this optimization. However, its security has not been properly analyzed, especially when we use deterministic commitments. In this article, we study the security of this optimization. We first show that for some parameters, there is an attack that exploits this optimization and breaks the scheme in time $O(2^{\\frac{\\lambda}{2}})$ while the claimed security is $\\lambda$ bits. This impacts in particular the recent Quasy-cyclic Stern signature scheme [BGMS22]. Our second result shows that there is an efficient fix to this attack. By adding a string $salt \\in \\{0,1\\}^{2\\lambda}$ to the scheme, and changing slightly how the pseudo-random strings are generated, we prove not only that our attack doesn't work but that for any attack, the scheme preserves $\\lambda$ bits of security, and this fix increases the total signature size by only $2\\lambda$ bits. We apply this construction to other optimizations on Stern's signature scheme, such as the use of Lee's metric or the use of hash trees, and we show how these optimizations improve the signature length of Stern's signature scheme.","sentences":["Stern's signature scheme is a historically important code-based signature scheme.","A crucial optimization of this scheme is to generate pseudo-random vectors and a permutation instead of random ones, and most proposals that are based on Stern's signature use this optimization.","However, its security has not been properly analyzed, especially when we use deterministic commitments.","In this article, we study the security of this optimization.","We first show that for some parameters, there is an attack that exploits this optimization and breaks the scheme in time $O(2^{\\frac{\\lambda}{2}})$ while the claimed security is $\\lambda$ bits.","This impacts in particular the recent Quasy-cyclic Stern signature scheme [BGMS22].","Our second result shows that there is an efficient fix to this attack.","By adding a string $salt \\in \\{0,1\\}^{2\\lambda}$ to the scheme, and changing slightly how the pseudo-random strings are generated, we prove not only that our attack doesn't work but that for any attack, the scheme preserves $\\lambda$ bits of security, and this fix increases the total signature size by only $2\\lambda$ bits.","We apply this construction to other optimizations on Stern's signature scheme, such as the use of Lee's metric or the use of hash trees, and we show how these optimizations improve the signature length of Stern's signature scheme."],"url":"http://arxiv.org/abs/2408.15843v1"}
{"created":"2024-08-28 14:52:37","title":"EdgeLinker: Practical Blockchain-based Framework for Healthcare Fog Applications to Enhance Security in Edge-IoT Data Communications","abstract":"The pervasive adoption of Internet of Things (IoT) has significantly advanced healthcare digitization and modernization. Nevertheless, the sensitive nature of medical data presents security and privacy challenges. On the other hand, resource constraints of IoT devices often necessitates cloud services for data handling, introducing single points of failure, processing delays, and security vulnerabilities. Meanwhile, the blockchain technology offers potential solutions for enhancing security, decentralization, and data ownership. An ideal solution should ensure confidentiality, access control, and data integrity while being scalable, cost-effective, and integrable with the existing systems. However, current blockchain-based studies only address some of these requirements. Accordingly, this paper proposes EdgeLinker; a comprehensive solution incorporating Proof-of-Authority consensus, integrating smart contracts on the Ethereum blockchain for access control, and advanced cryptographic algorithms for secure data communication between IoT edge devices and the fog layer in healthcare fog applications. This novel framework has been implemented in a real-world fog testbed, using COTS fog devices. Based on a comprehensive set of evaluations, EdgeLinker demonstrates significant improvements in security and privacy with reasonable costs, making it an affordable and practical system for healthcare fog applications. Compared with the state-of-the-art, without significant changes in the write-time to the blockchain, EdgeLinker achieves a 35% improvement in data read time. Additionally, it is able to provide better throughput in both reading and writing transactions compared to the existing studies. EdgeLinker has been also examined in terms of energy, resource consumption and channel latency in both secure and non-secure modes, which has shown remarkable improvements.","sentences":["The pervasive adoption of Internet of Things (IoT) has significantly advanced healthcare digitization and modernization.","Nevertheless, the sensitive nature of medical data presents security and privacy challenges.","On the other hand, resource constraints of IoT devices often necessitates cloud services for data handling, introducing single points of failure, processing delays, and security vulnerabilities.","Meanwhile, the blockchain technology offers potential solutions for enhancing security, decentralization, and data ownership.","An ideal solution should ensure confidentiality, access control, and data integrity while being scalable, cost-effective, and integrable with the existing systems.","However, current blockchain-based studies only address some of these requirements.","Accordingly, this paper proposes EdgeLinker; a comprehensive solution incorporating Proof-of-Authority consensus, integrating smart contracts on the Ethereum blockchain for access control, and advanced cryptographic algorithms for secure data communication between IoT edge devices and the fog layer in healthcare fog applications.","This novel framework has been implemented in a real-world fog testbed, using COTS fog devices.","Based on a comprehensive set of evaluations, EdgeLinker demonstrates significant improvements in security and privacy with reasonable costs, making it an affordable and practical system for healthcare fog applications.","Compared with the state-of-the-art, without significant changes in the write-time to the blockchain, EdgeLinker achieves a 35% improvement in data read time.","Additionally, it is able to provide better throughput in both reading and writing transactions compared to the existing studies.","EdgeLinker has been also examined in terms of energy, resource consumption and channel latency in both secure and non-secure modes, which has shown remarkable improvements."],"url":"http://arxiv.org/abs/2408.15838v1"}
{"created":"2024-08-28 14:48:37","title":"Knowledge Navigator: LLM-guided Browsing Framework for Exploratory Search in Scientific Literature","abstract":"The exponential growth of scientific literature necessitates advanced tools for effective knowledge exploration. We present Knowledge Navigator, a system designed to enhance exploratory search abilities by organizing and structuring the retrieved documents from broad topical queries into a navigable, two-level hierarchy of named and descriptive scientific topics and subtopics. This structured organization provides an overall view of the research themes in a domain, while also enabling iterative search and deeper knowledge discovery within specific subtopics by allowing users to refine their focus and retrieve additional relevant documents. Knowledge Navigator combines LLM capabilities with cluster-based methods to enable an effective browsing method. We demonstrate our approach's effectiveness through automatic and manual evaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC. Our code, prompts, and benchmarks are made publicly available.","sentences":["The exponential growth of scientific literature necessitates advanced tools for effective knowledge exploration.","We present Knowledge Navigator, a system designed to enhance exploratory search abilities by organizing and structuring the retrieved documents from broad topical queries into a navigable, two-level hierarchy of named and descriptive scientific topics and subtopics.","This structured organization provides an overall view of the research themes in a domain, while also enabling iterative search and deeper knowledge discovery within specific subtopics by allowing users to refine their focus and retrieve additional relevant documents.","Knowledge Navigator combines LLM capabilities with cluster-based methods to enable an effective browsing method.","We demonstrate our approach's effectiveness through automatic and manual evaluations on two novel benchmarks, CLUSTREC-COVID and SCITOC.","Our code, prompts, and benchmarks are made publicly available."],"url":"http://arxiv.org/abs/2408.15836v1"}
{"created":"2024-08-28 14:47:34","title":"Network transferability of adversarial patches in real-time object detection","abstract":"Adversarial patches in computer vision can be used, to fool deep neural networks and manipulate their decision-making process. One of the most prominent examples of adversarial patches are evasion attacks for object detectors. By covering parts of objects of interest, these patches suppress the detections and thus make the target object 'invisible' to the object detector. Since these patches are usually optimized on a specific network with a specific train dataset, the transferability across multiple networks and datasets is not given. This paper addresses these issues and investigates the transferability across numerous object detector architectures. Our extensive evaluation across various models on two distinct datasets indicates that patches optimized with larger models provide better network transferability than patches that are optimized with smaller models.","sentences":["Adversarial patches in computer vision can be used, to fool deep neural networks and manipulate their decision-making process.","One of the most prominent examples of adversarial patches are evasion attacks for object detectors.","By covering parts of objects of interest, these patches suppress the detections and thus make the target object 'invisible' to the object detector.","Since these patches are usually optimized on a specific network with a specific train dataset, the transferability across multiple networks and datasets is not given.","This paper addresses these issues and investigates the transferability across numerous object detector architectures.","Our extensive evaluation across various models on two distinct datasets indicates that patches optimized with larger models provide better network transferability than patches that are optimized with smaller models."],"url":"http://arxiv.org/abs/2408.15833v1"}
{"created":"2024-08-28 14:45:31","title":"Towards Optimized Parallel Robots for Human-Robot Collaboration by Combined Structural and Dimensional Synthesis","abstract":"Parallel robots (PR) offer potential for human-robot collaboration (HRC) due to their lower moving masses and higher speeds. However, the parallel leg chains increase the risks of collision and clamping. In this work, these hazards are described by kinematics and kinetostatics models to minimize them as objective functions by a combined structural and dimensional synthesis in a particle-swarm optimization. In addition to the risk of clamping within and between kinematic chains, the back-drivability is quantified to theoretically guarantee detectability via motor current. Another HRC-relevant objective function is the largest eigenvalue of the mass matrix formulated in the operational-space coordinates to consider collision effects. Multi-objective optimization leads to different Pareto-optimal PR structures. The results show that the optimization leads to significant improvement of the HRC criteria and that a Hexa structure (6-RUS) is to be favored concerning the objective functions and due to its simpler joint structure.","sentences":["Parallel robots (PR) offer potential for human-robot collaboration (HRC) due to their lower moving masses and higher speeds.","However, the parallel leg chains increase the risks of collision and clamping.","In this work, these hazards are described by kinematics and kinetostatics models to minimize them as objective functions by a combined structural and dimensional synthesis in a particle-swarm optimization.","In addition to the risk of clamping within and between kinematic chains, the back-drivability is quantified to theoretically guarantee detectability via motor current.","Another HRC-relevant objective function is the largest eigenvalue of the mass matrix formulated in the operational-space coordinates to consider collision effects.","Multi-objective optimization leads to different Pareto-optimal PR structures.","The results show that the optimization leads to significant improvement of the HRC criteria and that a Hexa structure (6-RUS) is to be favored concerning the objective functions and due to its simpler joint structure."],"url":"http://arxiv.org/abs/2408.15831v1"}
{"created":"2024-08-28 14:44:42","title":"SITransformer: Shared Information-Guided Transformer for Extreme Multimodal Summarization","abstract":"Extreme Multimodal Summarization with Multimodal Output (XMSMO) becomes an attractive summarization approach by integrating various types of information to create extremely concise yet informative summaries for individual modalities. Existing methods overlook the issue that multimodal data often contains more topic irrelevant information, which can mislead the model into producing inaccurate summaries especially for extremely short ones. In this paper, we propose SITransformer, a \\textbf{S}hared \\textbf{I}nformation-guided \\textbf{T}ransformer for extreme multimodal summarization. It has a shared information guided pipeline which involves a cross-modal shared information extractor and a cross-modal interaction module. The extractor formulates semantically shared salient information from different modalities by devising a novel filtering process consisting of a differentiable top-k selector and a shared-information guided gating unit. As a result, the common, salient, and relevant contents across modalities are identified. Next, a transformer with cross-modal attentions is developed for intra- and inter-modality learning with the shared information guidance to produce the extreme summary. Comprehensive experiments demonstrate that SITransformer significantly enhances the summarization quality for both video and text summaries for XMSMO. Our code will be publicly available at https://github.com/SichengLeoLiu/MMAsia24-XMSMO.","sentences":["Extreme Multimodal Summarization with Multimodal Output (XMSMO) becomes an attractive summarization approach by integrating various types of information to create extremely concise yet informative summaries for individual modalities.","Existing methods overlook the issue that multimodal data often contains more topic irrelevant information, which can mislead the model into producing inaccurate summaries especially for extremely short ones.","In this paper, we propose SITransformer, a \\textbf{S}hared \\textbf{I}nformation-guided \\textbf{T}ransformer for extreme multimodal summarization.","It has a shared information guided pipeline which involves a cross-modal shared information extractor and a cross-modal interaction module.","The extractor formulates semantically shared salient information from different modalities by devising a novel filtering process consisting of a differentiable top-k selector and a shared-information guided gating unit.","As a result, the common, salient, and relevant contents across modalities are identified.","Next, a transformer with cross-modal attentions is developed for intra- and inter-modality learning with the shared information guidance to produce the extreme summary.","Comprehensive experiments demonstrate that SITransformer significantly enhances the summarization quality for both video and text summaries for XMSMO.","Our code will be publicly available at https://github.com/SichengLeoLiu/MMAsia24-XMSMO."],"url":"http://arxiv.org/abs/2408.15829v1"}
{"created":"2024-08-28 14:40:15","title":"Automatic Differential Diagnosis using Transformer-Based Multi-Label Sequence Classification","abstract":"As the field of artificial intelligence progresses, assistive technologies are becoming more widely used across all industries. The healthcare industry is no different, with numerous studies being done to develop assistive tools for healthcare professionals. Automatic diagnostic systems are one such beneficial tool that can assist with a variety of tasks, including collecting patient information, analyzing test results, and diagnosing patients. However, the idea of developing systems that can provide a differential diagnosis has been largely overlooked in most of these research studies. In this study, we propose a transformer-based approach for providing differential diagnoses based on a patient's age, sex, medical history, and symptoms. We use the DDXPlus dataset, which provides differential diagnosis information for patients based on 49 disease types. Firstly, we propose a method to process the tabular patient data from the dataset and engineer them into patient reports to make them suitable for our research. In addition, we introduce two data modification modules to diversify the training data and consequently improve the robustness of the models. We approach the task as a multi-label classification problem and conduct extensive experiments using four transformer models. All the models displayed promising results by achieving over 97% F1 score on the held-out test set. Moreover, we design additional behavioral tests to get a broader understanding of the models. In particular, for one of our test cases, we prepared a custom test set of 100 samples with the assistance of a doctor. The results on the custom set showed that our proposed data modification modules improved the model's generalization capabilities. We hope our findings will provide future researchers with valuable insights and inspire them to develop reliable systems for automatic differential diagnosis.","sentences":["As the field of artificial intelligence progresses, assistive technologies are becoming more widely used across all industries.","The healthcare industry is no different, with numerous studies being done to develop assistive tools for healthcare professionals.","Automatic diagnostic systems are one such beneficial tool that can assist with a variety of tasks, including collecting patient information, analyzing test results, and diagnosing patients.","However, the idea of developing systems that can provide a differential diagnosis has been largely overlooked in most of these research studies.","In this study, we propose a transformer-based approach for providing differential diagnoses based on a patient's age, sex, medical history, and symptoms.","We use the DDXPlus dataset, which provides differential diagnosis information for patients based on 49 disease types.","Firstly, we propose a method to process the tabular patient data from the dataset and engineer them into patient reports to make them suitable for our research.","In addition, we introduce two data modification modules to diversify the training data and consequently improve the robustness of the models.","We approach the task as a multi-label classification problem and conduct extensive experiments using four transformer models.","All the models displayed promising results by achieving over 97% F1 score on the held-out test set.","Moreover, we design additional behavioral tests to get a broader understanding of the models.","In particular, for one of our test cases, we prepared a custom test set of 100 samples with the assistance of a doctor.","The results on the custom set showed that our proposed data modification modules improved the model's generalization capabilities.","We hope our findings will provide future researchers with valuable insights and inspire them to develop reliable systems for automatic differential diagnosis."],"url":"http://arxiv.org/abs/2408.15827v1"}
{"created":"2024-08-28 14:34:34","title":"Automating Pruning in Top-Down Enumeration for Program Synthesis Problems with Monotonic Semantics","abstract":"In top-down enumeration for program synthesis, abstraction-based pruning uses an abstract domain to approximate the set of possible values that a partial program, when completed, can output on a given input. If the set does not contain the desired output, the partial program and all its possible completions can be pruned. In its general form, abstraction-based pruning requires manually designed, domain-specific abstract domains and semantics, and thus has only been used in domain-specific synthesizers.   This paper provides sufficient conditions under which a form of abstraction-based pruning can be automated for arbitrary synthesis problems in the general-purpose Semantics-Guided Synthesis (SemGuS) framework without requiring manually-defined abstract domains. We show that if the semantics of the language for which we are synthesizing programs exhibits some monotonicity properties, one can obtain an abstract interval-based semantics for free from the concrete semantics of the programming language, and use such semantics to effectively prune the search space. We also identify a condition that ensures such abstract semantics can be used to compute a precise abstraction of the set of values that a program derivable from a given hole in a partial program can produce. These precise abstractions make abstraction-based pruning more effective.   We implement our approach in a tool, Moito, which can tackle synthesis problems defined in the SemGuS framework. Moito can automate interval-based pruning without any a-priori knowledge of the problem domain, and solve synthesis problems that previously required domain-specific, abstraction-based synthesizers -- e.g., synthesis of regular expressions, CSV file schema, and imperative programs from examples.","sentences":["In top-down enumeration for program synthesis, abstraction-based pruning uses an abstract domain to approximate the set of possible values that a partial program, when completed, can output on a given input.","If the set does not contain the desired output, the partial program and all its possible completions can be pruned.","In its general form, abstraction-based pruning requires manually designed, domain-specific abstract domains and semantics, and thus has only been used in domain-specific synthesizers.   ","This paper provides sufficient conditions under which a form of abstraction-based pruning can be automated for arbitrary synthesis problems in the general-purpose Semantics-Guided Synthesis (SemGuS) framework without requiring manually-defined abstract domains.","We show that if the semantics of the language for which we are synthesizing programs exhibits some monotonicity properties, one can obtain an abstract interval-based semantics for free from the concrete semantics of the programming language, and use such semantics to effectively prune the search space.","We also identify a condition that ensures such abstract semantics can be used to compute a precise abstraction of the set of values that a program derivable from a given hole in a partial program can produce.","These precise abstractions make abstraction-based pruning more effective.   ","We implement our approach in a tool, Moito, which can tackle synthesis problems defined in the SemGuS framework.","Moito can automate interval-based pruning without any a-priori knowledge of the problem domain, and solve synthesis problems that previously required domain-specific, abstraction-based synthesizers -- e.g., synthesis of regular expressions, CSV file schema, and imperative programs from examples."],"url":"http://arxiv.org/abs/2408.15822v1"}
{"created":"2024-08-28 14:27:33","title":"Unifying Model Execution and Deductive Verification with Interaction Trees in Isabelle/HOL","abstract":"Model execution allows us to prototype and analyse software engineering models by stepping through their possible behaviours, using techniques like animation and simulation. On the other hand, deductive verification allows us to construct formal proofs demonstrating satisfaction of certain critical properties in support of high-assurance software engineering. To ensure coherent results between execution and proof, we need unifying semantics and automation. In this paper, we mechanise Interaction Trees (ITrees) in Isabelle/HOL to produce an execution and verification framework. ITrees are coinductive structures that allow us to encode infinite labelled transition systems, yet they are inherently executable. We use ITrees to create verification tools for stateful imperative programs, concurrent programs with message passing in the form of the CSP and \\Circus languages, and abstract system models in the style of the Z and B methods. We demonstrate how ITrees can account for diverse semantic presentations, such as structural operational semantics, a relational program model, and CSP's failures-divergences trace model. Finally, we demonstrate how ITrees can be executed using the Isabelle code generator to support the animation of models.","sentences":["Model execution allows us to prototype and analyse software engineering models by stepping through their possible behaviours, using techniques like animation and simulation.","On the other hand, deductive verification allows us to construct formal proofs demonstrating satisfaction of certain critical properties in support of high-assurance software engineering.","To ensure coherent results between execution and proof, we need unifying semantics and automation.","In this paper, we mechanise Interaction Trees (ITrees) in Isabelle/HOL to produce an execution and verification framework.","ITrees are coinductive structures that allow us to encode infinite labelled transition systems, yet they are inherently executable.","We use ITrees to create verification tools for stateful imperative programs, concurrent programs with message passing in the form of the CSP and \\Circus languages, and abstract system models in the style of the Z and B methods.","We demonstrate how ITrees can account for diverse semantic presentations, such as structural operational semantics, a relational program model, and CSP's failures-divergences trace model.","Finally, we demonstrate how ITrees can be executed using the Isabelle code generator to support the animation of models."],"url":"http://arxiv.org/abs/2408.15817v1"}
{"created":"2024-08-28 14:25:35","title":"Mining Field Data for Tree Species Recognition at Scale","abstract":"Individual tree species labels are particularly hard to acquire due to the expert knowledge needed and the limitations of photointerpretation. Here, we present a methodology to automatically mine species labels from public forest inventory data, using available pretrained tree detection models. We identify tree instances in aerial imagery and match them with field data with close to zero human involvement. We conduct a series of experiments on the resulting dataset, and show a beneficial effect when adding noisy or even unlabeled data points, highlighting a strong potential for large-scale individual species mapping.","sentences":["Individual tree species labels are particularly hard to acquire due to the expert knowledge needed and the limitations of photointerpretation.","Here, we present a methodology to automatically mine species labels from public forest inventory data, using available pretrained tree detection models.","We identify tree instances in aerial imagery and match them with field data with close to zero human involvement.","We conduct a series of experiments on the resulting dataset, and show a beneficial effect when adding noisy or even unlabeled data points, highlighting a strong potential for large-scale individual species mapping."],"url":"http://arxiv.org/abs/2408.15816v1"}
{"created":"2024-08-28 14:24:48","title":"MR-Adopt: Automatic Deduction of Input Transformation Function for Metamorphic Testing","abstract":"While a recent study reveals that many developer-written test cases can encode a reusable Metamorphic Relation (MR), over 70% of them directly hard-code the source input and follow-up input in the encoded relation. Such encoded MRs, which do not contain an explicit input transformation to transform the source inputs to corresponding follow-up inputs, cannot be reused with new source inputs to enhance test adequacy.   In this paper, we propose MR-Adopt (Automatic Deduction Of inPut Transformation) to automatically deduce the input transformation from the hard-coded source and follow-up inputs, aiming to enable the encoded MRs to be reused with new source inputs. With typically only one pair of source and follow-up inputs available in an MR-encoded test case as the example, we leveraged LLMs to understand the intention of the test case and generate additional examples of source-followup input pairs. This helps to guide the generation of input transformations generalizable to multiple source inputs. Besides, to mitigate the issue that LLMs generate erroneous code, we refine LLM-generated transformations by removing MR- irrelevant code elements with data-flow analysis. Finally, we assess candidate transformations based on encoded output relations and select the best transformation as the result. Evaluation results show that MR-Adopt can generate input transformations applicable to all experimental source inputs for 72.00% of encoded MRs, which is 33.33% more than using vanilla GPT-3.5. By incorporating MR- Adopt-generated input transformations, encoded MR-based test cases can effectively enhance the test adequacy, increasing the line coverage and mutation score by 10.62% and 18.91%, respectively.","sentences":["While a recent study reveals that many developer-written test cases can encode a reusable Metamorphic Relation (MR), over 70% of them directly hard-code the source input and follow-up input in the encoded relation.","Such encoded MRs, which do not contain an explicit input transformation to transform the source inputs to corresponding follow-up inputs, cannot be reused with new source inputs to enhance test adequacy.   ","In this paper, we propose MR-Adopt (Automatic Deduction Of inPut Transformation) to automatically deduce the input transformation from the hard-coded source and follow-up inputs, aiming to enable the encoded MRs to be reused with new source inputs.","With typically only one pair of source and follow-up inputs available in an MR-encoded test case as the example, we leveraged LLMs to understand the intention of the test case and generate additional examples of source-followup input pairs.","This helps to guide the generation of input transformations generalizable to multiple source inputs.","Besides, to mitigate the issue that LLMs generate erroneous code, we refine LLM-generated transformations by removing MR- irrelevant code elements with data-flow analysis.","Finally, we assess candidate transformations based on encoded output relations and select the best transformation as the result.","Evaluation results show that MR-Adopt can generate input transformations applicable to all experimental source inputs for 72.00% of encoded MRs, which is 33.33% more than using vanilla GPT-3.5.","By incorporating MR- Adopt-generated input transformations, encoded MR-based test cases can effectively enhance the test adequacy, increasing the line coverage and mutation score by 10.62% and 18.91%, respectively."],"url":"http://arxiv.org/abs/2408.15815v1"}
{"created":"2024-08-28 14:14:33","title":"DQFormer: Towards Unified LiDAR Panoptic Segmentation with Decoupled Queries","abstract":"LiDAR panoptic segmentation, which jointly performs instance and semantic segmentation for things and stuff classes, plays a fundamental role in LiDAR perception tasks. While most existing methods explicitly separate these two segmentation tasks and utilize different branches (i.e., semantic and instance branches), some recent methods have embraced the query-based paradigm to unify LiDAR panoptic segmentation. However, the distinct spatial distribution and inherent characteristics of objects(things) and their surroundings(stuff) in 3D scenes lead to challenges, including the mutual competition of things/stuff and the ambiguity of classification/segmentation. In this paper, we propose decoupling things/stuff queries according to their intrinsic properties for individual decoding and disentangling classification/segmentation to mitigate ambiguity. To this end, we propose a novel framework dubbed DQFormer to implement semantic and instance segmentation in a unified workflow. Specifically, we design a decoupled query generator to propose informative queries with semantics by localizing things/stuff positions and fusing multi-level BEV embeddings. Moreover, a query-oriented mask decoder is introduced to decode corresponding segmentation masks by performing masked cross-attention between queries and mask embeddings. Finally, the decoded masks are combined with the semantics of the queries to produce panoptic results. Extensive experiments on nuScenes and SemanticKITTI datasets demonstrate the superiority of our DQFormer framework.","sentences":["LiDAR panoptic segmentation, which jointly performs instance and semantic segmentation for things and stuff classes, plays a fundamental role in LiDAR perception tasks.","While most existing methods explicitly separate these two segmentation tasks and utilize different branches (i.e., semantic and instance branches), some recent methods have embraced the query-based paradigm to unify LiDAR panoptic segmentation.","However, the distinct spatial distribution and inherent characteristics of objects(things) and their surroundings(stuff) in 3D scenes lead to challenges, including the mutual competition of things/stuff and the ambiguity of classification/segmentation.","In this paper, we propose decoupling things/","stuff queries according to their intrinsic properties for individual decoding and disentangling classification/segmentation to mitigate ambiguity.","To this end, we propose a novel framework dubbed DQFormer to implement semantic and instance segmentation in a unified workflow.","Specifically, we design a decoupled query generator to propose informative queries with semantics by localizing things/stuff positions and fusing multi-level BEV embeddings.","Moreover, a query-oriented mask decoder is introduced to decode corresponding segmentation masks by performing masked cross-attention between queries and mask embeddings.","Finally, the decoded masks are combined with the semantics of the queries to produce panoptic results.","Extensive experiments on nuScenes and SemanticKITTI datasets demonstrate the superiority of our DQFormer framework."],"url":"http://arxiv.org/abs/2408.15813v1"}
{"created":"2024-08-28 14:10:57","title":"Multi-view Pose Fusion for Occlusion-Aware 3D Human Pose Estimation","abstract":"Robust 3D human pose estimation is crucial to ensure safe and effective human-robot collaboration. Accurate human perception,however, is particularly challenging in these scenarios due to strong occlusions and limited camera viewpoints. Current 3D human pose estimation approaches are rather vulnerable in such conditions. In this work we present a novel approach for robust 3D human pose estimation in the context of human-robot collaboration. Instead of relying on noisy 2D features triangulation, we perform multi-view fusion on 3D skeletons provided by absolute monocular methods. Accurate 3D pose estimation is then obtained via reprojection error optimization, introducing limbs length symmetry constraints. We evaluate our approach on the public dataset Human3.6M and on a novel version Human3.6M-Occluded, derived adding synthetic occlusions on the camera views with the purpose of testing pose estimation algorithms under severe occlusions. We further validate our method on real human-robot collaboration workcells, in which we strongly surpass current 3D human pose estimation methods. Our approach outperforms state-of-the-art multi-view human pose estimation techniques and demonstrates superior capabilities in handling challenging scenarios with strong occlusions, representing a reliable and effective solution for real human-robot collaboration setups.","sentences":["Robust 3D human pose estimation is crucial to ensure safe and effective human-robot collaboration.","Accurate human perception,however, is particularly challenging in these scenarios due to strong occlusions and limited camera viewpoints.","Current 3D human pose estimation approaches are rather vulnerable in such conditions.","In this work we present a novel approach for robust 3D human pose estimation in the context of human-robot collaboration.","Instead of relying on noisy 2D features triangulation, we perform multi-view fusion on 3D skeletons provided by absolute monocular methods.","Accurate 3D pose estimation is then obtained via reprojection error optimization, introducing limbs length symmetry constraints.","We evaluate our approach on the public dataset Human3.6M and on a novel version Human3.6M-Occluded, derived adding synthetic occlusions on the camera views with the purpose of testing pose estimation algorithms under severe occlusions.","We further validate our method on real human-robot collaboration workcells, in which we strongly surpass current 3D human pose estimation methods.","Our approach outperforms state-of-the-art multi-view human pose estimation techniques and demonstrates superior capabilities in handling challenging scenarios with strong occlusions, representing a reliable and effective solution for real human-robot collaboration setups."],"url":"http://arxiv.org/abs/2408.15810v1"}
{"created":"2024-08-28 14:08:24","title":"Object Detection for Vehicle Dashcams using Transformers","abstract":"The use of intelligent automation is growing significantly in the automotive industry, as it assists drivers and fleet management companies, thus increasing their productivity. Dash cams are now been used for this purpose which enables the instant identification and understanding of multiple objects and occurrences in the surroundings. In this paper, we propose a novel approach for object detection in dashcams using transformers. Our system is based on the state-of-the-art DEtection TRansformer (DETR), which has demonstrated strong performance in a variety of conditions, including different weather and illumination scenarios. The use of transformers allows for the consideration of contextual information in decisionmaking, improving the accuracy of object detection. To validate our approach, we have trained our DETR model on a dataset that represents real-world conditions. Our results show that the use of intelligent automation through transformers can significantly enhance the capabilities of dashcam systems. The model achieves an mAP of 0.95 on detection.","sentences":["The use of intelligent automation is growing significantly in the automotive industry, as it assists drivers and fleet management companies, thus increasing their productivity.","Dash cams are now been used for this purpose which enables the instant identification and understanding of multiple objects and occurrences in the surroundings.","In this paper, we propose a novel approach for object detection in dashcams using transformers.","Our system is based on the state-of-the-art DEtection TRansformer (DETR), which has demonstrated strong performance in a variety of conditions, including different weather and illumination scenarios.","The use of transformers allows for the consideration of contextual information in decisionmaking, improving the accuracy of object detection.","To validate our approach, we have trained our DETR model on a dataset that represents real-world conditions.","Our results show that the use of intelligent automation through transformers can significantly enhance the capabilities of dashcam systems.","The model achieves an mAP of 0.95 on detection."],"url":"http://arxiv.org/abs/2408.15809v1"}
{"created":"2024-08-28 13:53:27","title":"Visual Prompt Engineering for Medical Vision Language Models in Radiology","abstract":"Medical image classification in radiology faces significant challenges, particularly in generalizing to unseen pathologies. In contrast, CLIP offers a promising solution by leveraging multimodal learning to improve zero-shot classification performance. However, in the medical domain, lesions can be small and might not be well represented in the embedding space. Therefore, in this paper, we explore the potential of visual prompt engineering to enhance the capabilities of Vision Language Models (VLMs) in radiology. Leveraging BiomedCLIP, trained on extensive biomedical image-text pairs, we investigate the impact of embedding visual markers directly within radiological images to guide the model's attention to critical regions. Our evaluation on the JSRT dataset, focusing on lung nodule malignancy classification, demonstrates that incorporating visual prompts $\\unicode{x2013}$ such as arrows, circles, and contours $\\unicode{x2013}$ significantly improves classification metrics including AUROC, AUPRC, F1 score, and accuracy. Moreover, the study provides attention maps, showcasing enhanced model interpretability and focus on clinically relevant areas. These findings underscore the efficacy of visual prompt engineering as a straightforward yet powerful approach to advance VLM performance in medical image analysis.","sentences":["Medical image classification in radiology faces significant challenges, particularly in generalizing to unseen pathologies.","In contrast, CLIP offers a promising solution by leveraging multimodal learning to improve zero-shot classification performance.","However, in the medical domain, lesions can be small and might not be well represented in the embedding space.","Therefore, in this paper, we explore the potential of visual prompt engineering to enhance the capabilities of Vision Language Models (VLMs) in radiology.","Leveraging BiomedCLIP, trained on extensive biomedical image-text pairs, we investigate the impact of embedding visual markers directly within radiological images to guide the model's attention to critical regions.","Our evaluation on the JSRT dataset, focusing on lung nodule malignancy classification, demonstrates that incorporating visual prompts $\\unicode{x2013}$ such as arrows, circles, and contours $\\unicode{x2013}$ significantly improves classification metrics including AUROC, AUPRC, F1 score, and accuracy.","Moreover, the study provides attention maps, showcasing enhanced model interpretability and focus on clinically relevant areas.","These findings underscore the efficacy of visual prompt engineering as a straightforward yet powerful approach to advance VLM performance in medical image analysis."],"url":"http://arxiv.org/abs/2408.15802v1"}
{"created":"2024-08-28 13:52:19","title":"Scaling Up Summarization: Leveraging Large Language Models for Long Text Extractive Summarization","abstract":"In an era where digital text is proliferating at an unprecedented rate, efficient summarization tools are becoming indispensable. While Large Language Models (LLMs) have been successfully applied in various NLP tasks, their role in extractive text summarization remains underexplored. This paper introduces EYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive Summarization), a framework that leverages LLMs, specifically LLAMA2-7B and ChatGLM2-6B, for extractive summarization of lengthy text documents. Instead of abstractive methods, which often suffer from issues like factual inaccuracies and hallucinations, EYEGLAXS focuses on extractive summarization to ensure factual and grammatical integrity. Utilizing state-of-the-art techniques such as Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS addresses the computational and resource challenges typically associated with LLMs. The system sets new performance benchmarks on well-known datasets like PubMed and ArXiv. Furthermore, we extend our research through additional analyses that explore the adaptability of LLMs in handling different sequence lengths and their efficiency in training on smaller datasets. These contributions not only set a new standard in the field but also open up promising avenues for future research in extractive text summarization.","sentences":["In an era where digital text is proliferating at an unprecedented rate, efficient summarization tools are becoming indispensable.","While Large Language Models (LLMs) have been successfully applied in various NLP tasks, their role in extractive text summarization remains underexplored.","This paper introduces EYEGLAXS (Easy Yet Efficient larGe LAnguage model for eXtractive Summarization), a framework that leverages LLMs, specifically LLAMA2-7B and ChatGLM2-6B, for extractive summarization of lengthy text documents.","Instead of abstractive methods, which often suffer from issues like factual inaccuracies and hallucinations, EYEGLAXS focuses on extractive summarization to ensure factual and grammatical integrity.","Utilizing state-of-the-art techniques such as Flash Attention and Parameter-Efficient Fine-Tuning (PEFT), EYEGLAXS addresses the computational and resource challenges typically associated with LLMs.","The system sets new performance benchmarks on well-known datasets like PubMed and ArXiv.","Furthermore, we extend our research through additional analyses that explore the adaptability of LLMs in handling different sequence lengths and their efficiency in training on smaller datasets.","These contributions not only set a new standard in the field but also open up promising avenues for future research in extractive text summarization."],"url":"http://arxiv.org/abs/2408.15801v1"}
{"created":"2024-08-28 13:51:52","title":"Emulating Brain-like Rapid Learning in Neuromorphic Edge Computing","abstract":"Achieving personalized intelligence at the edge with real-time learning capabilities holds enormous promise in enhancing our daily experiences and helping decision making, planning, and sensing. However, efficient and reliable edge learning remains difficult with current technology due to the lack of personalized data, insufficient hardware capabilities, and inherent challenges posed by online learning.   Over time and across multiple developmental stages, the brain has evolved to efficiently incorporate new knowledge by gradually building on previous knowledge. In this work, we emulate the multiple stages of learning with digital neuromorphic technology that simulates the neural and synaptic processes of the brain using two stages of learning. First, a meta-training stage trains the hyperparameters of synaptic plasticity for one-shot learning using a differentiable simulation of the neuromorphic hardware. This meta-training process refines a hardware local three-factor synaptic plasticity rule and its associated hyperparameters to align with the trained task domain. In a subsequent deployment stage, these optimized hyperparameters enable fast, data-efficient, and accurate learning of new classes. We demonstrate our approach using event-driven vision sensor data and the Intel Loihi neuromorphic processor with its plasticity dynamics, achieving real-time one-shot learning of new classes that is vastly improved over transfer learning. Our methodology can be deployed with arbitrary plasticity models and can be applied to situations demanding quick learning and adaptation at the edge, such as navigating unfamiliar environments or learning unexpected categories of data through user engagement.","sentences":["Achieving personalized intelligence at the edge with real-time learning capabilities holds enormous promise in enhancing our daily experiences and helping decision making, planning, and sensing.","However, efficient and reliable edge learning remains difficult with current technology due to the lack of personalized data, insufficient hardware capabilities, and inherent challenges posed by online learning.   ","Over time and across multiple developmental stages, the brain has evolved to efficiently incorporate new knowledge by gradually building on previous knowledge.","In this work, we emulate the multiple stages of learning with digital neuromorphic technology that simulates the neural and synaptic processes of the brain using two stages of learning.","First, a meta-training stage trains the hyperparameters of synaptic plasticity for one-shot learning using a differentiable simulation of the neuromorphic hardware.","This meta-training process refines a hardware local three-factor synaptic plasticity rule and its associated hyperparameters to align with the trained task domain.","In a subsequent deployment stage, these optimized hyperparameters enable fast, data-efficient, and accurate learning of new classes.","We demonstrate our approach using event-driven vision sensor data and the Intel Loihi neuromorphic processor with its plasticity dynamics, achieving real-time one-shot learning of new classes that is vastly improved over transfer learning.","Our methodology can be deployed with arbitrary plasticity models and can be applied to situations demanding quick learning and adaptation at the edge, such as navigating unfamiliar environments or learning unexpected categories of data through user engagement."],"url":"http://arxiv.org/abs/2408.15800v1"}
{"created":"2024-08-28 13:42:28","title":"Evaluating Named Entity Recognition Using Few-Shot Prompting with Large Language Models","abstract":"This paper evaluates Few-Shot Prompting with Large Language Models for Named Entity Recognition (NER). Traditional NER systems rely on extensive labeled datasets, which are costly and time-consuming to obtain. Few-Shot Prompting or in-context learning enables models to recognize entities with minimal examples. We assess state-of-the-art models like GPT-4 in NER tasks, comparing their few-shot performance to fully supervised benchmarks. Results show that while there is a performance gap, large models excel in adapting to new entity types and domains with very limited data. We also explore the effects of prompt engineering, guided output format and context length on performance. This study underscores Few-Shot Learning's potential to reduce the need for large labeled datasets, enhancing NER scalability and accessibility.","sentences":["This paper evaluates Few-Shot Prompting with Large Language Models for Named Entity Recognition (NER).","Traditional NER systems rely on extensive labeled datasets, which are costly and time-consuming to obtain.","Few-Shot Prompting or in-context learning enables models to recognize entities with minimal examples.","We assess state-of-the-art models like GPT-4 in NER tasks, comparing their few-shot performance to fully supervised benchmarks.","Results show that while there is a performance gap, large models excel in adapting to new entity types and domains with very limited data.","We also explore the effects of prompt engineering, guided output format and context length on performance.","This study underscores Few-Shot Learning's potential to reduce the need for large labeled datasets, enhancing NER scalability and accessibility."],"url":"http://arxiv.org/abs/2408.15796v1"}
