{"created":"2025-01-27 18:59:57","title":"RelightVid: Temporal-Consistent Diffusion Model for Video Relighting","abstract":"Diffusion models have demonstrated remarkable success in image generation and editing, with recent advancements enabling albedo-preserving image relighting. However, applying these models to video relighting remains challenging due to the lack of paired video relighting datasets and the high demands for output fidelity and temporal consistency, further complicated by the inherent randomness of diffusion models. To address these challenges, we introduce RelightVid, a flexible framework for video relighting that can accept background video, text prompts, or environment maps as relighting conditions. Trained on in-the-wild videos with carefully designed illumination augmentations and rendered videos under extreme dynamic lighting, RelightVid achieves arbitrary video relighting with high temporal consistency without intrinsic decomposition while preserving the illumination priors of its image backbone.","sentences":["Diffusion models have demonstrated remarkable success in image generation and editing, with recent advancements enabling albedo-preserving image relighting.","However, applying these models to video relighting remains challenging due to the lack of paired video relighting datasets and the high demands for output fidelity and temporal consistency, further complicated by the inherent randomness of diffusion models.","To address these challenges, we introduce RelightVid, a flexible framework for video relighting that can accept background video, text prompts, or environment maps as relighting conditions.","Trained on in-the-wild videos with carefully designed illumination augmentations and rendered videos under extreme dynamic lighting, RelightVid achieves arbitrary video relighting with high temporal consistency without intrinsic decomposition while preserving the illumination priors of its image backbone."],"url":"http://arxiv.org/abs/2501.16330v1"}
{"created":"2025-01-27 18:59:55","title":"sDREAMER: Self-distilled Mixture-of-Modality-Experts Transformer for Automatic Sleep Staging","abstract":"Automatic sleep staging based on electroencephalography (EEG) and electromyography (EMG) signals is an important aspect of sleep-related research. Current sleep staging methods suffer from two major drawbacks. First, there are limited information interactions between modalities in the existing methods. Second, current methods do not develop unified models that can handle different sources of input. To address these issues, we propose a novel sleep stage scoring model sDREAMER, which emphasizes cross-modality interaction and per-channel performance. Specifically, we develop a mixture-of-modality-expert (MoME) model with three pathways for EEG, EMG, and mixed signals with partially shared weights. We further propose a self-distillation training scheme for further information interaction across modalities. Our model is trained with multi-channel inputs and can make classifications on either single-channel or multi-channel inputs. Experiments demonstrate that our model outperforms the existing transformer-based sleep scoring methods for multi-channel inference. For single-channel inference, our model also outperforms the transformer-based models trained with single-channel signals.","sentences":["Automatic sleep staging based on electroencephalography (EEG) and electromyography (EMG) signals is an important aspect of sleep-related research.","Current sleep staging methods suffer from two major drawbacks.","First, there are limited information interactions between modalities in the existing methods.","Second, current methods do not develop unified models that can handle different sources of input.","To address these issues, we propose a novel sleep stage scoring model sDREAMER, which emphasizes cross-modality interaction and per-channel performance.","Specifically, we develop a mixture-of-modality-expert (MoME) model with three pathways for EEG, EMG, and mixed signals with partially shared weights.","We further propose a self-distillation training scheme for further information interaction across modalities.","Our model is trained with multi-channel inputs and can make classifications on either single-channel or multi-channel inputs.","Experiments demonstrate that our model outperforms the existing transformer-based sleep scoring methods for multi-channel inference.","For single-channel inference, our model also outperforms the transformer-based models trained with single-channel signals."],"url":"http://arxiv.org/abs/2501.16329v1"}
{"created":"2025-01-27 18:59:51","title":"LUCY: Linguistic Understanding and Control Yielding Early Stage of Her","abstract":"The film Her features Samantha, a sophisticated AI audio agent who is capable of understanding both linguistic and paralinguistic information in human speech and delivering real-time responses that are natural, informative and sensitive to emotional subtleties. Moving one step toward more sophisticated audio agent from recent advancement in end-to-end (E2E) speech systems, we propose LUCY, a E2E speech model that (1) senses and responds to user's emotion, (2) deliver responses in a succinct and natural style, and (3) use external tool to answer real-time inquiries. Experiment results show that LUCY is better at emotion control than peer models, generating emotional responses based on linguistic emotional instructions and responding to paralinguistic emotional cues. Lucy is also able to generate responses in a more natural style, as judged by external language models, without sacrificing much performance on general question answering. Finally, LUCY can leverage function calls to answer questions that are out of its knowledge scope.","sentences":["The film Her features Samantha, a sophisticated AI audio agent who is capable of understanding both linguistic and paralinguistic information in human speech and delivering real-time responses that are natural, informative and sensitive to emotional subtleties.","Moving one step toward more sophisticated audio agent from recent advancement in end-to-end (E2E) speech systems, we propose LUCY, a E2E speech model that (1) senses and responds to user's emotion, (2) deliver responses in a succinct and natural style, and (3) use external tool to answer real-time inquiries.","Experiment results show that LUCY is better at emotion control than peer models, generating emotional responses based on linguistic emotional instructions and responding to paralinguistic emotional cues.","Lucy is also able to generate responses in a more natural style, as judged by external language models, without sacrificing much performance on general question answering.","Finally, LUCY can leverage function calls to answer questions that are out of its knowledge scope."],"url":"http://arxiv.org/abs/2501.16327v1"}
{"created":"2025-01-27 18:59:02","title":"Movement- and Traffic-based User Identification in Commercial Virtual Reality Applications: Threats and Opportunities","abstract":"With the unprecedented diffusion of virtual reality, the number of application scenarios is continuously growing. As commercial and gaming applications become pervasive, the need for the secure and convenient identification of users, often overlooked by the research in immersive media, is becoming more and more pressing. Networked scenarios such as Cloud gaming or cooperative virtual training and teleoperation require both a user-friendly and streamlined experience and user privacy and security. In this work, we investigate the possibility of identifying users from their movement patterns and data traffic traces while playing four commercial games, using a publicly available dataset. If, on the one hand, this paves the way for easy identification and automatic customization of the virtual reality content, it also represents a serious threat to users' privacy due to network analysis-based fingerprinting. Based on this, we analyze the threats and opportunities for virtual reality users' security and privacy.","sentences":["With the unprecedented diffusion of virtual reality, the number of application scenarios is continuously growing.","As commercial and gaming applications become pervasive, the need for the secure and convenient identification of users, often overlooked by the research in immersive media, is becoming more and more pressing.","Networked scenarios such as Cloud gaming or cooperative virtual training and teleoperation require both a user-friendly and streamlined experience and user privacy and security.","In this work, we investigate the possibility of identifying users from their movement patterns and data traffic traces while playing four commercial games, using a publicly available dataset.","If, on the one hand, this paves the way for easy identification and automatic customization of the virtual reality content, it also represents a serious threat to users' privacy due to network analysis-based fingerprinting.","Based on this, we analyze the threats and opportunities for virtual reality users' security and privacy."],"url":"http://arxiv.org/abs/2501.16326v1"}
{"created":"2025-01-27 18:58:04","title":"Tailored Forecasting from Short Time Series via Meta-learning","abstract":"Machine learning (ML) models can be effective for forecasting the dynamics of unknown systems from time-series data, but they often require large amounts of data and struggle to generalize across systems with varying dynamics. Combined, these issues make forecasting from short time series particularly challenging. To address this problem, we introduce Meta-learning for Tailored Forecasting from Related Time Series (METAFORS), which uses related systems with longer time-series data to supplement limited data from the system of interest. By leveraging a library of models trained on related systems, METAFORS builds tailored models to forecast system evolution with limited data. Using a reservoir computing implementation and testing on simulated chaotic systems, we demonstrate METAFORS' ability to predict both short-term dynamics and long-term statistics, even when test and related systems exhibit significantly different behaviors and the available data are scarce, highlighting its robustness and versatility in data-limited scenarios.","sentences":["Machine learning (ML) models can be effective for forecasting the dynamics of unknown systems from time-series data, but they often require large amounts of data and struggle to generalize across systems with varying dynamics.","Combined, these issues make forecasting from short time series particularly challenging.","To address this problem, we introduce Meta-learning for Tailored Forecasting from Related Time Series (METAFORS), which uses related systems with longer time-series data to supplement limited data from the system of interest.","By leveraging a library of models trained on related systems, METAFORS builds tailored models to forecast system evolution with limited data.","Using a reservoir computing implementation and testing on simulated chaotic systems, we demonstrate METAFORS' ability to predict both short-term dynamics and long-term statistics, even when test and related systems exhibit significantly different behaviors and the available data are scarce, highlighting its robustness and versatility in data-limited scenarios."],"url":"http://arxiv.org/abs/2501.16325v1"}
{"created":"2025-01-27 18:56:22","title":"Implicit Bias in Matrix Factorization and its Explicit Realization in a New Architecture","abstract":"Gradient descent for matrix factorization is known to exhibit an implicit bias toward approximately low-rank solutions. While existing theories often assume the boundedness of iterates, empirically the bias persists even with unbounded sequences. We thus hypothesize that implicit bias is driven by divergent dynamics markedly different from the convergent dynamics for data fitting. Using this perspective, we introduce a new factorization model: $X\\approx UDV^\\top$, where $U$ and $V$ are constrained within norm balls, while $D$ is a diagonal factor allowing the model to span the entire search space. Our experiments reveal that this model exhibits a strong implicit bias regardless of initialization and step size, yielding truly (rather than approximately) low-rank solutions. Furthermore, drawing parallels between matrix factorization and neural networks, we propose a novel neural network model featuring constrained layers and diagonal components. This model achieves strong performance across various regression and classification tasks while finding low-rank solutions, resulting in efficient and lightweight networks.","sentences":["Gradient descent for matrix factorization is known to exhibit an implicit bias toward approximately low-rank solutions.","While existing theories often assume the boundedness of iterates, empirically the bias persists even with unbounded sequences.","We thus hypothesize that implicit bias is driven by divergent dynamics markedly different from the convergent dynamics for data fitting.","Using this perspective, we introduce a new factorization model: $X\\approx UDV^\\top$, where $U$ and $V$ are constrained within norm balls, while $D$ is a diagonal factor allowing the model to span the entire search space.","Our experiments reveal that this model exhibits a strong implicit bias regardless of initialization and step size, yielding truly (rather than approximately) low-rank solutions.","Furthermore, drawing parallels between matrix factorization and neural networks, we propose a novel neural network model featuring constrained layers and diagonal components.","This model achieves strong performance across various regression and classification tasks while finding low-rank solutions, resulting in efficient and lightweight networks."],"url":"http://arxiv.org/abs/2501.16322v1"}
{"created":"2025-01-27 18:55:21","title":"Adaptive Iterative Compression for High-Resolution Files: an Approach Focused on Preserving Visual Quality in Cinematic Workflows","abstract":"This study presents an iterative adaptive compression model for high-resolution DPX-derived TIFF files used in cinematographic workflows and digital preservation. The model employs SSIM and PSNR metrics to dynamically adjust compression parameters across three configurations (C0, C1, C2), achieving storage reductions up to 83.4 % while maintaining high visual fidelity (SSIM > 0.95). Validation across three diverse productions - black and white classic, soft-palette drama, and complex action film - demonstrated the method's effectiveness in preserving critical visual elements while significantly reducing storage requirements. Professional evaluators reported 90% acceptance rate for the optimal C1 configuration, with artifacts remaining below perceptual threshold in critical areas. Comparative analysis with JPEG2000 and H.265 showed superior quality preservation at equivalent compression rates, particularly for high bit-depth content. While requiring additional computational overhead, the method's storage benefits and quality control capabilities make it suitable for professional workflows, with potential applications in medical imaging and cloud storage optimization.","sentences":["This study presents an iterative adaptive compression model for high-resolution DPX-derived TIFF files used in cinematographic workflows and digital preservation.","The model employs SSIM and PSNR metrics to dynamically adjust compression parameters across three configurations (C0, C1, C2), achieving storage reductions up to 83.4 % while maintaining high visual fidelity (SSIM > 0.95).","Validation across three diverse productions - black and white classic, soft-palette drama, and complex action film - demonstrated the method's effectiveness in preserving critical visual elements while significantly reducing storage requirements.","Professional evaluators reported 90% acceptance rate for the optimal C1 configuration, with artifacts remaining below perceptual threshold in critical areas.","Comparative analysis with JPEG2000 and H.265 showed superior quality preservation at equivalent compression rates, particularly for high bit-depth content.","While requiring additional computational overhead, the method's storage benefits and quality control capabilities make it suitable for professional workflows, with potential applications in medical imaging and cloud storage optimization."],"url":"http://arxiv.org/abs/2501.16319v1"}
{"created":"2025-01-27 18:49:38","title":"LinPrim: Linear Primitives for Differentiable Volumetric Rendering","abstract":"Volumetric rendering has become central to modern novel view synthesis methods, which use differentiable rendering to optimize 3D scene representations directly from observed views. While many recent works build on NeRF or 3D Gaussians, we explore an alternative volumetric scene representation. More specifically, we introduce two new scene representations based on linear primitives-octahedra and tetrahedra-both of which define homogeneous volumes bounded by triangular faces. This formulation aligns naturally with standard mesh-based tools, minimizing overhead for downstream applications. To optimize these primitives, we present a differentiable rasterizer that runs efficiently on GPUs, allowing end-to-end gradient-based optimization while maintaining realtime rendering capabilities. Through experiments on real-world datasets, we demonstrate comparable performance to state-of-the-art volumetric methods while requiring fewer primitives to achieve similar reconstruction fidelity. Our findings provide insights into the geometry of volumetric rendering and suggest that adopting explicit polyhedra can expand the design space of scene representations.","sentences":["Volumetric rendering has become central to modern novel view synthesis methods, which use differentiable rendering to optimize 3D scene representations directly from observed views.","While many recent works build on NeRF or 3D Gaussians, we explore an alternative volumetric scene representation.","More specifically, we introduce two new scene representations based on linear primitives-octahedra and tetrahedra-both of which define homogeneous volumes bounded by triangular faces.","This formulation aligns naturally with standard mesh-based tools, minimizing overhead for downstream applications.","To optimize these primitives, we present a differentiable rasterizer that runs efficiently on GPUs, allowing end-to-end gradient-based optimization while maintaining realtime rendering capabilities.","Through experiments on real-world datasets, we demonstrate comparable performance to state-of-the-art volumetric methods while requiring fewer primitives to achieve similar reconstruction fidelity.","Our findings provide insights into the geometry of volumetric rendering and suggest that adopting explicit polyhedra can expand the design space of scene representations."],"url":"http://arxiv.org/abs/2501.16312v1"}
{"created":"2025-01-27 18:49:11","title":"A Modular Program-Transformation Framework for Reducing Specifications to Reachability","abstract":"Software verification is a complex problem, and verification tools need significant tuning to achieve high performance. Due to this, many verifiers choose to specialize on reachability properties, or invest the time to implement known transformations from the given specification to reachability on their internal representations. To improve this situation, we provide transformations as stand-alone components, modifying the input program instead of the internal representation, enabling their usage as a preprocessing step by other verifiers. This way, we separate two concerns: improving the performance of reachability analyses and implementing efficient transformations of arbitrary specifications to reachability. We implement the transformations in a framework that is based on instrumentation automata, inspired by the BLAST query language. In our initial study, we support three important concrete specifications for C programs: termination, no-overflow, and memory cleanup. Moreover, we discuss the broader expressiveness of our framework and show how general liveness properties can be transformed to reachability. We demonstrate the effectiveness and efficiency of our transformations by comparing verifiers that support the specifications natively with verifiers for reachability applied on the transformed programs. The results are very promising: Our transformations can extend existing verifiers to be effective on specifications that they do not support natively, and that the efficiency is often similar to verifiers that natively support the considered specifications.","sentences":["Software verification is a complex problem, and verification tools need significant tuning to achieve high performance.","Due to this, many verifiers choose to specialize on reachability properties, or invest the time to implement known transformations from the given specification to reachability on their internal representations.","To improve this situation, we provide transformations as stand-alone components, modifying the input program instead of the internal representation, enabling their usage as a preprocessing step by other verifiers.","This way, we separate two concerns: improving the performance of reachability analyses and implementing efficient transformations of arbitrary specifications to reachability.","We implement the transformations in a framework that is based on instrumentation automata, inspired by the BLAST query language.","In our initial study, we support three important concrete specifications for C programs: termination, no-overflow, and memory cleanup.","Moreover, we discuss the broader expressiveness of our framework and show how general liveness properties can be transformed to reachability.","We demonstrate the effectiveness and efficiency of our transformations by comparing verifiers that support the specifications natively with verifiers for reachability applied on the transformed programs.","The results are very promising: Our transformations can extend existing verifiers to be effective on specifications that they do not support natively, and that the efficiency is often similar to verifiers that natively support the considered specifications."],"url":"http://arxiv.org/abs/2501.16310v1"}
{"created":"2025-01-27 18:46:15","title":"Privacy-aware Nash Equilibrium Synthesis with Partially Ordered LTL$_f$ Objectives","abstract":"Nash equilibrium is a fundamental solution concept for modeling the behavior of self-interested agents. We develop an algorithm to synthesize pure Nash equilibria in two-player deterministic games on graphs where players have partial preferences over objectives expressed with linear temporal logic over finite traces. Previous approaches for Nash equilibrium synthesis assume that players' preferences are common knowledge. Instead, we allow players' preferences to remain private but enable communication between players. The algorithm we design synthesizes Nash equilibria for a complete-information game, but synthesizes these equilibria in an incomplete-information setting where players' preferences are private. The algorithm is privacy-aware, as instead of requiring that players share private preferences, the algorithm reduces the information sharing to a query interface. Through this interface, players exchange information about states in the game from which they can enforce a more desirable outcome. We prove the algorithm's completeness by showing that it either returns an equilibrium or certifies that one does not exist. We then demonstrate, via numerical examples, the existence of games where the queries the players exchange are insufficient to reconstruct players' preferences, highlighting the privacy-aware nature of the algorithm we propose.","sentences":["Nash equilibrium is a fundamental solution concept for modeling the behavior of self-interested agents.","We develop an algorithm to synthesize pure Nash equilibria in two-player deterministic games on graphs where players have partial preferences over objectives expressed with linear temporal logic over finite traces.","Previous approaches for Nash equilibrium synthesis assume that players' preferences are common knowledge.","Instead, we allow players' preferences to remain private but enable communication between players.","The algorithm we design synthesizes Nash equilibria for a complete-information game, but synthesizes these equilibria in an incomplete-information setting where players' preferences are private.","The algorithm is privacy-aware, as instead of requiring that players share private preferences, the algorithm reduces the information sharing to a query interface.","Through this interface, players exchange information about states in the game from which they can enforce a more desirable outcome.","We prove the algorithm's completeness by showing that it either returns an equilibrium or certifies that one does not exist.","We then demonstrate, via numerical examples, the existence of games where the queries the players exchange are insufficient to reconstruct players' preferences, highlighting the privacy-aware nature of the algorithm we propose."],"url":"http://arxiv.org/abs/2501.16307v1"}
{"created":"2025-01-27 18:45:39","title":"Exploring Data-Driven Advocacy in Home Health Care Work","abstract":"This paper explores opportunities and challenges for data-driven advocacy to support home care workers, an often overlooked group of low-wage, frontline health workers. First, we investigate what data to collect and how to collect it in ways that preserve privacy and avoid burdening workers. Second, we examine how workers and advocates could use collected data to strengthen individual and collective advocacy efforts. Our qualitative study with 11 workers and 15 advocates highlights tensions between workers' desires for individual and immediate benefits and advocates' preferences to prioritize more collective and long-term benefits. We also uncover discrepancies between participants' expectations for how data might transform advocacy and their on-the-ground experiences collecting and using real data. Finally, we discuss future directions for data-driven worker advocacy, including combining different kinds of data to ameliorate challenges, leveraging advocates as data stewards, and accounting for workers' and organizations' heterogeneous goals.","sentences":["This paper explores opportunities and challenges for data-driven advocacy to support home care workers, an often overlooked group of low-wage, frontline health workers.","First, we investigate what data to collect and how to collect it in ways that preserve privacy and avoid burdening workers.","Second, we examine how workers and advocates could use collected data to strengthen individual and collective advocacy efforts.","Our qualitative study with 11 workers and 15 advocates highlights tensions between workers' desires for individual and immediate benefits and advocates' preferences to prioritize more collective and long-term benefits.","We also uncover discrepancies between participants' expectations for how data might transform advocacy and their on-the-ground experiences collecting and using real data.","Finally, we discuss future directions for data-driven worker advocacy, including combining different kinds of data to ameliorate challenges, leveraging advocates as data stewards, and accounting for workers' and organizations' heterogeneous goals."],"url":"http://arxiv.org/abs/2501.16305v1"}
{"created":"2025-01-27 18:45:07","title":"RAPID: Retrieval-Augmented Parallel Inference Drafting for Text-Based Video Event Retrieval","abstract":"Retrieving events from videos using text queries has become increasingly challenging due to the rapid growth of multimedia content. Existing methods for text-based video event retrieval often focus heavily on object-level descriptions, overlooking the crucial role of contextual information. This limitation is especially apparent when queries lack sufficient context, such as missing location details or ambiguous background elements. To address these challenges, we propose a novel system called RAPID (Retrieval-Augmented Parallel Inference Drafting), which leverages advancements in Large Language Models (LLMs) and prompt-based learning to semantically correct and enrich user queries with relevant contextual information. These enriched queries are then processed through parallel retrieval, followed by an evaluation step to select the most relevant results based on their alignment with the original query. Through extensive experiments on our custom-developed dataset, we demonstrate that RAPID significantly outperforms traditional retrieval methods, particularly for contextually incomplete queries. Our system was validated for both speed and accuracy through participation in the Ho Chi Minh City AI Challenge 2024, where it successfully retrieved events from over 300 hours of video. Further evaluation comparing RAPID with the baseline proposed by the competition organizers demonstrated its superior effectiveness, highlighting the strength and robustness of our approach.","sentences":["Retrieving events from videos using text queries has become increasingly challenging due to the rapid growth of multimedia content.","Existing methods for text-based video event retrieval often focus heavily on object-level descriptions, overlooking the crucial role of contextual information.","This limitation is especially apparent when queries lack sufficient context, such as missing location details or ambiguous background elements.","To address these challenges, we propose a novel system called RAPID (Retrieval-Augmented Parallel Inference Drafting), which leverages advancements in Large Language Models (LLMs) and prompt-based learning to semantically correct and enrich user queries with relevant contextual information.","These enriched queries are then processed through parallel retrieval, followed by an evaluation step to select the most relevant results based on their alignment with the original query.","Through extensive experiments on our custom-developed dataset, we demonstrate that RAPID significantly outperforms traditional retrieval methods, particularly for contextually incomplete queries.","Our system was validated for both speed and accuracy through participation in the Ho Chi Minh City AI Challenge 2024, where it successfully retrieved events from over 300 hours of video.","Further evaluation comparing RAPID with the baseline proposed by the competition organizers demonstrated its superior effectiveness, highlighting the strength and robustness of our approach."],"url":"http://arxiv.org/abs/2501.16303v1"}
{"created":"2025-01-27 18:42:48","title":"Matryoshka Re-Ranker: A Flexible Re-Ranking Architecture With Configurable Depth and Width","abstract":"Large language models (LLMs) provide powerful foundations to perform fine-grained text re-ranking. However, they are often prohibitive in reality due to constraints on computation bandwidth. In this work, we propose a \\textbf{flexible} architecture called \\textbf{Matroyshka Re-Ranker}, which is designed to facilitate \\textbf{runtime customization} of model layers and sequence lengths at each layer based on users' configurations. Consequently, the LLM-based re-rankers can be made applicable across various real-world situations. The increased flexibility may come at the cost of precision loss. To address this problem, we introduce a suite of techniques to optimize the performance. First, we propose \\textbf{cascaded self-distillation}, where each sub-architecture learns to preserve a precise re-ranking performance from its super components, whose predictions can be exploited as smooth and informative teacher signals. Second, we design a \\textbf{factorized compensation mechanism}, where two collaborative Low-Rank Adaptation modules, vertical and horizontal, are jointly employed to compensate for the precision loss resulted from arbitrary combinations of layer and sequence compression. We perform comprehensive experiments based on the passage and document retrieval datasets from MSMARCO, along with all public datasets from BEIR benchmark. In our experiments, Matryoshka Re-Ranker substantially outperforms the existing methods, while effectively preserving its superior performance across various forms of compression and different application scenarios.","sentences":["Large language models (LLMs) provide powerful foundations to perform fine-grained text re-ranking.","However, they are often prohibitive in reality due to constraints on computation bandwidth.","In this work, we propose a \\textbf{flexible} architecture called \\textbf{Matroyshka Re-Ranker}, which is designed to facilitate \\textbf{runtime customization} of model layers and sequence lengths at each layer based on users' configurations.","Consequently, the LLM-based re-rankers can be made applicable across various real-world situations.","The increased flexibility may come at the cost of precision loss.","To address this problem, we introduce a suite of techniques to optimize the performance.","First, we propose \\textbf{cascaded self-distillation}, where each sub-architecture learns to preserve a precise re-ranking performance from its super components, whose predictions can be exploited as smooth and informative teacher signals.","Second, we design a \\textbf{factorized compensation mechanism}, where two collaborative Low-Rank Adaptation modules, vertical and horizontal, are jointly employed to compensate for the precision loss resulted from arbitrary combinations of layer and sequence compression.","We perform comprehensive experiments based on the passage and document retrieval datasets from MSMARCO, along with all public datasets from BEIR benchmark.","In our experiments, Matryoshka Re-Ranker substantially outperforms the existing methods, while effectively preserving its superior performance across various forms of compression and different application scenarios."],"url":"http://arxiv.org/abs/2501.16302v1"}
{"created":"2025-01-27 18:38:36","title":"Large Models in Dialogue for Active Perception and Anomaly Detection","abstract":"Autonomous aerial monitoring is an important task aimed at gathering information from areas that may not be easily accessible by humans. At the same time, this task often requires recognizing anomalies from a significant distance or not previously encountered in the past. In this paper, we propose a novel framework that leverages the advanced capabilities provided by Large Language Models (LLMs) to actively collect information and perform anomaly detection in novel scenes. To this end, we propose an LLM based model dialogue approach, in which two deep learning models engage in a dialogue to actively control a drone to increase perception and anomaly detection accuracy. We conduct our experiments in a high fidelity simulation environment where an LLM is provided with a predetermined set of natural language movement commands mapped into executable code functions. Additionally, we deploy a multimodal Visual Question Answering (VQA) model charged with the task of visual question answering and captioning. By engaging the two models in conversation, the LLM asks exploratory questions while simultaneously flying a drone into different parts of the scene, providing a novel way to implement active perception. By leveraging LLMs reasoning ability, we output an improved detailed description of the scene going beyond existing static perception approaches. In addition to information gathering, our approach is utilized for anomaly detection and our results demonstrate the proposed methods effectiveness in informing and alerting about potential hazards.","sentences":["Autonomous aerial monitoring is an important task aimed at gathering information from areas that may not be easily accessible by humans.","At the same time, this task often requires recognizing anomalies from a significant distance or not previously encountered in the past.","In this paper, we propose a novel framework that leverages the advanced capabilities provided by Large Language Models (LLMs) to actively collect information and perform anomaly detection in novel scenes.","To this end, we propose an LLM based model dialogue approach, in which two deep learning models engage in a dialogue to actively control a drone to increase perception and anomaly detection accuracy.","We conduct our experiments in a high fidelity simulation environment where an LLM is provided with a predetermined set of natural language movement commands mapped into executable code functions.","Additionally, we deploy a multimodal Visual Question Answering (VQA) model charged with the task of visual question answering and captioning.","By engaging the two models in conversation, the LLM asks exploratory questions while simultaneously flying a drone into different parts of the scene, providing a novel way to implement active perception.","By leveraging LLMs reasoning ability, we output an improved detailed description of the scene going beyond existing static perception approaches.","In addition to information gathering, our approach is utilized for anomaly detection and our results demonstrate the proposed methods effectiveness in informing and alerting about potential hazards."],"url":"http://arxiv.org/abs/2501.16300v1"}
{"created":"2025-01-27 18:36:46","title":"Uncoded Download in Lagrange-Coded Elastic Computing with Straggler Tolerance","abstract":"Coded elastic computing, introduced by Yang et al. in 2018, is a technique designed to mitigate the impact of elasticity in cloud computing systems, where machines can be preempted or be added during computing rounds. This approach utilizes maximum distance separable (MDS) coding for both storage and download in matrix-matrix multiplications. The proposed scheme is unable to tolerate stragglers and has high encoding complexity and upload cost. In 2023, we addressed these limitations by employing uncoded storage and Lagrange-coded download. However, it results in a large storage size. To address the challenges of storage size and upload cost, in this paper, we focus on Lagrange-coded elastic computing based on uncoded download. We propose a new class of elastic computing schemes, using Lagrange-coded storage with uncoded download (LCSUD). Our proposed schemes address both elasticity and straggler challenges while achieving lower storage size, reduced encoding complexity, and upload cost compared to existing methods.","sentences":["Coded elastic computing, introduced by Yang et al. in 2018, is a technique designed to mitigate the impact of elasticity in cloud computing systems, where machines can be preempted or be added during computing rounds.","This approach utilizes maximum distance separable (MDS) coding for both storage and download in matrix-matrix multiplications.","The proposed scheme is unable to tolerate stragglers and has high encoding complexity and upload cost.","In 2023, we addressed these limitations by employing uncoded storage and Lagrange-coded download.","However, it results in a large storage size.","To address the challenges of storage size and upload cost, in this paper, we focus on Lagrange-coded elastic computing based on uncoded download.","We propose a new class of elastic computing schemes, using Lagrange-coded storage with uncoded download (LCSUD).","Our proposed schemes address both elasticity and straggler challenges while achieving lower storage size, reduced encoding complexity, and upload cost compared to existing methods."],"url":"http://arxiv.org/abs/2501.16298v1"}
{"created":"2025-01-27 18:36:10","title":"FALCON: Resolving Visual Redundancy and Fragmentation in High-resolution Multimodal Large Language Models via Visual Registers","abstract":"The incorporation of high-resolution visual input equips multimodal large language models (MLLMs) with enhanced visual perception capabilities for real-world tasks. However, most existing high-resolution MLLMs rely on a cropping-based approach to process images, which leads to fragmented visual encoding and a sharp increase in redundant tokens. To tackle these issues, we propose the FALCON model. FALCON introduces a novel visual register technique to simultaneously: 1) Eliminate redundant tokens at the stage of visual encoding. To directly address the visual redundancy present in the output of vision encoder, we propose a Register-based Representation Compacting (ReCompact) mechanism. This mechanism introduces a set of learnable visual registers designed to adaptively aggregate essential information while discarding redundancy. It enables the encoder to produce a more compact visual representation with a minimal number of output tokens, thus eliminating the need for an additional compression module. 2) Ensure continuity in visual encoding. To address the potential encoding errors caused by fragmented visual inputs, we develop a Register Interactive Attention (ReAtten) module. This module facilitates effective and efficient information exchange across sub-images by enabling interactions between visual registers. It ensures the continuity of visual semantics throughout the encoding. We conduct comprehensive experiments with FALCON on high-resolution benchmarks across a wide range of scenarios. FALCON demonstrates superior performance with a remarkable 9-fold and 16-fold reduction in visual tokens.","sentences":["The incorporation of high-resolution visual input equips multimodal large language models (MLLMs) with enhanced visual perception capabilities for real-world tasks.","However, most existing high-resolution MLLMs rely on a cropping-based approach to process images, which leads to fragmented visual encoding and a sharp increase in redundant tokens.","To tackle these issues, we propose the FALCON model.","FALCON introduces a novel visual register technique to simultaneously: 1) Eliminate redundant tokens at the stage of visual encoding.","To directly address the visual redundancy present in the output of vision encoder, we propose a Register-based Representation Compacting (ReCompact) mechanism.","This mechanism introduces a set of learnable visual registers designed to adaptively aggregate essential information while discarding redundancy.","It enables the encoder to produce a more compact visual representation with a minimal number of output tokens, thus eliminating the need for an additional compression module.","2) Ensure continuity in visual encoding.","To address the potential encoding errors caused by fragmented visual inputs, we develop a Register Interactive Attention (ReAtten) module.","This module facilitates effective and efficient information exchange across sub-images by enabling interactions between visual registers.","It ensures the continuity of visual semantics throughout the encoding.","We conduct comprehensive experiments with FALCON on high-resolution benchmarks across a wide range of scenarios.","FALCON demonstrates superior performance with a remarkable 9-fold and 16-fold reduction in visual tokens."],"url":"http://arxiv.org/abs/2501.16297v1"}
{"created":"2025-01-27 18:35:33","title":"Entanglement-Assisted Coding for Arbitrary Linear Computations Over a Quantum MAC","abstract":"We study a linear computation problem over a quantum multiple access channel (LC-QMAC), where $S$ servers share an entangled state and separately store classical data streams $W_1,\\cdots, W_S$ over a finite field $\\mathbb{F}_d$. A user aims to compute $K$ linear combinations of these data streams, represented as $Y = \\mathbf{V}_1 W_1 + \\mathbf{V}_2 W_2 + \\cdots + \\mathbf{V}_S W_S \\in \\mathbb{F}_d^{K \\times 1}$. To this end, each server encodes its classical information into its local quantum subsystem and transmits it to the user, who retrieves the desired computations via quantum measurements. In this work, we propose an achievable scheme for LC-QMAC based on the stabilizer formalism and the ideas from entanglement-assisted quantum error-correcting codes (EAQECC). Specifically, given any linear computation matrix, we construct a self-orthogonal matrix that can be implemented using the stabilizer formalism. Also, we apply precoding matrices to minimize the number of auxiliary qudits required. Our scheme achieves more computations per qudit, i.e., a higher computation rate, compared to the best-known methods in the literature, and attains the capacity in certain cases.","sentences":["We study a linear computation problem over a quantum multiple access channel (LC-QMAC), where $S$ servers share an entangled state and separately store classical data streams $W_1,\\cdots, W_S$ over a finite field $\\mathbb{F}_d$. A user aims to compute $K$ linear combinations of these data streams, represented as $Y = \\mathbf{V}_1 W_1 + \\mathbf{V}_2 W_2 + \\cdots + \\mathbf{V}_S W_S \\in \\mathbb{F}_d^{K","\\times 1}$.","To this end, each server encodes its classical information into its local quantum subsystem and transmits it to the user, who retrieves the desired computations via quantum measurements.","In this work, we propose an achievable scheme for LC-QMAC based on the stabilizer formalism and the ideas from entanglement-assisted quantum error-correcting codes (EAQECC).","Specifically, given any linear computation matrix, we construct a self-orthogonal matrix that can be implemented using the stabilizer formalism.","Also, we apply precoding matrices to minimize the number of auxiliary qudits required.","Our scheme achieves more computations per qudit, i.e., a higher computation rate, compared to the best-known methods in the literature, and attains the capacity in certain cases."],"url":"http://arxiv.org/abs/2501.16296v1"}
{"created":"2025-01-27 18:35:05","title":"Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity","abstract":"State Space Models (SSMs) have emerged as efficient alternatives to Transformers for sequential modeling, but their inability to leverage modality-specific features limits their performance in multi-modal pretraining. Here, we propose Mixture-of-Mamba, a novel SSM architecture that introduces modality-aware sparsity through modality-specific parameterization of the Mamba block. Building on Mixture-of-Transformers (W. Liang et al. arXiv:2411.04996; 2024), we extend the benefits of modality-aware sparsity to SSMs while preserving their computational efficiency. We evaluate Mixture-of-Mamba across three multi-modal pretraining settings: Transfusion (interleaved text and continuous image tokens with diffusion loss), Chameleon (interleaved text and discrete image tokens), and an extended three-modality framework incorporating speech. Mixture-of-Mamba consistently reaches the same loss values at earlier training steps with significantly reduced computational costs. In the Transfusion setting, Mixture-of-Mamba achieves equivalent image loss using only 34.76% of the training FLOPs at the 1.4B scale. In the Chameleon setting, Mixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs at the 1.4B scale, and similar text loss with just 65.40% of the FLOPs. In the three-modality setting, MoM matches speech loss at 24.80% of the FLOPs at the 1.4B scale. Our ablation study highlights the synergistic effects of decoupling projection components, where joint decoupling yields greater gains than individual modifications. These results establish modality-aware sparsity as a versatile and effective design principle, extending its impact from Transformers to SSMs and setting new benchmarks in multi-modal pretraining. Our code can be accessed at https://github.com/Weixin-Liang/Mixture-of-Mamba","sentences":["State Space Models (SSMs) have emerged as efficient alternatives to Transformers for sequential modeling, but their inability to leverage modality-specific features limits their performance in multi-modal pretraining.","Here, we propose Mixture-of-Mamba, a novel SSM architecture that introduces modality-aware sparsity through modality-specific parameterization of the Mamba block.","Building on Mixture-of-Transformers (W. Liang et al. arXiv:2411.04996; 2024), we extend the benefits of modality-aware sparsity to SSMs while preserving their computational efficiency.","We evaluate Mixture-of-Mamba across three multi-modal pretraining settings: Transfusion (interleaved text and continuous image tokens with diffusion loss), Chameleon (interleaved text and discrete image tokens), and an extended three-modality framework incorporating speech.","Mixture-of-Mamba consistently reaches the same loss values at earlier training steps with significantly reduced computational costs.","In the Transfusion setting, Mixture-of-Mamba achieves equivalent image loss using only 34.76% of the training FLOPs at the 1.4B scale.","In the Chameleon setting, Mixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs at the 1.4B scale, and similar text loss with just 65.40% of the FLOPs.","In the three-modality setting, MoM matches speech loss at 24.80% of the FLOPs at the 1.4B scale.","Our ablation study highlights the synergistic effects of decoupling projection components, where joint decoupling yields greater gains than individual modifications.","These results establish modality-aware sparsity as a versatile and effective design principle, extending its impact from Transformers to SSMs and setting new benchmarks in multi-modal pretraining.","Our code can be accessed at https://github.com/Weixin-Liang/Mixture-of-Mamba"],"url":"http://arxiv.org/abs/2501.16295v1"}
{"created":"2025-01-27 18:28:15","title":"Sequential Decision Making in Stochastic Games with Incomplete Preferences over Temporal Objectives","abstract":"Ensuring that AI systems make strategic decisions aligned with the specified preferences in adversarial sequential interactions is a critical challenge for developing trustworthy AI systems, especially when the environment is stochastic and players' incomplete preferences leave some outcomes unranked. We study the problem of synthesizing preference-satisfying strategies in two-player stochastic games on graphs where players have opposite (possibly incomplete) preferences over a set of temporal goals. We represent these goals using linear temporal logic over finite traces (LTLf), which enables modeling the nuances of human preferences where temporal goals need not be mutually exclusive and comparison between some goals may be unspecified. We introduce a solution concept of non-dominated almost-sure winning, which guarantees to achieve a most preferred outcome aligned with specified preferences while maintaining robustness against the adversarial behaviors of the opponent. Our results show that strategy profiles based on this concept are Nash equilibria in the game where players are risk-averse, thus providing a practical framework for evaluating and ensuring stable, preference-aligned outcomes in the game. Using a drone delivery example, we demonstrate that our contributions offer valuable insights not only for synthesizing rational behavior under incomplete preferences but also for designing games that motivate the desired behavior from the players in adversarial conditions.","sentences":["Ensuring that AI systems make strategic decisions aligned with the specified preferences in adversarial sequential interactions is a critical challenge for developing trustworthy AI systems, especially when the environment is stochastic and players' incomplete preferences leave some outcomes unranked.","We study the problem of synthesizing preference-satisfying strategies in two-player stochastic games on graphs where players have opposite (possibly incomplete) preferences over a set of temporal goals.","We represent these goals using linear temporal logic over finite traces (LTLf), which enables modeling the nuances of human preferences where temporal goals need not be mutually exclusive and comparison between some goals may be unspecified.","We introduce a solution concept of non-dominated almost-sure winning, which guarantees to achieve a most preferred outcome aligned with specified preferences while maintaining robustness against the adversarial behaviors of the opponent.","Our results show that strategy profiles based on this concept are Nash equilibria in the game where players are risk-averse, thus providing a practical framework for evaluating and ensuring stable, preference-aligned outcomes in the game.","Using a drone delivery example, we demonstrate that our contributions offer valuable insights not only for synthesizing rational behavior under incomplete preferences but also for designing games that motivate the desired behavior from the players in adversarial conditions."],"url":"http://arxiv.org/abs/2501.16291v1"}
{"created":"2025-01-27 18:25:35","title":"Multi-view Structural Convolution Network for Domain-Invariant Point Cloud Recognition of Autonomous Vehicles","abstract":"Point cloud representation has recently become a research hotspot in the field of computer vision and has been utilized for autonomous vehicles. However, adapting deep learning networks for point cloud data recognition is challenging due to the variability in datasets and sensor technologies. This variability underscores the necessity for adaptive techniques to maintain accuracy under different conditions. In this paper, we present the Multi-View Structural Convolution Network (MSCN) designed for domain-invariant point cloud recognition. MSCN comprises Structural Convolution Layers (SCL) that extract local context geometric features from point clouds and Structural Aggregation Layers (SAL) that extract and aggregate both local and overall context features from point clouds. Additionally, our MSCN enhances feature representation robustness by training with unseen domain point clouds derived from source domain point clouds. This method acquires domain-invariant features and exhibits robust, consistent performance across various point cloud datasets, ensuring compatibility with diverse sensor configurations without the need for parameter adjustments. This highlights MSCN's potential to significantly improve the reliability and domain invariant features in different environments. Our code is available at https://github.com/MLMLab/MSCN.","sentences":["Point cloud representation has recently become a research hotspot in the field of computer vision and has been utilized for autonomous vehicles.","However, adapting deep learning networks for point cloud data recognition is challenging due to the variability in datasets and sensor technologies.","This variability underscores the necessity for adaptive techniques to maintain accuracy under different conditions.","In this paper, we present the Multi-View Structural Convolution Network (MSCN) designed for domain-invariant point cloud recognition.","MSCN comprises Structural Convolution Layers (SCL) that extract local context geometric features from point clouds and Structural Aggregation Layers (SAL) that extract and aggregate both local and overall context features from point clouds.","Additionally, our MSCN enhances feature representation robustness by training with unseen domain point clouds derived from source domain point clouds.","This method acquires domain-invariant features and exhibits robust, consistent performance across various point cloud datasets, ensuring compatibility with diverse sensor configurations without the need for parameter adjustments.","This highlights MSCN's potential to significantly improve the reliability and domain invariant features in different environments.","Our code is available at https://github.com/MLMLab/MSCN."],"url":"http://arxiv.org/abs/2501.16289v1"}
{"created":"2025-01-27 18:25:04","title":"Upside Down Reinforcement Learning with Policy Generators","abstract":"Upside Down Reinforcement Learning (UDRL) is a promising framework for solving reinforcement learning problems which focuses on learning command-conditioned policies. In this work, we extend UDRL to the task of learning a command-conditioned generator of deep neural network policies. We accomplish this using Hypernetworks - a variant of Fast Weight Programmers, which learn to decode input commands representing a desired expected return into command-specific weight matrices. Our method, dubbed Upside Down Reinforcement Learning with Policy Generators (UDRLPG), streamlines comparable techniques by removing the need for an evaluator or critic to update the weights of the generator. To counteract the increased variance in last returns caused by not having an evaluator, we decouple the sampling probability of the buffer from the absolute number of policies in it, which, together with a simple weighting strategy, improves the empirical convergence of the algorithm. Compared with existing algorithms, UDRLPG achieves competitive performance and high returns, sometimes outperforming more complex architectures. Our experiments show that a trained generator can generalize to create policies that achieve unseen returns zero-shot. The proposed method appears to be effective in mitigating some of the challenges associated with learning highly multimodal functions. Altogether, we believe that UDRLPG represents a promising step forward in achieving greater empirical sample efficiency in RL. A full implementation of UDRLPG is publicly available at https://github.com/JacopoD/udrlpg_","sentences":["Upside Down Reinforcement Learning (UDRL) is a promising framework for solving reinforcement learning problems which focuses on learning command-conditioned policies.","In this work, we extend UDRL to the task of learning a command-conditioned generator of deep neural network policies.","We accomplish this using Hypernetworks - a variant of Fast Weight Programmers, which learn to decode input commands representing a desired expected return into command-specific weight matrices.","Our method, dubbed Upside Down Reinforcement Learning with Policy Generators (UDRLPG), streamlines comparable techniques by removing the need for an evaluator or critic to update the weights of the generator.","To counteract the increased variance in last returns caused by not having an evaluator, we decouple the sampling probability of the buffer from the absolute number of policies in it, which, together with a simple weighting strategy, improves the empirical convergence of the algorithm.","Compared with existing algorithms, UDRLPG achieves competitive performance and high returns, sometimes outperforming more complex architectures.","Our experiments show that a trained generator can generalize to create policies that achieve unseen returns zero-shot.","The proposed method appears to be effective in mitigating some of the challenges associated with learning highly multimodal functions.","Altogether, we believe that UDRLPG represents a promising step forward in achieving greater empirical sample efficiency in RL.","A full implementation of UDRLPG is publicly available at https://github.com/JacopoD/udrlpg_"],"url":"http://arxiv.org/abs/2501.16288v1"}
{"created":"2025-01-27 18:23:59","title":"A Unified Representation of Density-Power-Based Divergences Reducible to M-Estimation","abstract":"Density-power-based divergences are known to provide robust inference procedures against outliers, and their extensions have been widely studied. A characteristic of successful divergences is that the estimation problem can be reduced to M-estimation. In this paper, we define a norm-based Bregman density power divergence (NB-DPD) -- density-power-based divergence with functional flexibility within the framework of Bregman divergences that can be reduced to M-estimation. We show that, by specifying the function $\\phi_\\gamma$, NB-DPD reduces to well-known divergences, such as the density power divergence and the $\\gamma$-divergence. Furthermore, by examining the combinations of functions $\\phi_\\gamma$ corresponding to existing divergences, we show that a new divergence connecting these existing divergences can be derived. Finally, we show that the redescending property, one of the key indicators of robustness, holds only for the $\\gamma$-divergence.","sentences":["Density-power-based divergences are known to provide robust inference procedures against outliers, and their extensions have been widely studied.","A characteristic of successful divergences is that the estimation problem can be reduced to M-estimation.","In this paper, we define a norm-based Bregman density power divergence (NB-DPD) -- density-power-based divergence with functional flexibility within the framework of Bregman divergences that can be reduced to M-estimation.","We show that, by specifying the function $\\phi_\\gamma$, NB-DPD reduces to well-known divergences, such as the density power divergence and the $\\gamma$-divergence.","Furthermore, by examining the combinations of functions $\\phi_\\gamma$ corresponding to existing divergences, we show that a new divergence connecting these existing divergences can be derived.","Finally, we show that the redescending property, one of the key indicators of robustness, holds only for the $\\gamma$-divergence."],"url":"http://arxiv.org/abs/2501.16287v1"}
{"created":"2025-01-27 18:11:06","title":"Do LLMs Have Visualization Literacy? An Evaluation on Modified Visualizations to Test Generalization in Data Interpretation","abstract":"In this paper, we assess the visualization literacy of two prominent Large Language Models (LLMs): OpenAI's Generative Pretrained Transformers (GPT), the backend of ChatGPT, and Google's Gemini, previously known as Bard, to establish benchmarks for assessing their visualization capabilities. While LLMs have shown promise in generating chart descriptions, captions, and design suggestions, their potential for evaluating visualizations remains under-explored. Collecting data from humans for evaluations has been a bottleneck for visualization research in terms of both time and money, and if LLMs were able to serve, even in some limited role, as evaluators, they could be a significant resource. To investigate the feasibility of using LLMs in the visualization evaluation process, we explore the extent to which LLMs possess visualization literacy -- a crucial factor for their effective utility in the field. We conducted a series of experiments using a modified 53-item Visualization Literacy Assessment Test (VLAT) for GPT-4 and Gemini. Our findings indicate that the LLMs we explored currently fail to achieve the same levels of visualization literacy when compared to data from the general public reported in VLAT, and LLMs heavily relied on their pre-existing knowledge to answer questions instead of utilizing the information provided by the visualization when answering questions.","sentences":["In this paper, we assess the visualization literacy of two prominent Large Language Models (LLMs): OpenAI's Generative Pretrained Transformers (GPT), the backend of ChatGPT, and Google's Gemini, previously known as Bard, to establish benchmarks for assessing their visualization capabilities.","While LLMs have shown promise in generating chart descriptions, captions, and design suggestions, their potential for evaluating visualizations remains under-explored.","Collecting data from humans for evaluations has been a bottleneck for visualization research in terms of both time and money, and if LLMs were able to serve, even in some limited role, as evaluators, they could be a significant resource.","To investigate the feasibility of using LLMs in the visualization evaluation process, we explore the extent to which LLMs possess visualization literacy -- a crucial factor for their effective utility in the field.","We conducted a series of experiments using a modified 53-item Visualization Literacy Assessment Test (VLAT) for GPT-4 and Gemini.","Our findings indicate that the LLMs we explored currently fail to achieve the same levels of visualization literacy when compared to data from the general public reported in VLAT, and LLMs heavily relied on their pre-existing knowledge to answer questions instead of utilizing the information provided by the visualization when answering questions."],"url":"http://arxiv.org/abs/2501.16277v1"}
{"created":"2025-01-27 18:10:34","title":"URAG: Implementing a Unified Hybrid RAG for Precise Answers in University Admission Chatbots -- A Case Study at HCMUT","abstract":"With the rapid advancement of Artificial Intelligence, particularly in Natural Language Processing, Large Language Models (LLMs) have become pivotal in educational question-answering systems, especially university admission chatbots. Concepts such as Retrieval-Augmented Generation (RAG) and other advanced techniques have been developed to enhance these systems by integrating specific university data, enabling LLMs to provide informed responses on admissions and academic counseling. However, these enhanced RAG techniques often involve high operational costs and require the training of complex, specialized modules, which poses challenges for practical deployment. Additionally, in the educational context, it is crucial to provide accurate answers to prevent misinformation, a task that LLM-based systems find challenging without appropriate strategies and methods. In this paper, we introduce the Unified RAG (URAG) Framework, a hybrid approach that significantly improves the accuracy of responses, particularly for critical queries. Experimental results demonstrate that URAG enhances our in-house, lightweight model to perform comparably to state-of-the-art commercial models. Moreover, to validate its practical applicability, we conducted a case study at our educational institution, which received positive feedback and acclaim. This study not only proves the effectiveness of URAG but also highlights its feasibility for real-world implementation in educational settings.","sentences":["With the rapid advancement of Artificial Intelligence, particularly in Natural Language Processing, Large Language Models (LLMs) have become pivotal in educational question-answering systems, especially university admission chatbots.","Concepts such as Retrieval-Augmented Generation (RAG) and other advanced techniques have been developed to enhance these systems by integrating specific university data, enabling LLMs to provide informed responses on admissions and academic counseling.","However, these enhanced RAG techniques often involve high operational costs and require the training of complex, specialized modules, which poses challenges for practical deployment.","Additionally, in the educational context, it is crucial to provide accurate answers to prevent misinformation, a task that LLM-based systems find challenging without appropriate strategies and methods.","In this paper, we introduce the Unified RAG (URAG) Framework, a hybrid approach that significantly improves the accuracy of responses, particularly for critical queries.","Experimental results demonstrate that URAG enhances our in-house, lightweight model to perform comparably to state-of-the-art commercial models.","Moreover, to validate its practical applicability, we conducted a case study at our educational institution, which received positive feedback and acclaim.","This study not only proves the effectiveness of URAG but also highlights its feasibility for real-world implementation in educational settings."],"url":"http://arxiv.org/abs/2501.16276v1"}
{"created":"2025-01-27 18:06:48","title":"What is Formal Verification without Specifications? A Survey on mining LTL Specifications","abstract":"Virtually all verification techniques using formal methods rely on the availability of a formal specification, which describes the design requirements precisely. However, formulating specifications remains a manual task that is notoriously challenging and error-prone. To address this bottleneck in formal verification, recent research has thus focussed on automatically generating specifications for formal verification from examples of (desired and undesired) system behavior. In this survey, we list and compare recent advances in mining specifications in Linear Temporal Logic (LTL), the de facto standard specification language for reactive systems. Several approaches have been designed for learning LTL formulas, which address different aspects and settings of specification design. Moreover, the approaches rely on a diverse range of techniques such as constraint solving, neural network training, enumerative search, etc. We survey the current state-of-the-art techniques and compare them for the convenience of the formal methods practitioners.","sentences":["Virtually all verification techniques using formal methods rely on the availability of a formal specification, which describes the design requirements precisely.","However, formulating specifications remains a manual task that is notoriously challenging and error-prone.","To address this bottleneck in formal verification, recent research has thus focussed on automatically generating specifications for formal verification from examples of (desired and undesired) system behavior.","In this survey, we list and compare recent advances in mining specifications in Linear Temporal Logic (LTL), the de facto standard specification language for reactive systems.","Several approaches have been designed for learning LTL formulas, which address different aspects and settings of specification design.","Moreover, the approaches rely on a diverse range of techniques such as constraint solving, neural network training, enumerative search, etc.","We survey the current state-of-the-art techniques and compare them for the convenience of the formal methods practitioners."],"url":"http://arxiv.org/abs/2501.16274v1"}
{"created":"2025-01-27 18:06:36","title":"Return of the Encoder: Maximizing Parameter Efficiency for SLMs","abstract":"The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing. For small language models (SLMs) - those with 1 billion parameters or fewer - our systematic analysis across GPU, CPU, and NPU platforms reveals that encoder-decoder architectures achieve 47% lower first-token latency and 4.7x higher throughput compared to decoder-only models on edge devices. These gains may be attributed to encoder-decoder's one-time input processing and efficient separation of understanding and generation phases.   We introduce a novel knowledge distillation framework that enables encoder-decoder models to leverage capabilities from large scalable decoder-only teachers while preserving their architectural advantages, achieving up to 6 average performance points improvement across diverse tasks, with significant gains in asymmetric sequence tasks where input and output distributions can benefit from different processing approaches.   When combined with modern advances like Rotary Positional Embeddings (RoPE) and Vision encoders, our systematic investigation demonstrates that encoder-decoder architectures provide a more practical path toward deploying capable language models in resource-constrained environments. Our findings challenge the prevailing trend toward decoder-only scaling, showing that architectural choices become increasingly crucial as parameter budgets decrease, particularly for on-device and edge deployments where computational efficiency is paramount.","sentences":["The dominance of large decoder-only language models has overshadowed encoder-decoder architectures, despite their fundamental efficiency advantages in sequence processing.","For small language models (SLMs) - those with 1 billion parameters or fewer - our systematic analysis across GPU, CPU, and NPU platforms reveals that encoder-decoder architectures achieve 47% lower first-token latency and 4.7x higher throughput compared to decoder-only models on edge devices.","These gains may be attributed to encoder-decoder's one-time input processing and efficient separation of understanding and generation phases.   ","We introduce a novel knowledge distillation framework that enables encoder-decoder models to leverage capabilities from large scalable decoder-only teachers while preserving their architectural advantages, achieving up to 6 average performance points improvement across diverse tasks, with significant gains in asymmetric sequence tasks where input and output distributions can benefit from different processing approaches.   ","When combined with modern advances like Rotary Positional Embeddings (RoPE) and Vision encoders, our systematic investigation demonstrates that encoder-decoder architectures provide a more practical path toward deploying capable language models in resource-constrained environments.","Our findings challenge the prevailing trend toward decoder-only scaling, showing that architectural choices become increasingly crucial as parameter budgets decrease, particularly for on-device and edge deployments where computational efficiency is paramount."],"url":"http://arxiv.org/abs/2501.16273v1"}
{"created":"2025-01-27 18:05:28","title":"From Molecules to Mixtures: Learning Representations of Olfactory Mixture Similarity using Inductive Biases","abstract":"Olfaction -- how molecules are perceived as odors to humans -- remains poorly understood. Recently, the principal odor map (POM) was introduced to digitize the olfactory properties of single compounds. However, smells in real life are not pure single molecules, but complex mixtures of molecules, whose representations remain relatively under-explored. In this work, we introduce POMMix, an extension of the POM to represent mixtures. Our representation builds upon the symmetries of the problem space in a hierarchical manner: (1) graph neural networks for building molecular embeddings, (2) attention mechanisms for aggregating molecular representations into mixture representations, and (3) cosine prediction heads to encode olfactory perceptual distance in the mixture embedding space. POMMix achieves state-of-the-art predictive performance across multiple datasets. We also evaluate the generalizability of the representation on multiple splits when applied to unseen molecules and mixture sizes. Our work advances the effort to digitize olfaction, and highlights the synergy of domain expertise and deep learning in crafting expressive representations in low-data regimes.","sentences":["Olfaction -- how molecules are perceived as odors to humans -- remains poorly understood.","Recently, the principal odor map (POM) was introduced to digitize the olfactory properties of single compounds.","However, smells in real life are not pure single molecules, but complex mixtures of molecules, whose representations remain relatively under-explored.","In this work, we introduce POMMix, an extension of the POM to represent mixtures.","Our representation builds upon the symmetries of the problem space in a hierarchical manner: (1) graph neural networks for building molecular embeddings, (2) attention mechanisms for aggregating molecular representations into mixture representations, and (3) cosine prediction heads to encode olfactory perceptual distance in the mixture embedding space.","POMMix achieves state-of-the-art predictive performance across multiple datasets.","We also evaluate the generalizability of the representation on multiple splits when applied to unseen molecules and mixture sizes.","Our work advances the effort to digitize olfaction, and highlights the synergy of domain expertise and deep learning in crafting expressive representations in low-data regimes."],"url":"http://arxiv.org/abs/2501.16271v1"}
{"created":"2025-01-27 18:03:00","title":"Training Dynamics of In-Context Learning in Linear Attention","abstract":"While attention-based models have demonstrated the remarkable ability of in-context learning, the theoretical understanding of how these models acquired this ability through gradient descent training is still preliminary. Towards answering this question, we study the gradient descent dynamics of multi-head linear self-attention trained for in-context linear regression. We examine two parametrizations of linear self-attention: one with the key and query weights merged as a single matrix (common in theoretical studies), and one with separate key and query matrices (closer to practical settings). For the merged parametrization, we show the training dynamics has two fixed points and the loss trajectory exhibits a single, abrupt drop. We derive an analytical time-course solution for a certain class of datasets and initialization. For the separate parametrization, we show the training dynamics has exponentially many fixed points and the loss exhibits saddle-to-saddle dynamics, which we reduce to scalar ordinary differential equations. During training, the model implements principal component regression in context with the number of principal components increasing over training time. Overall, we characterize how in-context learning abilities evolve during gradient descent training of linear attention, revealing dynamics of abrupt acquisition versus progressive improvements in models with different parametrizations.","sentences":["While attention-based models have demonstrated the remarkable ability of in-context learning, the theoretical understanding of how these models acquired this ability through gradient descent training is still preliminary.","Towards answering this question, we study the gradient descent dynamics of multi-head linear self-attention trained for in-context linear regression.","We examine two parametrizations of linear self-attention: one with the key and query weights merged as a single matrix (common in theoretical studies), and one with separate key and query matrices (closer to practical settings).","For the merged parametrization, we show the training dynamics has two fixed points and the loss trajectory exhibits a single, abrupt drop.","We derive an analytical time-course solution for a certain class of datasets and initialization.","For the separate parametrization, we show the training dynamics has exponentially many fixed points and the loss exhibits saddle-to-saddle dynamics, which we reduce to scalar ordinary differential equations.","During training, the model implements principal component regression in context with the number of principal components increasing over training time.","Overall, we characterize how in-context learning abilities evolve during gradient descent training of linear attention, revealing dynamics of abrupt acquisition versus progressive improvements in models with different parametrizations."],"url":"http://arxiv.org/abs/2501.16265v1"}
{"created":"2025-01-27 17:55:39","title":"Improving DBMS Scheduling Decisions with Fine-grained Performance Prediction on Concurrent Queries -- Extended","abstract":"Query scheduling is a critical task that directly impacts query performance in database management systems (DBMS). Deeply integrated schedulers, which require changes to DBMS internals, are usually customized for a specific engine and can take months to implement. In contrast, non-intrusive schedulers make coarse-grained decisions, such as controlling query admission and re-ordering query execution, without requiring modifications to DBMS internals. They require much less engineering effort and can be applied across a wide range of DBMS engines, offering immediate benefits to end users. However, most existing non-intrusive scheduling systems rely on simplified cost models and heuristics that cannot accurately model query interactions under concurrency and different system states, possibly leading to suboptimal scheduling decisions.   This work introduces IconqSched, a new, principled non-intrusive scheduler that optimizes the execution order and timing of queries to enhance total end-to-end runtime as experienced by the user query queuing time plus system runtime. Unlike previous approaches, IconqSched features a novel fine-grained predictor, Iconq, which treats the DBMS as a black box and accurately estimates the system runtime of concurrently executed queries under different system states. Using these predictions, IconqSched is able to capture system runtime variations across different query mixes and system loads. It then employs a greedy scheduling algorithm to effectively determine which queries to submit and when to submit them. We compare IconqSched to other schedulers in terms of end-to-end runtime using real workload traces. On Postgres, IconqSched reduces end-to-end runtime by 16.2%-28.2% on average and 33.6%-38.9% in the tail. Similarly, on Redshift, it reduces end-to-end runtime by 10.3%-14.1% on average and 14.9%-22.2% in the tail.","sentences":["Query scheduling is a critical task that directly impacts query performance in database management systems (DBMS).","Deeply integrated schedulers, which require changes to DBMS internals, are usually customized for a specific engine and can take months to implement.","In contrast, non-intrusive schedulers make coarse-grained decisions, such as controlling query admission and re-ordering query execution, without requiring modifications to DBMS internals.","They require much less engineering effort and can be applied across a wide range of DBMS engines, offering immediate benefits to end users.","However, most existing non-intrusive scheduling systems rely on simplified cost models and heuristics that cannot accurately model query interactions under concurrency and different system states, possibly leading to suboptimal scheduling decisions.   ","This work introduces IconqSched, a new, principled non-intrusive scheduler that optimizes the execution order and timing of queries to enhance total end-to-end runtime as experienced by the user query queuing time plus system runtime.","Unlike previous approaches, IconqSched features a novel fine-grained predictor, Iconq, which treats the DBMS as a black box and accurately estimates the system runtime of concurrently executed queries under different system states.","Using these predictions, IconqSched is able to capture system runtime variations across different query mixes and system loads.","It then employs a greedy scheduling algorithm to effectively determine which queries to submit and when to submit them.","We compare IconqSched to other schedulers in terms of end-to-end runtime using real workload traces.","On Postgres, IconqSched reduces end-to-end runtime by 16.2%-28.2% on average and 33.6%-38.9% in the tail.","Similarly, on Redshift, it reduces end-to-end runtime by 10.3%-14.1% on average and 14.9%-22.2% in the tail."],"url":"http://arxiv.org/abs/2501.16256v1"}
{"created":"2025-01-27 17:55:37","title":"A foundation model for human-AI collaboration in medical literature mining","abstract":"Systematic literature review is essential for evidence-based medicine, requiring comprehensive analysis of clinical trial publications. However, the application of artificial intelligence (AI) models for medical literature mining has been limited by insufficient training and evaluation across broad therapeutic areas and diverse tasks. Here, we present LEADS, an AI foundation model for study search, screening, and data extraction from medical literature. The model is trained on 633,759 instruction data points in LEADSInstruct, curated from 21,335 systematic reviews, 453,625 clinical trial publications, and 27,015 clinical trial registries. We showed that LEADS demonstrates consistent improvements over four cutting-edge generic large language models (LLMs) on six tasks. Furthermore, LEADS enhances expert workflows by providing supportive references following expert requests, streamlining processes while maintaining high-quality results. A study with 16 clinicians and medical researchers from 14 different institutions revealed that experts collaborating with LEADS achieved a recall of 0.81 compared to 0.77 experts working alone in study selection, with a time savings of 22.6%. In data extraction tasks, experts using LEADS achieved an accuracy of 0.85 versus 0.80 without using LEADS, alongside a 26.9% time savings. These findings highlight the potential of specialized medical literature foundation models to outperform generic models, delivering significant quality and efficiency benefits when integrated into expert workflows for medical literature mining.","sentences":["Systematic literature review is essential for evidence-based medicine, requiring comprehensive analysis of clinical trial publications.","However, the application of artificial intelligence (AI) models for medical literature mining has been limited by insufficient training and evaluation across broad therapeutic areas and diverse tasks.","Here, we present LEADS, an AI foundation model for study search, screening, and data extraction from medical literature.","The model is trained on 633,759 instruction data points in LEADSInstruct, curated from 21,335 systematic reviews, 453,625 clinical trial publications, and 27,015 clinical trial registries.","We showed that LEADS demonstrates consistent improvements over four cutting-edge generic large language models (LLMs) on six tasks.","Furthermore, LEADS enhances expert workflows by providing supportive references following expert requests, streamlining processes while maintaining high-quality results.","A study with 16 clinicians and medical researchers from 14 different institutions revealed that experts collaborating with LEADS achieved a recall of 0.81 compared to 0.77 experts working alone in study selection, with a time savings of 22.6%.","In data extraction tasks, experts using LEADS achieved an accuracy of 0.85 versus 0.80 without using LEADS, alongside a 26.9% time savings.","These findings highlight the potential of specialized medical literature foundation models to outperform generic models, delivering significant quality and efficiency benefits when integrated into expert workflows for medical literature mining."],"url":"http://arxiv.org/abs/2501.16255v1"}
{"created":"2025-01-27 17:54:31","title":"Multi-Agent Geospatial Copilots for Remote Sensing Workflows","abstract":"We present GeoLLM-Squad, a geospatial Copilot that introduces the novel multi-agent paradigm to remote sensing (RS) workflows. Unlike existing single-agent approaches that rely on monolithic large language models (LLM), GeoLLM-Squad separates agentic orchestration from geospatial task-solving, by delegating RS tasks to specialized sub-agents. Built on the open-source AutoGen and GeoLLM-Engine frameworks, our work enables the modular integration of diverse applications, spanning urban monitoring, forestry protection, climate analysis, and agriculture studies. Our results demonstrate that while single-agent systems struggle to scale with increasing RS task complexity, GeoLLM-Squad maintains robust performance, achieving a 17% improvement in agentic correctness over state-of-the-art baselines. Our findings highlight the potential of multi-agent AI in advancing RS workflows.","sentences":["We present GeoLLM-Squad, a geospatial Copilot that introduces the novel multi-agent paradigm to remote sensing (RS) workflows.","Unlike existing single-agent approaches that rely on monolithic large language models (LLM), GeoLLM-Squad separates agentic orchestration from geospatial task-solving, by delegating RS tasks to specialized sub-agents.","Built on the open-source AutoGen and GeoLLM-Engine frameworks, our work enables the modular integration of diverse applications, spanning urban monitoring, forestry protection, climate analysis, and agriculture studies.","Our results demonstrate that while single-agent systems struggle to scale with increasing RS task complexity, GeoLLM-Squad maintains robust performance, achieving a 17% improvement in agentic correctness over state-of-the-art baselines.","Our findings highlight the potential of multi-agent AI in advancing RS workflows."],"url":"http://arxiv.org/abs/2501.16254v1"}
{"created":"2025-01-27 17:51:51","title":"Runtime Analysis of the Compact Genetic Algorithm on the LeadingOnes Benchmark","abstract":"The compact genetic algorithm (cGA) is one of the simplest estimation-of-distribution algorithms (EDAs). Next to the univariate marginal distribution algorithm (UMDA) -- another simple EDA -- , the cGA has been subject to extensive mathematical runtime analyses, often showcasing a similar or even superior performance to competing approaches. Surprisingly though, up to date and in contrast to the UMDA and many other heuristics, we lack a rigorous runtime analysis of the cGA on the LeadingOnes benchmark -- one of the most studied theory benchmarks in the domain of evolutionary computation.   We fill this gap in the literature by conducting a formal runtime analysis of the cGA on LeadingOnes. For the cGA's single parameter -- called the hypothetical population size -- at least polylogarithmically larger than the problem size, we prove that the cGA samples the optimum of LeadingOnes with high probability within a number of function evaluations quasi-linear in the problem size and linear in the hypothetical population size. For the best hypothetical population size, our result matches, up to polylogarithmic factors, the typical quadratic runtime that many randomized search heuristics exhibit on LeadingOnes. Our analysis exhibits some noteworthy differences in the working principles of the two algorithms which were not visible in previous works.","sentences":["The compact genetic algorithm (cGA) is one of the simplest estimation-of-distribution algorithms (EDAs).","Next to the univariate marginal distribution algorithm (UMDA) -- another simple EDA -- , the cGA has been subject to extensive mathematical runtime analyses, often showcasing a similar or even superior performance to competing approaches.","Surprisingly though, up to date and in contrast to the UMDA and many other heuristics, we lack a rigorous runtime analysis of the cGA on the LeadingOnes benchmark -- one of the most studied theory benchmarks in the domain of evolutionary computation.   ","We fill this gap in the literature by conducting a formal runtime analysis of the cGA on LeadingOnes.","For the cGA's single parameter -- called the hypothetical population size -- at least polylogarithmically larger than the problem size, we prove that the cGA samples the optimum of LeadingOnes with high probability within a number of function evaluations quasi-linear in the problem size and linear in the hypothetical population size.","For the best hypothetical population size, our result matches, up to polylogarithmic factors, the typical quadratic runtime that many randomized search heuristics exhibit on LeadingOnes.","Our analysis exhibits some noteworthy differences in the working principles of the two algorithms which were not visible in previous works."],"url":"http://arxiv.org/abs/2501.16250v1"}
{"created":"2025-01-27 17:48:48","title":"Zero-Shot Decision Tree Construction via Large Language Models","abstract":"This paper introduces a novel algorithm for constructing decision trees using large language models (LLMs) in a zero-shot manner based on Classification and Regression Trees (CART) principles. Traditional decision tree induction methods rely heavily on labeled data to recursively partition data using criteria such as information gain or the Gini index. In contrast, we propose a method that uses the pre-trained knowledge embedded in LLMs to build decision trees without requiring training data. Our approach leverages LLMs to perform operations essential for decision tree construction, including attribute discretization, probability calculation, and Gini index computation based on the probabilities. We show that these zero-shot decision trees can outperform baseline zero-shot methods and achieve competitive performance compared to supervised data-driven decision trees on tabular datasets. The decision trees constructed via this method provide transparent and interpretable models, addressing data scarcity while preserving interpretability. This work establishes a new baseline in low-data machine learning, offering a principled, knowledge-driven alternative to data-driven tree construction.","sentences":["This paper introduces a novel algorithm for constructing decision trees using large language models (LLMs) in a zero-shot manner based on Classification and Regression Trees (CART) principles.","Traditional decision tree induction methods rely heavily on labeled data to recursively partition data using criteria such as information gain or the Gini index.","In contrast, we propose a method that uses the pre-trained knowledge embedded in LLMs to build decision trees without requiring training data.","Our approach leverages LLMs to perform operations essential for decision tree construction, including attribute discretization, probability calculation, and Gini index computation based on the probabilities.","We show that these zero-shot decision trees can outperform baseline zero-shot methods and achieve competitive performance compared to supervised data-driven decision trees on tabular datasets.","The decision trees constructed via this method provide transparent and interpretable models, addressing data scarcity while preserving interpretability.","This work establishes a new baseline in low-data machine learning, offering a principled, knowledge-driven alternative to data-driven tree construction."],"url":"http://arxiv.org/abs/2501.16247v1"}
{"created":"2025-01-27 17:43:51","title":"CLISC: Bridging clip and sam by enhanced cam for unsupervised brain tumor segmentation","abstract":"Brain tumor segmentation is important for diagnosis of the tumor, and current deep-learning methods rely on a large set of annotated images for training, with high annotation costs. Unsupervised segmentation is promising to avoid human annotations while the performance is often limited. In this study, we present a novel unsupervised segmentation approach that leverages the capabilities of foundation models, and it consists of three main steps: (1) A vision-language model (i.e., CLIP) is employed to obtain image-level pseudo-labels for training a classification network. Class Activation Mapping (CAM) is then employed to extract Regions of Interest (ROIs), where an adaptive masking-based data augmentation is used to enhance ROI identification.(2) The ROIs are used to generate bounding box and point prompts for the Segment Anything Model (SAM) to obtain segmentation pseudo-labels. (3) A 3D segmentation network is trained with the SAM-derived pseudo-labels, where low-quality pseudo-labels are filtered out in a self-learning process based on the similarity between the SAM's output and the network's prediction. Evaluation on the BraTS2020 dataset demonstrates that our approach obtained an average Dice Similarity Score (DSC) of 85.60%, outperforming five state-of-the-art unsupervised segmentation methods by more than 10 percentage points. Besides, our approach outperforms directly using SAM for zero-shot inference, and its performance is close to fully supervised learning.","sentences":["Brain tumor segmentation is important for diagnosis of the tumor, and current deep-learning methods rely on a large set of annotated images for training, with high annotation costs.","Unsupervised segmentation is promising to avoid human annotations while the performance is often limited.","In this study, we present a novel unsupervised segmentation approach that leverages the capabilities of foundation models, and it consists of three main steps: (1) A vision-language model (i.e., CLIP) is employed to obtain image-level pseudo-labels for training a classification network.","Class Activation Mapping (CAM) is then employed to extract Regions of Interest (ROIs), where an adaptive masking-based data augmentation is used to enhance ROI identification.(2)","The ROIs are used to generate bounding box and point prompts for the Segment Anything Model (SAM) to obtain segmentation pseudo-labels.","(3) A 3D segmentation network is trained with the SAM-derived pseudo-labels, where low-quality pseudo-labels are filtered out in a self-learning process based on the similarity between the SAM's output and the network's prediction.","Evaluation on the BraTS2020 dataset demonstrates that our approach obtained an average Dice Similarity Score (DSC) of 85.60%, outperforming five state-of-the-art unsupervised segmentation methods by more than 10 percentage points.","Besides, our approach outperforms directly using SAM for zero-shot inference, and its performance is close to fully supervised learning."],"url":"http://arxiv.org/abs/2501.16246v1"}
{"created":"2025-01-27 17:42:20","title":"SP-IMPact: A Framework for Static Partitioning Interference Mitigation and Performance Analysis","abstract":"Modern embedded systems are evolving toward complex, heterogeneous architectures to accommodate increasingly demanding applications. Driven by SWAP-C constraints, this shift has led to consolidating multiple systems onto single hardware platforms. Static Partitioning Hypervisors offer a promising solution to partition hardware resources and provide spatial isolation between critical workloads. However, shared resources like the Last-Level Cache and system bus can introduce temporal interference between virtual machines (VMs), negatively impacting performance and predictability. Over the past decade, academia and industry have developed interference mitigation techniques, such as cache partitioning and memory bandwidth reservation. However, configuring these techniques is complex and time-consuming. Cache partitioning requires balancing cache sections across VMs, while memory bandwidth reservation needs tuning bandwidth budgets and periods. Testing all configurations is impractical and often leads to suboptimal results. Moreover, understanding how these techniques interact is limited, as their combined use can produce compounded or conflicting effects on performance. Static analysis tools estimating worst-case execution times offer guidance for configuring mitigation techniques but often fail to capture the complexity of modern multi-core systems. They typically focus on limited shared resources while neglecting others, such as IOMMUs and interrupt controllers. To address these challenges, we present SP-IMPact, an open-source framework for analyzing and guiding interference mitigation configurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth reservation, while evaluating their interactions and cumulative impact. By providing insights on real hardware, SP-IMPact helps optimize configurations for mixed-criticality systems, ensuring performance and predictability.","sentences":["Modern embedded systems are evolving toward complex, heterogeneous architectures to accommodate increasingly demanding applications.","Driven by SWAP-C constraints, this shift has led to consolidating multiple systems onto single hardware platforms.","Static Partitioning Hypervisors offer a promising solution to partition hardware resources and provide spatial isolation between critical workloads.","However, shared resources like the Last-Level Cache and system bus can introduce temporal interference between virtual machines (VMs), negatively impacting performance and predictability.","Over the past decade, academia and industry have developed interference mitigation techniques, such as cache partitioning and memory bandwidth reservation.","However, configuring these techniques is complex and time-consuming.","Cache partitioning requires balancing cache sections across VMs, while memory bandwidth reservation needs tuning bandwidth budgets and periods.","Testing all configurations is impractical and often leads to suboptimal results.","Moreover, understanding how these techniques interact is limited, as their combined use can produce compounded or conflicting effects on performance.","Static analysis tools estimating worst-case execution times offer guidance for configuring mitigation techniques but often fail to capture the complexity of modern multi-core systems.","They typically focus on limited shared resources while neglecting others, such as IOMMUs and interrupt controllers.","To address these challenges, we present SP-IMPact, an open-source framework for analyzing and guiding interference mitigation configurations.","SP-IMPact supports (i) cache coloring and (ii) memory bandwidth reservation, while evaluating their interactions and cumulative impact.","By providing insights on real hardware, SP-IMPact helps optimize configurations for mixed-criticality systems, ensuring performance and predictability."],"url":"http://arxiv.org/abs/2501.16245v1"}
{"created":"2025-01-27 17:36:06","title":"Phase Transitions in Large Language Models and the $O(N)$ Model","abstract":"Large language models (LLMs) exhibit unprecedentedly rich scaling behaviors. In physics, scaling behavior is closely related to phase transitions, critical phenomena, and field theory. To investigate the phase transition phenomena in LLMs, we reformulated the Transformer architecture as an $O(N)$ model. Our study reveals two distinct phase transitions corresponding to the temperature used in text generation and the model's parameter size, respectively. The first phase transition enables us to estimate the internal dimension of the model, while the second phase transition is of \\textit{higher-depth} and signals the emergence of new capabilities. As an application, the energy of the $O(N)$ model can be used to evaluate whether an LLM's parameters are sufficient to learn the training data.","sentences":["Large language models (LLMs) exhibit unprecedentedly rich scaling behaviors.","In physics, scaling behavior is closely related to phase transitions, critical phenomena, and field theory.","To investigate the phase transition phenomena in LLMs, we reformulated the Transformer architecture as an $O(N)$ model.","Our study reveals two distinct phase transitions corresponding to the temperature used in text generation and the model's parameter size, respectively.","The first phase transition enables us to estimate the internal dimension of the model, while the second phase transition is of \\textit{higher-depth} and signals the emergence of new capabilities.","As an application, the energy of the $O(N)$ model can be used to evaluate whether an LLM's parameters are sufficient to learn the training data."],"url":"http://arxiv.org/abs/2501.16241v1"}
{"created":"2025-01-27 17:35:48","title":"AiGet: Transforming Everyday Moments into Hidden Knowledge Discovery with AI Assistance on Smart Glasses","abstract":"Unlike the free exploration of childhood, the demands of daily life reduce our motivation to explore our surroundings, leading to missed opportunities for informal learning. Traditional tools for knowledge acquisition are reactive, relying on user initiative and limiting their ability to uncover hidden interests. Through formative studies, we introduce AiGet, a proactive AI assistant integrated with AR smart glasses, designed to seamlessly embed informal learning into low-demand daily activities (e.g., casual walking and shopping). AiGet analyzes real-time user gaze patterns, environmental context, and user profiles, leveraging large language models to deliver personalized, context-aware knowledge with low disruption to primary tasks. In-lab evaluations and real-world testing, including continued use over multiple days, demonstrate AiGet's effectiveness in uncovering overlooked yet surprising interests, enhancing primary task enjoyment, reviving curiosity, and deepening connections with the environment. We further propose design guidelines for AI-assisted informal learning, focused on transforming everyday moments into enriching learning experiences.","sentences":["Unlike the free exploration of childhood, the demands of daily life reduce our motivation to explore our surroundings, leading to missed opportunities for informal learning.","Traditional tools for knowledge acquisition are reactive, relying on user initiative and limiting their ability to uncover hidden interests.","Through formative studies, we introduce AiGet, a proactive AI assistant integrated with AR smart glasses, designed to seamlessly embed informal learning into low-demand daily activities (e.g., casual walking and shopping).","AiGet analyzes real-time user gaze patterns, environmental context, and user profiles, leveraging large language models to deliver personalized, context-aware knowledge with low disruption to primary tasks.","In-lab evaluations and real-world testing, including continued use over multiple days, demonstrate AiGet's effectiveness in uncovering overlooked yet surprising interests, enhancing primary task enjoyment, reviving curiosity, and deepening connections with the environment.","We further propose design guidelines for AI-assisted informal learning, focused on transforming everyday moments into enriching learning experiences."],"url":"http://arxiv.org/abs/2501.16240v1"}
{"created":"2025-01-27 17:35:39","title":"Distilling foundation models for robust and efficient models in digital pathology","abstract":"In recent years, the advent of foundation models (FM) for digital pathology has relied heavily on scaling the pre-training datasets and the model size, yielding large and powerful models. While it resulted in improving the performance on diverse downstream tasks, it also introduced increased computational cost and inference time. In this work, we explore the distillation of a large foundation model into a smaller one, reducing the number of parameters by several orders of magnitude. Leveraging distillation techniques, our distilled model, H0-mini, achieves nearly comparable performance to large FMs at a significantly reduced inference cost. It is evaluated on several public benchmarks, achieving 3rd place on the HEST benchmark and 5th place on the EVA benchmark. Additionally, a robustness analysis conducted on the PLISM dataset demonstrates that our distilled model reaches excellent robustness to variations in staining and scanning conditions, significantly outperforming other state-of-the art models. This opens new perspectives to design lightweight and robust models for digital pathology, without compromising on performance.","sentences":["In recent years, the advent of foundation models (FM) for digital pathology has relied heavily on scaling the pre-training datasets and the model size, yielding large and powerful models.","While it resulted in improving the performance on diverse downstream tasks, it also introduced increased computational cost and inference time.","In this work, we explore the distillation of a large foundation model into a smaller one, reducing the number of parameters by several orders of magnitude.","Leveraging distillation techniques, our distilled model, H0-mini, achieves nearly comparable performance to large FMs at a significantly reduced inference cost.","It is evaluated on several public benchmarks, achieving 3rd place on the HEST benchmark and 5th place on the EVA benchmark.","Additionally, a robustness analysis conducted on the PLISM dataset demonstrates that our distilled model reaches excellent robustness to variations in staining and scanning conditions, significantly outperforming other state-of-the art models.","This opens new perspectives to design lightweight and robust models for digital pathology, without compromising on performance."],"url":"http://arxiv.org/abs/2501.16239v1"}
{"created":"2025-01-27 17:34:04","title":"Application of Structured State Space Models to High energy physics with locality-sensitive hashing","abstract":"Modern high-energy physics (HEP) experiments are increasingly challenged by the vast size and complexity of their datasets, particularly regarding large-scale point cloud processing and long sequences. In this study, to address these challenges, we explore the application of structured state space models (SSMs), proposing one of the first trials to integrate local-sensitive hashing into either a hybrid or pure Mamba Model. Our results demonstrate that pure SSMs could serve as powerful backbones for HEP problems involving tasks for long sequence data with local inductive bias. By integrating locality-sensitive hashing into Mamba blocks, we achieve significant improvements over traditional backbones in key HEP tasks, surpassing them in inference speed and physics metrics while reducing computational overhead. In key tests, our approach demonstrated promising results, presenting a viable alternative to traditional transformer backbones by significantly reducing FLOPS while maintaining robust performance.","sentences":["Modern high-energy physics (HEP) experiments are increasingly challenged by the vast size and complexity of their datasets, particularly regarding large-scale point cloud processing and long sequences.","In this study, to address these challenges, we explore the application of structured state space models (SSMs), proposing one of the first trials to integrate local-sensitive hashing into either a hybrid or pure Mamba Model.","Our results demonstrate that pure SSMs could serve as powerful backbones for HEP problems involving tasks for long sequence data with local inductive bias.","By integrating locality-sensitive hashing into Mamba blocks, we achieve significant improvements over traditional backbones in key HEP tasks, surpassing them in inference speed and physics metrics while reducing computational overhead.","In key tests, our approach demonstrated promising results, presenting a viable alternative to traditional transformer backbones by significantly reducing FLOPS while maintaining robust performance."],"url":"http://arxiv.org/abs/2501.16237v1"}
{"created":"2025-01-27 17:33:49","title":"Unveiling Ethereum's P2P Network: The Role of Chain and Client Diversity","abstract":"The Ethereum network, built on the devp2p protocol stack, was designed to function as a \"world computer\" by supporting decentralized applications through a shared P2P infrastructure. However, the proliferation of blockchain forks has increased network diversity, complicating node discovery and reducing efficiency. Ethereum mainnet nodes cannot easily distinguish between peers from different blockchains until after establishing an expensive TCP connection, encryption, and protocol handshake. This inefficiency is further worsened by client diversity, where differences in software implementations cause protocol incompatibilities and connection failures. This paper introduces a monitoring tool that tracks devp2p message exchanges and client statuses to analyze connection dynamics and protocol variations. Our findings highlight issues such as inefficiencies in node discovery and client incompatibility, including timeouts in Geth during the discovery process. The study emphasizes the need to consider chain and client diversity when assessing the health and performance of the post-merge Ethereum network.","sentences":["The Ethereum network, built on the devp2p protocol stack, was designed to function as a \"world computer\" by supporting decentralized applications through a shared P2P infrastructure.","However, the proliferation of blockchain forks has increased network diversity, complicating node discovery and reducing efficiency.","Ethereum mainnet nodes cannot easily distinguish between peers from different blockchains until after establishing an expensive TCP connection, encryption, and protocol handshake.","This inefficiency is further worsened by client diversity, where differences in software implementations cause protocol incompatibilities and connection failures.","This paper introduces a monitoring tool that tracks devp2p message exchanges and client statuses to analyze connection dynamics and protocol variations.","Our findings highlight issues such as inefficiencies in node discovery and client incompatibility, including timeouts in Geth during the discovery process.","The study emphasizes the need to consider chain and client diversity when assessing the health and performance of the post-merge Ethereum network."],"url":"http://arxiv.org/abs/2501.16236v1"}
{"created":"2025-01-27 17:33:38","title":"Echoes of Discord: Forecasting Hater Reactions to Counterspeech","abstract":"Hate speech (HS) erodes the inclusiveness of online users and propagates negativity and division. Counterspeech has been recognized as a way to mitigate the harmful consequences. While some research has investigated the impact of user-generated counterspeech on social media platforms, few have examined and modeled haters' reactions toward counterspeech, despite the immediate alteration of haters' attitudes being an important aspect of counterspeech. This study fills the gap by analyzing the impact of counterspeech from the hater's perspective, focusing on whether the counterspeech leads the hater to reenter the conversation and if the reentry is hateful. We compile the Reddit Echoes of Hate dataset (ReEco), which consists of triple-turn conversations featuring haters' reactions, to assess the impact of counterspeech. The linguistic analysis sheds insights on the language of counterspeech to hate eliciting different haters' reactions. Experimental results demonstrate that the 3-way classification model outperforms the two-stage reaction predictor, which first predicts reentry and then determines the reentry type. We conclude the study with an assessment showing the most common errors identified by the best-performing model.","sentences":["Hate speech (HS) erodes the inclusiveness of online users and propagates negativity and division.","Counterspeech has been recognized as a way to mitigate the harmful consequences.","While some research has investigated the impact of user-generated counterspeech on social media platforms, few have examined and modeled haters' reactions toward counterspeech, despite the immediate alteration of haters' attitudes being an important aspect of counterspeech.","This study fills the gap by analyzing the impact of counterspeech from the hater's perspective, focusing on whether the counterspeech leads the hater to reenter the conversation and if the reentry is hateful.","We compile the Reddit Echoes of Hate dataset (ReEco), which consists of triple-turn conversations featuring haters' reactions, to assess the impact of counterspeech.","The linguistic analysis sheds insights on the language of counterspeech to hate eliciting different haters' reactions.","Experimental results demonstrate that the 3-way classification model outperforms the two-stage reaction predictor, which first predicts reentry and then determines the reentry type.","We conclude the study with an assessment showing the most common errors identified by the best-performing model."],"url":"http://arxiv.org/abs/2501.16235v1"}
{"created":"2025-01-27 17:29:17","title":"MIND-EEG: Multi-granularity Integration Network with Discrete Codebook for EEG-based Emotion Recognition","abstract":"Emotion recognition using electroencephalogram (EEG) signals has broad potential across various domains. EEG signals have ability to capture rich spatial information related to brain activity, yet effectively modeling and utilizing these spatial relationships remains a challenge. Existing methods struggle with simplistic spatial structure modeling, failing to capture complex node interactions, and lack generalizable spatial connection representations, failing to balance the dynamic nature of brain networks with the need for discriminative and generalizable features. To address these challenges, we propose the Multi-granularity Integration Network with Discrete Codebook for EEG-based Emotion Recognition (MIND-EEG). The framework employs a multi-granularity approach, integrating global and regional spatial information through a Global State Encoder, an Intra-Regional Functionality Encoder, and an Inter-Regional Interaction Encoder to comprehensively model brain activity. Additionally, we introduce a discrete codebook mechanism for constructing network structures via vector quantization, ensuring compact and meaningful brain network representations while mitigating over-smoothing and enhancing model generalization. The proposed framework effectively captures the dynamic and diverse nature of EEG signals, enabling robust emotion recognition. Extensive comparisons and analyses demonstrate the effectiveness of MIND-EEG, and the source code is publicly available at https://anonymous.4open.science/r/MIND_EEG.","sentences":["Emotion recognition using electroencephalogram (EEG) signals has broad potential across various domains.","EEG signals have ability to capture rich spatial information related to brain activity, yet effectively modeling and utilizing these spatial relationships remains a challenge.","Existing methods struggle with simplistic spatial structure modeling, failing to capture complex node interactions, and lack generalizable spatial connection representations, failing to balance the dynamic nature of brain networks with the need for discriminative and generalizable features.","To address these challenges, we propose the Multi-granularity Integration Network with Discrete Codebook for EEG-based Emotion Recognition (MIND-EEG).","The framework employs a multi-granularity approach, integrating global and regional spatial information through a Global State Encoder, an Intra-Regional Functionality Encoder, and an Inter-Regional Interaction Encoder to comprehensively model brain activity.","Additionally, we introduce a discrete codebook mechanism for constructing network structures via vector quantization, ensuring compact and meaningful brain network representations while mitigating over-smoothing and enhancing model generalization.","The proposed framework effectively captures the dynamic and diverse nature of EEG signals, enabling robust emotion recognition.","Extensive comparisons and analyses demonstrate the effectiveness of MIND-EEG, and the source code is publicly available at https://anonymous.4open.science/r/MIND_EEG."],"url":"http://arxiv.org/abs/2501.16230v1"}
{"created":"2025-01-27 17:21:33","title":"PDC-ViT : Source Camera Identification using Pixel Difference Convolution and Vision Transformer","abstract":"Source camera identification has emerged as a vital solution to unlock incidents involving critical cases like terrorism, violence, and other criminal activities. The ability to trace the origin of an image/video can aid law enforcement agencies in gathering evidence and constructing the timeline of events. Moreover, identifying the owner of a certain device narrows down the area of search in a criminal investigation where smartphone devices are involved. This paper proposes a new pixel-based method for source camera identification, integrating Pixel Difference Convolution (PDC) with a Vision Transformer network (ViT), and named PDC-ViT. While the PDC acts as the backbone for feature extraction by exploiting Angular PDC (APDC) and Radial PDC (RPDC). These techniques enhance the capability to capture subtle variations in pixel information, which are crucial for distinguishing between different source cameras. The second part of the methodology focuses on classification, which is based on a Vision Transformer network. Unlike traditional methods that utilize image patches directly for training the classification network, the proposed approach uniquely inputs PDC features into the Vision Transformer network. To demonstrate the effectiveness of the PDC-ViT approach, it has been assessed on five different datasets, which include various image contents and video scenes. The method has also been compared with state-of-the-art source camera identification methods. Experimental results demonstrate the effectiveness and superiority of the proposed system in terms of accuracy and robustness when compared to its competitors. For example, our proposed PDC-ViT has achieved an accuracy of 94.30%, 84%, 94.22% and 92.29% using the Vision dataset, Daxing dataset, Socrates dataset and QUFVD dataset, respectively.","sentences":["Source camera identification has emerged as a vital solution to unlock incidents involving critical cases like terrorism, violence, and other criminal activities.","The ability to trace the origin of an image/video can aid law enforcement agencies in gathering evidence and constructing the timeline of events.","Moreover, identifying the owner of a certain device narrows down the area of search in a criminal investigation where smartphone devices are involved.","This paper proposes a new pixel-based method for source camera identification, integrating Pixel Difference Convolution (PDC) with a Vision Transformer network (ViT), and named PDC-ViT.","While the PDC acts as the backbone for feature extraction by exploiting Angular PDC (APDC) and Radial PDC (RPDC).","These techniques enhance the capability to capture subtle variations in pixel information, which are crucial for distinguishing between different source cameras.","The second part of the methodology focuses on classification, which is based on a Vision Transformer network.","Unlike traditional methods that utilize image patches directly for training the classification network, the proposed approach uniquely inputs PDC features into the Vision Transformer network.","To demonstrate the effectiveness of the PDC-ViT approach, it has been assessed on five different datasets, which include various image contents and video scenes.","The method has also been compared with state-of-the-art source camera identification methods.","Experimental results demonstrate the effectiveness and superiority of the proposed system in terms of accuracy and robustness when compared to its competitors.","For example, our proposed PDC-ViT has achieved an accuracy of 94.30%, 84%, 94.22% and 92.29% using the Vision dataset, Daxing dataset, Socrates dataset and QUFVD dataset, respectively."],"url":"http://arxiv.org/abs/2501.16227v1"}
{"created":"2025-01-27 17:20:04","title":"Language-Based Bayesian Optimization Research Assistant (BORA)","abstract":"Many important scientific problems involve multivariate optimization coupled with slow and laborious experimental measurements. These complex, high-dimensional searches can be defined by non-convex optimization landscapes that resemble needle-in-a-haystack surfaces, leading to entrapment in local minima. Contextualizing optimizers with human domain knowledge is a powerful approach to guide searches to localized fruitful regions. However, this approach is susceptible to human confirmation bias and it is also challenging for domain experts to keep track of the rapidly expanding scientific literature. Here, we propose the use of Large Language Models (LLMs) for contextualizing Bayesian optimization (BO) via a hybrid optimization framework that intelligently and economically blends stochastic inference with domain knowledge-based insights from the LLM, which is used to suggest new, better-performing areas of the search space for exploration. Our method fosters user engagement by offering real-time commentary on the optimization progress, explaining the reasoning behind the search strategies. We validate the effectiveness of our approach on synthetic benchmarks with up to 15 independent variables and demonstrate the ability of LLMs to reason in four real-world experimental tasks where context-aware suggestions boost optimization performance substantially.","sentences":["Many important scientific problems involve multivariate optimization coupled with slow and laborious experimental measurements.","These complex, high-dimensional searches can be defined by non-convex optimization landscapes that resemble needle-in-a-haystack surfaces, leading to entrapment in local minima.","Contextualizing optimizers with human domain knowledge is a powerful approach to guide searches to localized fruitful regions.","However, this approach is susceptible to human confirmation bias and it is also challenging for domain experts to keep track of the rapidly expanding scientific literature.","Here, we propose the use of Large Language Models (LLMs) for contextualizing Bayesian optimization (BO) via a hybrid optimization framework that intelligently and economically blends stochastic inference with domain knowledge-based insights from the LLM, which is used to suggest new, better-performing areas of the search space for exploration.","Our method fosters user engagement by offering real-time commentary on the optimization progress, explaining the reasoning behind the search strategies.","We validate the effectiveness of our approach on synthetic benchmarks with up to 15 independent variables and demonstrate the ability of LLMs to reason in four real-world experimental tasks where context-aware suggestions boost optimization performance substantially."],"url":"http://arxiv.org/abs/2501.16224v1"}
{"created":"2025-01-27 17:13:03","title":"SPECIAL: Zero-shot Hyperspectral Image Classification With CLIP","abstract":"Hyperspectral image (HSI) classification aims at categorizing each pixel in an HSI into a specific land cover class, which is crucial for applications like remote sensing, environmental monitoring, and agriculture. Although deep learning-based HSI classification methods have achieved significant advancements, existing methods still rely on manually labeled data for training, which is both time-consuming and labor-intensive.To address this limitation, we introduce a novel zero-shot hyperspectral image classification framework based on CLIP (SPECIAL), aiming to eliminate the need for manual annotations. The SPECIAL framework consists of two main stages: (1) CLIP-based pseudo-label generation, and (2) noisy label learning. In the first stage, HSI is spectrally interpolated to produce RGB bands. These bands are subsequently classified using CLIP, resulting in noisy pseudo-labels that are accompanied by confidence scores.To improve the quality of these labels, we propose a scaling strategy that fuses predictions from multiple spatial scales. In the second stage, spectral information and a label refinement technique are incorporated to mitigate label noise and further enhance classification accuracy. Experimental results on three benchmark datasets demonstrate that our SPECIAL outperforms existing methods in zero-shot HSI classification, showing its potential for more practical applications. The code is available at https://github.com/LiPang/SPECIAL.","sentences":["Hyperspectral image (HSI) classification aims at categorizing each pixel in an HSI into a specific land cover class, which is crucial for applications like remote sensing, environmental monitoring, and agriculture.","Although deep learning-based HSI classification methods have achieved significant advancements, existing methods still rely on manually labeled data for training, which is both time-consuming and labor-intensive.","To address this limitation, we introduce a novel zero-shot hyperspectral image classification framework based on CLIP (SPECIAL), aiming to eliminate the need for manual annotations.","The SPECIAL framework consists of two main stages: (1) CLIP-based pseudo-label generation, and (2) noisy label learning.","In the first stage, HSI is spectrally interpolated to produce RGB bands.","These bands are subsequently classified using CLIP, resulting in noisy pseudo-labels that are accompanied by confidence scores.","To improve the quality of these labels, we propose a scaling strategy that fuses predictions from multiple spatial scales.","In the second stage, spectral information and a label refinement technique are incorporated to mitigate label noise and further enhance classification accuracy.","Experimental results on three benchmark datasets demonstrate that our SPECIAL outperforms existing methods in zero-shot HSI classification, showing its potential for more practical applications.","The code is available at https://github.com/LiPang/SPECIAL."],"url":"http://arxiv.org/abs/2501.16222v1"}
{"created":"2025-01-27 17:10:33","title":"Automatic Calibration of a Multi-Camera System with Limited Overlapping Fields of View for 3D Surgical Scene Reconstruction","abstract":"Purpose: The purpose of this study is to develop an automated and accurate external camera calibration method for multi-camera systems used in 3D surgical scene reconstruction (3D-SSR), eliminating the need for operator intervention or specialized expertise. The method specifically addresses the problem of limited overlapping fields of view caused by significant variations in optical zoom levels and camera locations.   Methods: We contribute a novel, fast, and fully automatic calibration method based on the projection of multi-scale markers (MSMs) using a ceiling-mounted projector. MSMs consist of 2D patterns projected at varying scales, ensuring accurate extraction of well distributed point correspondences across significantly different viewpoints and zoom levels. Validation is performed using both synthetic and real data captured in a mock-up OR, with comparisons to traditional manual marker-based methods as well as markerless calibration methods.   Results: The method achieves accuracy comparable to manual, operator-dependent calibration methods while exhibiting higher robustness under conditions of significant differences in zoom levels. Additionally, we show that state-of-the-art Structure-from-Motion (SfM) pipelines are ineffective in 3D-SSR settings, even when additional texture is projected onto the OR floor.   Conclusion: The use of a ceiling-mounted entry-level projector proves to be an effective alternative to operator-dependent, traditional marker-based methods, paving the way for fully automated 3D-SSR.","sentences":["Purpose: The purpose of this study is to develop an automated and accurate external camera calibration method for multi-camera systems used in 3D surgical scene reconstruction (3D-SSR), eliminating the need for operator intervention or specialized expertise.","The method specifically addresses the problem of limited overlapping fields of view caused by significant variations in optical zoom levels and camera locations.   ","Methods: We contribute a novel, fast, and fully automatic calibration method based on the projection of multi-scale markers (MSMs) using a ceiling-mounted projector.","MSMs consist of 2D patterns projected at varying scales, ensuring accurate extraction of well distributed point correspondences across significantly different viewpoints and zoom levels.","Validation is performed using both synthetic and real data captured in a mock-up OR, with comparisons to traditional manual marker-based methods as well as markerless calibration methods.   ","Results:","The method achieves accuracy comparable to manual, operator-dependent calibration methods while exhibiting higher robustness under conditions of significant differences in zoom levels.","Additionally, we show that state-of-the-art Structure-from-Motion (SfM) pipelines are ineffective in 3D-SSR settings, even when additional texture is projected onto the OR floor.   ","Conclusion: The use of a ceiling-mounted entry-level projector proves to be an effective alternative to operator-dependent, traditional marker-based methods, paving the way for fully automated 3D-SSR."],"url":"http://arxiv.org/abs/2501.16221v1"}
{"created":"2025-01-27 17:09:47","title":"DBRouting: Routing End User Queries to Databases for Answerability","abstract":"Enterprise level data is often distributed across multiple sources and identifying the correct set-of data-sources with relevant information for a knowledge request is a fundamental challenge. In this work, we define the novel task of routing an end-user query to the appropriate data-source, where the data-sources are databases. We synthesize datasets by extending existing datasets designed for NL-to-SQL semantic parsing. We create baselines on these datasets by using open-source LLMs, using both pre-trained and task specific embeddings fine-tuned using the training data. With these baselines we demonstrate that open-source LLMs perform better than embedding based approach, but suffer from token length limitations. Embedding based approaches benefit from task specific fine-tuning, more so when there is availability of data in terms of database specific questions for training. We further find that the task becomes more difficult (i) with an increase in the number of data-sources, (ii) having data-sources closer in terms of their domains,(iii) having databases without external domain knowledge required to interpret its entities and (iv) with ambiguous and complex queries requiring more fine-grained understanding of the data-sources or logical reasoning for routing to an appropriate source. This calls for the need for developing more sophisticated solutions to better address the task.","sentences":["Enterprise level data is often distributed across multiple sources and identifying the correct set-of data-sources with relevant information for a knowledge request is a fundamental challenge.","In this work, we define the novel task of routing an end-user query to the appropriate data-source, where the data-sources are databases.","We synthesize datasets by extending existing datasets designed for NL-to-SQL semantic parsing.","We create baselines on these datasets by using open-source LLMs, using both pre-trained and task specific embeddings fine-tuned using the training data.","With these baselines we demonstrate that open-source LLMs perform better than embedding based approach, but suffer from token length limitations.","Embedding based approaches benefit from task specific fine-tuning, more so when there is availability of data in terms of database specific questions for training.","We further find that the task becomes more difficult (i) with an increase in the number of data-sources, (ii) having data-sources closer in terms of their domains,(iii) having databases without external domain knowledge required to interpret its entities and (iv) with ambiguous and complex queries requiring more fine-grained understanding of the data-sources or logical reasoning for routing to an appropriate source.","This calls for the need for developing more sophisticated solutions to better address the task."],"url":"http://arxiv.org/abs/2501.16220v1"}
{"created":"2025-01-27 17:08:36","title":"Evaluation of isolation design flow (IDF) for Single Chip Cryptography (SCC) application","abstract":"Field Programmable Gate Arrays (FPGAs) are increasingly in various applications. This is due to the fact that they provide flexibility to reprogram and modify in realtime with minimum effort. The increasing usage of FPGA must also ensure its end users with a guarantee that they are able to overcome failures and work in the harshest of environments. Today's end user demands a guarantee that the system he/she is buying remains functional.   FPGA Vendor Xilinx provides its users with that guarantee in the name of Isolation Design flow or IDF for short. Xilinx claims that IDF can help ensure users reliability while providing flexibility. If true, application that can benefit from IDF are vast, such as Avionics, Unmanned probes, Self-driving fully autonomous vehicles, etc.   This thesis puts the Xilinx claim regarding IDF to the test by implementing a single-chip cryptographic application, namely Advanced Encryption Standard, according to the rules and regulations defined by isolation design flow. This thesis does so by replicating and injecting faults in the system that conforms to the rules of IDF and records its behavior firsthand to observe IDF effectiveness. This thesis can also help end users and system evaluators interested in effectiveness of IDF with an independent view other than Xilinx by providing them with statistical data collected to remove all doubts.","sentences":["Field Programmable Gate Arrays (FPGAs) are increasingly in various applications.","This is due to the fact that they provide flexibility to reprogram and modify in realtime with minimum effort.","The increasing usage of FPGA must also ensure its end users with a guarantee that they are able to overcome failures and work in the harshest of environments.","Today's end user demands a guarantee that the system he/she is buying remains functional.   ","FPGA Vendor Xilinx provides its users with that guarantee in the name of Isolation Design flow or IDF for short.","Xilinx claims that IDF can help ensure users reliability while providing flexibility.","If true, application that can benefit from IDF are vast, such as Avionics, Unmanned probes, Self-driving fully autonomous vehicles, etc.   ","This thesis puts the Xilinx claim regarding IDF to the test by implementing a single-chip cryptographic application, namely Advanced Encryption Standard, according to the rules and regulations defined by isolation design flow.","This thesis does so by replicating and injecting faults in the system that conforms to the rules of IDF and records its behavior firsthand to observe IDF effectiveness.","This thesis can also help end users and system evaluators interested in effectiveness of IDF with an independent view other than Xilinx by providing them with statistical data collected to remove all doubts."],"url":"http://arxiv.org/abs/2501.16217v1"}
{"created":"2025-01-27 17:07:20","title":"Enhancing Visual Inspection Capability of Multi-Modal Large Language Models on Medical Time Series with Supportive Conformalized and Interpretable Small Specialized Models","abstract":"Large language models (LLMs) exhibit remarkable capabilities in visual inspection of medical time-series data, achieving proficiency comparable to human clinicians. However, their broad scope limits domain-specific precision, and proprietary weights hinder fine-tuning for specialized datasets. In contrast, small specialized models (SSMs) excel in targeted tasks but lack the contextual reasoning required for complex clinical decision-making. To address these challenges, we propose ConMIL (Conformalized Multiple Instance Learning), a decision-support SSM that integrates seamlessly with LLMs. By using Multiple Instance Learning (MIL) to identify clinically significant signal segments and conformal prediction for calibrated set-valued outputs, ConMIL enhances LLMs' interpretative capabilities for medical time-series analysis. Experimental results demonstrate that ConMIL significantly improves the performance of state-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically, \\ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for confident samples in arrhythmia detection and sleep staging, compared to standalone LLM accuracy of 46.13% and 13.16%. These findings highlight the potential of ConMIL to bridge task-specific precision and broader contextual reasoning, enabling more reliable and interpretable AI-driven clinical decision support.","sentences":["Large language models (LLMs) exhibit remarkable capabilities in visual inspection of medical time-series data, achieving proficiency comparable to human clinicians.","However, their broad scope limits domain-specific precision, and proprietary weights hinder fine-tuning for specialized datasets.","In contrast, small specialized models (SSMs) excel in targeted tasks but lack the contextual reasoning required for complex clinical decision-making.","To address these challenges, we propose ConMIL (Conformalized Multiple Instance Learning), a decision-support SSM that integrates seamlessly with LLMs.","By using Multiple Instance Learning (MIL) to identify clinically significant signal segments and conformal prediction for calibrated set-valued outputs, ConMIL enhances LLMs' interpretative capabilities for medical time-series analysis.","Experimental results demonstrate that ConMIL significantly improves the performance of state-of-the-art LLMs, such as ChatGPT4.0 and Qwen2-VL-7B. Specifically, \\ConMIL{}-supported Qwen2-VL-7B achieves 94.92% and 96.82% precision for confident samples in arrhythmia detection and sleep staging, compared to standalone LLM accuracy of 46.13% and 13.16%.","These findings highlight the potential of ConMIL to bridge task-specific precision and broader contextual reasoning, enabling more reliable and interpretable AI-driven clinical decision support."],"url":"http://arxiv.org/abs/2501.16215v1"}
{"created":"2025-01-27 17:06:56","title":"Provence: efficient and robust context pruning for retrieval-augmented generation","abstract":"Retrieval-augmented generation improves various aspects of large language models (LLMs) generation, but suffers from computational overhead caused by long contexts as well as the propagation of irrelevant retrieved information into generated responses. Context pruning deals with both aspects, by removing irrelevant parts of retrieved contexts before LLM generation. Existing context pruning approaches are however limited, and do not provide a universal model that would be both efficient and robust in a wide range of scenarios, e.g., when contexts contain a variable amount of relevant information or vary in length, or when evaluated on various domains. In this work, we close this gap and introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts), an efficient and robust context pruner for Question Answering, which dynamically detects the needed amount of pruning for a given context and can be used out-of-the-box for various domains. The three key ingredients of Provence are formulating the context pruning task as sequence labeling, unifying context pruning capabilities with context reranking, and training on diverse data. Our experimental results show that Provence enables context pruning with negligible to no drop in performance, in various domains and settings, at almost no cost in a standard RAG pipeline. We also conduct a deeper analysis alongside various ablations to provide insights into training context pruners for future work.","sentences":["Retrieval-augmented generation improves various aspects of large language models (LLMs) generation, but suffers from computational overhead caused by long contexts as well as the propagation of irrelevant retrieved information into generated responses.","Context pruning deals with both aspects, by removing irrelevant parts of retrieved contexts before LLM generation.","Existing context pruning approaches are however limited, and do not provide a universal model that would be both efficient and robust in a wide range of scenarios, e.g., when contexts contain a variable amount of relevant information or vary in length, or when evaluated on various domains.","In this work, we close this gap and introduce Provence (Pruning and Reranking Of retrieVEd relevaNt ContExts), an efficient and robust context pruner for Question Answering, which dynamically detects the needed amount of pruning for a given context and can be used out-of-the-box for various domains.","The three key ingredients of Provence are formulating the context pruning task as sequence labeling, unifying context pruning capabilities with context reranking, and training on diverse data.","Our experimental results show that Provence enables context pruning with negligible to no drop in performance, in various domains and settings, at almost no cost in a standard RAG pipeline.","We also conduct a deeper analysis alongside various ablations to provide insights into training context pruners for future work."],"url":"http://arxiv.org/abs/2501.16214v1"}
{"created":"2025-01-27 17:04:12","title":"An FPGA-Based Neuro-Fuzzy Sensor for Personalized Driving Assistance","abstract":"Advanced driving-assistance systems (ADAS) are intended to automatize driver tasks, as well as improve driving and vehicle safety. This work proposes an intelligent neuro-fuzzy sensor for driving style (DS) recognition, suitable for ADAS enhancement. The development of the driving style intelligent sensor uses naturalistic driving data from the SHRP2 study, which includes data from a CAN bus, inertial measurement unit, and front radar. The system has been successfully implemented using a field-programmable gate array (FPGA) device of the Xilinx Zynq programmable system-on-chip (PSoC). It can mimic the typical timing parameters of a group of drivers as well as tune these typical parameters to model individual DSs. The neuro-fuzzy intelligent sensor provides high-speed real-time active ADAS implementation and is able to personalize its behavior into safe margins without driver intervention. In particular, the personalization procedure of the time headway (THW) parameter for an ACC in steady car following was developed, achieving a performance of 0.53 microseconds. This performance fulfilled the requirements of cutting-edge active ADAS specifications.","sentences":["Advanced driving-assistance systems (ADAS) are intended to automatize driver tasks, as well as improve driving and vehicle safety.","This work proposes an intelligent neuro-fuzzy sensor for driving style (DS) recognition, suitable for ADAS enhancement.","The development of the driving style intelligent sensor uses naturalistic driving data from the SHRP2 study, which includes data from a CAN bus, inertial measurement unit, and front radar.","The system has been successfully implemented using a field-programmable gate array (FPGA) device of the Xilinx Zynq programmable system-on-chip (PSoC).","It can mimic the typical timing parameters of a group of drivers as well as tune these typical parameters to model individual DSs.","The neuro-fuzzy intelligent sensor provides high-speed real-time active ADAS implementation and is able to personalize its behavior into safe margins without driver intervention.","In particular, the personalization procedure of the time headway (THW) parameter for an ACC in steady car following was developed, achieving a performance of 0.53 microseconds.","This performance fulfilled the requirements of cutting-edge active ADAS specifications."],"url":"http://arxiv.org/abs/2501.16212v1"}
{"created":"2025-01-27 17:01:45","title":"UDBE: Unsupervised Diffusion-based Brightness Enhancement in Underwater Images","abstract":"Activities in underwater environments are paramount in several scenarios, which drives the continuous development of underwater image enhancement techniques. A major challenge in this domain is the depth at which images are captured, with increasing depth resulting in a darker environment. Most existing methods for underwater image enhancement focus on noise removal and color adjustment, with few works dedicated to brightness enhancement. This work introduces a novel unsupervised learning approach to underwater image enhancement using a diffusion model. Our method, called UDBE, is based on conditional diffusion to maintain the brightness details of the unpaired input images. The input image is combined with a color map and a Signal-Noise Relation map (SNR) to ensure stable training and prevent color distortion in the output images. The results demonstrate that our approach achieves an impressive accuracy rate in the datasets UIEB, SUIM and RUIE, well-established underwater image benchmarks. Additionally, the experiments validate the robustness of our approach, regarding the image quality metrics PSNR, SSIM, UIQM, and UISM, indicating the good performance of the brightness enhancement process. The source code is available here: https://github.com/gusanagy/UDBE.","sentences":["Activities in underwater environments are paramount in several scenarios, which drives the continuous development of underwater image enhancement techniques.","A major challenge in this domain is the depth at which images are captured, with increasing depth resulting in a darker environment.","Most existing methods for underwater image enhancement focus on noise removal and color adjustment, with few works dedicated to brightness enhancement.","This work introduces a novel unsupervised learning approach to underwater image enhancement using a diffusion model.","Our method, called UDBE, is based on conditional diffusion to maintain the brightness details of the unpaired input images.","The input image is combined with a color map and a Signal-Noise Relation map (SNR) to ensure stable training and prevent color distortion in the output images.","The results demonstrate that our approach achieves an impressive accuracy rate in the datasets UIEB, SUIM and RUIE, well-established underwater image benchmarks.","Additionally, the experiments validate the robustness of our approach, regarding the image quality metrics PSNR, SSIM, UIQM, and UISM, indicating the good performance of the brightness enhancement process.","The source code is available here: https://github.com/gusanagy/UDBE."],"url":"http://arxiv.org/abs/2501.16211v1"}
{"created":"2025-01-27 17:01:41","title":"New Frontiers in Fighting Misinformation","abstract":"Despite extensive research and development of tools and technologies for misinformation tracking and detection, we often find ourselves largely on the losing side of the battle against misinformation. In an era where misinformation poses a substantial threat to public discourse, trust in information sources, and societal and political stability, it is imperative that we regularly revisit and reorient our work strategies. While we have made significant strides in understanding how and why misinformation spreads, we must now broaden our focus and explore how technology can help realise new approaches to address this complex challenge more efficiently.","sentences":["Despite extensive research and development of tools and technologies for misinformation tracking and detection, we often find ourselves largely on the losing side of the battle against misinformation.","In an era where misinformation poses a substantial threat to public discourse, trust in information sources, and societal and political stability, it is imperative that we regularly revisit and reorient our work strategies.","While we have made significant strides in understanding how and why misinformation spreads, we must now broaden our focus and explore how technology can help realise new approaches to address this complex challenge more efficiently."],"url":"http://arxiv.org/abs/2501.16210v1"}
{"created":"2025-01-27 17:00:56","title":"From Informal to Formal -- Incorporating and Evaluating LLMs on Natural Language Requirements to Verifiable Formal Proofs","abstract":"The research in AI-based formal mathematical reasoning has shown an unstoppable growth trend. These studies have excelled in mathematical competitions like IMO, showing significant progress. However, these studies intertwined multiple skills simultaneously, i.e., problem-solving, reasoning, and writing formal specifications, making it hard to precisely identify the LLMs' strengths and weaknesses in each task. This paper focuses on formal verification, an immediate application scenario of formal reasoning, and decomposes it into six sub-tasks. We constructed 18k high-quality instruction-response pairs across five mainstream formal specification languages (Coq, Lean4, Dafny, ACSL, and TLA+) in six formal-verification-related tasks by distilling GPT-4o. They are split into a 14k+ fine-tuning dataset FM-alpaca and a 4k benchmark FM-Bench. We found that LLMs are good at writing proof segments when given either the code, or the detailed description of proof steps. Also, the fine-tuning brought about a nearly threefold improvement at most. Interestingly, we observed that fine-tuning with formal data also enhances mathematics, reasoning, and coding abilities. We hope our findings inspire further research. Fine-tuned models are released to facilitate subsequent studies","sentences":["The research in AI-based formal mathematical reasoning has shown an unstoppable growth trend.","These studies have excelled in mathematical competitions like IMO, showing significant progress.","However, these studies intertwined multiple skills simultaneously, i.e., problem-solving, reasoning, and writing formal specifications, making it hard to precisely identify the LLMs' strengths and weaknesses in each task.","This paper focuses on formal verification, an immediate application scenario of formal reasoning, and decomposes it into six sub-tasks.","We constructed 18k high-quality instruction-response pairs across five mainstream formal specification languages (Coq, Lean4, Dafny, ACSL, and TLA+) in six formal-verification-related tasks by distilling GPT-4o.","They are split into a 14k+ fine-tuning dataset FM-alpaca and a 4k benchmark FM-Bench.","We found that LLMs are good at writing proof segments when given either the code, or the detailed description of proof steps.","Also, the fine-tuning brought about a nearly threefold improvement at most.","Interestingly, we observed that fine-tuning with formal data also enhances mathematics, reasoning, and coding abilities.","We hope our findings inspire further research.","Fine-tuned models are released to facilitate subsequent studies"],"url":"http://arxiv.org/abs/2501.16207v1"}
{"created":"2025-01-27 16:58:33","title":"EPOCH: Enabling Preemption Operation for Context Saving in Heterogeneous FPGA Systems","abstract":"FPGAs are increasingly used in multi-tenant cloud environments to offload compute-intensive tasks from the main CPU. The operating system (OS) plays a vital role in identifying tasks suitable for offloading and coordinating between the CPU and FPGA for seamless task execution. The OS leverages preemption to manage CPU efficiently and balance CPU time; however, preempting tasks running on FPGAs without context loss remains challenging. Despite growing reliance on FPGAs, vendors have yet to deliver a solution that fully preserves and restores task context.   This paper presents EPOCH, the first out-of-the-box framework to seamlessly preserve the state of tasks running on multi-tenant cloud FPGAs. EPOCH enables interrupting a tenant's execution at any arbitrary clock cycle, capturing its state, and saving this 'state snapshot' in off-chip memory with fine-grain granularity. Subsequently, when task resumption is required, EPOCH can resume execution from the saved 'state snapshot', eliminating the need to restart the task from scratch. EPOCH automates intricate processes, shields users from complexities, and synchronizes all underlying logic in a common clock domain, mitigating timing violations and ensuring seamless handling of interruptions.   EPOCH proficiently captures the state of fundamental FPGA elements, such as look-up tables, flip-flops, block--RAMs, and digital signal processing units. On real hardware, ZynQ-XC7Z020 SoC, the proposed solution achieves context save and restore operations per frame in 62.2us and 67.4us, respectively.","sentences":["FPGAs are increasingly used in multi-tenant cloud environments to offload compute-intensive tasks from the main CPU.","The operating system (OS) plays a vital role in identifying tasks suitable for offloading and coordinating between the CPU and FPGA for seamless task execution.","The OS leverages preemption to manage CPU efficiently and balance CPU time; however, preempting tasks running on FPGAs without context loss remains challenging.","Despite growing reliance on FPGAs, vendors have yet to deliver a solution that fully preserves and restores task context.   ","This paper presents EPOCH, the first out-of-the-box framework to seamlessly preserve the state of tasks running on multi-tenant cloud FPGAs.","EPOCH enables interrupting a tenant's execution at any arbitrary clock cycle, capturing its state, and saving this 'state snapshot' in off-chip memory with fine-grain granularity.","Subsequently, when task resumption is required, EPOCH can resume execution from the saved 'state snapshot', eliminating the need to restart the task from scratch.","EPOCH automates intricate processes, shields users from complexities, and synchronizes all underlying logic in a common clock domain, mitigating timing violations and ensuring seamless handling of interruptions.   ","EPOCH proficiently captures the state of fundamental FPGA elements, such as look-up tables, flip-flops, block--RAMs, and digital signal processing units.","On real hardware, ZynQ-XC7Z020 SoC, the proposed solution achieves context save and restore operations per frame in 62.2us and 67.4us, respectively."],"url":"http://arxiv.org/abs/2501.16205v1"}
{"created":"2025-01-27 16:48:39","title":"HERITRACE: A User-Friendly Semantic Data Editor with Change Tracking and Provenance Management for Cultural Heritage Institutions","abstract":"HERITRACE is a data editor designed for galleries, libraries, archives and museums, aimed at simplifying data curation while enabling non-technical domain experts to manage data intuitively without losing its semantic integrity. While the semantic nature of RDF can pose a barrier to data curation due to its complexity, HERITRACE conceals this intricacy while preserving the advantages of semantic representation. The system natively supports provenance management and change tracking, ensuring transparency and accountability throughout the curation process. Although HERITRACE functions effectively out of the box, it offers a straightforward customization interface for technical staff, enabling adaptation to the specific data model required by a given collection. Current applications include the ParaText project, and its adoption is already planned for OpenCitations. Future developments will focus on integrating the RDF Mapping Language (RML) to enhance compatibility with non-RDF data formats, further expanding its applicability in digital heritage management.","sentences":["HERITRACE is a data editor designed for galleries, libraries, archives and museums, aimed at simplifying data curation while enabling non-technical domain experts to manage data intuitively without losing its semantic integrity.","While the semantic nature of RDF can pose a barrier to data curation due to its complexity, HERITRACE conceals this intricacy while preserving the advantages of semantic representation.","The system natively supports provenance management and change tracking, ensuring transparency and accountability throughout the curation process.","Although HERITRACE functions effectively out of the box, it offers a straightforward customization interface for technical staff, enabling adaptation to the specific data model required by a given collection.","Current applications include the ParaText project, and its adoption is already planned for OpenCitations.","Future developments will focus on integrating the RDF Mapping Language (RML) to enhance compatibility with non-RDF data formats, further expanding its applicability in digital heritage management."],"url":"http://arxiv.org/abs/2501.16197v1"}
{"created":"2025-01-27 16:46:34","title":"Posting Patterns of Members of Parental Subreddits","abstract":"Online forums (e.g., Reddit) are used by many parents to discuss their challenges, needs, and receive support. While studies have investigated the contents of posts made to popular parental subreddits revealing the family health concerns being expressed, little is known about parents' posting patterns or other issues they engage in. In this study, we explore the posting activity of users of 55 parental subreddits. Exploring posts made by these users (667K) across Reddit (34M posts) reveals that over 85% of posters are not one-time users of Reddit and actively engage with the community. Studying cross-posting patterns also reveals the use of subreddits dedicated to other topics such as relationship and health advice (e.g., r/AskDocs, r/relationship_advice) by this population. As a result, for a comprehensive understanding of the type of information posters share and seek, future work should investigate sub-communities outside of parental-specific ones. Finally, we expand the list of parental subreddits, compiling a total of 115 subreddits that could be utilized in future studies of parental concerns.","sentences":["Online forums (e.g., Reddit) are used by many parents to discuss their challenges, needs, and receive support.","While studies have investigated the contents of posts made to popular parental subreddits revealing the family health concerns being expressed, little is known about parents' posting patterns or other issues they engage in.","In this study, we explore the posting activity of users of 55 parental subreddits.","Exploring posts made by these users (667K) across Reddit (34M posts) reveals that over 85% of posters are not one-time users of Reddit and actively engage with the community.","Studying cross-posting patterns also reveals the use of subreddits dedicated to other topics such as relationship and health advice (e.g., r/AskDocs, r/relationship_advice) by this population.","As a result, for a comprehensive understanding of the type of information posters share and seek, future work should investigate sub-communities outside of parental-specific ones.","Finally, we expand the list of parental subreddits, compiling a total of 115 subreddits that could be utilized in future studies of parental concerns."],"url":"http://arxiv.org/abs/2501.16193v1"}
{"created":"2025-01-27 16:45:34","title":"Raiders of the Lost Dependency: Fixing Dependency Conflicts in Python using LLMs","abstract":"Fixing Python dependency issues is a tedious and error-prone task for developers, who must manually identify and resolve environment dependencies and version constraints of third-party modules and Python interpreters. Researchers have attempted to automate this process by relying on large knowledge graphs and database lookup tables. However, these traditional approaches face limitations due to the variety of dependency error types, large sets of possible module versions, and conflicts among transitive dependencies. This study explores the potential of using large language models (LLMs) to automatically fix dependency issues in Python programs. We introduce PLLM (pronounced \"plum\"), a novel technique that employs retrieval-augmented generation (RAG) to help an LLM infer Python versions and required modules for a given Python file. PLLM builds a testing environment that iteratively (1) prompts the LLM for module combinations, (2) tests the suggested changes, and (3) provides feedback (error messages) to the LLM to refine the fix. This feedback cycle leverages natural language processing (NLP) to intelligently parse and interpret build error messages. We benchmark PLLM on the Gistable HG2.9K dataset, a collection of challenging single-file Python gists. We compare PLLM against two state-of-the-art automatic dependency inference approaches, namely PyEGo and ReadPyE, w.r.t. the ability to resolve dependency issues. Our results indicate that PLLM can fix more dependency issues than the two baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%) over PyEGo. Our deeper analyses suggest that PLLM is particularly beneficial for projects with many dependencies and for specific third-party numerical and machine-learning modules. Our findings demonstrate the potential of LLM-based approaches to iteratively resolve Python dependency issues.","sentences":["Fixing Python dependency issues is a tedious and error-prone task for developers, who must manually identify and resolve environment dependencies and version constraints of third-party modules and Python interpreters.","Researchers have attempted to automate this process by relying on large knowledge graphs and database lookup tables.","However, these traditional approaches face limitations due to the variety of dependency error types, large sets of possible module versions, and conflicts among transitive dependencies.","This study explores the potential of using large language models (LLMs) to automatically fix dependency issues in Python programs.","We introduce PLLM (pronounced \"plum\"), a novel technique that employs retrieval-augmented generation (RAG) to help an LLM infer Python versions and required modules for a given Python file.","PLLM builds a testing environment that iteratively (1) prompts the LLM for module combinations, (2) tests the suggested changes, and (3) provides feedback (error messages) to the LLM to refine the fix.","This feedback cycle leverages natural language processing (NLP) to intelligently parse and interpret build error messages.","We benchmark PLLM on the Gistable HG2.9K dataset, a collection of challenging single-file Python gists.","We compare PLLM against two state-of-the-art automatic dependency inference approaches, namely PyEGo and ReadPyE, w.r.t.","the ability to resolve dependency issues.","Our results indicate that PLLM can fix more dependency issues than the two baselines, with +218 (+15.97%) more fixes over ReadPyE and +281 (+21.58%) over PyEGo.","Our deeper analyses suggest that PLLM is particularly beneficial for projects with many dependencies and for specific third-party numerical and machine-learning modules.","Our findings demonstrate the potential of LLM-based approaches to iteratively resolve Python dependency issues."],"url":"http://arxiv.org/abs/2501.16191v1"}
{"created":"2025-01-27 16:42:48","title":"Learn to Optimize Resource Allocation under QoS Constraint of AR","abstract":"This paper studies the uplink and downlink power allocation for interactive augmented reality (AR) services, where live video captured by an AR device is uploaded to the network edge and then the augmented video is subsequently downloaded. By modeling the AR transmission process as a tandem queuing system, we derive an upper bound for the probabilistic quality of service (QoS) requirement concerning end-to-end latency and reliability. The resource allocation with the QoS constraints results in a functional optimization problem. To address it, we design a deep neural network to learn the power allocation policy, leveraging the structure of optimal power allocation to enhance learning performance. Simulation results demonstrate that the proposed method effectively reduces transmit powers while meeting the QoS requirement.","sentences":["This paper studies the uplink and downlink power allocation for interactive augmented reality (AR) services, where live video captured by an AR device is uploaded to the network edge and then the augmented video is subsequently downloaded.","By modeling the AR transmission process as a tandem queuing system, we derive an upper bound for the probabilistic quality of service (QoS) requirement concerning end-to-end latency and reliability.","The resource allocation with the QoS constraints results in a functional optimization problem.","To address it, we design a deep neural network to learn the power allocation policy, leveraging the structure of optimal power allocation to enhance learning performance.","Simulation results demonstrate that the proposed method effectively reduces transmit powers while meeting the QoS requirement."],"url":"http://arxiv.org/abs/2501.16186v1"}
{"created":"2025-01-27 16:32:08","title":"Cryptographic Compression","abstract":"We introduce a protocol called ENCORE which simultaneously compresses and encrypts data in a one-pass process that can be implemented efficiently and possesses a number of desirable features as a streaming encoder/decoder. Motivated by the observation that both lossless compression and encryption consist of performing an invertible transformation whose output is close to a uniform distribution over bit streams, we show that these can be done simultaneously, at least for ``typical'' data with a stable distribution, i.e., approximated reasonably well by the output of a Markov model. The strategy is to transform the data into a dyadic distribution whose Huffman encoding is close to uniform, and then store the transformations made to said data in a compressed secondary stream interwoven into the first with a user-defined encryption protocol. The result is an encoding which we show exhibits a modified version of Yao's ``next-bit test'' while requiring many fewer bits of entropy than standard encryption. Numerous open questions remain, particularly regarding results that we suspect can be strengthened considerably.","sentences":["We introduce a protocol called ENCORE which simultaneously compresses and encrypts data in a one-pass process that can be implemented efficiently and possesses a number of desirable features as a streaming encoder/decoder.","Motivated by the observation that both lossless compression and encryption consist of performing an invertible transformation whose output is close to a uniform distribution over bit streams, we show that these can be done simultaneously, at least for ``typical'' data with a stable distribution, i.e., approximated reasonably well by the output of a Markov model.","The strategy is to transform the data into a dyadic distribution whose Huffman encoding is close to uniform, and then store the transformations made to said data in a compressed secondary stream interwoven into the first with a user-defined encryption protocol.","The result is an encoding which we show exhibits a modified version of Yao's ``next-bit test'' while requiring many fewer bits of entropy than standard encryption.","Numerous open questions remain, particularly regarding results that we suspect can be strengthened considerably."],"url":"http://arxiv.org/abs/2501.16184v1"}
{"created":"2025-01-27 16:29:17","title":"The Linear Attention Resurrection in Vision Transformer","abstract":"Vision Transformers (ViTs) have recently taken computer vision by storm. However, the softmax attention underlying ViTs comes with a quadratic complexity in time and memory, hindering the application of ViTs to high-resolution images. We revisit the attention design and propose a linear attention method to address the limitation, which doesn't sacrifice ViT's core advantage of capturing global representation like existing methods (e.g. local window attention of Swin). We further investigate the key difference between linear attention and softmax attention. Our empirical results suggest that linear attention lacks a fundamental property of concentrating the distribution of the attention matrix. Inspired by this observation, we introduce a local concentration module to enhance linear attention. By incorporating enhanced linear global attention and local window attention, we propose a new ViT architecture, dubbed L$^2$ViT. Notably, L$^2$ViT can effectively capture both global interactions and local representations while enjoying linear computational complexity. Extensive experiments demonstrate the strong performance of L$^2$ViT. On image classification, L$^2$ViT achieves 84.4% Top-1 accuracy on ImageNet-1K without any extra training data or label. By further pre-training on ImageNet-22k, it attains 87.0% when fine-tuned with resolution 384$^2$. For downstream tasks, L$^2$ViT delivers favorable performance as a backbone on object detection as well as semantic segmentation.","sentences":["Vision Transformers (ViTs) have recently taken computer vision by storm.","However, the softmax attention underlying ViTs comes with a quadratic complexity in time and memory, hindering the application of ViTs to high-resolution images.","We revisit the attention design and propose a linear attention method to address the limitation, which doesn't sacrifice ViT's core advantage of capturing global representation like existing methods (e.g. local window attention of Swin).","We further investigate the key difference between linear attention and softmax attention.","Our empirical results suggest that linear attention lacks a fundamental property of concentrating the distribution of the attention matrix.","Inspired by this observation, we introduce a local concentration module to enhance linear attention.","By incorporating enhanced linear global attention and local window attention, we propose a new ViT architecture, dubbed L$^2$ViT. Notably, L$^2$ViT can effectively capture both global interactions and local representations while enjoying linear computational complexity.","Extensive experiments demonstrate the strong performance of L$^2$ViT. On image classification, L$^2$ViT achieves 84.4% Top-1 accuracy on ImageNet-1K without any extra training data or label.","By further pre-training on ImageNet-22k, it attains 87.0% when fine-tuned with resolution 384$^2$. For downstream tasks, L$^2$ViT delivers favorable performance as a backbone on object detection as well as semantic segmentation."],"url":"http://arxiv.org/abs/2501.16182v1"}
{"created":"2025-01-27 16:27:41","title":"Can summarization approximate simplification? A gold standard comparison","abstract":"This study explores the overlap between text summarization and simplification outputs. While summarization evaluation methods are streamlined, simplification lacks cohesion, prompting the question: how closely can abstractive summarization resemble gold-standard simplification? We address this by applying two BART-based BRIO summarization methods to the Newsela corpus, comparing outputs with manually annotated simplifications and achieving a top ROUGE-L score of 0.654. This provides insight into where summarization and simplification outputs converge and differ.","sentences":["This study explores the overlap between text summarization and simplification outputs.","While summarization evaluation methods are streamlined, simplification lacks cohesion, prompting the question: how closely can abstractive summarization resemble gold-standard simplification?","We address this by applying two BART-based BRIO summarization methods to the Newsela corpus, comparing outputs with manually annotated simplifications and achieving a top ROUGE-L score of 0.654.","This provides insight into where summarization and simplification outputs converge and differ."],"url":"http://arxiv.org/abs/2501.16181v1"}
{"created":"2025-01-27 16:26:07","title":"SWIFT: Mapping Sub-series with Wavelet Decomposition Improves Time Series Forecasting","abstract":"In recent work on time-series prediction, Transformers and even large language models have garnered significant attention due to their strong capabilities in sequence modeling. However, in practical deployments, time-series prediction often requires operation in resource-constrained environments, such as edge devices, which are unable to handle the computational overhead of large models. To address such scenarios, some lightweight models have been proposed, but they exhibit poor performance on non-stationary sequences. In this paper, we propose $\\textit{SWIFT}$, a lightweight model that is not only powerful, but also efficient in deployment and inference for Long-term Time Series Forecasting (LTSF). Our model is based on three key points: (i) Utilizing wavelet transform to perform lossless downsampling of time series. (ii) Achieving cross-band information fusion with a learnable filter. (iii) Using only one shared linear layer or one shallow MLP for sub-series' mapping. We conduct comprehensive experiments, and the results show that $\\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on multiple datasets, offering a promising method for edge computing and deployment in this task. Moreover, it is noteworthy that the number of parameters in $\\textit{SWIFT-Linear}$ is only 25\\% of what it would be with a single-layer linear model for time-domain prediction. Our code is available at https://github.com/LancelotXWX/SWIFT.","sentences":["In recent work on time-series prediction, Transformers and even large language models have garnered significant attention due to their strong capabilities in sequence modeling.","However, in practical deployments, time-series prediction often requires operation in resource-constrained environments, such as edge devices, which are unable to handle the computational overhead of large models.","To address such scenarios, some lightweight models have been proposed, but they exhibit poor performance on non-stationary sequences.","In this paper, we propose $\\textit{SWIFT}$, a lightweight model that is not only powerful, but also efficient in deployment and inference for Long-term Time Series Forecasting (LTSF).","Our model is based on three key points: (i) Utilizing wavelet transform to perform lossless downsampling of time series.","(ii) Achieving cross-band information fusion with a learnable filter.","(iii) Using only one shared linear layer or one shallow MLP for sub-series' mapping.","We conduct comprehensive experiments, and the results show that $\\textit{SWIFT}$ achieves state-of-the-art (SOTA) performance on multiple datasets, offering a promising method for edge computing and deployment in this task.","Moreover, it is noteworthy that the number of parameters in $\\textit{SWIFT-Linear}$ is only 25\\% of what it would be with a single-layer linear model for time-domain prediction.","Our code is available at https://github.com/LancelotXWX/SWIFT."],"url":"http://arxiv.org/abs/2501.16178v1"}
{"created":"2025-01-27 16:23:45","title":"BAG: Body-Aligned 3D Wearable Asset Generation","abstract":"While recent advancements have shown remarkable progress in general 3D shape generation models, the challenge of leveraging these approaches to automatically generate wearable 3D assets remains unexplored. To this end, we present BAG, a Body-aligned Asset Generation method to output 3D wearable asset that can be automatically dressed on given 3D human bodies. This is achived by controlling the 3D generation process using human body shape and pose information. Specifically, we first build a general single-image to consistent multiview image diffusion model, and train it on the large Objaverse dataset to achieve diversity and generalizability. Then we train a Controlnet to guide the multiview generator to produce body-aligned multiview images. The control signal utilizes the multiview 2D projections of the target human body, where pixel values represent the XYZ coordinates of the body surface in a canonical space. The body-conditioned multiview diffusion generates body-aligned multiview images, which are then fed into a native 3D diffusion model to produce the 3D shape of the asset. Finally, by recovering the similarity transformation using multiview silhouette supervision and addressing asset-body penetration with physics simulators, the 3D asset can be accurately fitted onto the target human body. Experimental results demonstrate significant advantages over existing methods in terms of image prompt-following capability, shape diversity, and shape quality. Our project page is available at https://bag-3d.github.io/.","sentences":["While recent advancements have shown remarkable progress in general 3D shape generation models, the challenge of leveraging these approaches to automatically generate wearable 3D assets remains unexplored.","To this end, we present BAG, a Body-aligned Asset Generation method to output 3D wearable asset that can be automatically dressed on given 3D human bodies.","This is achived by controlling the 3D generation process using human body shape and pose information.","Specifically, we first build a general single-image to consistent multiview image diffusion model, and train it on the large Objaverse dataset to achieve diversity and generalizability.","Then we train a Controlnet to guide the multiview generator to produce body-aligned multiview images.","The control signal utilizes the multiview 2D projections of the target human body, where pixel values represent the XYZ coordinates of the body surface in a canonical space.","The body-conditioned multiview diffusion generates body-aligned multiview images, which are then fed into a native 3D diffusion model to produce the 3D shape of the asset.","Finally, by recovering the similarity transformation using multiview silhouette supervision and addressing asset-body penetration with physics simulators, the 3D asset can be accurately fitted onto the target human body.","Experimental results demonstrate significant advantages over existing methods in terms of image prompt-following capability, shape diversity, and shape quality.","Our project page is available at https://bag-3d.github.io/."],"url":"http://arxiv.org/abs/2501.16177v1"}
{"created":"2025-01-27 16:14:33","title":"Will Systems of LLM Agents Cooperate: An Investigation into a Social Dilemma","abstract":"As autonomous agents become more prevalent, understanding their collective behaviour in strategic interactions is crucial. This study investigates the emergent cooperative tendencies of systems of Large Language Model (LLM) agents in a social dilemma. Unlike previous research where LLMs output individual actions, we prompt state-of-the-art LLMs to generate complete strategies for iterated Prisoner's Dilemma. Using evolutionary game theory, we simulate populations of agents with different strategic dispositions (aggressive, cooperative, or neutral) and observe their evolutionary dynamics. Our findings reveal that different LLMs exhibit distinct biases affecting the relative success of aggressive versus cooperative strategies. This research provides insights into the potential long-term behaviour of systems of deployed LLM-based autonomous agents and highlights the importance of carefully considering the strategic environments in which they operate.","sentences":["As autonomous agents become more prevalent, understanding their collective behaviour in strategic interactions is crucial.","This study investigates the emergent cooperative tendencies of systems of Large Language Model (LLM) agents in a social dilemma.","Unlike previous research where LLMs output individual actions, we prompt state-of-the-art LLMs to generate complete strategies for iterated Prisoner's Dilemma.","Using evolutionary game theory, we simulate populations of agents with different strategic dispositions (aggressive, cooperative, or neutral) and observe their evolutionary dynamics.","Our findings reveal that different LLMs exhibit distinct biases affecting the relative success of aggressive versus cooperative strategies.","This research provides insights into the potential long-term behaviour of systems of deployed LLM-based autonomous agents and highlights the importance of carefully considering the strategic environments in which they operate."],"url":"http://arxiv.org/abs/2501.16173v1"}
{"created":"2025-01-27 16:07:26","title":"Ringmaster ASGD: The First Asynchronous SGD with Optimal Time Complexity","abstract":"Asynchronous Stochastic Gradient Descent (Asynchronous SGD) is a cornerstone method for parallelizing learning in distributed machine learning. However, its performance suffers under arbitrarily heterogeneous computation times across workers, leading to suboptimal time complexity and inefficiency as the number of workers scales. While several Asynchronous SGD variants have been proposed, recent findings by Tyurin & Richt\\'arik (NeurIPS 2023) reveal that none achieve optimal time complexity, leaving a significant gap in the literature. In this paper, we propose Ringmaster ASGD, a novel Asynchronous SGD method designed to address these limitations and tame the inherent challenges of Asynchronous SGD. We establish, through rigorous theoretical analysis, that Ringmaster ASGD achieves optimal time complexity under arbitrarily heterogeneous and dynamically fluctuating worker computation times. This makes it the first Asynchronous SGD method to meet the theoretical lower bounds for time complexity in such scenarios.","sentences":["Asynchronous Stochastic Gradient Descent (Asynchronous SGD) is a cornerstone method for parallelizing learning in distributed machine learning.","However, its performance suffers under arbitrarily heterogeneous computation times across workers, leading to suboptimal time complexity and inefficiency as the number of workers scales.","While several Asynchronous SGD variants have been proposed, recent findings by Tyurin & Richt\\'arik (NeurIPS 2023) reveal that none achieve optimal time complexity, leaving a significant gap in the literature.","In this paper, we propose Ringmaster ASGD, a novel Asynchronous SGD method designed to address these limitations and tame the inherent challenges of Asynchronous SGD.","We establish, through rigorous theoretical analysis, that Ringmaster ASGD achieves optimal time complexity under arbitrarily heterogeneous and dynamically fluctuating worker computation times.","This makes it the first Asynchronous SGD method to meet the theoretical lower bounds for time complexity in such scenarios."],"url":"http://arxiv.org/abs/2501.16168v1"}
{"created":"2025-01-27 16:03:14","title":"Demystifying OS Kernel Fuzzing with a Novel Taxonomy","abstract":"The Operating System (OS) kernel is foundational in modern computing, especially with the proliferation of diverse computing devices. However, its development also comes with vulnerabilities that can lead to severe security breaches. Kernel fuzzing, a technique used to uncover these vulnerabilities, poses distinct challenges when compared to userspace fuzzing. These include the complexity of configuring the testing environment and addressing the statefulness inherent to both the kernel and the fuzzing process. Despite the significant interest from the security community, a comprehensive understanding of kernel fuzzing remains lacking, hindering further progress in the field. In this paper, we present the first systematic study dedicated to OS kernel fuzzing. It begins by summarizing the progress of 99 academic studies from top-tier venues between 2017 and 2024. Following this, we introduce a stage-based fuzzing model and a novel fuzzing taxonomy that highlights nine core functionalities unique to kernel fuzzing. These functionalities are examined alongside their corresponding methodological approaches based on qualitative evaluation criteria. Our systematization identifies challenges in meeting functionality requirements and proposes potential technical solutions. Finally, we outline promising and practical future directions to guide forthcoming research in kernel security, supported in part by insights derived from our case study.","sentences":["The Operating System (OS) kernel is foundational in modern computing, especially with the proliferation of diverse computing devices.","However, its development also comes with vulnerabilities that can lead to severe security breaches.","Kernel fuzzing, a technique used to uncover these vulnerabilities, poses distinct challenges when compared to userspace fuzzing.","These include the complexity of configuring the testing environment and addressing the statefulness inherent to both the kernel and the fuzzing process.","Despite the significant interest from the security community, a comprehensive understanding of kernel fuzzing remains lacking, hindering further progress in the field.","In this paper, we present the first systematic study dedicated to OS kernel fuzzing.","It begins by summarizing the progress of 99 academic studies from top-tier venues between 2017 and 2024.","Following this, we introduce a stage-based fuzzing model and a novel fuzzing taxonomy that highlights nine core functionalities unique to kernel fuzzing.","These functionalities are examined alongside their corresponding methodological approaches based on qualitative evaluation criteria.","Our systematization identifies challenges in meeting functionality requirements and proposes potential technical solutions.","Finally, we outline promising and practical future directions to guide forthcoming research in kernel security, supported in part by insights derived from our case study."],"url":"http://arxiv.org/abs/2501.16165v1"}
{"created":"2025-01-27 15:59:58","title":"MetaDecorator: Generating Immersive Virtual Tours through Multimodality","abstract":"MetaDecorator, is a framework that empowers users to personalize virtual spaces. By leveraging text-driven prompts and image synthesis techniques, MetaDecorator adorns static panoramas captured by 360{\\deg} imaging devices, transforming them into uniquely styled and visually appealing environments. This significantly enhances the realism and engagement of virtual tours compared to traditional offerings. Beyond the core framework, we also discuss the integration of Large Language Models (LLMs) and haptics in the VR application to provide a more immersive experience.","sentences":["MetaDecorator, is a framework that empowers users to personalize virtual spaces.","By leveraging text-driven prompts and image synthesis techniques, MetaDecorator adorns static panoramas captured by 360{\\deg} imaging devices, transforming them into uniquely styled and visually appealing environments.","This significantly enhances the realism and engagement of virtual tours compared to traditional offerings.","Beyond the core framework, we also discuss the integration of Large Language Models (LLMs) and haptics in the VR application to provide a more immersive experience."],"url":"http://arxiv.org/abs/2501.16164v1"}
{"created":"2025-01-27 15:56:12","title":"Comprehensive Benchmarking Environment for Worker Flexibility in Flexible Job Shop Scheduling Problems","abstract":"In Production Scheduling, the Flexible Job Shop Scheduling Problem (FJSSP) aims to optimize a sequence of operations and assign each to an eligible machine with varying processing times. For integration of the workforce, each machine also requires a worker to be present to process an operation which additionally affects the processing times. The resulting problem is called Flexible Job Shop Scheduling Problem with Worker Flexibility (FJSSP-W). The FJSSP has been approached with various problem representations, including Mixed Integer Linear Programming (MILP), Constrained Programming (CP), and Simulation-based Optimization (SBO). In the latter area in particular, there exists a large number of specialized Evolutionary Algorithms (EA) like Particle Swarm Optimization (PSO) or Genetic Algorithms (GA). Yet, the solvers are often developed for single use cases only, and validated on a few selected test instances, let alone compared with results from solvers using other problem representations. While suitable approaches do also exist, the design of the FJSSP-W instances is not standardized and the algorithms are hardly comparable. This calls for a systematic benchmarking environment that provides a comprehensive set of FJSSP(-W) instances and supports targeted algorithm development. It will facilitate the comparison of algorithmic performance in the face of different problem characteristics. The present paper presents a collection of 402 commonly accepted FJSSP instances and proposes an approach to extend these with worker flexibility. In addition, we present a detailed procedure for the evaluation of scheduling algorithms on these problem sets and provide suitable model representations for this purpose. We provide complexity characteristics for all presented instances as well as baseline results of common commercial solvers to facilitate the validation of new algorithmic developments.","sentences":["In Production Scheduling, the Flexible Job Shop Scheduling Problem (FJSSP) aims to optimize a sequence of operations and assign each to an eligible machine with varying processing times.","For integration of the workforce, each machine also requires a worker to be present to process an operation which additionally affects the processing times.","The resulting problem is called Flexible Job Shop Scheduling Problem with Worker Flexibility (FJSSP-W).","The FJSSP has been approached with various problem representations, including Mixed Integer Linear Programming (MILP), Constrained Programming (CP), and Simulation-based Optimization (SBO).","In the latter area in particular, there exists a large number of specialized Evolutionary Algorithms (EA) like Particle Swarm Optimization (PSO) or Genetic Algorithms (GA).","Yet, the solvers are often developed for single use cases only, and validated on a few selected test instances, let alone compared with results from solvers using other problem representations.","While suitable approaches do also exist, the design of the FJSSP-W instances is not standardized and the algorithms are hardly comparable.","This calls for a systematic benchmarking environment that provides a comprehensive set of FJSSP(-W) instances and supports targeted algorithm development.","It will facilitate the comparison of algorithmic performance in the face of different problem characteristics.","The present paper presents a collection of 402 commonly accepted FJSSP instances and proposes an approach to extend these with worker flexibility.","In addition, we present a detailed procedure for the evaluation of scheduling algorithms on these problem sets and provide suitable model representations for this purpose.","We provide complexity characteristics for all presented instances as well as baseline results of common commercial solvers to facilitate the validation of new algorithmic developments."],"url":"http://arxiv.org/abs/2501.16159v1"}
{"created":"2025-01-27 15:49:24","title":"CITYWALK: Enhancing LLM-Based C++ Unit Test Generation via Project-Dependency Awareness and Language-Specific Knowledge","abstract":"Unit testing plays a pivotal role in the software development lifecycle, as it ensures code quality. However, writing high-quality unit tests remains a time-consuming task for developers in practice. More recently, the application of large language models (LLMs) in automated unit test generation has demonstrated promising results. Existing approaches primarily focus on interpreted programming languages (e.g., Java), while mature solutions tailored to compiled programming languages like C++ are yet to be explored. The intricate language features of C++, such as pointers, templates, and virtual functions, pose particular challenges for LLMs in generating both executable and high-coverage unit tests. To tackle the aforementioned problems, this paper introduces CITYWALK, a novel LLM-based framework for C++ unit test generation. CITYWALK enhances LLMs by providing a comprehensive understanding of the dependency relationships within the project under test via program analysis. Furthermore, CITYWALK incorporates language-specific knowledge about C++ derived from project documentation and empirical observations, significantly improving the correctness of the LLM-generated unit tests. We implement CITYWALK by employing the widely popular LLM GPT-4o. The experimental results show that CITYWALK outperforms current state-of-the-art approaches on a collection of eight popular C++ projects. Our findings demonstrate the effectiveness of CITYWALK in generating high-quality C++ unit tests.","sentences":["Unit testing plays a pivotal role in the software development lifecycle, as it ensures code quality.","However, writing high-quality unit tests remains a time-consuming task for developers in practice.","More recently, the application of large language models (LLMs) in automated unit test generation has demonstrated promising results.","Existing approaches primarily focus on interpreted programming languages (e.g., Java), while mature solutions tailored to compiled programming languages like C++ are yet to be explored.","The intricate language features of C++, such as pointers, templates, and virtual functions, pose particular challenges for LLMs in generating both executable and high-coverage unit tests.","To tackle the aforementioned problems, this paper introduces CITYWALK, a novel LLM-based framework for C++ unit test generation.","CITYWALK enhances LLMs by providing a comprehensive understanding of the dependency relationships within the project under test via program analysis.","Furthermore, CITYWALK incorporates language-specific knowledge about C++ derived from project documentation and empirical observations, significantly improving the correctness of the LLM-generated unit tests.","We implement CITYWALK by employing the widely popular LLM GPT-4o.","The experimental results show that CITYWALK outperforms current state-of-the-art approaches on a collection of eight popular C++ projects.","Our findings demonstrate the effectiveness of CITYWALK in generating high-quality C++ unit tests."],"url":"http://arxiv.org/abs/2501.16155v1"}
{"created":"2025-01-27 15:48:57","title":"AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Chain-of-Thought","abstract":"Large language models (LLMs) have shown impressive multilingual capabilities through pretraining on diverse corpora. While these models show strong reasoning abilities, their performance varies significantly across languages due to uneven training data distribution. Existing approaches using machine translation, and extensive multilingual pretraining and cross-lingual tuning face scalability challenges and often fail to capture nuanced reasoning processes across languages. In this paper, we introduce AdaCoT (Adaptive Chain-of-Thought), a framework that enhances multilingual reasoning by dynamically routing thought processes through intermediary \"thinking languages\" before generating target-language responses. AdaCoT leverages a language-agnostic core and incorporates an adaptive, reward-based mechanism for selecting optimal reasoning pathways without requiring additional pretraining. Our comprehensive evaluation across multiple benchmarks demonstrates substantial improvements in both factual reasoning quality and cross-lingual consistency, with particularly strong performance gains in low-resource language settings. The results suggest that adaptive reasoning paths can effectively bridge the performance gap between high and low-resource languages while maintaining cultural and linguistic nuances.","sentences":["Large language models (LLMs) have shown impressive multilingual capabilities through pretraining on diverse corpora.","While these models show strong reasoning abilities, their performance varies significantly across languages due to uneven training data distribution.","Existing approaches using machine translation, and extensive multilingual pretraining and cross-lingual tuning face scalability challenges and often fail to capture nuanced reasoning processes across languages.","In this paper, we introduce AdaCoT","(Adaptive Chain-of-Thought), a framework that enhances multilingual reasoning by dynamically routing thought processes through intermediary \"thinking languages\" before generating target-language responses.","AdaCoT","leverages a language-agnostic core and incorporates an adaptive, reward-based mechanism for selecting optimal reasoning pathways without requiring additional pretraining.","Our comprehensive evaluation across multiple benchmarks demonstrates substantial improvements in both factual reasoning quality and cross-lingual consistency, with particularly strong performance gains in low-resource language settings.","The results suggest that adaptive reasoning paths can effectively bridge the performance gap between high and low-resource languages while maintaining cultural and linguistic nuances."],"url":"http://arxiv.org/abs/2501.16154v1"}
{"created":"2025-01-27 15:46:38","title":"MILP initialization for solving parabolic PDEs with PINNs","abstract":"Physics-Informed Neural Networks (PINNs) are a powerful deep learning method capable of providing solutions and parameter estimations of physical systems. Given the complexity of their neural network structure, the convergence speed is still limited compared to numerical methods, mainly when used in applications that model realistic systems. The network initialization follows a random distribution of the initial weights, as in the case of traditional neural networks, which could lead to severe model convergence bottlenecks. To overcome this problem, we follow current studies that deal with optimal initial weights in traditional neural networks. In this paper, we use a convex optimization model to improve the initialization of the weights in PINNs and accelerate convergence. We investigate two optimization models as a first training step, defined as pre-training, one involving only the boundaries and one including physics. The optimization is focused on the first layer of the neural network part of the PINN model, while the other weights are randomly initialized. We test the methods using a practical application of the heat diffusion equation to model the temperature distribution of power transformers. The PINN model with boundary pre-training is the fastest converging method at the current stage.","sentences":["Physics-Informed Neural Networks (PINNs) are a powerful deep learning method capable of providing solutions and parameter estimations of physical systems.","Given the complexity of their neural network structure, the convergence speed is still limited compared to numerical methods, mainly when used in applications that model realistic systems.","The network initialization follows a random distribution of the initial weights, as in the case of traditional neural networks, which could lead to severe model convergence bottlenecks.","To overcome this problem, we follow current studies that deal with optimal initial weights in traditional neural networks.","In this paper, we use a convex optimization model to improve the initialization of the weights in PINNs and accelerate convergence.","We investigate two optimization models as a first training step, defined as pre-training, one involving only the boundaries and one including physics.","The optimization is focused on the first layer of the neural network part of the PINN model, while the other weights are randomly initialized.","We test the methods using a practical application of the heat diffusion equation to model the temperature distribution of power transformers.","The PINN model with boundary pre-training is the fastest converging method at the current stage."],"url":"http://arxiv.org/abs/2501.16153v1"}
{"created":"2025-01-27 15:44:02","title":"AI Agents for Computer Use: A Review of Instruction-based Computer Control, GUI Automation, and Operator Assistants","abstract":"Instruction-based computer control agents (CCAs) execute complex action sequences on personal computers or mobile devices to fulfill tasks using the same graphical user interfaces as a human user would, provided instructions in natural language. This review offers a comprehensive overview of the emerging field of instruction-based computer control, examining available agents -- their taxonomy, development, and respective resources -- and emphasizing the shift from manually designed, specialized agents to leveraging foundation models such as large language models (LLMs) and vision-language models (VLMs). We formalize the problem and establish a taxonomy of the field to analyze agents from three perspectives: (a) the environment perspective, analyzing computer environments; (b) the interaction perspective, describing observations spaces (e.g., screenshots, HTML) and action spaces (e.g., mouse and keyboard actions, executable code); and (c) the agent perspective, focusing on the core principle of how an agent acts and learns to act. Our framework encompasses both specialized and foundation agents, facilitating their comparative analysis and revealing how prior solutions in specialized agents, such as an environment learning step, can guide the development of more capable foundation agents. Additionally, we review current CCA datasets and CCA evaluation methods and outline the challenges to deploying such agents in a productive setting. In total, we review and classify 86 CCAs and 33 related datasets. By highlighting trends, limitations, and future research directions, this work presents a comprehensive foundation to obtain a broad understanding of the field and push its future development.","sentences":["Instruction-based computer control agents (CCAs) execute complex action sequences on personal computers or mobile devices to fulfill tasks using the same graphical user interfaces as a human user would, provided instructions in natural language.","This review offers a comprehensive overview of the emerging field of instruction-based computer control, examining available agents -- their taxonomy, development, and respective resources -- and emphasizing the shift from manually designed, specialized agents to leveraging foundation models such as large language models (LLMs) and vision-language models (VLMs).","We formalize the problem and establish a taxonomy of the field to analyze agents from three perspectives: (a) the environment perspective, analyzing computer environments; (b) the interaction perspective, describing observations spaces (e.g., screenshots, HTML) and action spaces (e.g., mouse and keyboard actions, executable code); and (c) the agent perspective, focusing on the core principle of how an agent acts and learns to act.","Our framework encompasses both specialized and foundation agents, facilitating their comparative analysis and revealing how prior solutions in specialized agents, such as an environment learning step, can guide the development of more capable foundation agents.","Additionally, we review current CCA datasets and CCA evaluation methods and outline the challenges to deploying such agents in a productive setting.","In total, we review and classify 86 CCAs and 33 related datasets.","By highlighting trends, limitations, and future research directions, this work presents a comprehensive foundation to obtain a broad understanding of the field and push its future development."],"url":"http://arxiv.org/abs/2501.16150v1"}
{"created":"2025-01-27 15:43:04","title":"PATCH: Empowering Large Language Model with Programmer-Intent Guidance and Collaborative-Behavior Simulation for Automatic Bug Fixing","abstract":"Bug fixing holds significant importance in software development and maintenance. Recent research has made substantial strides in exploring the potential of large language models (LLMs) for automatically resolving software bugs. However, a noticeable gap in existing approaches lies in the oversight of collaborative facets intrinsic to bug resolution, treating the process as a single-stage endeavor. Moreover, most approaches solely take the buggy code snippet as input for LLMs during the patch generation stage. To mitigate the aforementioned limitations, we introduce a novel stage-wise framework named PATCH. Specifically, we first augment the buggy code snippet with corresponding dependence context and intent information to better guide LLMs in generating the correct candidate patches. Additionally, by taking inspiration from bug management practices, we decompose the bug-fixing task into four distinct stages: bug reporting, bug diagnosis, patch generation, and patch verification. These stages are performed interactively by LLMs, aiming to simulate the collaborative behavior of programmers during the resolution of software bugs. By harnessing these collective contributions, PATCH effectively enhances the bug-fixing capability of LLMs. We implement PATCH by employing the powerful dialogue-based LLM ChatGPT. Our evaluation on the widely used bug-fixing benchmark BFP demonstrates that PATCH has achieved better performance than state-of-the-art LLMs.","sentences":["Bug fixing holds significant importance in software development and maintenance.","Recent research has made substantial strides in exploring the potential of large language models (LLMs) for automatically resolving software bugs.","However, a noticeable gap in existing approaches lies in the oversight of collaborative facets intrinsic to bug resolution, treating the process as a single-stage endeavor.","Moreover, most approaches solely take the buggy code snippet as input for LLMs during the patch generation stage.","To mitigate the aforementioned limitations, we introduce a novel stage-wise framework named PATCH.","Specifically, we first augment the buggy code snippet with corresponding dependence context and intent information to better guide LLMs in generating the correct candidate patches.","Additionally, by taking inspiration from bug management practices, we decompose the bug-fixing task into four distinct stages: bug reporting, bug diagnosis, patch generation, and patch verification.","These stages are performed interactively by LLMs, aiming to simulate the collaborative behavior of programmers during the resolution of software bugs.","By harnessing these collective contributions, PATCH effectively enhances the bug-fixing capability of LLMs.","We implement PATCH by employing the powerful dialogue-based LLM ChatGPT.","Our evaluation on the widely used bug-fixing benchmark BFP demonstrates that PATCH has achieved better performance than state-of-the-art LLMs."],"url":"http://arxiv.org/abs/2501.16149v1"}
{"created":"2025-01-27 15:41:19","title":"Efficient Portrait Matte Creation With Layer Diffusion and Connectivity Priors","abstract":"Learning effective deep portrait matting models requires training data of both high quality and large quantity. Neither quality nor quantity can be easily met for portrait matting, however. Since the most accurate ground-truth portrait mattes are acquired in front of the green screen, it is almost impossible to harvest a large-scale portrait matting dataset in reality. This work shows that one can leverage text prompts and the recent Layer Diffusion model to generate high-quality portrait foregrounds and extract latent portrait mattes. However, the portrait mattes cannot be readily in use due to significant generation artifacts. Inspired by the connectivity priors observed in portrait images, that is, the border of portrait foregrounds always appears connected, a connectivity-aware approach is introduced to refine portrait mattes. Building on this, a large-scale portrait matting dataset is created, termed LD-Portrait-20K, with $20,051$ portrait foregrounds and high-quality alpha mattes. Extensive experiments demonstrated the value of the LD-Portrait-20K dataset, with models trained on it significantly outperforming those trained on other datasets. In addition, comparisons with the chroma keying algorithm and an ablation study on dataset capacity further confirmed the effectiveness of the proposed matte creation approach. Further, the dataset also contributes to state-of-the-art video portrait matting, implemented by simple video segmentation and a trimap-based image matting model trained on this dataset.","sentences":["Learning effective deep portrait matting models requires training data of both high quality and large quantity.","Neither quality nor quantity can be easily met for portrait matting, however.","Since the most accurate ground-truth portrait mattes are acquired in front of the green screen, it is almost impossible to harvest a large-scale portrait matting dataset in reality.","This work shows that one can leverage text prompts and the recent Layer Diffusion model to generate high-quality portrait foregrounds and extract latent portrait mattes.","However, the portrait mattes cannot be readily in use due to significant generation artifacts.","Inspired by the connectivity priors observed in portrait images, that is, the border of portrait foregrounds always appears connected, a connectivity-aware approach is introduced to refine portrait mattes.","Building on this, a large-scale portrait matting dataset is created, termed LD-Portrait-20K, with $20,051$ portrait foregrounds and high-quality alpha mattes.","Extensive experiments demonstrated the value of the LD-Portrait-20K dataset, with models trained on it significantly outperforming those trained on other datasets.","In addition, comparisons with the chroma keying algorithm and an ablation study on dataset capacity further confirmed the effectiveness of the proposed matte creation approach.","Further, the dataset also contributes to state-of-the-art video portrait matting, implemented by simple video segmentation and a trimap-based image matting model trained on this dataset."],"url":"http://arxiv.org/abs/2501.16147v1"}
{"created":"2025-01-27 15:39:39","title":"Toward Efficient Generalization in 3D Human Pose Estimation via a Canonical Domain Approach","abstract":"Recent advancements in deep learning methods have significantly improved the performance of 3D Human Pose Estimation (HPE). However, performance degradation caused by domain gaps between source and target domains remains a major challenge to generalization, necessitating extensive data augmentation and/or fine-tuning for each specific target domain. To address this issue more efficiently, we propose a novel canonical domain approach that maps both the source and target domains into a unified canonical domain, alleviating the need for additional fine-tuning in the target domain. To construct the canonical domain, we introduce a canonicalization process to generate a novel canonical 2D-3D pose mapping that ensures 2D-3D pose consistency and simplifies 2D-3D pose patterns, enabling more efficient training of lifting networks. The canonicalization of both domains is achieved through the following steps: (1) in the source domain, the lifting network is trained within the canonical domain; (2) in the target domain, input 2D poses are canonicalized prior to inference by leveraging the properties of perspective projection and known camera intrinsics. Consequently, the trained network can be directly applied to the target domain without requiring additional fine-tuning. Experiments conducted with various lifting networks and publicly available datasets (e.g., Human3.6M, Fit3D, MPI-INF-3DHP) demonstrate that the proposed method substantially improves generalization capability across datasets while using the same data volume.","sentences":["Recent advancements in deep learning methods have significantly improved the performance of 3D Human Pose Estimation (HPE).","However, performance degradation caused by domain gaps between source and target domains remains a major challenge to generalization, necessitating extensive data augmentation and/or fine-tuning for each specific target domain.","To address this issue more efficiently, we propose a novel canonical domain approach that maps both the source and target domains into a unified canonical domain, alleviating the need for additional fine-tuning in the target domain.","To construct the canonical domain, we introduce a canonicalization process to generate a novel canonical 2D-3D pose mapping that ensures 2D-3D pose consistency and simplifies 2D-3D pose patterns, enabling more efficient training of lifting networks.","The canonicalization of both domains is achieved through the following steps: (1) in the source domain, the lifting network is trained within the canonical domain; (2) in the target domain, input 2D poses are canonicalized prior to inference by leveraging the properties of perspective projection and known camera intrinsics.","Consequently, the trained network can be directly applied to the target domain without requiring additional fine-tuning.","Experiments conducted with various lifting networks and publicly available datasets (e.g., Human3.6M, Fit3D, MPI-INF-3DHP) demonstrate that the proposed method substantially improves generalization capability across datasets while using the same data volume."],"url":"http://arxiv.org/abs/2501.16146v1"}
{"created":"2025-01-27 15:39:20","title":"Capacity-Achieving Input Distribution of the Additive Uniform Noise Channel With Peak Amplitude and Cost Constraint","abstract":"Under which condition is quantization optimal? We address this question in the context of the additive uniform noise channel under peak amplitude and power constraints. We compute analytically the capacity-achieving input distribution as a function of the noise level, the average power constraint and the exponent of the power constraint. We found that when the cost constraint is tight and the cost function is concave, the capacity-achieving input distribution is discrete, whereas when the cost function is convex, the support of the capacity-achieving input distribution spans the entire interval.","sentences":["Under which condition is quantization optimal?","We address this question in the context of the additive uniform noise channel under peak amplitude and power constraints.","We compute analytically the capacity-achieving input distribution as a function of the noise level, the average power constraint and the exponent of the power constraint.","We found that when the cost constraint is tight and the cost function is concave, the capacity-achieving input distribution is discrete, whereas when the cost function is convex, the support of the capacity-achieving input distribution spans the entire interval."],"url":"http://arxiv.org/abs/2501.16145v1"}
{"created":"2025-01-27 15:36:51","title":"Disruption-aware Microservice Re-orchestration for Cost-efficient Multi-cloud Deployments","abstract":"Multi-cloud environments enable a cost-efficient scaling of cloud-native applications across geographically distributed virtual nodes with different pricing models. In this context, the resource fragmentation caused by frequent changes in the resource demands of deployed microservices, along with the allocation or termination of new and existing microservices, increases the deployment cost. Therefore, re-orchestrating deployed microservices on a cheaper configuration of multi-cloud nodes offers a practical solution to restore the cost efficiency of deployment. However, the rescheduling procedure causes frequent service interruptions due to the continuous termination and rebooting of the containerized microservices. Moreover, it may potentially interfere with and delay other deployment operations, compromising the stability of the running applications. To address this issue, we formulate a multi-objective integer linear programming problem that computes a microservice rescheduling solution capable of providing minimum deployment cost without significantly affecting the service continuity. At the same time, the proposed formulation also preserves the quality of service (QoS) requirements, including latency, expressed through microservice colocation constraints. Additionally, we present a heuristic algorithm to approximate the optimal solution, striking a balance between cost reduction and service disruption mitigation. We integrate the proposed approach as a custom plugin of the Kubernetes scheduler. Results reveal that our approach significantly reduces multi-cloud deployment costs and service disruptions compared to the default Kubernetes scheduler implementation, while ensuring QoS requirements are consistently met.","sentences":["Multi-cloud environments enable a cost-efficient scaling of cloud-native applications across geographically distributed virtual nodes with different pricing models.","In this context, the resource fragmentation caused by frequent changes in the resource demands of deployed microservices, along with the allocation or termination of new and existing microservices, increases the deployment cost.","Therefore, re-orchestrating deployed microservices on a cheaper configuration of multi-cloud nodes offers a practical solution to restore the cost efficiency of deployment.","However, the rescheduling procedure causes frequent service interruptions due to the continuous termination and rebooting of the containerized microservices.","Moreover, it may potentially interfere with and delay other deployment operations, compromising the stability of the running applications.","To address this issue, we formulate a multi-objective integer linear programming problem that computes a microservice rescheduling solution capable of providing minimum deployment cost without significantly affecting the service continuity.","At the same time, the proposed formulation also preserves the quality of service (QoS) requirements, including latency, expressed through microservice colocation constraints.","Additionally, we present a heuristic algorithm to approximate the optimal solution, striking a balance between cost reduction and service disruption mitigation.","We integrate the proposed approach as a custom plugin of the Kubernetes scheduler.","Results reveal that our approach significantly reduces multi-cloud deployment costs and service disruptions compared to the default Kubernetes scheduler implementation, while ensuring QoS requirements are consistently met."],"url":"http://arxiv.org/abs/2501.16143v1"}
{"created":"2025-01-27 15:36:37","title":"Towards General-Purpose Model-Free Reinforcement Learning","abstract":"Reinforcement learning (RL) promises a framework for near-universal problem-solving. In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices. Recently, powerful model-based RL methods have shown impressive general results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability. In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings. To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories. We evaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and general baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms.","sentences":["Reinforcement learning (RL) promises a framework for near-universal problem-solving.","In practice however, RL algorithms are often tailored to specific benchmarks, relying on carefully tuned hyperparameters and algorithmic choices.","Recently, powerful model-based RL methods have shown impressive general results across benchmarks but come at the cost of increased complexity and slow run times, limiting their broader applicability.","In this paper, we attempt to find a unifying model-free deep RL algorithm that can address a diverse class of domains and problem settings.","To achieve this, we leverage model-based representations that approximately linearize the value function, taking advantage of the denser task objectives used by model-based RL while avoiding the costs associated with planning or simulated trajectories.","We evaluate our algorithm, MR.Q, on a variety of common RL benchmarks with a single set of hyperparameters and show a competitive performance against domain-specific and general baselines, providing a concrete step towards building general-purpose model-free deep RL algorithms."],"url":"http://arxiv.org/abs/2501.16142v1"}
{"created":"2025-01-27 15:28:05","title":"Quantifying the Self-Interest Level of Markov Social Dilemmas","abstract":"This paper introduces a novel method for estimating the self-interest level of computationally intractable Markov social dilemmas. We extend the concept of self-interest level from normal-form games to Markov games, providing a quantitative measure of the minimum reward exchange required to incentivize cooperation by aligning individual and collective interests. We demonstrate our method on three environments from the Melting Pot suite: which represent either common-pool resources or public goods. Our results show that the proposed method successfully identifies a threshold at which learning agents transition from selfish to cooperative equilibria in a Markov social dilemma. This work contributes to the fields of Cooperative AI and multiagent reinforcement learning by providing a practical tool for analysing complex, multistep social dilemmas. Our findings offer insights into how reward structures can promote or hinger cooperation in challenging multiagent scenarios, with potential applications in areas such as mechanism design.","sentences":["This paper introduces a novel method for estimating the self-interest level of computationally intractable Markov social dilemmas.","We extend the concept of self-interest level from normal-form games to Markov games, providing a quantitative measure of the minimum reward exchange required to incentivize cooperation by aligning individual and collective interests.","We demonstrate our method on three environments from the Melting Pot suite: which represent either common-pool resources or public goods.","Our results show that the proposed method successfully identifies a threshold at which learning agents transition from selfish to cooperative equilibria in a Markov social dilemma.","This work contributes to the fields of Cooperative AI and multiagent reinforcement learning by providing a practical tool for analysing complex, multistep social dilemmas.","Our findings offer insights into how reward structures can promote or hinger cooperation in challenging multiagent scenarios, with potential applications in areas such as mechanism design."],"url":"http://arxiv.org/abs/2501.16138v1"}
{"created":"2025-01-27 15:25:26","title":"Evaluation of NMT-Assisted Grammar Transfer for a Multi-Language Configurable Data-to-Text System","abstract":"One approach for multilingual data-to-text generation is to translate grammatical configurations upfront from the source language into each target language. These configurations are then used by a surface realizer and in document planning stages to generate output. In this paper, we describe a rule-based NLG implementation of this approach where the configuration is translated by Neural Machine Translation (NMT) combined with a one-time human review, and introduce a cross-language grammar dependency model to create a multilingual NLG system that generates text from the source data, scaling the generation phase without a human in the loop. Additionally, we introduce a method for human post-editing evaluation on the automatically translated text. Our evaluation on the SportSett:Basketball dataset shows that our NLG system performs well, underlining its grammatical correctness in translation tasks.","sentences":["One approach for multilingual data-to-text generation is to translate grammatical configurations upfront from the source language into each target language.","These configurations are then used by a surface realizer and in document planning stages to generate output.","In this paper, we describe a rule-based NLG implementation of this approach where the configuration is translated by Neural Machine Translation (NMT) combined with a one-time human review, and introduce a cross-language grammar dependency model to create a multilingual NLG system that generates text from the source data, scaling the generation phase without a human in the loop.","Additionally, we introduce a method for human post-editing evaluation on the automatically translated text.","Our evaluation on the SportSett:Basketball dataset shows that our NLG system performs well, underlining its grammatical correctness in translation tasks."],"url":"http://arxiv.org/abs/2501.16135v1"}
{"created":"2025-01-27 15:20:50","title":"Optimized Self-supervised Training with BEST-RQ for Speech Recognition","abstract":"Self-supervised learning has been successfully used for various speech related tasks, including automatic speech recognition. BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ) has achieved state-of-the-art results in speech recognition. In this work, we further optimize the BEST-RQ approach using Kullback-Leibler divergence as an additional regularizing loss and multi-codebook extension per cluster derived from low-level feature clustering. Preliminary experiments on train-100 split of LibriSpeech result in a relative improvement of 11.2% on test-clean by using multiple codebooks, utilizing a combination of cross-entropy and Kullback-Leibler divergence further reduces the word error rate by 4.5%. The proposed optimizations on full LibriSpeech pre-training and fine-tuning result in relative word error rate improvements of up to 23.8% on test-clean and 30.6% on test-other using 6 codebooks. Furthermore, the proposed setup leads to faster convergence in pre-training and fine-tuning and additionally stabilizes the pre-training.","sentences":["Self-supervised learning has been successfully used for various speech related tasks, including automatic speech recognition.","BERT-based Speech pre-Training with Random-projection Quantizer (BEST-RQ) has achieved state-of-the-art results in speech recognition.","In this work, we further optimize the BEST-RQ approach using Kullback-Leibler divergence as an additional regularizing loss and multi-codebook extension per cluster derived from low-level feature clustering.","Preliminary experiments on train-100 split of LibriSpeech result in a relative improvement of 11.2% on test-clean by using multiple codebooks, utilizing a combination of cross-entropy and Kullback-Leibler divergence further reduces the word error rate by 4.5%.","The proposed optimizations on full LibriSpeech pre-training and fine-tuning result in relative word error rate improvements of up to 23.8% on test-clean and 30.6% on test-other using 6 codebooks.","Furthermore, the proposed setup leads to faster convergence in pre-training and fine-tuning and additionally stabilizes the pre-training."],"url":"http://arxiv.org/abs/2501.16131v1"}
{"created":"2025-01-27 15:20:41","title":"ReFill: Reinforcement Learning for Fill-In Minimization","abstract":"Efficiently solving sparse linear systems $Ax=b$, where $A$ is a large, sparse, symmetric positive semi-definite matrix, is a core challenge in scientific computing, machine learning, and optimization. A major bottleneck in Gaussian elimination for these systems is fill-in, the creation of non-zero entries that increase memory and computational cost. Minimizing fill-in is NP-hard, and existing heuristics like Minimum Degree and Nested Dissection offer limited adaptability across diverse problem instances.   We introduce \\textit{ReFill}, a reinforcement learning framework enhanced by Graph Neural Networks (GNNs) to learn adaptive ordering strategies for fill-in minimization. ReFill trains a GNN-based heuristic to predict efficient elimination orders, outperforming traditional heuristics by dynamically adapting to the structure of input matrices. Experiments demonstrate that ReFill outperforms strong heuristics in reducing fill-in, highlighting the untapped potential of learning-based methods for this well-studied classical problem.","sentences":["Efficiently solving sparse linear systems $Ax=b$, where $A$ is a large, sparse, symmetric positive semi-definite matrix, is a core challenge in scientific computing, machine learning, and optimization.","A major bottleneck in Gaussian elimination for these systems is fill-in, the creation of non-zero entries that increase memory and computational cost.","Minimizing fill-in is NP-hard, and existing heuristics like Minimum Degree and Nested Dissection offer limited adaptability across diverse problem instances.   ","We introduce \\textit{ReFill}, a reinforcement learning framework enhanced by Graph Neural Networks (GNNs) to learn adaptive ordering strategies for fill-in minimization.","ReFill trains a GNN-based heuristic to predict efficient elimination orders, outperforming traditional heuristics by dynamically adapting to the structure of input matrices.","Experiments demonstrate that ReFill outperforms strong heuristics in reducing fill-in, highlighting the untapped potential of learning-based methods for this well-studied classical problem."],"url":"http://arxiv.org/abs/2501.16130v1"}
