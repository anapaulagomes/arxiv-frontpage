{"created":"2024-11-26 18:59:58","title":"Video-Guided Foley Sound Generation with Multimodal Controls","abstract":"Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design. To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video. Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow). MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning. A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation. Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods. Please see our project page for video results: https://ificl.github.io/MultiFoley/","sentences":["Generating sound effects for videos often requires creating artistic sound effects that diverge significantly from real-life sources and flexible control in the sound design.","To address this problem, we introduce MultiFoley, a model designed for video-guided sound generation that supports multimodal conditioning through text, audio, and video.","Given a silent video and a text prompt, MultiFoley allows users to create clean sounds (e.g., skateboard wheels spinning without wind noise) or more whimsical sounds (e.g., making a lion's roar sound like a cat's meow).","MultiFoley also allows users to choose reference audio from sound effects (SFX) libraries or partial videos for conditioning.","A key novelty of our model lies in its joint training on both internet video datasets with low-quality audio and professional SFX recordings, enabling high-quality, full-bandwidth (48kHz) audio generation.","Through automated evaluations and human studies, we demonstrate that MultiFoley successfully generates synchronized high-quality sounds across varied conditional inputs and outperforms existing methods.","Please see our project page for video results: https://ificl.github.io/MultiFoley/"],"url":"http://arxiv.org/abs/2411.17698v1"}
{"created":"2024-11-26 18:59:22","title":"StableAnimator: High-Quality Identity-Preserving Human Image Animation","abstract":"Current diffusion models for human image animation struggle to ensure identity (ID) consistency. This paper presents StableAnimator, the first end-to-end ID-preserving video diffusion framework, which synthesizes high-quality videos without any post-processing, conditioned on a reference image and a sequence of poses. Building upon a video diffusion model, StableAnimator contains carefully designed modules for both training and inference striving for identity consistency. In particular, StableAnimator begins by computing image and face embeddings with off-the-shelf extractors, respectively and face embeddings are further refined by interacting with image embeddings using a global content-aware Face Encoder. Then, StableAnimator introduces a novel distribution-aware ID Adapter that prevents interference caused by temporal layers while preserving ID via alignment. During inference, we propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to further enhance the face quality. We demonstrate that solving the HJB equation can be integrated into the diffusion denoising process, and the resulting solution constrains the denoising path and thus benefits ID preservation. Experiments on multiple benchmarks show the effectiveness of StableAnimator both qualitatively and quantitatively.","sentences":["Current diffusion models for human image animation struggle to ensure identity (ID) consistency.","This paper presents StableAnimator, the first end-to-end ID-preserving video diffusion framework, which synthesizes high-quality videos without any post-processing, conditioned on a reference image and a sequence of poses.","Building upon a video diffusion model, StableAnimator contains carefully designed modules for both training and inference striving for identity consistency.","In particular, StableAnimator begins by computing image and face embeddings with off-the-shelf extractors, respectively and face embeddings are further refined by interacting with image embeddings using a global content-aware Face Encoder.","Then, StableAnimator introduces a novel distribution-aware ID Adapter that prevents interference caused by temporal layers while preserving ID via alignment.","During inference, we propose a novel Hamilton-Jacobi-Bellman (HJB) equation-based optimization to further enhance the face quality.","We demonstrate that solving the HJB equation can be integrated into the diffusion denoising process, and the resulting solution constrains the denoising path and thus benefits ID preservation.","Experiments on multiple benchmarks show the effectiveness of StableAnimator both qualitatively and quantitatively."],"url":"http://arxiv.org/abs/2411.17697v1"}
{"created":"2024-11-26 18:59:11","title":"ScribbleLight: Single Image Indoor Relighting with Scribbles","abstract":"Image-based relighting of indoor rooms creates an immersive virtual understanding of the space, which is useful for interior design, virtual staging, and real estate. Relighting indoor rooms from a single image is especially challenging due to complex illumination interactions between multiple lights and cluttered objects featuring a large variety in geometrical and material complexity. Recently, generative models have been successfully applied to image-based relighting conditioned on a target image or a latent code, albeit without detailed local lighting control. In this paper, we introduce ScribbleLight, a generative model that supports local fine-grained control of lighting effects through scribbles that describe changes in lighting. Our key technical novelty is an Albedo-conditioned Stable Image Diffusion model that preserves the intrinsic color and texture of the original image after relighting and an encoder-decoder-based ControlNet architecture that enables geometry-preserving lighting effects with normal map and scribble annotations. We demonstrate ScribbleLight's ability to create different lighting effects (e.g., turning lights on/off, adding highlights, cast shadows, or indirect lighting from unseen lights) from sparse scribble annotations.","sentences":["Image-based relighting of indoor rooms creates an immersive virtual understanding of the space, which is useful for interior design, virtual staging, and real estate.","Relighting indoor rooms from a single image is especially challenging due to complex illumination interactions between multiple lights and cluttered objects featuring a large variety in geometrical and material complexity.","Recently, generative models have been successfully applied to image-based relighting conditioned on a target image or a latent code, albeit without detailed local lighting control.","In this paper, we introduce ScribbleLight, a generative model that supports local fine-grained control of lighting effects through scribbles that describe changes in lighting.","Our key technical novelty is an Albedo-conditioned Stable Image Diffusion model that preserves the intrinsic color and texture of the original image after relighting and an encoder-decoder-based ControlNet architecture that enables geometry-preserving lighting effects with normal map and scribble annotations.","We demonstrate ScribbleLight's ability to create different lighting effects (e.g., turning lights on/off, adding highlights, cast shadows, or indirect lighting from unseen lights) from sparse scribble annotations."],"url":"http://arxiv.org/abs/2411.17696v1"}
{"created":"2024-11-26 18:58:20","title":"Adaptive Deployment of Untrusted LLMs Reduces Distributed Threats","abstract":"As large language models (LLMs) become increasingly capable, it is prudent to assess whether safety measures remain effective even if LLMs intentionally try to bypass them. Previous work introduced control evaluations, an adversarial framework for testing deployment strategies of untrusted models (i.e., models which might be trying to bypass safety measures). While prior work treats a single failure as unacceptable, we perform control evaluations in a \"distributed threat setting\" -- a setting where no single action is catastrophic and no single action provides overwhelming evidence of misalignment. We approach this problem with a two-level deployment framework that uses an adaptive macro-protocol to choose between micro-protocols. Micro-protocols operate on a single task, using a less capable, but extensively tested (trusted) model to harness and monitor the untrusted model. Meanwhile, the macro-protocol maintains an adaptive credence on the untrusted model's alignment based on its past actions, using it to pick between safer and riskier micro-protocols. We evaluate our method in a code generation testbed where a red team attempts to generate subtly backdoored code with an LLM whose deployment is safeguarded by a blue team. We plot Pareto frontiers of safety (# of non-backdoored solutions) and usefulness (# of correct solutions). At a given level of usefulness, our adaptive deployment strategy reduces the number of backdoors by 80% compared to non-adaptive baselines.","sentences":["As large language models (LLMs) become increasingly capable, it is prudent to assess whether safety measures remain effective even if LLMs intentionally try to bypass them.","Previous work introduced control evaluations, an adversarial framework for testing deployment strategies of untrusted models (i.e., models which might be trying to bypass safety measures).","While prior work treats a single failure as unacceptable, we perform control evaluations in a \"distributed threat setting\" -- a setting where no single action is catastrophic and no single action provides overwhelming evidence of misalignment.","We approach this problem with a two-level deployment framework that uses an adaptive macro-protocol to choose between micro-protocols.","Micro-protocols operate on a single task, using a less capable, but extensively tested (trusted) model to harness and monitor the untrusted model.","Meanwhile, the macro-protocol maintains an adaptive credence on the untrusted model's alignment based on its past actions, using it to pick between safer and riskier micro-protocols.","We evaluate our method in a code generation testbed where a red team attempts to generate subtly backdoored code with an LLM whose deployment is safeguarded by a blue team.","We plot Pareto frontiers of safety (# of non-backdoored solutions) and usefulness (# of correct solutions).","At a given level of usefulness, our adaptive deployment strategy reduces the number of backdoors by 80% compared to non-adaptive baselines."],"url":"http://arxiv.org/abs/2411.17693v1"}
{"created":"2024-11-26 18:57:58","title":"Low-Bit Quantization Favors Undertrained LLMs: Scaling Laws for Quantized LLMs with 100T Training Tokens","abstract":"We reveal that low-bit quantization favors undertrained large language models (LLMs) by observing that models with larger sizes or fewer training tokens experience less quantization-induced degradation (QiD) when applying low-bit quantization, whereas smaller models with extensive training tokens suffer significant QiD. To gain deeper insights into this trend, we study over 1500 quantized LLM checkpoints of various sizes and at different training levels (undertrained or fully trained) in a controlled setting, deriving scaling laws for understanding the relationship between QiD and factors such as the number of training tokens, model size and bit width.   With the derived scaling laws, we propose a novel perspective that we can use QiD to measure an LLM's training levels and determine the number of training tokens required for fully training LLMs of various sizes. Moreover, we use the scaling laws to predict the quantization performance of different-sized LLMs trained with 100 trillion tokens. Our projection shows that the low-bit quantization performance of future models, which are expected to be trained with over 100 trillion tokens, may NOT be desirable. This poses a potential challenge for low-bit quantization in the future and highlights the need for awareness of a model's training level when evaluating low-bit quantization research. To facilitate future research on this problem, we release all the 1500+ quantized checkpoints used in this work at https://huggingface.co/Xu-Ouyang.","sentences":["We reveal that low-bit quantization favors undertrained large language models (LLMs) by observing that models with larger sizes or fewer training tokens experience less quantization-induced degradation (QiD) when applying low-bit quantization, whereas smaller models with extensive training tokens suffer significant QiD. To gain deeper insights into this trend, we study over 1500 quantized LLM checkpoints of various sizes and at different training levels (undertrained or fully trained) in a controlled setting, deriving scaling laws for understanding the relationship between QiD and factors such as the number of training tokens, model size and bit width.   ","With the derived scaling laws, we propose a novel perspective that we can use QiD to measure an LLM's training levels and determine the number of training tokens required for fully training LLMs of various sizes.","Moreover, we use the scaling laws to predict the quantization performance of different-sized LLMs trained with 100 trillion tokens.","Our projection shows that the low-bit quantization performance of future models, which are expected to be trained with over 100 trillion tokens, may NOT be desirable.","This poses a potential challenge for low-bit quantization in the future and highlights the need for awareness of a model's training level when evaluating low-bit quantization research.","To facilitate future research on this problem, we release all the 1500+ quantized checkpoints used in this work at https://huggingface.co/Xu-Ouyang."],"url":"http://arxiv.org/abs/2411.17691v1"}
{"created":"2024-11-26 18:57:29","title":"Visatronic: A Multimodal Decoder-Only Model for Speech Synthesis","abstract":"In this paper, we propose a new task -- generating speech from videos of people and their transcripts (VTTS) -- to motivate new techniques for multimodal speech generation. This task generalizes the task of generating speech from cropped lip videos, and is also more complicated than the task of generating generic audio clips (e.g., dog barking) from videos and text. Multilingual versions of the task could lead to new techniques for cross-lingual dubbing. We also present a decoder-only multimodal model for this task, which we call Visatronic. This model embeds vision, text and speech directly into the common subspace of a transformer model and uses an autoregressive loss to learn a generative model of discretized mel-spectrograms conditioned on speaker videos and transcripts of their speech. By embedding all modalities into a common subspace, Visatronic can achieve improved results over models that use only text or video as input. Further, it presents a much simpler approach for multimodal speech generation compared to prevailing approaches which rely on lip-detectors and complicated architectures to fuse modalities while producing better results. Since the model is flexible enough to accommodate different ways of ordering inputs as a sequence, we carefully explore different strategies to better understand the best way to propagate information to the generative steps. To facilitate further research on VTTS, we will release (i) our code, (ii) clean transcriptions for the large-scale VoxCeleb2 dataset, and (iii) a standardized evaluation protocol for VTTS incorporating both objective and subjective metrics.","sentences":["In this paper, we propose a new task -- generating speech from videos of people and their transcripts (VTTS) -- to motivate new techniques for multimodal speech generation.","This task generalizes the task of generating speech from cropped lip videos, and is also more complicated than the task of generating generic audio clips (e.g., dog barking) from videos and text.","Multilingual versions of the task could lead to new techniques for cross-lingual dubbing.","We also present a decoder-only multimodal model for this task, which we call Visatronic.","This model embeds vision, text and speech directly into the common subspace of a transformer model and uses an autoregressive loss to learn a generative model of discretized mel-spectrograms conditioned on speaker videos and transcripts of their speech.","By embedding all modalities into a common subspace, Visatronic can achieve improved results over models that use only text or video as input.","Further, it presents a much simpler approach for multimodal speech generation compared to prevailing approaches which rely on lip-detectors and complicated architectures to fuse modalities while producing better results.","Since the model is flexible enough to accommodate different ways of ordering inputs as a sequence, we carefully explore different strategies to better understand the best way to propagate information to the generative steps.","To facilitate further research on VTTS, we will release (i) our code, (ii) clean transcriptions for the large-scale VoxCeleb2 dataset, and (iii) a standardized evaluation protocol for VTTS incorporating both objective and subjective metrics."],"url":"http://arxiv.org/abs/2411.17690v1"}
{"created":"2024-11-26 18:55:49","title":"GenDeg: Diffusion-Based Degradation Synthesis for Generalizable All-in-One Image Restoration","abstract":"Deep learning-based models for All-In-One Image Restoration (AIOR) have achieved significant advancements in recent years. However, their practical applicability is limited by poor generalization to samples outside the training distribution. This limitation arises primarily from insufficient diversity in degradation variations and scenes within existing datasets, resulting in inadequate representations of real-world scenarios. Additionally, capturing large-scale real-world paired data for degradations such as haze, low-light, and raindrops is often cumbersome and sometimes infeasible. In this paper, we leverage the generative capabilities of latent diffusion models to synthesize high-quality degraded images from their clean counterparts. Specifically, we introduce GenDeg, a degradation and intensity-aware conditional diffusion model capable of producing diverse degradation patterns on clean images. Using GenDeg, we synthesize over 550k samples across six degradation types: haze, rain, snow, motion blur, low-light, and raindrops. These generated samples are integrated with existing datasets to form the GenDS dataset, comprising over 750k samples. Our experiments reveal that image restoration models trained on the GenDS dataset exhibit significant improvements in out-of-distribution performance compared to those trained solely on existing datasets. Furthermore, we provide comprehensive analyses on the implications of diffusion model-based synthetic degradations for AIOR. The code will be made publicly available.","sentences":["Deep learning-based models for All-In-One Image Restoration (AIOR) have achieved significant advancements in recent years.","However, their practical applicability is limited by poor generalization to samples outside the training distribution.","This limitation arises primarily from insufficient diversity in degradation variations and scenes within existing datasets, resulting in inadequate representations of real-world scenarios.","Additionally, capturing large-scale real-world paired data for degradations such as haze, low-light, and raindrops is often cumbersome and sometimes infeasible.","In this paper, we leverage the generative capabilities of latent diffusion models to synthesize high-quality degraded images from their clean counterparts.","Specifically, we introduce GenDeg, a degradation and intensity-aware conditional diffusion model capable of producing diverse degradation patterns on clean images.","Using GenDeg, we synthesize over 550k samples across six degradation types: haze, rain, snow, motion blur, low-light, and raindrops.","These generated samples are integrated with existing datasets to form the GenDS dataset, comprising over 750k samples.","Our experiments reveal that image restoration models trained on the GenDS dataset exhibit significant improvements in out-of-distribution performance compared to those trained solely on existing datasets.","Furthermore, we provide comprehensive analyses on the implications of diffusion model-based synthetic degradations for AIOR.","The code will be made publicly available."],"url":"http://arxiv.org/abs/2411.17687v1"}
{"created":"2024-11-26 18:53:51","title":"Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration","abstract":"To accelerate the inference of heavy Multimodal Large Language Models (MLLMs), this study rethinks the current landscape of training-free token reduction research. We regret to find that the critical components of existing methods are tightly intertwined, with their interconnections and effects remaining unclear for comparison, transfer, and expansion. Therefore, we propose a unified ''filter-correlate-compress'' paradigm that decomposes the token reduction into three distinct stages within a pipeline, maintaining consistent design objectives and elements while allowing for unique implementations. We additionally demystify the popular works and subsume them into our paradigm to showcase its universality. Finally, we offer a suite of methods grounded in the paradigm, striking a balance between speed and accuracy throughout different phases of the inference. Experimental results across 10 benchmarks indicate that our methods can achieve up to an 82.4% reduction in FLOPs with a minimal impact on performance, simultaneously surpassing state-of-the-art training-free methods. Our project page is at https://ficoco-accelerate.github.io/.","sentences":["To accelerate the inference of heavy Multimodal Large Language Models (MLLMs), this study rethinks the current landscape of training-free token reduction research.","We regret to find that the critical components of existing methods are tightly intertwined, with their interconnections and effects remaining unclear for comparison, transfer, and expansion.","Therefore, we propose a unified ''filter-correlate-compress'' paradigm that decomposes the token reduction into three distinct stages within a pipeline, maintaining consistent design objectives and elements while allowing for unique implementations.","We additionally demystify the popular works and subsume them into our paradigm to showcase its universality.","Finally, we offer a suite of methods grounded in the paradigm, striking a balance between speed and accuracy throughout different phases of the inference.","Experimental results across 10 benchmarks indicate that our methods can achieve up to an 82.4% reduction in FLOPs with a minimal impact on performance, simultaneously surpassing state-of-the-art training-free methods.","Our project page is at https://ficoco-accelerate.github.io/."],"url":"http://arxiv.org/abs/2411.17686v1"}
{"created":"2024-11-26 18:52:06","title":"Attamba: Attending To Multi-Token States","abstract":"When predicting the next token in a sequence, vanilla transformers compute attention over all previous tokens, resulting in quadratic scaling of compute with sequence length. State-space models compress the entire sequence of tokens into a fixed-dimensional representation to improve efficiency, while other architectures achieve sub-quadratic complexity via low-rank projections or sparse attention patterns over the sequence. In this paper, we introduce Attamba, a novel architecture that uses state-space models to compress chunks of tokens and applies attention on these compressed key-value representations. We find that replacing key and value projections in a transformer with SSMs can improve model quality and enable flexible token chunking, resulting in 24% improved perplexity with transformer of similar KV-Cache and attention footprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity trade-off. Attamba can perform attention on chunked-sequences of variable length, enabling a smooth transition between quadratic and linear scaling, offering adaptable efficiency gains.","sentences":["When predicting the next token in a sequence, vanilla transformers compute attention over all previous tokens, resulting in quadratic scaling of compute with sequence length.","State-space models compress the entire sequence of tokens into a fixed-dimensional representation to improve efficiency, while other architectures achieve sub-quadratic complexity via low-rank projections or sparse attention patterns over the sequence.","In this paper, we introduce Attamba, a novel architecture that uses state-space models to compress chunks of tokens and applies attention on these compressed key-value representations.","We find that replacing key and value projections in a transformer with SSMs can improve model quality and enable flexible token chunking, resulting in 24% improved perplexity with transformer of similar KV-Cache and attention footprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity trade-off.","Attamba can perform attention on chunked-sequences of variable length, enabling a smooth transition between quadratic and linear scaling, offering adaptable efficiency gains."],"url":"http://arxiv.org/abs/2411.17685v1"}
{"created":"2024-11-26 18:48:23","title":"RealSeal: Revolutionizing Media Authentication with Real-Time Realism Scoring","abstract":"The growing threat of deepfakes and manipulated media necessitates a radical rethinking of media authentication. Existing methods for watermarking synthetic data fall short, as they can be easily removed or altered, and current deepfake detection algorithms do not achieve perfect accuracy. Provenance techniques, which rely on metadata to verify content origin, fail to address the fundamental problem of staged or fake media.   This paper introduces a groundbreaking paradigm shift in media authentication by advocating for the watermarking of real content at its source, as opposed to watermarking synthetic data. Our innovative approach employs multisensory inputs and machine learning to assess the realism of content in real-time and across different contexts. We propose embedding a robust realism score within the image metadata, fundamentally transforming how images are trusted and circulated. By combining established principles of human reasoning about reality, rooted in firmware and hardware security, with the sophisticated reasoning capabilities of contemporary machine learning systems, we develop a holistic approach that analyzes information from multiple perspectives.   This ambitious, blue sky approach represents a significant leap forward in the field, pushing the boundaries of media authenticity and trust. By embracing cutting-edge advancements in technology and interdisciplinary research, we aim to establish a new standard for verifying the authenticity of digital media.","sentences":["The growing threat of deepfakes and manipulated media necessitates a radical rethinking of media authentication.","Existing methods for watermarking synthetic data fall short, as they can be easily removed or altered, and current deepfake detection algorithms do not achieve perfect accuracy.","Provenance techniques, which rely on metadata to verify content origin, fail to address the fundamental problem of staged or fake media.   ","This paper introduces a groundbreaking paradigm shift in media authentication by advocating for the watermarking of real content at its source, as opposed to watermarking synthetic data.","Our innovative approach employs multisensory inputs and machine learning to assess the realism of content in real-time and across different contexts.","We propose embedding a robust realism score within the image metadata, fundamentally transforming how images are trusted and circulated.","By combining established principles of human reasoning about reality, rooted in firmware and hardware security, with the sophisticated reasoning capabilities of contemporary machine learning systems, we develop a holistic approach that analyzes information from multiple perspectives.   ","This ambitious, blue sky approach represents a significant leap forward in the field, pushing the boundaries of media authenticity and trust.","By embracing cutting-edge advancements in technology and interdisciplinary research, we aim to establish a new standard for verifying the authenticity of digital media."],"url":"http://arxiv.org/abs/2411.17684v1"}
{"created":"2024-11-26 18:44:39","title":"Enhancing Character-Level Understanding in LLMs through Token Internal Structure Learning","abstract":"Tokenization techniques such as Byte-Pair Encoding (BPE) and Byte-Level BPE (BBPE) have significantly improved the computational efficiency and vocabulary representation stability of large language models (LLMs) by segmenting text into tokens. However, this segmentation often obscures the internal character structures and sequences within tokens, preventing models from fully learning these intricate details during training. Consequently, LLMs struggle to comprehend the character compositions and positional relationships within tokens, especially when fine-tuned on downstream tasks with limited data. In this paper, we introduce Token Internal Position Awareness (TIPA), a novel approach that enhances LLMs' understanding of internal token structures by training them on reverse character prediction tasks using the tokenizer's own vocabulary. This method enables models to effectively learn and generalize character positions and internal structures. Experimental results demonstrate that LLMs trained with TIPA outperform baseline models in predicting character positions at the token level. Furthermore, when applied to the downstream task of Chinese Spelling Correction (CSC), TIPA not only accelerates model convergence but also significantly improves task performance.","sentences":["Tokenization techniques such as Byte-Pair Encoding (BPE) and Byte-Level BPE (BBPE) have significantly improved the computational efficiency and vocabulary representation stability of large language models (LLMs) by segmenting text into tokens.","However, this segmentation often obscures the internal character structures and sequences within tokens, preventing models from fully learning these intricate details during training.","Consequently, LLMs struggle to comprehend the character compositions and positional relationships within tokens, especially when fine-tuned on downstream tasks with limited data.","In this paper, we introduce Token Internal Position Awareness (TIPA), a novel approach that enhances LLMs' understanding of internal token structures by training them on reverse character prediction tasks using the tokenizer's own vocabulary.","This method enables models to effectively learn and generalize character positions and internal structures.","Experimental results demonstrate that LLMs trained with TIPA outperform baseline models in predicting character positions at the token level.","Furthermore, when applied to the downstream task of Chinese Spelling Correction (CSC), TIPA not only accelerates model convergence but also significantly improves task performance."],"url":"http://arxiv.org/abs/2411.17679v1"}
{"created":"2024-11-26 18:38:38","title":"Instance-Aware Graph Prompt Learning","abstract":"Graph neural networks stand as the predominant technique for graph representation learning owing to their strong expressive power, yet the performance highly depends on the availability of high-quality labels in an end-to-end manner. Thus the pretraining and fine-tuning paradigm has been proposed to mitigate the label cost issue. Subsequently, the gap between the pretext tasks and downstream tasks has spurred the development of graph prompt learning which inserts a set of graph prompts into the original graph data with minimal parameters while preserving competitive performance. However, the current exploratory works are still limited since they all concentrate on learning fixed task-specific prompts which may not generalize well across the diverse instances that the task comprises. To tackle this challenge, we introduce Instance-Aware Graph Prompt Learning (IA-GPL) in this paper, aiming to generate distinct prompts tailored to different input instances. The process involves generating intermediate prompts for each instance using a lightweight architecture, quantizing these prompts through trainable codebook vectors, and employing the exponential moving average technique to ensure stable training. Extensive experiments conducted on multiple datasets and settings showcase the superior performance of IA-GPL compared to state-of-the-art baselines.","sentences":["Graph neural networks stand as the predominant technique for graph representation learning owing to their strong expressive power, yet the performance highly depends on the availability of high-quality labels in an end-to-end manner.","Thus the pretraining and fine-tuning paradigm has been proposed to mitigate the label cost issue.","Subsequently, the gap between the pretext tasks and downstream tasks has spurred the development of graph prompt learning which inserts a set of graph prompts into the original graph data with minimal parameters while preserving competitive performance.","However, the current exploratory works are still limited since they all concentrate on learning fixed task-specific prompts which may not generalize well across the diverse instances that the task comprises.","To tackle this challenge, we introduce Instance-Aware Graph Prompt Learning (IA-GPL) in this paper, aiming to generate distinct prompts tailored to different input instances.","The process involves generating intermediate prompts for each instance using a lightweight architecture, quantizing these prompts through trainable codebook vectors, and employing the exponential moving average technique to ensure stable training.","Extensive experiments conducted on multiple datasets and settings showcase the superior performance of IA-GPL compared to state-of-the-art baselines."],"url":"http://arxiv.org/abs/2411.17676v1"}
{"created":"2024-11-26 18:35:24","title":"Push the Limit of Multi-modal Emotion Recognition by Prompting LLMs with Receptive-Field-Aware Attention Weighting","abstract":"Understanding the emotions in a dialogue usually requires external knowledge to accurately understand the contents. As the LLMs become more and more powerful, we do not want to settle on the limited ability of the pre-trained language model. However, the LLMs either can only process text modality or are too expensive to process the multimedia information. We aim to utilize both the power of LLMs and the supplementary features from the multimedia modalities. In this paper, we present a framework, Lantern, that can improve the performance of a certain vanilla model by prompting large language models with receptive-field-aware attention weighting. This framework trained a multi-task vanilla model to produce probabilities of emotion classes and dimension scores. These predictions are fed into the LLMs as references to adjust the predicted probabilities of each emotion class with its external knowledge and contextual understanding. We slice the dialogue into different receptive fields, and each sample is included in exactly t receptive fields. Finally, the predictions of LLMs are merged with a receptive-field-aware attention-driven weighting module. In the experiments, vanilla models CORECT and SDT are deployed in Lantern with GPT-4 or Llama-3.1-405B. The experiments in IEMOCAP with 4-way and 6-way settings demonstrated that the Lantern can significantly improve the performance of current vanilla models by up to 1.23% and 1.80%.","sentences":["Understanding the emotions in a dialogue usually requires external knowledge to accurately understand the contents.","As the LLMs become more and more powerful, we do not want to settle on the limited ability of the pre-trained language model.","However, the LLMs either can only process text modality or are too expensive to process the multimedia information.","We aim to utilize both the power of LLMs and the supplementary features from the multimedia modalities.","In this paper, we present a framework, Lantern, that can improve the performance of a certain vanilla model by prompting large language models with receptive-field-aware attention weighting.","This framework trained a multi-task vanilla model to produce probabilities of emotion classes and dimension scores.","These predictions are fed into the LLMs as references to adjust the predicted probabilities of each emotion class with its external knowledge and contextual understanding.","We slice the dialogue into different receptive fields, and each sample is included in exactly t receptive fields.","Finally, the predictions of LLMs are merged with a receptive-field-aware attention-driven weighting module.","In the experiments, vanilla models CORECT and SDT are deployed in Lantern with GPT-4 or Llama-3.1-405B.","The experiments in IEMOCAP with 4-way and 6-way settings demonstrated that the Lantern can significantly improve the performance of current vanilla models by up to 1.23% and 1.80%."],"url":"http://arxiv.org/abs/2411.17674v1"}
{"created":"2024-11-26 18:32:06","title":"SketchAgent: Language-Driven Sequential Sketch Generation","abstract":"Sketching serves as a versatile tool for externalizing ideas, enabling rapid exploration and visual communication that spans various disciplines. While artificial systems have driven substantial advances in content creation and human-computer interaction, capturing the dynamic and abstract nature of human sketching remains challenging. In this work, we introduce SketchAgent, a language-driven, sequential sketch generation method that enables users to create, modify, and refine sketches through dynamic, conversational interactions. Our approach requires no training or fine-tuning. Instead, we leverage the sequential nature and rich prior knowledge of off-the-shelf multimodal large language models (LLMs). We present an intuitive sketching language, introduced to the model through in-context examples, enabling it to \"draw\" using string-based actions. These are processed into vector graphics and then rendered to create a sketch on a pixel canvas, which can be accessed again for further tasks. By drawing stroke by stroke, our agent captures the evolving, dynamic qualities intrinsic to sketching. We demonstrate that SketchAgent can generate sketches from diverse prompts, engage in dialogue-driven drawing, and collaborate meaningfully with human users.","sentences":["Sketching serves as a versatile tool for externalizing ideas, enabling rapid exploration and visual communication that spans various disciplines.","While artificial systems have driven substantial advances in content creation and human-computer interaction, capturing the dynamic and abstract nature of human sketching remains challenging.","In this work, we introduce SketchAgent, a language-driven, sequential sketch generation method that enables users to create, modify, and refine sketches through dynamic, conversational interactions.","Our approach requires no training or fine-tuning.","Instead, we leverage the sequential nature and rich prior knowledge of off-the-shelf multimodal large language models (LLMs).","We present an intuitive sketching language, introduced to the model through in-context examples, enabling it to \"draw\" using string-based actions.","These are processed into vector graphics and then rendered to create a sketch on a pixel canvas, which can be accessed again for further tasks.","By drawing stroke by stroke, our agent captures the evolving, dynamic qualities intrinsic to sketching.","We demonstrate that SketchAgent can generate sketches from diverse prompts, engage in dialogue-driven drawing, and collaborate meaningfully with human users."],"url":"http://arxiv.org/abs/2411.17673v1"}
{"created":"2024-11-26 18:31:14","title":"Synthetic Data Generation with LLM for Improved Depression Prediction","abstract":"Automatic detection of depression is a rapidly growing field of research at the intersection of psychology and machine learning. However, with its exponential interest comes a growing concern for data privacy and scarcity due to the sensitivity of such a topic. In this paper, we propose a pipeline for Large Language Models (LLMs) to generate synthetic data to improve the performance of depression prediction models. Starting from unstructured, naturalistic text data from recorded transcripts of clinical interviews, we utilize an open-source LLM to generate synthetic data through chain-of-thought prompting. This pipeline involves two key steps: the first step is the generation of the synopsis and sentiment analysis based on the original transcript and depression score, while the second is the generation of the synthetic synopsis/sentiment analysis based on the summaries generated in the first step and a new depression score. Not only was the synthetic data satisfactory in terms of fidelity and privacy-preserving metrics, it also balanced the distribution of severity in the training dataset, thereby significantly enhancing the model's capability in predicting the intensity of the patient's depression. By leveraging LLMs to generate synthetic data that can be augmented to limited and imbalanced real-world datasets, we demonstrate a novel approach to addressing data scarcity and privacy concerns commonly faced in automatic depression detection, all while maintaining the statistical integrity of the original dataset. This approach offers a robust framework for future mental health research and applications.","sentences":["Automatic detection of depression is a rapidly growing field of research at the intersection of psychology and machine learning.","However, with its exponential interest comes a growing concern for data privacy and scarcity due to the sensitivity of such a topic.","In this paper, we propose a pipeline for Large Language Models (LLMs) to generate synthetic data to improve the performance of depression prediction models.","Starting from unstructured, naturalistic text data from recorded transcripts of clinical interviews, we utilize an open-source LLM to generate synthetic data through chain-of-thought prompting.","This pipeline involves two key steps: the first step is the generation of the synopsis and sentiment analysis based on the original transcript and depression score, while the second is the generation of the synthetic synopsis/sentiment analysis based on the summaries generated in the first step and a new depression score.","Not only was the synthetic data satisfactory in terms of fidelity and privacy-preserving metrics, it also balanced the distribution of severity in the training dataset, thereby significantly enhancing the model's capability in predicting the intensity of the patient's depression.","By leveraging LLMs to generate synthetic data that can be augmented to limited and imbalanced real-world datasets, we demonstrate a novel approach to addressing data scarcity and privacy concerns commonly faced in automatic depression detection, all while maintaining the statistical integrity of the original dataset.","This approach offers a robust framework for future mental health research and applications."],"url":"http://arxiv.org/abs/2411.17672v1"}
{"created":"2024-11-26 18:30:20","title":"Linguistic Laws Meet Protein Sequences: A Comparative Analysis of Subword Tokenization Methods","abstract":"Tokenization is a crucial step in processing protein sequences for machine learning models, as proteins are complex sequences of amino acids that require meaningful segmentation to capture their functional and structural properties. However, existing subword tokenization methods, developed primarily for human language, may be inadequate for protein sequences, which have unique patterns and constraints. This study evaluates three prominent tokenization approaches, Byte-Pair Encoding (BPE), WordPiece, and SentencePiece, across varying vocabulary sizes (400-6400), analyzing their effectiveness in protein sequence representation, domain boundary preservation, and adherence to established linguistic laws. Our comprehensive analysis reveals distinct behavioral patterns among these tokenizers, with vocabulary size significantly influencing their performance. BPE demonstrates better contextual specialization and marginally better domain boundary preservation at smaller vocabularies, while SentencePiece achieves better encoding efficiency, leading to lower fertility scores. WordPiece offers a balanced compromise between these characteristics. However, all tokenizers show limitations in maintaining protein domain integrity, particularly as vocabulary size increases. Analysis of linguistic law adherence shows partial compliance with Zipf's and Brevity laws but notable deviations from Menzerath's law, suggesting that protein sequences may follow distinct organizational principles from natural languages. These findings highlight the limitations of applying traditional NLP tokenization methods to protein sequences and emphasize the need for developing specialized tokenization strategies that better account for the unique characteristics of proteins.","sentences":["Tokenization is a crucial step in processing protein sequences for machine learning models, as proteins are complex sequences of amino acids that require meaningful segmentation to capture their functional and structural properties.","However, existing subword tokenization methods, developed primarily for human language, may be inadequate for protein sequences, which have unique patterns and constraints.","This study evaluates three prominent tokenization approaches, Byte-Pair Encoding (BPE), WordPiece, and SentencePiece, across varying vocabulary sizes (400-6400), analyzing their effectiveness in protein sequence representation, domain boundary preservation, and adherence to established linguistic laws.","Our comprehensive analysis reveals distinct behavioral patterns among these tokenizers, with vocabulary size significantly influencing their performance.","BPE demonstrates better contextual specialization and marginally better domain boundary preservation at smaller vocabularies, while SentencePiece achieves better encoding efficiency, leading to lower fertility scores.","WordPiece offers a balanced compromise between these characteristics.","However, all tokenizers show limitations in maintaining protein domain integrity, particularly as vocabulary size increases.","Analysis of linguistic law adherence shows partial compliance with Zipf's and Brevity laws but notable deviations from Menzerath's law, suggesting that protein sequences may follow distinct organizational principles from natural languages.","These findings highlight the limitations of applying traditional NLP tokenization methods to protein sequences and emphasize the need for developing specialized tokenization strategies that better account for the unique characteristics of proteins."],"url":"http://arxiv.org/abs/2411.17669v1"}
{"created":"2024-11-26 18:29:44","title":"Anytime Acceleration of Gradient Descent","abstract":"This work investigates stepsize-based acceleration of gradient descent with {\\em anytime} convergence guarantees. For smooth (non-strongly) convex optimization, we propose a stepsize schedule that allows gradient descent to achieve convergence guarantees of $O(T^{-1.03})$ for any stopping time $T$, where the stepsize schedule is predetermined without prior knowledge of the stopping time. This result provides an affirmative answer to a COLT open problem \\citep{kornowski2024open} regarding whether stepsize-based acceleration can yield anytime convergence rates of $o(T^{-1})$. We further extend our theory to yield anytime convergence guarantees of $\\exp(-\\Omega(T/\\kappa^{0.97}))$ for smooth and strongly convex optimization, with $\\kappa$ being the condition number.","sentences":["This work investigates stepsize-based acceleration of gradient descent with {\\em anytime} convergence guarantees.","For smooth (non-strongly) convex optimization, we propose a stepsize schedule that allows gradient descent to achieve convergence guarantees of $O(T^{-1.03})$ for any stopping time $T$, where the stepsize schedule is predetermined without prior knowledge of the stopping time.","This result provides an affirmative answer to a COLT open problem \\citep{kornowski2024open} regarding whether stepsize-based acceleration can yield anytime convergence rates of $o(T^{-1})$. We further extend our theory to yield anytime convergence guarantees of $\\exp(-\\Omega(T/\\kappa^{0.97}))$ for smooth and strongly convex optimization, with $\\kappa$ being the condition number."],"url":"http://arxiv.org/abs/2411.17668v1"}
{"created":"2024-11-26 18:29:11","title":"How do Multimodal Foundation Models Encode Text and Speech? An Analysis of Cross-Lingual and Cross-Modal Representations","abstract":"Multimodal foundation models aim to create a unified representation space that abstracts away from surface features like language syntax or modality differences. To investigate this, we study the internal representations of three recent models, analyzing the model activations from semantically equivalent sentences across languages in the text and speech modalities. Our findings reveal that: 1) Cross-modal representations converge over model layers, except in the initial layers specialized at text and speech processing. 2) Length adaptation is crucial for reducing the cross-modal gap between text and speech, although current approaches' effectiveness is primarily limited to high-resource languages. 3) Speech exhibits larger cross-lingual differences than text. 4) For models not explicitly trained for modality-agnostic representations, the modality gap is more prominent than the language gap.","sentences":["Multimodal foundation models aim to create a unified representation space that abstracts away from surface features like language syntax or modality differences.","To investigate this, we study the internal representations of three recent models, analyzing the model activations from semantically equivalent sentences across languages in the text and speech modalities.","Our findings reveal that: 1) Cross-modal representations converge over model layers, except in the initial layers specialized at text and speech processing.","2) Length adaptation is crucial for reducing the cross-modal gap between text and speech, although current approaches' effectiveness is primarily limited to high-resource languages.","3)","Speech exhibits larger cross-lingual differences than text.","4) For models not explicitly trained for modality-agnostic representations, the modality gap is more prominent than the language gap."],"url":"http://arxiv.org/abs/2411.17666v1"}
{"created":"2024-11-26 18:26:17","title":"RoboPEPP: Vision-Based Robot Pose and Joint Angle Estimation through Embedding Predictive Pre-Training","abstract":"Vision-based pose estimation of articulated robots with unknown joint angles has applications in collaborative robotics and human-robot interaction tasks. Current frameworks use neural network encoders to extract image features and downstream layers to predict joint angles and robot pose. While images of robots inherently contain rich information about the robot's physical structures, existing methods often fail to leverage it fully; therefore, limiting performance under occlusions and truncations. To address this, we introduce RoboPEPP, a method that fuses information about the robot's physical model into the encoder using a masking-based self-supervised embedding-predictive architecture. Specifically, we mask the robot's joints and pre-train an encoder-predictor model to infer the joints' embeddings from surrounding unmasked regions, enhancing the encoder's understanding of the robot's physical model. The pre-trained encoder-predictor pair, along with joint angle and keypoint prediction networks, is then fine-tuned for pose and joint angle estimation. Random masking of input during fine-tuning and keypoint filtering during evaluation further improves robustness. Our method, evaluated on several datasets, achieves the best results in robot pose and joint angle estimation while being the least sensitive to occlusions and requiring the lowest execution time.","sentences":["Vision-based pose estimation of articulated robots with unknown joint angles has applications in collaborative robotics and human-robot interaction tasks.","Current frameworks use neural network encoders to extract image features and downstream layers to predict joint angles and robot pose.","While images of robots inherently contain rich information about the robot's physical structures, existing methods often fail to leverage it fully; therefore, limiting performance under occlusions and truncations.","To address this, we introduce RoboPEPP, a method that fuses information about the robot's physical model into the encoder using a masking-based self-supervised embedding-predictive architecture.","Specifically, we mask the robot's joints and pre-train an encoder-predictor model to infer the joints' embeddings from surrounding unmasked regions, enhancing the encoder's understanding of the robot's physical model.","The pre-trained encoder-predictor pair, along with joint angle and keypoint prediction networks, is then fine-tuned for pose and joint angle estimation.","Random masking of input during fine-tuning and keypoint filtering during evaluation further improves robustness.","Our method, evaluated on several datasets, achieves the best results in robot pose and joint angle estimation while being the least sensitive to occlusions and requiring the lowest execution time."],"url":"http://arxiv.org/abs/2411.17662v1"}
{"created":"2024-11-26 18:25:57","title":"BERT or FastText? A Comparative Analysis of Contextual as well as Non-Contextual Embeddings","abstract":"Natural Language Processing (NLP) for low-resource languages presents significant challenges, particularly due to the scarcity of high-quality annotated data and linguistic resources. The choice of embeddings plays a critical role in enhancing the performance of NLP tasks, such as news classification, sentiment analysis, and hate speech detection, especially for low-resource languages like Marathi. In this study, we investigate the impact of various embedding techniques- Contextual BERT-based, Non-Contextual BERT-based, and FastText-based on NLP classification tasks specific to the Marathi language. Our research includes a thorough evaluation of both compressed and uncompressed embeddings, providing a comprehensive overview of how these embeddings perform across different scenarios. Specifically, we compare two BERT model embeddings, Muril and MahaBERT, as well as two FastText model embeddings, IndicFT and MahaFT. Our evaluation includes applying embeddings to a Multiple Logistic Regression (MLR) classifier for task performance assessment, as well as TSNE visualizations to observe the spatial distribution of these embeddings. The results demonstrate that contextual embeddings outperform non-contextual embeddings. Furthermore, BERT-based non-contextual embeddings extracted from the first BERT embedding layer yield better results than FastText-based embeddings, suggesting a potential alternative to FastText embeddings.","sentences":["Natural Language Processing (NLP) for low-resource languages presents significant challenges, particularly due to the scarcity of high-quality annotated data and linguistic resources.","The choice of embeddings plays a critical role in enhancing the performance of NLP tasks, such as news classification, sentiment analysis, and hate speech detection, especially for low-resource languages like Marathi.","In this study, we investigate the impact of various embedding techniques-","Contextual BERT-based, Non-Contextual BERT-based, and FastText-based on NLP classification tasks specific to the Marathi language.","Our research includes a thorough evaluation of both compressed and uncompressed embeddings, providing a comprehensive overview of how these embeddings perform across different scenarios.","Specifically, we compare two BERT model embeddings, Muril and MahaBERT, as well as two FastText model embeddings, IndicFT and MahaFT.","Our evaluation includes applying embeddings to a Multiple Logistic Regression (MLR) classifier for task performance assessment, as well as TSNE visualizations to observe the spatial distribution of these embeddings.","The results demonstrate that contextual embeddings outperform non-contextual embeddings.","Furthermore, BERT-based non-contextual embeddings extracted from the first BERT embedding layer yield better results than FastText-based embeddings, suggesting a potential alternative to FastText embeddings."],"url":"http://arxiv.org/abs/2411.17661v1"}
{"created":"2024-11-26 18:25:51","title":"DROID-Splat: Combining end-to-end SLAM with 3D Gaussian Splatting","abstract":"Recent progress in scene synthesis makes standalone SLAM systems purely based on optimizing hyperprimitives with a Rendering objective possible \\cite{monogs}.   However, the tracking performance still lacks behind traditional \\cite{orbslam} and end-to-end SLAM systems \\cite{droid}.   An optimal trade-off between robustness, speed and accuracy has not yet been reached, especially for monocular video.   In this paper, we introduce a SLAM system based on an end-to-end Tracker and extend it with a Renderer based on recent 3D Gaussian Splatting techniques.   Our framework \\textbf{DroidSplat} achieves both SotA tracking and rendering results on common SLAM benchmarks.   We implemented multiple building blocks of modern SLAM systems to run in parallel, allowing for fast inference on common consumer GPU's.   Recent progress in monocular depth prediction and camera calibration allows our system to achieve strong results even on in-the-wild data without known camera intrinsics.   Code will be available at \\url{https://github.com/ChenHoy/DROID-Splat}.","sentences":["Recent progress in scene synthesis makes standalone SLAM systems purely based on optimizing hyperprimitives with a Rendering objective possible \\cite{monogs}.   ","However, the tracking performance still lacks behind traditional \\cite{orbslam} and end-to-end SLAM systems \\cite{droid}.   ","An optimal trade-off between robustness, speed and accuracy has not yet been reached, especially for monocular video.   ","In this paper, we introduce a SLAM system based on an end-to-end Tracker and extend it with a Renderer based on recent 3D Gaussian Splatting techniques.   ","Our framework \\textbf{DroidSplat} achieves both SotA tracking and rendering results on common SLAM benchmarks.   ","We implemented multiple building blocks of modern SLAM systems to run in parallel, allowing for fast inference on common consumer GPU's.   ","Recent progress in monocular depth prediction and camera calibration allows our system to achieve strong results even on in-the-wild data without known camera intrinsics.   ","Code will be available at \\url{https://github.com/ChenHoy/DROID-Splat}."],"url":"http://arxiv.org/abs/2411.17660v1"}
{"created":"2024-11-26 18:16:56","title":"Toward High-Performance LLM Serving: A Simulation-Based Approach for Identifying Optimal Parallelism","abstract":"Serving Large Language Models (LLMs) efficiently has become crucial. LLMs are often served with multiple devices using techniques like data, pipeline, and tensor parallelisms. Each parallelism presents trade-offs between computation, memory, and communication overhead, making it challenging to determine the optimal parallel execution plan. Moreover, input workloads also impact parallelism strategies. Tasks with long prompts like article summarization are compute-intensive, while tasks with long generation lengths like code generation are often memory-intensive; these differing characteristics result in distinct optimal execution plans. Since searching for the optimal plan via actual deployment is prohibitively expensive, we propose APEX, an LLM serving system simulator that efficiently identifies an optimal parallel execution plan. APEX captures the complex characteristics of iteration-level batching, a technique widely used in SOTA LLM serving systems. APEX leverages the repetitive structure of LLMs to reduce design space, maintaining a similar simulation overhead, even when scaling to trillion scale models. APEX supports a wide range of LLMs, device clusters, etc., and it can be easily extended through its high-level templates. We run APEX simulations using a CPU and evaluate the identified optimal plans using 8 H100 GPUs, encompassing a wide range of LLMs and input workloads. We show that APEX can find optimal execution plans that are up to 4.42x faster than heuristic plans in terms of end-to-end serving latency. APEX also reports a set of metrics used in LLM serving systems, such as time per output token and time to first token. Furthermore, APEX can identify an optimal parallel execution plan within 15 minutes using a CPU. This is 71x faster and 1234x more cost-effective than actual deployment on a GPU cluster using cloud services. APEX will be open-sourced upon acceptance.","sentences":["Serving Large Language Models (LLMs) efficiently has become crucial.","LLMs are often served with multiple devices using techniques like data, pipeline, and tensor parallelisms.","Each parallelism presents trade-offs between computation, memory, and communication overhead, making it challenging to determine the optimal parallel execution plan.","Moreover, input workloads also impact parallelism strategies.","Tasks with long prompts like article summarization are compute-intensive, while tasks with long generation lengths like code generation are often memory-intensive; these differing characteristics result in distinct optimal execution plans.","Since searching for the optimal plan via actual deployment is prohibitively expensive, we propose APEX, an LLM serving system simulator that efficiently identifies an optimal parallel execution plan.","APEX captures the complex characteristics of iteration-level batching, a technique widely used in SOTA LLM serving systems.","APEX leverages the repetitive structure of LLMs to reduce design space, maintaining a similar simulation overhead, even when scaling to trillion scale models.","APEX supports a wide range of LLMs, device clusters, etc., and it can be easily extended through its high-level templates.","We run APEX simulations using a CPU and evaluate the identified optimal plans using 8 H100 GPUs, encompassing a wide range of LLMs and input workloads.","We show that APEX can find optimal execution plans that are up to 4.42x faster than heuristic plans in terms of end-to-end serving latency.","APEX also reports a set of metrics used in LLM serving systems, such as time per output token and time to first token.","Furthermore, APEX can identify an optimal parallel execution plan within 15 minutes using a CPU.","This is 71x faster and 1234x more cost-effective than actual deployment on a GPU cluster using cloud services.","APEX will be open-sourced upon acceptance."],"url":"http://arxiv.org/abs/2411.17651v1"}
{"created":"2024-11-26 18:10:54","title":"SAMWISE: Infusing wisdom in SAM2 for Text-Driven Video Segmentation","abstract":"Referring Video Object Segmentation (RVOS) relies on natural language expressions to segment an object in a video clip. Existing methods restrict reasoning either to independent short clips, losing global context, or process the entire video offline, impairing their application in a streaming fashion. In this work, we aim to surpass these limitations and design an RVOS method capable of effectively operating in streaming-like scenarios while retaining contextual information from past frames. We build upon the Segment-Anything 2 (SAM2) model, that provides robust segmentation and tracking capabilities and is naturally suited for streaming processing. We make SAM2 wiser, by empowering it with natural language understanding and explicit temporal modeling at the feature extraction stage, without fine-tuning its weights, and without outsourcing modality interaction to external models. To this end, we introduce a novel adapter module that injects temporal information and multi-modal cues in the feature extraction process. We further reveal the phenomenon of tracking bias in SAM2 and propose a learnable module to adjust its tracking focus when the current frame features suggest a new object more aligned with the caption. Our proposed method, SAMWISE, achieves state-of-the-art across various benchmarks, by adding a negligible overhead of just 4.2 M parameters. The code is available at https://github.com/ClaudiaCuttano/SAMWISE","sentences":["Referring Video Object Segmentation (RVOS) relies on natural language expressions to segment an object in a video clip.","Existing methods restrict reasoning either to independent short clips, losing global context, or process the entire video offline, impairing their application in a streaming fashion.","In this work, we aim to surpass these limitations and design an RVOS method capable of effectively operating in streaming-like scenarios while retaining contextual information from past frames.","We build upon the Segment-Anything 2 (SAM2) model, that provides robust segmentation and tracking capabilities and is naturally suited for streaming processing.","We make SAM2 wiser, by empowering it with natural language understanding and explicit temporal modeling at the feature extraction stage, without fine-tuning its weights, and without outsourcing modality interaction to external models.","To this end, we introduce a novel adapter module that injects temporal information and multi-modal cues in the feature extraction process.","We further reveal the phenomenon of tracking bias in SAM2 and propose a learnable module to adjust its tracking focus when the current frame features suggest a new object more aligned with the caption.","Our proposed method, SAMWISE, achieves state-of-the-art across various benchmarks, by adding a negligible overhead of just 4.2 M parameters.","The code is available at https://github.com/ClaudiaCuttano/SAMWISE"],"url":"http://arxiv.org/abs/2411.17646v1"}
{"created":"2024-11-26 18:10:51","title":"Explainable AI for Classifying UTI Risk Groups Using a Real-World Linked EHR and Pathology Lab Dataset","abstract":"The use of machine learning and AI on electronic health records (EHRs) holds substantial potential for clinical insight. However, this approach faces significant challenges due to data heterogeneity, sparsity, temporal misalignment, and limited labeled outcomes. In this context, we leverage a linked EHR dataset of approximately one million de-identified individuals from Bristol, North Somerset, and South Gloucestershire, UK, to characterize urinary tract infections (UTIs) and develop predictive models focused on data quality, fairness and transparency. A comprehensive data pre-processing and curation pipeline transforms the raw EHR data into a structured format suitable for AI modeling. Given the limited availability and biases of ground truth UTI outcomes, we introduce a UTI risk estimation framework informed by clinical expertise to estimate UTI risk across individual patient timelines. Using this framework, we built pairwise XGBoost models to differentiate UTI risk categories with explainable AI techniques to identify key predictors while ensuring interpretability. Our findings reveal differences in clinical and demographic factors across risk groups, offering insights into UTI risk stratification and progression. This study demonstrates the added value of AI-driven insights into UTI clinical decision-making while prioritizing interpretability, transparency, and fairness, underscoring the importance of sound data practices in advancing health outcomes.","sentences":["The use of machine learning and AI on electronic health records (EHRs) holds substantial potential for clinical insight.","However, this approach faces significant challenges due to data heterogeneity, sparsity, temporal misalignment, and limited labeled outcomes.","In this context, we leverage a linked EHR dataset of approximately one million de-identified individuals from Bristol, North Somerset, and South Gloucestershire, UK, to characterize urinary tract infections (UTIs) and develop predictive models focused on data quality, fairness and transparency.","A comprehensive data pre-processing and curation pipeline transforms the raw EHR data into a structured format suitable for AI modeling.","Given the limited availability and biases of ground truth UTI outcomes, we introduce a UTI risk estimation framework informed by clinical expertise to estimate UTI risk across individual patient timelines.","Using this framework, we built pairwise XGBoost models to differentiate UTI risk categories with explainable AI techniques to identify key predictors while ensuring interpretability.","Our findings reveal differences in clinical and demographic factors across risk groups, offering insights into UTI risk stratification and progression.","This study demonstrates the added value of AI-driven insights into UTI clinical decision-making while prioritizing interpretability, transparency, and fairness, underscoring the importance of sound data practices in advancing health outcomes."],"url":"http://arxiv.org/abs/2411.17645v1"}
{"created":"2024-11-26 18:08:39","title":"A robust image encryption scheme based on new 4-D hyperchaotic system and elliptic curve","abstract":"In this work, a new 4-D hyperchaotic system for image encryption is proposed and its effectiveness is demonstrated by incorporating it into an existing Elliptic Curve Cryptography (ECC) mapping scheme. The proposed system is considered simple because it consists of eight terms with two nonlinearities. The system exhibits high sensitivity to initial conditions, which makes it suitable for encryption purposes. The two-stage encryption process, involving confusion and diffusion, is employed to protect the confidentiality of digital images. The simulation results demonstrate the effectiveness of the hyperchaotic system in terms of security and performance when combined with the ECC mapping scheme. This approach can be applied in various domains including healthcare, military, and entertainment to ensure the robust encryption of digital images.","sentences":["In this work, a new 4-D hyperchaotic system for image encryption is proposed and its effectiveness is demonstrated by incorporating it into an existing Elliptic Curve Cryptography (ECC) mapping scheme.","The proposed system is considered simple because it consists of eight terms with two nonlinearities.","The system exhibits high sensitivity to initial conditions, which makes it suitable for encryption purposes.","The two-stage encryption process, involving confusion and diffusion, is employed to protect the confidentiality of digital images.","The simulation results demonstrate the effectiveness of the hyperchaotic system in terms of security and performance when combined with the ECC mapping scheme.","This approach can be applied in various domains including healthcare, military, and entertainment to ensure the robust encryption of digital images."],"url":"http://arxiv.org/abs/2411.17643v1"}
{"created":"2024-11-26 17:55:37","title":"On Limitations of LLM as Annotator for Low Resource Languages","abstract":"Low-resource languages face significant challenges due to the lack of sufficient linguistic data, resources, and tools for tasks such as supervised learning, annotation, and classification. This shortage hinders the development of accurate models and datasets, making it difficult to perform critical NLP tasks like sentiment analysis or hate speech detection. To bridge this gap, Large Language Models (LLMs) present an opportunity for potential annotators, capable of generating datasets and resources for these underrepresented languages. In this paper, we focus on Marathi, a low-resource language, and evaluate the performance of both closed-source and open-source LLMs as annotators. We assess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and 9B), and Llama 3.1 (8B) on classification tasks including sentiment analysis, news classification, and hate speech detection. Our findings reveal that while LLMs excel in annotation tasks for high-resource languages like English, they still fall short when applied to Marathi. Even advanced closed models like Gemini and GPT underperform in comparison to BERT-based baselines, highlighting the limitations of LLMs as annotators for low-resource languages.","sentences":["Low-resource languages face significant challenges due to the lack of sufficient linguistic data, resources, and tools for tasks such as supervised learning, annotation, and classification.","This shortage hinders the development of accurate models and datasets, making it difficult to perform critical NLP tasks like sentiment analysis or hate speech detection.","To bridge this gap, Large Language Models (LLMs) present an opportunity for potential annotators, capable of generating datasets and resources for these underrepresented languages.","In this paper, we focus on Marathi, a low-resource language, and evaluate the performance of both closed-source and open-source LLMs as annotators.","We assess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and 9B), and Llama 3.1 (8B) on classification tasks including sentiment analysis, news classification, and hate speech detection.","Our findings reveal that while LLMs excel in annotation tasks for high-resource languages like English, they still fall short when applied to Marathi.","Even advanced closed models like Gemini and GPT underperform in comparison to BERT-based baselines, highlighting the limitations of LLMs as annotators for low-resource languages."],"url":"http://arxiv.org/abs/2411.17637v1"}
{"created":"2024-11-26 17:53:44","title":"MALMM: Multi-Agent Large Language Models for Zero-Shot Robotics Manipulation","abstract":"Large Language Models (LLMs) have demonstrated remarkable planning abilities across various domains, including robotics manipulation and navigation. While recent efforts in robotics have leveraged LLMs both for high-level and low-level planning, these approaches often face significant challenges, such as hallucinations in long-horizon tasks and limited adaptability due to the generation of plans in a single pass without real-time feedback. To address these limitations, we propose a novel multi-agent LLM framework, Multi-Agent Large Language Model for Manipulation (MALMM) that distributes high-level planning and low-level control code generation across specialized LLM agents, supervised by an additional agent that dynamically manages transitions. By incorporating observations from the environment after each step, our framework effectively handles intermediate failures and enables adaptive re-planning. Unlike existing methods, our approach does not rely on pre-trained skill policies or in-context learning examples and generalizes to a variety of new tasks. We evaluate our approach on nine RLBench tasks, including long-horizon tasks, and demonstrate its ability to solve robotics manipulation in a zero-shot setting, thereby overcoming key limitations of existing LLM-based manipulation methods.","sentences":["Large Language Models (LLMs) have demonstrated remarkable planning abilities across various domains, including robotics manipulation and navigation.","While recent efforts in robotics have leveraged LLMs both for high-level and low-level planning, these approaches often face significant challenges, such as hallucinations in long-horizon tasks and limited adaptability due to the generation of plans in a single pass without real-time feedback.","To address these limitations, we propose a novel multi-agent LLM framework, Multi-Agent Large Language Model for Manipulation (MALMM) that distributes high-level planning and low-level control code generation across specialized LLM agents, supervised by an additional agent that dynamically manages transitions.","By incorporating observations from the environment after each step, our framework effectively handles intermediate failures and enables adaptive re-planning.","Unlike existing methods, our approach does not rely on pre-trained skill policies or in-context learning examples and generalizes to a variety of new tasks.","We evaluate our approach on nine RLBench tasks, including long-horizon tasks, and demonstrate its ability to solve robotics manipulation in a zero-shot setting, thereby overcoming key limitations of existing LLM-based manipulation methods."],"url":"http://arxiv.org/abs/2411.17636v1"}
{"created":"2024-11-26 17:41:44","title":"Learning Chemical Reaction Representation with Reactant-Product Alignment","abstract":"Organic synthesis stands as a cornerstone of chemical industry. The development of robust machine learning models to support tasks associated with organic reactions is of significant interest. However, current methods rely on hand-crafted features or direct adaptations of model architectures from other domains, which lacks feasibility as data scales increase or overlook the rich chemical information inherent in reactions. To address these issues, this paper introduces {\\modelname}, a novel chemical reaction representation learning model tailored for a variety of organic-reaction-related tasks. By integrating atomic correspondence between reactants and products, our model discerns the molecular transformations that occur during the reaction, thereby enhancing the comprehension of the reaction mechanism. We have designed an adapter structure to incorporate reaction conditions into the chemical reaction representation, allowing the model to handle diverse reaction conditions and adapt to various datasets and downstream tasks, e.g., reaction performance prediction. Additionally, we introduce a reaction-center aware attention mechanism that enables the model to concentrate on key functional groups, thereby generating potent representations for chemical reactions. Our model has been evaluated on a range of downstream tasks, including reaction condition prediction, reaction yield prediction, and reaction selectivity prediction. Experimental results indicate that our model markedly outperforms existing chemical reaction representation learning architectures across all tasks. Notably, our model significantly outperforms all the baselines with up to 25\\% (top-1) and 16\\% (top-10) increased accuracy over the strongest baseline on USPTO\\_CONDITION dataset for reaction condition prediction. We plan to open-source the code contingent upon the acceptance of the paper.","sentences":["Organic synthesis stands as a cornerstone of chemical industry.","The development of robust machine learning models to support tasks associated with organic reactions is of significant interest.","However, current methods rely on hand-crafted features or direct adaptations of model architectures from other domains, which lacks feasibility as data scales increase or overlook the rich chemical information inherent in reactions.","To address these issues, this paper introduces {\\modelname}, a novel chemical reaction representation learning model tailored for a variety of organic-reaction-related tasks.","By integrating atomic correspondence between reactants and products, our model discerns the molecular transformations that occur during the reaction, thereby enhancing the comprehension of the reaction mechanism.","We have designed an adapter structure to incorporate reaction conditions into the chemical reaction representation, allowing the model to handle diverse reaction conditions and adapt to various datasets and downstream tasks, e.g., reaction performance prediction.","Additionally, we introduce a reaction-center aware attention mechanism that enables the model to concentrate on key functional groups, thereby generating potent representations for chemical reactions.","Our model has been evaluated on a range of downstream tasks, including reaction condition prediction, reaction yield prediction, and reaction selectivity prediction.","Experimental results indicate that our model markedly outperforms existing chemical reaction representation learning architectures across all tasks.","Notably, our model significantly outperforms all the baselines with up to 25\\% (top-1) and 16\\% (top-10) increased accuracy over the strongest baseline on USPTO\\_CONDITION dataset for reaction condition prediction.","We plan to open-source the code contingent upon the acceptance of the paper."],"url":"http://arxiv.org/abs/2411.17629v1"}
{"created":"2024-11-26 17:37:21","title":"Semi-analytical model for the calculation of solar radiation pressure and its effects on a LEO satellite with predicting the change in position vectors using machine learning techniques","abstract":"The rapid increase in the deployment of Low Earth Orbit (LEO) satellites, catering to diverse applications such as communication, Earth observation, environmental monitoring, and scientific research, has significantly amplified the complexity of trajectory management. The current work focuses on calculating and analyzing perturbation effects on a satellite's anticipated trajectory in LEO, considering Solar Radiation Pressure (SRP) as the main perturbing force. The acceleration due to SRP and it's effects on the satellite was calculated using a custom-built Python module mainly based on the hypothesis of the cannonball model. The study demonstrates the effectiveness of the proposed model through comprehensive simulations and comparisons with existing analytical and numerical methods. Here, the primary Keplerian orbital characteristics were employed to analyze a simulated low-earth orbit LEO satellite, initially visualizing the satellite's trajectory and ground tracks at a designated altitude. The study also focuses on a comparative analysis of ground stations, primarily considering the main regions of the subcontinent, with revisit time as the key parameter for comparison. In the end, we combine analytical techniques with Machine Learning (ML) algorithms to predict changes in the position vectors of the satellite. Using ML techniques, the model can adaptively learn and refine predictions based on historical data and real-time input, thus improving accuracy over time. In addition, the incorporation of analytical methods allows for a deeper understanding of the underlying physics governing satellite motion, enabling more precise adjustments and corrections.","sentences":["The rapid increase in the deployment of Low Earth Orbit (LEO) satellites, catering to diverse applications such as communication, Earth observation, environmental monitoring, and scientific research, has significantly amplified the complexity of trajectory management.","The current work focuses on calculating and analyzing perturbation effects on a satellite's anticipated trajectory in LEO, considering Solar Radiation Pressure (SRP) as the main perturbing force.","The acceleration due to SRP and it's effects on the satellite was calculated using a custom-built Python module mainly based on the hypothesis of the cannonball model.","The study demonstrates the effectiveness of the proposed model through comprehensive simulations and comparisons with existing analytical and numerical methods.","Here, the primary Keplerian orbital characteristics were employed to analyze a simulated low-earth orbit LEO satellite, initially visualizing the satellite's trajectory and ground tracks at a designated altitude.","The study also focuses on a comparative analysis of ground stations, primarily considering the main regions of the subcontinent, with revisit time as the key parameter for comparison.","In the end, we combine analytical techniques with Machine Learning (ML) algorithms to predict changes in the position vectors of the satellite.","Using ML techniques, the model can adaptively learn and refine predictions based on historical data and real-time input, thus improving accuracy over time.","In addition, the incorporation of analytical methods allows for a deeper understanding of the underlying physics governing satellite motion, enabling more precise adjustments and corrections."],"url":"http://arxiv.org/abs/2411.17626v1"}
{"created":"2024-11-26 17:37:12","title":"Data-driven development of cycle prediction models for lithium metal batteries using multi modal mining","abstract":"Recent advances in data-driven research have shown great potential in understanding the intricate relationships between materials and their performances. Herein, we introduce a novel multi modal data-driven approach employing an Automatic Battery data Collector (ABC) that integrates a large language model (LLM) with an automatic graph mining tool, Material Graph Digitizer (MatGD). This platform enables state-of-the-art accurate extraction of battery material data and cyclability performance metrics from diverse textual and graphical data sources. From the database derived through the ABC platform, we developed machine learning models that can accurately predict the capacity and stability of lithium metal batteries, which is the first-ever model developed to achieve such predictions. Our models were also experimentally validated, confirming practical applicability and reliability of our data-driven approach.","sentences":["Recent advances in data-driven research have shown great potential in understanding the intricate relationships between materials and their performances.","Herein, we introduce a novel multi modal data-driven approach employing an Automatic Battery data Collector (ABC) that integrates a large language model (LLM) with an automatic graph mining tool, Material Graph Digitizer (MatGD).","This platform enables state-of-the-art accurate extraction of battery material data and cyclability performance metrics from diverse textual and graphical data sources.","From the database derived through the ABC platform, we developed machine learning models that can accurately predict the capacity and stability of lithium metal batteries, which is the first-ever model developed to achieve such predictions.","Our models were also experimentally validated, confirming practical applicability and reliability of our data-driven approach."],"url":"http://arxiv.org/abs/2411.17625v1"}
{"created":"2024-11-26 17:34:59","title":"Machine Learning and Multi-source Remote Sensing in Forest Carbon Stock Estimation: A Review","abstract":"Quantifying forest carbon is crucial for informing decisions and policies that will protect the planet. Machine learning (ML) and remote sensing (RS) techniques have been used to do this task more effectively, yet there lacks a systematic review on the most recent ML methods and RS combinations, especially with the consideration of forest characteristics. This study systematically analyzed 25 papers meeting strict inclusion criteria from over 80 related studies, identifying 28 ML methods and key combinations of RS data. Random Forest had the most frequent appearance (88\\% of studies), while Extreme Gradient Boosting showed superior performance in 75\\% of the studies in which it was compared with other methods. Sentinel-1 emerged as the most utilized remote sensing source, with multi-sensor approaches (e.g., Sentinel-1, Sentinel-2, and LiDAR) proving especially effective. Our findings provide grounds for recommending best practices in integrating machine learning and remote sensing for accurate and scalable forest carbon stock estimation.","sentences":["Quantifying forest carbon is crucial for informing decisions and policies that will protect the planet.","Machine learning (ML) and remote sensing (RS) techniques have been used to do this task more effectively, yet there lacks a systematic review on the most recent ML methods and RS combinations, especially with the consideration of forest characteristics.","This study systematically analyzed 25 papers meeting strict inclusion criteria from over 80 related studies, identifying 28 ML methods and key combinations of RS data.","Random Forest had the most frequent appearance (88\\% of studies), while Extreme Gradient Boosting showed superior performance in 75\\% of the studies in which it was compared with other methods.","Sentinel-1 emerged as the most utilized remote sensing source, with multi-sensor approaches (e.g., Sentinel-1, Sentinel-2, and LiDAR) proving especially effective.","Our findings provide grounds for recommending best practices in integrating machine learning and remote sensing for accurate and scalable forest carbon stock estimation."],"url":"http://arxiv.org/abs/2411.17624v1"}
{"created":"2024-11-26 17:33:02","title":"A Combined Feature Embedding Tools for Multi-Class Software Defect and Identification","abstract":"In software, a vulnerability is a defect in a program that attackers might utilize to acquire unauthorized access, alter system functions, and acquire information. These vulnerabilities arise from programming faults, design flaws, incorrect setups, and a lack of security protective measures. To mitigate these vulnerabilities, regular software upgrades, code reviews, safe development techniques, and the use of security tools to find and fix problems have been important. Several ways have been delivered in recent studies to address difficulties related to software vulnerabilities. However, previous approaches have significant limitations, notably in feature embedding and precisely recognizing specific vulnerabilities. To overcome these drawbacks, we present CodeGraphNet, an experimental method that combines GraphCodeBERT and Graph Convolutional Network (GCN) approaches, where, CodeGraphNet reveals data in a high-dimensional vector space, with comparable or related properties grouped closer together. This method captures intricate relation- ships between features, providing for more exact identification and separation of vulnerabilities. Using this feature embedding approach, we employed four machine learning models, applying both independent testing and 10-fold cross-validation. The DeepTree model, which is a hybrid of a Decision Tree and a Neural Network, outperforms state-of-the-art approaches. In additional validation, we evaluated our model using feature embeddings from LSA, GloVe, FastText, CodeBERT and GraphCodeBERT, and found that the CodeGraphNet method presented improved vulnerability identification with 98% of accuracy. Our model was tested on a real-time dataset to determine its capacity to handle real-world data and to focus on defect localization, which might influence future studies.","sentences":["In software, a vulnerability is a defect in a program that attackers might utilize to acquire unauthorized access, alter system functions, and acquire information.","These vulnerabilities arise from programming faults, design flaws, incorrect setups, and a lack of security protective measures.","To mitigate these vulnerabilities, regular software upgrades, code reviews, safe development techniques, and the use of security tools to find and fix problems have been important.","Several ways have been delivered in recent studies to address difficulties related to software vulnerabilities.","However, previous approaches have significant limitations, notably in feature embedding and precisely recognizing specific vulnerabilities.","To overcome these drawbacks, we present CodeGraphNet, an experimental method that combines GraphCodeBERT and Graph Convolutional Network (GCN) approaches, where, CodeGraphNet reveals data in a high-dimensional vector space, with comparable or related properties grouped closer together.","This method captures intricate relation- ships between features, providing for more exact identification and separation of vulnerabilities.","Using this feature embedding approach, we employed four machine learning models, applying both independent testing and 10-fold cross-validation.","The DeepTree model, which is a hybrid of a Decision Tree and a Neural Network, outperforms state-of-the-art approaches.","In additional validation, we evaluated our model using feature embeddings from LSA, GloVe, FastText, CodeBERT and GraphCodeBERT, and found that the CodeGraphNet method presented improved vulnerability identification with 98% of accuracy.","Our model was tested on a real-time dataset to determine its capacity to handle real-world data and to focus on defect localization, which might influence future studies."],"url":"http://arxiv.org/abs/2411.17621v1"}
{"created":"2024-11-26 17:28:10","title":"Accelerating Vision Diffusion Transformers with Skip Branches","abstract":"Diffusion Transformers (DiT), an emerging image and video generation model architecture, has demonstrated great potential because of its high generation quality and scalability properties. Despite the impressive performance, its practical deployment is constrained by computational complexity and redundancy in the sequential denoising process. While feature caching across timesteps has proven effective in accelerating diffusion models, its application to DiT is limited by fundamental architectural differences from U-Net-based approaches. Through empirical analysis of DiT feature dynamics, we identify that significant feature variation between DiT blocks presents a key challenge for feature reusability. To address this, we convert standard DiT into Skip-DiT with skip branches to enhance feature smoothness. Further, we introduce Skip-Cache which utilizes the skip branches to cache DiT features across timesteps at the inference time. We validated effectiveness of our proposal on different DiT backbones for video and image generation, showcasing skip branches to help preserve generation quality and achieve higher speedup. Experimental results indicate that Skip-DiT achieves a 1.5x speedup almost for free and a 2.2x speedup with only a minor reduction in quantitative metrics. Code is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.","sentences":["Diffusion Transformers (DiT), an emerging image and video generation model architecture, has demonstrated great potential because of its high generation quality and scalability properties.","Despite the impressive performance, its practical deployment is constrained by computational complexity and redundancy in the sequential denoising process.","While feature caching across timesteps has proven effective in accelerating diffusion models, its application to DiT is limited by fundamental architectural differences from U-Net-based approaches.","Through empirical analysis of DiT feature dynamics, we identify that significant feature variation between DiT blocks presents a key challenge for feature reusability.","To address this, we convert standard DiT into Skip-DiT with skip branches to enhance feature smoothness.","Further, we introduce Skip-Cache which utilizes the skip branches to cache DiT features across timesteps at the inference time.","We validated effectiveness of our proposal on different DiT backbones for video and image generation, showcasing skip branches to help preserve generation quality and achieve higher speedup.","Experimental results indicate that Skip-DiT achieves a 1.5x speedup almost for free and a 2.2x speedup with only a minor reduction in quantitative metrics.","Code is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."],"url":"http://arxiv.org/abs/2411.17616v1"}
{"created":"2024-11-26 17:27:18","title":"Automating Chapter-Level Classification for Electronic Theses and Dissertations","abstract":"Traditional archival practices for describing electronic theses and dissertations (ETDs) rely on broad, high-level metadata schemes that fail to capture the depth, complexity, and interdisciplinary nature of these long scholarly works. The lack of detailed, chapter-level content descriptions impedes researchers' ability to locate specific sections or themes, thereby reducing discoverability and overall accessibility. By providing chapter-level metadata information, we improve the effectiveness of ETDs as research resources. This makes it easier for scholars to navigate them efficiently and extract valuable insights. The absence of such metadata further obstructs interdisciplinary research by obscuring connections across fields, hindering new academic discoveries and collaboration. In this paper, we propose a machine learning and AI-driven solution to automatically categorize ETD chapters. This solution is intended to improve discoverability and promote understanding of chapters. Our approach enriches traditional archival practices by providing context-rich descriptions that facilitate targeted navigation and improved access. We aim to support interdisciplinary research and make ETDs more accessible. By providing chapter-level classification labels and using them to index in our developed prototype system, we make content in ETD chapters more discoverable and usable for a diverse range of scholarly needs. Implementing this AI-enhanced approach allows archives to serve researchers better, enabling efficient access to relevant information and supporting deeper engagement with ETDs. This will increase the impact of ETDs as research tools, foster interdisciplinary exploration, and reinforce the role of archives in scholarly communication within the data-intensive academic landscape.","sentences":["Traditional archival practices for describing electronic theses and dissertations (ETDs) rely on broad, high-level metadata schemes that fail to capture the depth, complexity, and interdisciplinary nature of these long scholarly works.","The lack of detailed, chapter-level content descriptions impedes researchers' ability to locate specific sections or themes, thereby reducing discoverability and overall accessibility.","By providing chapter-level metadata information, we improve the effectiveness of ETDs as research resources.","This makes it easier for scholars to navigate them efficiently and extract valuable insights.","The absence of such metadata further obstructs interdisciplinary research by obscuring connections across fields, hindering new academic discoveries and collaboration.","In this paper, we propose a machine learning and AI-driven solution to automatically categorize ETD chapters.","This solution is intended to improve discoverability and promote understanding of chapters.","Our approach enriches traditional archival practices by providing context-rich descriptions that facilitate targeted navigation and improved access.","We aim to support interdisciplinary research and make ETDs more accessible.","By providing chapter-level classification labels and using them to index in our developed prototype system, we make content in ETD chapters more discoverable and usable for a diverse range of scholarly needs.","Implementing this AI-enhanced approach allows archives to serve researchers better, enabling efficient access to relevant information and supporting deeper engagement with ETDs.","This will increase the impact of ETDs as research tools, foster interdisciplinary exploration, and reinforce the role of archives in scholarly communication within the data-intensive academic landscape."],"url":"http://arxiv.org/abs/2411.17614v1"}
{"created":"2024-11-26 17:21:53","title":"Modality-Incremental Learning with Disjoint Relevance Mapping Networks for Image-based Semantic Segmentation","abstract":"In autonomous driving, environment perception has significantly advanced with the utilization of deep learning techniques for diverse sensors such as cameras, depth sensors, or infrared sensors. The diversity in the sensor stack increases the safety and contributes to robustness against adverse weather and lighting conditions. However, the variance in data acquired from different sensors poses challenges. In the context of continual learning (CL), incremental learning is especially challenging for considerably large domain shifts, e.g. different sensor modalities. This amplifies the problem of catastrophic forgetting. To address this issue, we formulate the concept of modality-incremental learning and examine its necessity, by contrasting it with existing incremental learning paradigms. We propose the use of a modified Relevance Mapping Network (RMN) to incrementally learn new modalities while preserving performance on previously learned modalities, in which relevance maps are disjoint. Experimental results demonstrate that the prevention of shared connections in this approach helps alleviate the problem of forgetting within the constraints of a strict continual learning framework.","sentences":["In autonomous driving, environment perception has significantly advanced with the utilization of deep learning techniques for diverse sensors such as cameras, depth sensors, or infrared sensors.","The diversity in the sensor stack increases the safety and contributes to robustness against adverse weather and lighting conditions.","However, the variance in data acquired from different sensors poses challenges.","In the context of continual learning (CL), incremental learning is especially challenging for considerably large domain shifts, e.g. different sensor modalities.","This amplifies the problem of catastrophic forgetting.","To address this issue, we formulate the concept of modality-incremental learning and examine its necessity, by contrasting it with existing incremental learning paradigms.","We propose the use of a modified Relevance Mapping Network (RMN) to incrementally learn new modalities while preserving performance on previously learned modalities, in which relevance maps are disjoint.","Experimental results demonstrate that the prevention of shared connections in this approach helps alleviate the problem of forgetting within the constraints of a strict continual learning framework."],"url":"http://arxiv.org/abs/2411.17610v1"}
{"created":"2024-11-26 17:19:09","title":"Scaling Speech-Text Pre-training with Synthetic Interleaved Data","abstract":"Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower sampling rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.","sentences":["Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs).","Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs.","We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets.","Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech.","We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder.","This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower sampling rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality.","Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%.","We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain."],"url":"http://arxiv.org/abs/2411.17607v1"}
{"created":"2024-11-26 17:18:20","title":"HyperSeg: Towards Universal Visual Segmentation with Large Language Model","abstract":"This paper aims to address universal segmentation for image and video perception with the strong reasoning ability empowered by Visual Large Language Models (VLLMs). Despite significant progress in current unified segmentation methods, limitations in adaptation to both image and video scenarios, as well as the complex reasoning segmentation, make it difficult for them to handle various challenging instructions and achieve an accurate understanding of fine-grained vision-language correlations. We propose HyperSeg, the first VLLM-based universal segmentation model for pixel-level image and video perception, encompassing generic segmentation tasks and more complex reasoning perception tasks requiring powerful reasoning abilities and world knowledge. Besides, to fully leverage the recognition capabilities of VLLMs and the fine-grained visual information, HyperSeg incorporates hybrid entity recognition and fine-grained visual perceiver modules for various segmentation tasks. Combined with the temporal adapter, HyperSeg achieves a comprehensive understanding of temporal information. Experimental results validate the effectiveness of our insights in resolving universal image and video segmentation tasks, including the more complex reasoning perception tasks. Our code is available.","sentences":["This paper aims to address universal segmentation for image and video perception with the strong reasoning ability empowered by Visual Large Language Models (VLLMs).","Despite significant progress in current unified segmentation methods, limitations in adaptation to both image and video scenarios, as well as the complex reasoning segmentation, make it difficult for them to handle various challenging instructions and achieve an accurate understanding of fine-grained vision-language correlations.","We propose HyperSeg, the first VLLM-based universal segmentation model for pixel-level image and video perception, encompassing generic segmentation tasks and more complex reasoning perception tasks requiring powerful reasoning abilities and world knowledge.","Besides, to fully leverage the recognition capabilities of VLLMs and the fine-grained visual information, HyperSeg incorporates hybrid entity recognition and fine-grained visual perceiver modules for various segmentation tasks.","Combined with the temporal adapter, HyperSeg achieves a comprehensive understanding of temporal information.","Experimental results validate the effectiveness of our insights in resolving universal image and video segmentation tasks, including the more complex reasoning perception tasks.","Our code is available."],"url":"http://arxiv.org/abs/2411.17606v1"}
{"created":"2024-11-26 17:17:41","title":"Distractor-free Generalizable 3D Gaussian Splatting","abstract":"We present DGGS, a novel framework addressing the previously unexplored challenge of Distractor-free Generalizable 3D Gaussian Splatting (3DGS). It accomplishes two key objectives: fortifying generalizable 3DGS against distractor-laden data during both training and inference phases, while successfully extending cross-scene adaptation capabilities to conventional distractor-free approaches. To achieve these objectives, DGGS introduces a scene-agnostic reference-based mask prediction and refinement methodology during training phase, coupled with a training view selection strategy, effectively improving distractor prediction accuracy and training stability. Moreover, to address distractor-induced voids and artifacts during inference stage, we propose a two-stage inference framework for better reference selection based on the predicted distractor masks, complemented by a distractor pruning module to eliminate residual distractor effects. Extensive generalization experiments demonstrate DGGS's advantages under distractor-laden conditions. Additionally, experimental results show that our scene-agnostic mask inference achieves accuracy comparable to scene-specific trained methods. Homepage is \\url{https://github.com/bbbbby-99/DGGS}.","sentences":["We present DGGS, a novel framework addressing the previously unexplored challenge of Distractor-free Generalizable 3D Gaussian Splatting (3DGS).","It accomplishes two key objectives: fortifying generalizable 3DGS against distractor-laden data during both training and inference phases, while successfully extending cross-scene adaptation capabilities to conventional distractor-free approaches.","To achieve these objectives, DGGS introduces a scene-agnostic reference-based mask prediction and refinement methodology during training phase, coupled with a training view selection strategy, effectively improving distractor prediction accuracy and training stability.","Moreover, to address distractor-induced voids and artifacts during inference stage, we propose a two-stage inference framework for better reference selection based on the predicted distractor masks, complemented by a distractor pruning module to eliminate residual distractor effects.","Extensive generalization experiments demonstrate DGGS's advantages under distractor-laden conditions.","Additionally, experimental results show that our scene-agnostic mask inference achieves accuracy comparable to scene-specific trained methods.","Homepage is \\url{https://github.com/bbbbby-99/DGGS}."],"url":"http://arxiv.org/abs/2411.17605v1"}
{"created":"2024-11-26 17:11:10","title":"A Unified and Practical Approach for Generalized Deletion Propagation","abstract":"Deletion Propagation problems are a family of database problems that have been studied for over 40 years. They are variants of the classical view-update problem where intended tuple deletions in the view (output of a query) are propagated back to the source (input database) in a manner that obeys certain constraints while minimizing side effects. Problems from this family have been used in domains as diverse as GDPR compliance, effective SQL pedagogy, and query explanations. However, so far these variants, their complexity, and practical algorithms have always been studied in isolation. In this paper, we unify the Deletion Propagation (DP) in a single generalized framework that comes with several appealing benefits: (1) Our approach not only captures all prior deletion propagation variants but also introduces a whole family of new and well-motivated problems. (2) Our algorithmic solution is general and practical. It solves problems `course-grained instance-optimally', i.e., our algorithm is not only guaranteed to terminate in polynomial time (PTIME) for all currently known PTIME cases, it can also leverage regularities in the data without explicitly receiving them as input (knowing about certain structural properties in data is often a prerequisite for a specialized algorithm to be applicable). (3) At the same time, our approach is not only practical (easy-to-implement), it is also competitive with (and at times faster by orders of magnitude than) prior PTIME approaches specialized for each problem. For variants of the problem that have been studied only theoretically so far, we show the first experimental results. (4) Our approach is complete. It can solve all problem variants and covers all settings (even those that have been previously notoriously difficult to study, such as queries with self-joins, unions, and bag semantics), and it also allows us to provide new complexity results.","sentences":["Deletion Propagation problems are a family of database problems that have been studied for over 40 years.","They are variants of the classical view-update problem where intended tuple deletions in the view (output of a query) are propagated back to the source (input database) in a manner that obeys certain constraints while minimizing side effects.","Problems from this family have been used in domains as diverse as GDPR compliance, effective SQL pedagogy, and query explanations.","However, so far these variants, their complexity, and practical algorithms have always been studied in isolation.","In this paper, we unify the Deletion Propagation (DP) in a single generalized framework that comes with several appealing benefits: (1) Our approach not only captures all prior deletion propagation variants but also introduces a whole family of new and well-motivated problems.","(2) Our algorithmic solution is general and practical.","It solves problems `course-grained instance-optimally', i.e., our algorithm is not only guaranteed to terminate in polynomial time (PTIME) for all currently known PTIME cases, it can also leverage regularities in the data without explicitly receiving them as input (knowing about certain structural properties in data is often a prerequisite for a specialized algorithm to be applicable).","(3) At the same time, our approach is not only practical (easy-to-implement), it is also competitive with (and at times faster by orders of magnitude than) prior PTIME approaches specialized for each problem.","For variants of the problem that have been studied only theoretically so far, we show the first experimental results.","(4) Our approach is complete.","It can solve all problem variants and covers all settings (even those that have been previously notoriously difficult to study, such as queries with self-joins, unions, and bag semantics), and it also allows us to provide new complexity results."],"url":"http://arxiv.org/abs/2411.17603v1"}
{"created":"2024-11-26 17:06:58","title":"Making History Readable","abstract":"The Virginia Tech University Libraries (VTUL) Digital Library Platform (DLP) hosts digital collections that offer our users access to a wide variety of documents of historical and cultural importance. These collections are not only of academic importance but also provide our users with a glance at local historical events. Our DLP contains collections comprising digital objects featuring complex layouts, faded imagery, and hard-to-read handwritten text, which makes providing online access to these materials challenging. To address these issues, we integrate AI into our DLP workflow and convert the text in the digital objects into a machine-readable format. To enhance the user experience with our historical collections, we use custom AI agents for handwriting recognition, text extraction, and large language models (LLMs) for summarization. This poster highlights three collections focusing on handwritten letters, newspapers, and digitized topographic maps. We discuss the challenges with each collection and detail our approaches to address them. Our proposed methods aim to enhance the user experience by making the contents in these collections easier to search and navigate.","sentences":["The Virginia Tech University Libraries (VTUL) Digital Library Platform (DLP) hosts digital collections that offer our users access to a wide variety of documents of historical and cultural importance.","These collections are not only of academic importance but also provide our users with a glance at local historical events.","Our DLP contains collections comprising digital objects featuring complex layouts, faded imagery, and hard-to-read handwritten text, which makes providing online access to these materials challenging.","To address these issues, we integrate AI into our DLP workflow and convert the text in the digital objects into a machine-readable format.","To enhance the user experience with our historical collections, we use custom AI agents for handwriting recognition, text extraction, and large language models (LLMs) for summarization.","This poster highlights three collections focusing on handwritten letters, newspapers, and digitized topographic maps.","We discuss the challenges with each collection and detail our approaches to address them.","Our proposed methods aim to enhance the user experience by making the contents in these collections easier to search and navigate."],"url":"http://arxiv.org/abs/2411.17600v1"}
{"created":"2024-11-26 17:06:30","title":"Agentic AI for Improving Precision in Identifying Contributions to Sustainable Development Goals","abstract":"As research institutions increasingly commit to supporting the United Nations' Sustainable Development Goals (SDGs), there is a pressing need to accurately assess their research output against these goals. Current approaches, primarily reliant on keyword-based Boolean search queries, conflate incidental keyword matches with genuine contributions, reducing retrieval precision and complicating benchmarking efforts. This study investigates the application of autoregressive Large Language Models (LLMs) as evaluation agents to identify relevant scholarly contributions to SDG targets in scholarly publications. Using a dataset of academic abstracts retrieved via SDG-specific keyword queries, we demonstrate that small, locally-hosted LLMs can differentiate semantically relevant contributions to SDG targets from documents retrieved due to incidental keyword matches, addressing the limitations of traditional methods. By leveraging the contextual understanding of LLMs, this approach provides a scalable framework for improving SDG-related research metrics and informing institutional reporting.","sentences":["As research institutions increasingly commit to supporting the United Nations' Sustainable Development Goals (SDGs), there is a pressing need to accurately assess their research output against these goals.","Current approaches, primarily reliant on keyword-based Boolean search queries, conflate incidental keyword matches with genuine contributions, reducing retrieval precision and complicating benchmarking efforts.","This study investigates the application of autoregressive Large Language Models (LLMs) as evaluation agents to identify relevant scholarly contributions to SDG targets in scholarly publications.","Using a dataset of academic abstracts retrieved via SDG-specific keyword queries, we demonstrate that small, locally-hosted LLMs can differentiate semantically relevant contributions to SDG targets from documents retrieved due to incidental keyword matches, addressing the limitations of traditional methods.","By leveraging the contextual understanding of LLMs, this approach provides a scalable framework for improving SDG-related research metrics and informing institutional reporting."],"url":"http://arxiv.org/abs/2411.17598v1"}
{"created":"2024-11-26 17:05:56","title":"Arcee: An OCM-Solver","abstract":"The 2024 PACE Challenge focused on the One-Sided Crossing Minimization (OCM) problem, which aims to minimize edge crossings in a bipartite graph with a fixed order in one partition and a free order in the other. We describe our OCM solver submission that utilizes various reduction rules for OCM and, for the heuristic track, employs local search approaches as well as techniques to escape local minima. The exact and parameterized solver uses an ILP formulation and branch & bound to solve an equivalent Feedback Arc Set instance.","sentences":["The 2024 PACE Challenge focused on the One-Sided Crossing Minimization (OCM) problem, which aims to minimize edge crossings in a bipartite graph with a fixed order in one partition and a free order in the other.","We describe our OCM solver submission that utilizes various reduction rules for OCM and, for the heuristic track, employs local search approaches as well as techniques to escape local minima.","The exact and parameterized solver uses an ILP formulation and branch & bound to solve an equivalent Feedback Arc Set instance."],"url":"http://arxiv.org/abs/2411.17596v1"}
{"created":"2024-11-26 17:05:27","title":"Can artificial intelligence predict clinical trial outcomes?","abstract":"The increasing complexity and cost of clinical trials, particularly in the context of oncology and advanced therapies, pose significant challenges for drug development. This study evaluates the predictive capabilities of large language models (LLMs) such as GPT-3.5, GPT-4, and HINT in determining clinical trial outcomes. By leveraging a curated dataset of trials from ClinicalTrials.gov, we compare the models' performance using metrics including balanced accuracy, specificity, recall, and Matthews Correlation Coefficient (MCC). Results indicate that GPT-4o demonstrates robust performance in early trial phases, achieving high recall but facing limitations in specificity. Conversely, the HINT model excels in recognizing negative outcomes, particularly in later trial phases, offering a balanced approach across diverse endpoints. Oncology trials, characterized by high complexity, remain challenging for all models. Additionally, trial duration and disease categories influence predictive performance, with longer durations and complex diseases such as neoplasms reducing accuracy. This study highlights the complementary strengths of LLMs and HINT, providing insights into optimizing predictive tools for clinical trial design and risk management. Future advancements in LLMs are essential to address current gaps in handling negative outcomes and complex domains.","sentences":["The increasing complexity and cost of clinical trials, particularly in the context of oncology and advanced therapies, pose significant challenges for drug development.","This study evaluates the predictive capabilities of large language models (LLMs) such as GPT-3.5, GPT-4, and HINT in determining clinical trial outcomes.","By leveraging a curated dataset of trials from ClinicalTrials.gov, we compare the models' performance using metrics including balanced accuracy, specificity, recall, and Matthews Correlation Coefficient (MCC).","Results indicate that GPT-4o demonstrates robust performance in early trial phases, achieving high recall but facing limitations in specificity.","Conversely, the HINT model excels in recognizing negative outcomes, particularly in later trial phases, offering a balanced approach across diverse endpoints.","Oncology trials, characterized by high complexity, remain challenging for all models.","Additionally, trial duration and disease categories influence predictive performance, with longer durations and complex diseases such as neoplasms reducing accuracy.","This study highlights the complementary strengths of LLMs and HINT, providing insights into optimizing predictive tools for clinical trial design and risk management.","Future advancements in LLMs are essential to address current gaps in handling negative outcomes and complex domains."],"url":"http://arxiv.org/abs/2411.17595v1"}
{"created":"2024-11-26 17:01:27","title":"What Differentiates Educational Literature? A Multimodal Fusion Approach of Transformers and Computational Linguistics","abstract":"The integration of new literature into the English curriculum remains a challenge since educators often lack scalable tools to rapidly evaluate readability and adapt texts for diverse classroom needs. This study proposes to address this gap through a multimodal approach that combines transformer-based text classification with linguistic feature analysis to align texts with UK Key Stages. Eight state-of-the-art Transformers were fine-tuned on segmented text data, with BERT achieving the highest unimodal F1 score of 0.75. In parallel, 500 deep neural network topologies were searched for the classification of linguistic characteristics, achieving an F1 score of 0.392. The fusion of these modalities shows a significant improvement, with every multimodal approach outperforming all unimodal models. In particular, the ELECTRA Transformer fused with the neural network achieved an F1 score of 0.996. The proposed approach is finally encapsulated in a stakeholder-facing web application, providing non-technical stakeholder access to real-time insights on text complexity, reading difficulty, curriculum alignment, and recommendations for learning age range. The application empowers data-driven decision making and reduces manual workload by integrating AI-based recommendations into lesson planning for English literature.","sentences":["The integration of new literature into the English curriculum remains a challenge since educators often lack scalable tools to rapidly evaluate readability and adapt texts for diverse classroom needs.","This study proposes to address this gap through a multimodal approach that combines transformer-based text classification with linguistic feature analysis to align texts with UK Key Stages.","Eight state-of-the-art Transformers were fine-tuned on segmented text data, with BERT achieving the highest unimodal F1 score of 0.75.","In parallel, 500 deep neural network topologies were searched for the classification of linguistic characteristics, achieving an F1 score of 0.392.","The fusion of these modalities shows a significant improvement, with every multimodal approach outperforming all unimodal models.","In particular, the ELECTRA Transformer fused with the neural network achieved an F1 score of 0.996.","The proposed approach is finally encapsulated in a stakeholder-facing web application, providing non-technical stakeholder access to real-time insights on text complexity, reading difficulty, curriculum alignment, and recommendations for learning age range.","The application empowers data-driven decision making and reduces manual workload by integrating AI-based recommendations into lesson planning for English literature."],"url":"http://arxiv.org/abs/2411.17593v1"}
{"created":"2024-11-26 16:56:53","title":"VideoDirector: Precise Video Editing via Text-to-Video Models","abstract":"Despite the typical inversion-then-editing paradigm using text-to-image (T2I) models has demonstrated promising results, directly extending it to text-to-video (T2V) models still suffers severe artifacts such as color flickering and content distortion. Consequently, current video editing methods primarily rely on T2I models, which inherently lack temporal-coherence generative ability, often resulting in inferior editing results. In this paper, we attribute the failure of the typical editing paradigm to: 1) Tightly Spatial-temporal Coupling. The vanilla pivotal-based inversion strategy struggles to disentangle spatial-temporal information in the video diffusion model; 2) Complicated Spatial-temporal Layout. The vanilla cross-attention control is deficient in preserving the unedited content. To address these limitations, we propose a spatial-temporal decoupled guidance (STDG) and multi-frame null-text optimization strategy to provide pivotal temporal cues for more precise pivotal inversion. Furthermore, we introduce a self-attention control strategy to maintain higher fidelity for precise partial content editing. Experimental results demonstrate that our method (termed VideoDirector) effectively harnesses the powerful temporal generation capabilities of T2V models, producing edited videos with state-of-the-art performance in accuracy, motion smoothness, realism, and fidelity to unedited content.","sentences":["Despite the typical inversion-then-editing paradigm using text-to-image (T2I) models has demonstrated promising results, directly extending it to text-to-video (T2V) models still suffers severe artifacts such as color flickering and content distortion.","Consequently, current video editing methods primarily rely on T2I models, which inherently lack temporal-coherence generative ability, often resulting in inferior editing results.","In this paper, we attribute the failure of the typical editing paradigm to: 1) Tightly Spatial-temporal Coupling.","The vanilla pivotal-based inversion strategy struggles to disentangle spatial-temporal information in the video diffusion model; 2) Complicated Spatial-temporal Layout.","The vanilla cross-attention control is deficient in preserving the unedited content.","To address these limitations, we propose a spatial-temporal decoupled guidance (STDG) and multi-frame null-text optimization strategy to provide pivotal temporal cues for more precise pivotal inversion.","Furthermore, we introduce a self-attention control strategy to maintain higher fidelity for precise partial content editing.","Experimental results demonstrate that our method (termed VideoDirector) effectively harnesses the powerful temporal generation capabilities of T2V models, producing edited videos with state-of-the-art performance in accuracy, motion smoothness, realism, and fidelity to unedited content."],"url":"http://arxiv.org/abs/2411.17592v1"}
{"created":"2024-11-26 16:55:58","title":"Privacy-Preserving Behaviour of Chatbot Users: Steering Through Trust Dynamics","abstract":"Introduction: The use of chatbots is becoming increasingly important across various aspects of daily life. However, the privacy concerns associated with these communications have not yet been thoroughly addressed. The aim of this study was to investigate user awareness of privacy risks in chatbot interactions, the privacy-preserving behaviours users practice, and how these behaviours relate to their awareness of privacy threats, even when no immediate threat is perceived. Methods: We developed a novel \"privacy-safe\" setup to analyse user behaviour under the guarantees of anonymization and non-sharing. We employed a mixed-methods approach, starting with the quantification of broader trends by coding responses, followed by conducting a qualitative content analysis to gain deeper insights. Results: Overall, there was a substantial lack of understanding among users about how chatbot providers handle data (27% of the participants) and the basics of privacy risks (76% of the participants). Older users, in particular, expressed fears that chatbot providers might sell their data. Moreover, even users with privacy knowledge do not consistently exhibit privacy-preserving behaviours when assured of transparent data processing by chatbots. Notably, under-protective behaviours were observed among more expert users. Discussion: These findings highlight the need for a strategic approach to enhance user education on privacy concepts to ensure informed decision when interacting with chatbot technology. This includes the development of tools to help users monitor and control the information they share with chatbots","sentences":["Introduction: The use of chatbots is becoming increasingly important across various aspects of daily life.","However, the privacy concerns associated with these communications have not yet been thoroughly addressed.","The aim of this study was to investigate user awareness of privacy risks in chatbot interactions, the privacy-preserving behaviours users practice, and how these behaviours relate to their awareness of privacy threats, even when no immediate threat is perceived.","Methods: We developed a novel \"privacy-safe\" setup to analyse user behaviour under the guarantees of anonymization and non-sharing.","We employed a mixed-methods approach, starting with the quantification of broader trends by coding responses, followed by conducting a qualitative content analysis to gain deeper insights.","Results:","Overall, there was a substantial lack of understanding among users about how chatbot providers handle data (27% of the participants) and the basics of privacy risks (76% of the participants).","Older users, in particular, expressed fears that chatbot providers might sell their data.","Moreover, even users with privacy knowledge do not consistently exhibit privacy-preserving behaviours when assured of transparent data processing by chatbots.","Notably, under-protective behaviours were observed among more expert users.","Discussion:","These findings highlight the need for a strategic approach to enhance user education on privacy concepts to ensure informed decision when interacting with chatbot technology.","This includes the development of tools to help users monitor and control the information they share with chatbots"],"url":"http://arxiv.org/abs/2411.17589v1"}
{"created":"2024-11-26 16:51:52","title":"Multi-Objective Reinforcement Learning for Automated Resilient Cyber Defence","abstract":"Cyber-attacks pose a security threat to military command and control networks, Intelligence, Surveillance, and Reconnaissance (ISR) systems, and civilian critical national infrastructure. The use of artificial intelligence and autonomous agents in these attacks increases the scale, range, and complexity of this threat and the subsequent disruption they cause. Autonomous Cyber Defence (ACD) agents aim to mitigate this threat by responding at machine speed and at the scale required to address the problem. Sequential decision-making algorithms such as Deep Reinforcement Learning (RL) provide a promising route to create ACD agents. These algorithms focus on a single objective such as minimizing the intrusion of red agents on the network, by using a handcrafted weighted sum of rewards. This approach removes the ability to adapt the model during inference, and fails to address the many competing objectives present when operating and protecting these networks. Conflicting objectives, such as restoring a machine from a back-up image, must be carefully balanced with the cost of associated down-time, or the disruption to network traffic or services that might result. Instead of pursing a Single-Objective RL (SORL) approach, here we present a simple example of a multi-objective network defence game that requires consideration of both defending the network against red-agents and maintaining critical functionality of green-agents. Two Multi-Objective Reinforcement Learning (MORL) algorithms, namely Multi-Objective Proximal Policy Optimization (MOPPO), and Pareto-Conditioned Networks (PCN), are used to create two trained ACD agents whose performance is compared on our Multi-Objective Cyber Defence game. The benefits and limitations of MORL ACD agents in comparison to SORL ACD agents are discussed based on the investigations of this game.","sentences":["Cyber-attacks pose a security threat to military command and control networks, Intelligence, Surveillance, and Reconnaissance (ISR) systems, and civilian critical national infrastructure.","The use of artificial intelligence and autonomous agents in these attacks increases the scale, range, and complexity of this threat and the subsequent disruption they cause.","Autonomous Cyber Defence (ACD) agents aim to mitigate this threat by responding at machine speed and at the scale required to address the problem.","Sequential decision-making algorithms such as Deep Reinforcement Learning (RL) provide a promising route to create ACD agents.","These algorithms focus on a single objective such as minimizing the intrusion of red agents on the network, by using a handcrafted weighted sum of rewards.","This approach removes the ability to adapt the model during inference, and fails to address the many competing objectives present when operating and protecting these networks.","Conflicting objectives, such as restoring a machine from a back-up image, must be carefully balanced with the cost of associated down-time, or the disruption to network traffic or services that might result.","Instead of pursing a Single-Objective RL (SORL) approach, here we present a simple example of a multi-objective network defence game that requires consideration of both defending the network against red-agents and maintaining critical functionality of green-agents.","Two Multi-Objective Reinforcement Learning (MORL) algorithms, namely Multi-Objective Proximal Policy Optimization (MOPPO), and Pareto-Conditioned Networks (PCN), are used to create two trained ACD agents whose performance is compared on our Multi-Objective Cyber Defence game.","The benefits and limitations of MORL ACD agents in comparison to SORL ACD agents are discussed based on the investigations of this game."],"url":"http://arxiv.org/abs/2411.17585v1"}
{"created":"2024-11-26 16:51:11","title":"Pre-training for Action Recognition with Automatically Generated Fractal Datasets","abstract":"In recent years, interest in synthetic data has grown, particularly in the context of pre-training the image modality to support a range of computer vision tasks, including object classification, medical imaging etc. Previous work has demonstrated that synthetic samples, automatically produced by various generative processes, can replace real counterparts and yield strong visual representations. This approach resolves issues associated with real data such as collection and labeling costs, copyright and privacy.   We extend this trend to the video domain applying it to the task of action recognition. Employing fractal geometry, we present methods to automatically produce large-scale datasets of short synthetic video clips, which can be utilized for pre-training neural models. The generated video clips are characterized by notable variety, stemmed by the innate ability of fractals to generate complex multi-scale structures. To narrow the domain gap, we further identify key properties of real videos and carefully emulate them during pre-training. Through thorough ablations, we determine the attributes that strengthen downstream results and offer general guidelines for pre-training with synthetic videos. The proposed approach is evaluated by fine-tuning pre-trained models on established action recognition datasets HMDB51 and UCF101 as well as four other video benchmarks related to group action recognition, fine-grained action recognition and dynamic scenes. Compared to standard Kinetics pre-training, our reported results come close and are even superior on a portion of downstream datasets. Code and samples of synthetic videos are available at https://github.com/davidsvy/fractal_video .","sentences":["In recent years, interest in synthetic data has grown, particularly in the context of pre-training the image modality to support a range of computer vision tasks, including object classification, medical imaging etc.","Previous work has demonstrated that synthetic samples, automatically produced by various generative processes, can replace real counterparts and yield strong visual representations.","This approach resolves issues associated with real data such as collection and labeling costs, copyright and privacy.   ","We extend this trend to the video domain applying it to the task of action recognition.","Employing fractal geometry, we present methods to automatically produce large-scale datasets of short synthetic video clips, which can be utilized for pre-training neural models.","The generated video clips are characterized by notable variety, stemmed by the innate ability of fractals to generate complex multi-scale structures.","To narrow the domain gap, we further identify key properties of real videos and carefully emulate them during pre-training.","Through thorough ablations, we determine the attributes that strengthen downstream results and offer general guidelines for pre-training with synthetic videos.","The proposed approach is evaluated by fine-tuning pre-trained models on established action recognition datasets HMDB51 and UCF101 as well as four other video benchmarks related to group action recognition, fine-grained action recognition and dynamic scenes.","Compared to standard Kinetics pre-training, our reported results come close and are even superior on a portion of downstream datasets.","Code and samples of synthetic videos are available at https://github.com/davidsvy/fractal_video ."],"url":"http://arxiv.org/abs/2411.17584v1"}
{"created":"2024-11-26 16:49:16","title":"From Fairness to Infinity: Outcome-Indistinguishable (Omni)Prediction in Evolving Graphs","abstract":"Professional networks provide invaluable entree to opportunity through referrals and introductions. A rich literature shows they also serve to entrench and even exacerbate a status quo of privilege and disadvantage. Hiring platforms, equipped with the ability to nudge link formation, provide a tantalizing opening for beneficial structural change. We anticipate that key to this prospect will be the ability to estimate the likelihood of edge formation in an evolving graph. Outcome-indistinguishable prediction algorithms ensure that the modeled world is indistinguishable from the real world by a family of statistical tests. Omnipredictors ensure that predictions can be post-processed to yield loss minimization competitive with respect to a benchmark class of predictors for many losses simultaneously, with appropriate post-processing. We begin by observing that, by combining a slightly modified form of the online K29 star algorithm of Vovk (2007) with basic facts from the theory of reproducing kernel Hilbert spaces, one can derive simple and efficient online algorithms satisfying outcome indistinguishability and omniprediction, with guarantees that improve upon, or are complementary to, those currently known. This is of independent interest. We apply these techniques to evolving graphs, obtaining online outcome-indistinguishable omnipredictors for rich -- possibly infinite -- sets of distinguishers that capture properties of pairs of nodes, and their neighborhoods. This yields, inter alia, multicalibrated predictions of edge formation with respect to pairs of demographic groups, and the ability to simultaneously optimize loss as measured by a variety of social welfare functions.","sentences":["Professional networks provide invaluable entree to opportunity through referrals and introductions.","A rich literature shows they also serve to entrench and even exacerbate a status quo of privilege and disadvantage.","Hiring platforms, equipped with the ability to nudge link formation, provide a tantalizing opening for beneficial structural change.","We anticipate that key to this prospect will be the ability to estimate the likelihood of edge formation in an evolving graph.","Outcome-indistinguishable prediction algorithms ensure that the modeled world is indistinguishable from the real world by a family of statistical tests.","Omnipredictors ensure that predictions can be post-processed to yield loss minimization competitive with respect to a benchmark class of predictors for many losses simultaneously, with appropriate post-processing.","We begin by observing that, by combining a slightly modified form of the online K29 star algorithm of Vovk (2007) with basic facts from the theory of reproducing kernel Hilbert spaces, one can derive simple and efficient online algorithms satisfying outcome indistinguishability and omniprediction, with guarantees that improve upon, or are complementary to, those currently known.","This is of independent interest.","We apply these techniques to evolving graphs, obtaining online outcome-indistinguishable omnipredictors for rich -- possibly infinite -- sets of distinguishers that capture properties of pairs of nodes, and their neighborhoods.","This yields, inter alia, multicalibrated predictions of edge formation with respect to pairs of demographic groups, and the ability to simultaneously optimize loss as measured by a variety of social welfare functions."],"url":"http://arxiv.org/abs/2411.17582v1"}
{"created":"2024-11-26 16:46:47","title":"Revisiting Point Cloud Completion: Are We Ready For The Real-World?","abstract":"Point clouds acquired in constrained and challenging real-world settings are incomplete, non-uniformly sparse, or both. These obstacles present acute challenges for a vital task - point cloud completion. Using tools from Algebraic Topology and Persistent Homology ($\\mathcal{PH}$), we demonstrate that current benchmark synthetic point clouds lack rich topological features that are important constituents of point clouds captured in realistic settings. To facilitate research in this direction, we contribute the first real-world industrial point cloud dataset for point cloud completion, RealPC - a diverse set of rich and varied point clouds, consisting of $\\sim$ 40,000 pairs across 21 categories of industrial structures in railway establishments. Our benchmark results on several strong baselines reveal a striking observation - the existing methods are tailored for synthetic datasets and fail miserably in real-world settings. Building on our observation that RealPC consists of several 0 and 1-dimensional $\\mathcal{PH}$-based topological features, we demonstrate the potential of integrating Homology-based topological priors into existing works. More specifically, we present how 0-dimensional $\\mathcal{PH}$ priors, which extract the global topology of a complete shape in the form of a 3-D skeleton, can assist a model in generating topologically-consistent complete shapes.","sentences":["Point clouds acquired in constrained and challenging real-world settings are incomplete, non-uniformly sparse, or both.","These obstacles present acute challenges for a vital task - point cloud completion.","Using tools from Algebraic Topology and Persistent Homology ($\\mathcal{PH}$), we demonstrate that current benchmark synthetic point clouds lack rich topological features that are important constituents of point clouds captured in realistic settings.","To facilitate research in this direction, we contribute the first real-world industrial point cloud dataset for point cloud completion, RealPC - a diverse set of rich and varied point clouds, consisting of $\\sim$ 40,000 pairs across 21 categories of industrial structures in railway establishments.","Our benchmark results on several strong baselines reveal a striking observation - the existing methods are tailored for synthetic datasets and fail miserably in real-world settings.","Building on our observation that RealPC consists of several 0 and 1-dimensional $\\mathcal{PH}$-based topological features, we demonstrate the potential of integrating Homology-based topological priors into existing works.","More specifically, we present how 0-dimensional $\\mathcal{PH}$ priors, which extract the global topology of a complete shape in the form of a 3-D skeleton, can assist a model in generating topologically-consistent complete shapes."],"url":"http://arxiv.org/abs/2411.17580v1"}
{"created":"2024-11-26 16:41:09","title":"A Distractor-Aware Memory for Visual Object Tracking with SAM2","abstract":"Memory-based trackers are video object segmentation methods that form the target model by concatenating recently tracked frames into a memory buffer and localize the target by attending the current image to the buffered frames. While already achieving top performance on many benchmarks, it was the recent release of SAM2 that placed memory-based trackers into focus of the visual object tracking community. Nevertheless, modern trackers still struggle in the presence of distractors. We argue that a more sophisticated memory model is required, and propose a new distractor-aware memory model for SAM2 and an introspection-based update strategy that jointly addresses the segmentation accuracy as well as tracking robustness. The resulting tracker is denoted as SAM2.1++. We also propose a new distractor-distilled DiDi dataset to study the distractor problem better. SAM2.1++ outperforms SAM2.1 and related SAM memory extensions on seven benchmarks and sets a solid new state-of-the-art on six of them.","sentences":["Memory-based trackers are video object segmentation methods that form the target model by concatenating recently tracked frames into a memory buffer and localize the target by attending the current image to the buffered frames.","While already achieving top performance on many benchmarks, it was the recent release of SAM2 that placed memory-based trackers into focus of the visual object tracking community.","Nevertheless, modern trackers still struggle in the presence of distractors.","We argue that a more sophisticated memory model is required, and propose a new distractor-aware memory model for SAM2 and an introspection-based update strategy that jointly addresses the segmentation accuracy as well as tracking robustness.","The resulting tracker is denoted as SAM2.1++.","We also propose a new distractor-distilled DiDi dataset to study the distractor problem better.","SAM2.1++ outperforms SAM2.1 and related SAM memory extensions on seven benchmarks and sets a solid new state-of-the-art on six of them."],"url":"http://arxiv.org/abs/2411.17576v1"}
{"created":"2024-11-26 16:32:08","title":"Learning Explainable Treatment Policies with Clinician-Informed Representations: A Practical Approach","abstract":"Digital health interventions (DHIs) and remote patient monitoring (RPM) have shown great potential in improving chronic disease management through personalized care. However, barriers like limited efficacy and workload concerns hinder adoption of existing DHIs; while limited sample sizes and lack of interpretability limit the effectiveness and adoption of purely black-box algorithmic DHIs. In this paper, we address these challenges by developing a pipeline for learning explainable treatment policies for RPM-enabled DHIs. We apply our approach in the real-world setting of RPM using a DHI to improve glycemic control of youth with type 1 diabetes. Our main contribution is to reveal the importance of clinical domain knowledge in developing state and action representations for effective, efficient, and interpretable targeting policies. We observe that policies learned from clinician-informed representations are significantly more efficacious and efficient than policies learned from black-box representations. This work emphasizes the importance of collaboration between ML researchers and clinicians for developing effective DHIs in the real world.","sentences":["Digital health interventions (DHIs) and remote patient monitoring (RPM) have shown great potential in improving chronic disease management through personalized care.","However, barriers like limited efficacy and workload concerns hinder adoption of existing DHIs; while limited sample sizes and lack of interpretability limit the effectiveness and adoption of purely black-box algorithmic DHIs.","In this paper, we address these challenges by developing a pipeline for learning explainable treatment policies for RPM-enabled DHIs.","We apply our approach in the real-world setting of RPM using a DHI to improve glycemic control of youth with type 1 diabetes.","Our main contribution is to reveal the importance of clinical domain knowledge in developing state and action representations for effective, efficient, and interpretable targeting policies.","We observe that policies learned from clinician-informed representations are significantly more efficacious and efficient than policies learned from black-box representations.","This work emphasizes the importance of collaboration between ML researchers and clinicians for developing effective DHIs in the real world."],"url":"http://arxiv.org/abs/2411.17570v1"}
{"created":"2024-11-26 16:31:18","title":"RTL-Breaker: Assessing the Security of LLMs against Backdoor Attacks on HDL Code Generation","abstract":"Large language models (LLMs) have demonstrated remarkable potential with code generation/completion tasks for hardware design. In fact, LLM-based hardware description language (HDL) code generation has enabled the industry to realize complex designs more quickly, reducing the time and effort required in the development cycle. However, the increased reliance on such automation introduces critical security risks. Notably, given that LLMs have to be trained on vast datasets of codes that are typically sourced from publicly available repositories (often without thorough validation), LLMs are susceptible to so-called data poisoning or backdoor attacks. Here, attackers inject malicious code for the training data, which can be carried over into the HDL code generated by LLMs. This threat vector can compromise the security and integrity of entire hardware systems. In this work, we propose RTL-Breaker, a novel backdoor attack framework on LLM-based HDL code generation. RTL-Breaker provides an in-depth analysis for essential aspects of this novel problem: 1) various trigger mechanisms versus their effectiveness for inserting malicious modifications, and 2) side-effects by backdoor attacks on code generation in general, i.e., impact on code quality. RTL-Breaker emphasizes the urgent need for more robust measures to safeguard against such attacks. Toward that end, we open-source our framework and all data.","sentences":["Large language models (LLMs) have demonstrated remarkable potential with code generation/completion tasks for hardware design.","In fact, LLM-based hardware description language (HDL) code generation has enabled the industry to realize complex designs more quickly, reducing the time and effort required in the development cycle.","However, the increased reliance on such automation introduces critical security risks.","Notably, given that LLMs have to be trained on vast datasets of codes that are typically sourced from publicly available repositories (often without thorough validation), LLMs are susceptible to so-called data poisoning or backdoor attacks.","Here, attackers inject malicious code for the training data, which can be carried over into the HDL code generated by LLMs.","This threat vector can compromise the security and integrity of entire hardware systems.","In this work, we propose RTL-Breaker, a novel backdoor attack framework on LLM-based HDL code generation.","RTL-Breaker provides an in-depth analysis for essential aspects of this novel problem: 1) various trigger mechanisms versus their effectiveness for inserting malicious modifications, and 2) side-effects by backdoor attacks on code generation in general, i.e., impact on code quality.","RTL-Breaker emphasizes the urgent need for more robust measures to safeguard against such attacks.","Toward that end, we open-source our framework and all data."],"url":"http://arxiv.org/abs/2411.17569v1"}
{"created":"2024-11-26 16:27:37","title":"Outer-(ap)RAC Graphs","abstract":"An \\emph{outer-RAC drawing} of a graph is a straight-line drawing where all vertices are incident to the outer cell and all edge crossings occur at a right angle. If additionally, all crossing edges are either horizontal or vertical, we call the drawing \\emph{outer-apRAC} (\\emph{ap} for \\emph{axis-parallel)}. A graph is outer-(ap)RAC if it admits an outer-(ap)RAC drawing. We investigate the class of outer-(ap)RAC graphs. We show that the outer-RAC graphs are a proper subset of~the planar graphs with at most $2.5n-4$ edges where $n$ is the number of vertices. This density bound is tight, even for outer-apRAC graphs. Moreover, we provide an SPQR-tree based linear-time algorithm which computes an outer-RAC drawing for every given series-parallel graph of maximum degree four. As a complementing result, we present planar graphs of maximum degree four and series-parallel graphs of maximum degree five that are not outer-RAC. Finally, for series-parallel graphs of maximum degree three we show how to compute an outer-apRAC drawing in linear time.","sentences":["An \\emph{outer-RAC drawing} of a graph is a straight-line drawing where all vertices are incident to the outer cell and all edge crossings occur at a right angle.","If additionally, all crossing edges are either horizontal or vertical, we call the drawing \\emph{outer-apRAC} (\\emph{ap} for \\emph{axis-parallel)}.","A graph is outer-(ap)RAC if it admits an outer-(ap)RAC drawing.","We investigate the class of outer-(ap)RAC graphs.","We show that the outer-RAC graphs are a proper subset of~the planar graphs with at most $2.5n-4$ edges where $n$ is the number of vertices.","This density bound is tight, even for outer-apRAC graphs.","Moreover, we provide an SPQR-tree based linear-time algorithm which computes an outer-RAC drawing for every given series-parallel graph of maximum degree four.","As a complementing result, we present planar graphs of maximum degree four and series-parallel graphs of maximum degree five that are not outer-RAC.","Finally, for series-parallel graphs of maximum degree three we show how to compute an outer-apRAC drawing in linear time."],"url":"http://arxiv.org/abs/2411.17565v1"}
{"created":"2024-11-26 16:21:10","title":"Degrees of Freedom of Cache-Aided Interference Channels Assisted by Active Intelligent Reflecting Surfaces","abstract":"This paper studies cache-aided wireless networks in the presence of active intelligent reflecting surfaces (IRS) from an information-theoretic perspective. Specifically, we explore interference management in a cache-aided wireless network assisted by an active IRS, to enhance the achievable degrees of freedom (DoF). To this end, we jointly design the content placement, delivery phase, and phase shifts of the IRS and propose a one-shot achievable scheme. Our scheme exploits transmitters' cooperation, cache contents (as side information), interference alignment, and IRS capabilities, adapting to the network's parameters. We derive the achievable one-shot sum-DoF for different sizes of cache memories, network configurations, and numbers of IRS elements. Our results highlight the potential of deploying an IRS in cache-aided wireless communication systems, underscoring the enhancement of achievable DoF for various parameter regimes, particularly when the sizes of the caches (especially at the transmitters) are inadequate. Notably, we show that access to an IRS with a sufficient number of elements enables the achievement of the maximum possible DoF for various parameter regimes of interest.","sentences":["This paper studies cache-aided wireless networks in the presence of active intelligent reflecting surfaces (IRS) from an information-theoretic perspective.","Specifically, we explore interference management in a cache-aided wireless network assisted by an active IRS, to enhance the achievable degrees of freedom (DoF).","To this end, we jointly design the content placement, delivery phase, and phase shifts of the IRS and propose a one-shot achievable scheme.","Our scheme exploits transmitters' cooperation, cache contents (as side information), interference alignment, and IRS capabilities, adapting to the network's parameters.","We derive the achievable one-shot sum-DoF for different sizes of cache memories, network configurations, and numbers of IRS elements.","Our results highlight the potential of deploying an IRS in cache-aided wireless communication systems, underscoring the enhancement of achievable DoF for various parameter regimes, particularly when the sizes of the caches (especially at the transmitters) are inadequate.","Notably, we show that access to an IRS with a sufficient number of elements enables the achievement of the maximum possible DoF for various parameter regimes of interest."],"url":"http://arxiv.org/abs/2411.17559v1"}
{"created":"2024-11-26 16:21:03","title":"Natural Language Understanding and Inference with MLLM in Visual Question Answering: A Survey","abstract":"Visual Question Answering (VQA) is a challenge task that combines natural language processing and computer vision techniques and gradually becomes a benchmark test task in multimodal large language models (MLLMs). The goal of our survey is to provide an overview of the development of VQA and a detailed description of the latest models with high timeliness. This survey gives an up-to-date synthesis of natural language understanding of images and text, as well as the knowledge reasoning module based on image-question information on the core VQA tasks. In addition, we elaborate on recent advances in extracting and fusing modal information with vision-language pretraining models and multimodal large language models in VQA. We also exhaustively review the progress of knowledge reasoning in VQA by detailing the extraction of internal knowledge and the introduction of external knowledge. Finally, we present the datasets of VQA and different evaluation metrics and discuss possible directions for future work.","sentences":["Visual Question Answering (VQA) is a challenge task that combines natural language processing and computer vision techniques and gradually becomes a benchmark test task in multimodal large language models (MLLMs).","The goal of our survey is to provide an overview of the development of VQA and a detailed description of the latest models with high timeliness.","This survey gives an up-to-date synthesis of natural language understanding of images and text, as well as the knowledge reasoning module based on image-question information on the core VQA tasks.","In addition, we elaborate on recent advances in extracting and fusing modal information with vision-language pretraining models and multimodal large language models in VQA.","We also exhaustively review the progress of knowledge reasoning in VQA by detailing the extraction of internal knowledge and the introduction of external knowledge.","Finally, we present the datasets of VQA and different evaluation metrics and discuss possible directions for future work."],"url":"http://arxiv.org/abs/2411.17558v1"}
{"created":"2024-11-26 16:18:59","title":"A Bilayer Segmentation-Recombination Network for Accurate Segmentation of Overlapping C. elegans","abstract":"Caenorhabditis elegans (C. elegans) is an excellent model organism because of its short lifespan and high degree of homology with human genes, and it has been widely used in a variety of human health and disease models. However, the segmentation of C. elegans remains challenging due to the following reasons: 1) the activity trajectory of C. elegans is uncontrollable, and multiple nematodes often overlap, resulting in blurred boundaries of C. elegans. This makes it impossible to clearly study the life trajectory of a certain nematode; and 2) in the microscope images of overlapping C. elegans, the translucent tissues at the edges obscure each other, leading to inaccurate boundary segmentation. To solve these problems, a Bilayer Segmentation-Recombination Network (BR-Net) for the segmentation of C. elegans instances is proposed. The network consists of three parts: A Coarse Mask Segmentation Module (CMSM), a Bilayer Segmentation Module (BSM), and a Semantic Consistency Recombination Module (SCRM). The CMSM is used to extract the coarse mask, and we introduce a Unified Attention Module (UAM) in CMSM to make CMSM better aware of nematode instances. The Bilayer Segmentation Module (BSM) segments the aggregated C. elegans into overlapping and non-overlapping regions. This is followed by integration by the SCRM, where semantic consistency regularization is introduced to segment nematode instances more accurately. Finally, the effectiveness of the method is verified on the C. elegans dataset. The experimental results show that BR-Net exhibits good competitiveness and outperforms other recently proposed instance segmentation methods in processing C. elegans occlusion images.","sentences":["Caenorhabditis elegans (C. elegans) is an excellent model organism because of its short lifespan and high degree of homology with human genes, and it has been widely used in a variety of human health and disease models.","However, the segmentation of C. elegans remains challenging due to the following reasons: 1) the activity trajectory of C. elegans is uncontrollable, and multiple nematodes often overlap, resulting in blurred boundaries of C. elegans.","This makes it impossible to clearly study the life trajectory of a certain nematode; and 2) in the microscope images of overlapping C. elegans, the translucent tissues at the edges obscure each other, leading to inaccurate boundary segmentation.","To solve these problems, a Bilayer Segmentation-Recombination Network (BR-Net) for the segmentation of C. elegans instances is proposed.","The network consists of three parts: A Coarse Mask Segmentation Module (CMSM), a Bilayer Segmentation Module (BSM), and a Semantic Consistency Recombination Module (SCRM).","The CMSM is used to extract the coarse mask, and we introduce a Unified Attention Module (UAM) in CMSM to make CMSM better aware of nematode instances.","The Bilayer Segmentation Module (BSM) segments the aggregated C. elegans into overlapping and non-overlapping regions.","This is followed by integration by the SCRM, where semantic consistency regularization is introduced to segment nematode instances more accurately.","Finally, the effectiveness of the method is verified on the C. elegans dataset.","The experimental results show that BR-Net exhibits good competitiveness and outperforms other recently proposed instance segmentation methods in processing C. elegans occlusion images."],"url":"http://arxiv.org/abs/2411.17557v1"}
{"created":"2024-11-26 16:18:38","title":"Multiscale spatiotemporal heterogeneity analysis of bike-sharing system's self-loop phenomenon: Evidence from Shanghai","abstract":"Bike-sharing is an environmentally friendly shared mobility mode, but its self-loop phenomenon, where bikes are returned to the same station after several time usage, significantly impacts equity in accessing its services. Therefore, this study conducts a multiscale analysis with a spatial autoregressive model and double machine learning framework to assess socioeconomic features and geospatial location's impact on the self-loop phenomenon at metro stations and street scales. The results reveal that bike-sharing self-loop intensity exhibits significant spatial lag effect at street scale and is positively associated with residential land use. Marginal treatment effects of residential land use is higher on streets with middle-aged residents, high fixed employment, and low car ownership. The multimodal public transit condition reveals significant positive marginal treatment effects at both scales. To enhance bike-sharing cooperation, we advocate augmenting bicycle availability in areas with high metro usage and low bus coverage, alongside implementing adaptable redistribution strategies.","sentences":["Bike-sharing is an environmentally friendly shared mobility mode, but its self-loop phenomenon, where bikes are returned to the same station after several time usage, significantly impacts equity in accessing its services.","Therefore, this study conducts a multiscale analysis with a spatial autoregressive model and double machine learning framework to assess socioeconomic features and geospatial location's impact on the self-loop phenomenon at metro stations and street scales.","The results reveal that bike-sharing self-loop intensity exhibits significant spatial lag effect at street scale and is positively associated with residential land use.","Marginal treatment effects of residential land use is higher on streets with middle-aged residents, high fixed employment, and low car ownership.","The multimodal public transit condition reveals significant positive marginal treatment effects at both scales.","To enhance bike-sharing cooperation, we advocate augmenting bicycle availability in areas with high metro usage and low bus coverage, alongside implementing adaptable redistribution strategies."],"url":"http://arxiv.org/abs/2411.17555v1"}
{"created":"2024-11-26 16:15:49","title":"Navigating Spatial Inequities in Freight Truck Crash Severity via Counterfactual Inference in Los Angeles","abstract":"Freight truck-related crashes pose significant challenges, leading to substantial economic losses, injuries, and fatalities, with pronounced spatial disparities across different regions. This study adopts a transport geography perspective to examine spatial justice concerns by employing deep counterfactual inference models to analyze how socioeconomic disparities, road infrastructure, and environmental conditions influence the geographical distribution and severity of freight truck crashes. By integrating road network datasets, socioeconomic attributes, and crash records from the Los Angeles metropolitan area, this research provides a nuanced spatial analysis of how different communities are disproportionately impacted. The results reveal significant spatial disparities in crash severity across areas with varying population densities, income levels, and minority populations, highlighting the pivotal role of infrastructural and environmental improvements in mitigating these disparities. The findings offer insights into targeted, location-specific policy interventions, suggesting enhancements in road infrastructure, lighting, and traffic control systems, particularly in low-income and minority-concentrated areas. This research contributes to the literature on transport geography and spatial equity by providing data-driven insights into effective measures for reducing spatial injustices associated with freight truck-related crashes.","sentences":["Freight truck-related crashes pose significant challenges, leading to substantial economic losses, injuries, and fatalities, with pronounced spatial disparities across different regions.","This study adopts a transport geography perspective to examine spatial justice concerns by employing deep counterfactual inference models to analyze how socioeconomic disparities, road infrastructure, and environmental conditions influence the geographical distribution and severity of freight truck crashes.","By integrating road network datasets, socioeconomic attributes, and crash records from the Los Angeles metropolitan area, this research provides a nuanced spatial analysis of how different communities are disproportionately impacted.","The results reveal significant spatial disparities in crash severity across areas with varying population densities, income levels, and minority populations, highlighting the pivotal role of infrastructural and environmental improvements in mitigating these disparities.","The findings offer insights into targeted, location-specific policy interventions, suggesting enhancements in road infrastructure, lighting, and traffic control systems, particularly in low-income and minority-concentrated areas.","This research contributes to the literature on transport geography and spatial equity by providing data-driven insights into effective measures for reducing spatial injustices associated with freight truck-related crashes."],"url":"http://arxiv.org/abs/2411.17554v1"}
{"created":"2024-11-26 16:11:55","title":"Tracing Optimization for Performance Modeling and Regression Detection","abstract":"Software performance modeling plays a crucial role in developing and maintaining software systems. A performance model analytically describes the relationship between the performance of a system and its runtime activities. This process typically examines various aspects of a system's runtime behavior, such as the execution frequency of functions or methods, to forecast performance metrics like program execution time. By using performance models, developers can predict expected performance and thereby effectively identify and address unexpected performance regressions when actual performance deviates from the model's predictions. One common and precise method for capturing performance behavior is software tracing, which involves instrumenting the execution of a program, either at the kernel level (e.g., system calls) or application level (e.g., function calls). However, due to the nature of tracing, it can be highly resource-intensive, making it impractical for production environments where resources are limited. In this work, we propose statistical approaches to reduce tracing overhead by identifying and excluding performance-insensitive code regions, particularly application-level functions, from tracing while still building accurate performance models that can capture performance degradations. By selecting an optimal set of functions to be traced, we can construct optimized performance models that achieve an R-2 score of up to 99% and, sometimes, outperform full tracing models (models using non-optimized tracing data), while significantly reducing the tracing overhead by more than 80% in most cases. Our optimized performance models can also capture performance regressions in our studied programs effectively, demonstrating their usefulness in real-world scenarios. Our approach is fully automated, making it ready to be used in production environments with minimal human effort.","sentences":["Software performance modeling plays a crucial role in developing and maintaining software systems.","A performance model analytically describes the relationship between the performance of a system and its runtime activities.","This process typically examines various aspects of a system's runtime behavior, such as the execution frequency of functions or methods, to forecast performance metrics like program execution time.","By using performance models, developers can predict expected performance and thereby effectively identify and address unexpected performance regressions when actual performance deviates from the model's predictions.","One common and precise method for capturing performance behavior is software tracing, which involves instrumenting the execution of a program, either at the kernel level (e.g., system calls) or application level (e.g., function calls).","However, due to the nature of tracing, it can be highly resource-intensive, making it impractical for production environments where resources are limited.","In this work, we propose statistical approaches to reduce tracing overhead by identifying and excluding performance-insensitive code regions, particularly application-level functions, from tracing while still building accurate performance models that can capture performance degradations.","By selecting an optimal set of functions to be traced, we can construct optimized performance models that achieve an R-2 score of up to 99% and, sometimes, outperform full tracing models (models using non-optimized tracing data), while significantly reducing the tracing overhead by more than 80% in most cases.","Our optimized performance models can also capture performance regressions in our studied programs effectively, demonstrating their usefulness in real-world scenarios.","Our approach is fully automated, making it ready to be used in production environments with minimal human effort."],"url":"http://arxiv.org/abs/2411.17548v1"}
{"created":"2024-11-26 16:04:20","title":"Rapid Deployment of Domain-specific Hyperspectral Image Processors with Application to Autonomous Driving","abstract":"The article discusses the use of low cost System-On-Module (SOM) platforms for the implementation of efficient hyperspectral imaging (HSI) processors for application in autonomous driving. The work addresses the challenges of shaping and deploying multiple layer fully convolutional networks (FCN) for low-latency, on-board image semantic segmentation using resource- and power-constrained processing devices. The paper describes in detail the steps followed to redesign and customize a successfully trained HSI segmentation lightweight FCN that was previously tested on a high-end heterogeneous multiprocessing system-on-chip (MPSoC) to accommodate it to the constraints imposed by a low-cost SOM. This SOM features a lower-end but much cheaper MPSoC suitable for the deployment of automatic driving systems (ADS). In particular the article reports the data- and hardware-specific quantization techniques utilized to fit the FCN into a commercial fixed-point programmable AI coprocessor IP, and proposes a full customized post-training quantization scheme to reduce computation and storage costs without compromising segmentation accuracy.","sentences":["The article discusses the use of low cost System-On-Module (SOM) platforms for the implementation of efficient hyperspectral imaging (HSI) processors for application in autonomous driving.","The work addresses the challenges of shaping and deploying multiple layer fully convolutional networks (FCN) for low-latency, on-board image semantic segmentation using resource- and power-constrained processing devices.","The paper describes in detail the steps followed to redesign and customize a successfully trained HSI segmentation lightweight FCN that was previously tested on a high-end heterogeneous multiprocessing system-on-chip (MPSoC) to accommodate it to the constraints imposed by a low-cost SOM.","This SOM features a lower-end but much cheaper MPSoC suitable for the deployment of automatic driving systems (ADS).","In particular the article reports the data- and hardware-specific quantization techniques utilized to fit the FCN into a commercial fixed-point programmable AI coprocessor IP, and proposes a full customized post-training quantization scheme to reduce computation and storage costs without compromising segmentation accuracy."],"url":"http://arxiv.org/abs/2411.17543v1"}
{"created":"2024-11-26 16:03:51","title":"Metaverse Innovation Canvas: A Tool for Extended Reality Product/Service Development","abstract":"This study investigated the factors contributing to the failure of augmented reality (AR) and virtual reality (VR) startups in the emerging metaverse landscape. Through an in-depth analysis of 29 failed AR/VR startups from 2016 to 2022, key pitfalls were identified, such as a lack of scalability, poor usability, unclear value propositions, and the failure to address specific user problems. Grounded in these findings, we developed the Metaverse Innovation Canvas (MIC) a tailored business ideation framework for XR products and services. The canvas guides founders to define user problems, articulate unique XR value propositions, evaluate usability factors such as the motion-based interaction load, consider social/virtual economy opportunities, and plan for long term scalability. Unlike generalized models, specialized blocks prompt the consideration of critical XR factors from the outset. The canvas was evaluated through expert testing with startup consultants on five failed venture cases. The results highlighted the tool's effectiveness in surfacing overlooked usability issues and technology constraints upfront, enhancing the viability of future metaverse startups.","sentences":["This study investigated the factors contributing to the failure of augmented reality (AR) and virtual reality (VR) startups in the emerging metaverse landscape.","Through an in-depth analysis of 29 failed AR/VR startups from 2016 to 2022, key pitfalls were identified, such as a lack of scalability, poor usability, unclear value propositions, and the failure to address specific user problems.","Grounded in these findings, we developed the Metaverse Innovation Canvas (MIC) a tailored business ideation framework for XR products and services.","The canvas guides founders to define user problems, articulate unique XR value propositions, evaluate usability factors such as the motion-based interaction load, consider social/virtual economy opportunities, and plan for long term scalability.","Unlike generalized models, specialized blocks prompt the consideration of critical XR factors from the outset.","The canvas was evaluated through expert testing with startup consultants on five failed venture cases.","The results highlighted the tool's effectiveness in surfacing overlooked usability issues and technology constraints upfront, enhancing the viability of future metaverse startups."],"url":"http://arxiv.org/abs/2411.17541v1"}
{"created":"2024-11-26 15:55:15","title":"AI-Augmented Ethical Hacking: A Practical Examination of Manual Exploitation and Privilege Escalation in Linux Environments","abstract":"This study explores the application of generative AI (GenAI) within manual exploitation and privilege escalation tasks in Linux-based penetration testing environments, two areas critical to comprehensive cybersecurity assessments. Building on previous research into the role of GenAI in the ethical hacking lifecycle, this paper presents a hands-on experimental analysis conducted in a controlled virtual setup to evaluate the utility of GenAI in supporting these crucial, often manual, tasks. Our findings demonstrate that GenAI can streamline processes, such as identifying potential attack vectors and parsing complex outputs for sensitive data during privilege escalation. The study also identifies key benefits and challenges associated with GenAI, including enhanced efficiency and scalability, alongside ethical concerns related to data privacy, unintended discovery of vulnerabilities, and potential for misuse. This work contributes to the growing field of AI-assisted cybersecurity by emphasising the importance of human-AI collaboration, especially in contexts requiring careful decision-making, rather than the complete replacement of human input.","sentences":["This study explores the application of generative AI (GenAI) within manual exploitation and privilege escalation tasks in Linux-based penetration testing environments, two areas critical to comprehensive cybersecurity assessments.","Building on previous research into the role of GenAI in the ethical hacking lifecycle, this paper presents a hands-on experimental analysis conducted in a controlled virtual setup to evaluate the utility of GenAI in supporting these crucial, often manual, tasks.","Our findings demonstrate that GenAI can streamline processes, such as identifying potential attack vectors and parsing complex outputs for sensitive data during privilege escalation.","The study also identifies key benefits and challenges associated with GenAI, including enhanced efficiency and scalability, alongside ethical concerns related to data privacy, unintended discovery of vulnerabilities, and potential for misuse.","This work contributes to the growing field of AI-assisted cybersecurity by emphasising the importance of human-AI collaboration, especially in contexts requiring careful decision-making, rather than the complete replacement of human input."],"url":"http://arxiv.org/abs/2411.17539v1"}
{"created":"2024-11-26 15:53:28","title":"Isotropy Matters: Soft-ZCA Whitening of Embeddings for Semantic Code Search","abstract":"Low isotropy in an embedding space impairs performance on tasks involving semantic inference. Our study investigates the impact of isotropy on semantic code search performance and explores post-processing techniques to mitigate this issue. We analyze various code language models, examine isotropy in their embedding spaces, and its influence on search effectiveness. We propose a modified ZCA whitening technique to control isotropy levels in embeddings. Our results demonstrate that Soft-ZCA whitening improves the performance of pre-trained code language models and can complement contrastive fine-tuning. The code for our experiments is available at https://github.com/drndr/code\\_isotropy","sentences":["Low isotropy in an embedding space impairs performance on tasks involving semantic inference.","Our study investigates the impact of isotropy on semantic code search performance and explores post-processing techniques to mitigate this issue.","We analyze various code language models, examine isotropy in their embedding spaces, and its influence on search effectiveness.","We propose a modified ZCA whitening technique to control isotropy levels in embeddings.","Our results demonstrate that Soft-ZCA whitening improves the performance of pre-trained code language models and can complement contrastive fine-tuning.","The code for our experiments is available at https://github.com/drndr/code\\_isotropy"],"url":"http://arxiv.org/abs/2411.17538v1"}
{"created":"2024-11-26 15:51:25","title":"Box for Mask and Mask for Box: weak losses for multi-task partially supervised learning","abstract":"Object detection and semantic segmentation are both scene understanding tasks yet they differ in data structure and information level. Object detection requires box coordinates for object instances while semantic segmentation requires pixel-wise class labels. Making use of one task's information to train the other would be beneficial for multi-task partially supervised learning where each training example is annotated only for a single task, having the potential to expand training sets with different-task datasets. This paper studies various weak losses for partially annotated data in combination with existing supervised losses. We propose Box-for-Mask and Mask-for-Box strategies, and their combination BoMBo, to distil necessary information from one task annotations to train the other. Ablation studies and experimental results on VOC and COCO datasets show favorable results for the proposed idea. Source code and data splits can be found at https://github.com/lhoangan/multas.","sentences":["Object detection and semantic segmentation are both scene understanding tasks yet they differ in data structure and information level.","Object detection requires box coordinates for object instances while semantic segmentation requires pixel-wise class labels.","Making use of one task's information to train the other would be beneficial for multi-task partially supervised learning where each training example is annotated only for a single task, having the potential to expand training sets with different-task datasets.","This paper studies various weak losses for partially annotated data in combination with existing supervised losses.","We propose Box-for-Mask and Mask-for-Box strategies, and their combination BoMBo, to distil necessary information from one task annotations to train the other.","Ablation studies and experimental results on VOC and COCO datasets show favorable results for the proposed idea.","Source code and data splits can be found at https://github.com/lhoangan/multas."],"url":"http://arxiv.org/abs/2411.17536v1"}
{"created":"2024-11-26 15:51:15","title":"IMPROVE: Improving Medical Plausibility without Reliance on HumanValidation -- An Enhanced Prototype-Guided Diffusion Framework","abstract":"Generative models have proven to be very effective in generating synthetic medical images and find applications in downstream tasks such as enhancing rare disease datasets, long-tailed dataset augmentation, and scaling machine learning algorithms. For medical applications, the synthetically generated medical images by such models are still reasonable in quality when evaluated based on traditional metrics such as FID score, precision, and recall. However, these metrics fail to capture the medical/biological plausibility of the generated images. Human expert feedback has been used to get biological plausibility which demonstrates that these generated images have very low plausibility. Recently, the research community has further integrated this human feedback through Reinforcement Learning from Human Feedback(RLHF), which generates more medically plausible images. However, incorporating human feedback is a costly and slow process. In this work, we propose a novel approach to improve the medical plausibility of generated images without the need for human feedback. We introduce IMPROVE:Improving Medical Plausibility without Reliance on Human Validation - An Enhanced Prototype-Guided Diffusion Framework, a prototype-guided diffusion process for medical image generation and show that it substantially enhances the biological plausibility of the generated medical images without the need for any human feedback. We perform experiments on Bone Marrow and HAM10000 datasets and show that medical accuracy can be substantially increased without human feedback.","sentences":["Generative models have proven to be very effective in generating synthetic medical images and find applications in downstream tasks such as enhancing rare disease datasets, long-tailed dataset augmentation, and scaling machine learning algorithms.","For medical applications, the synthetically generated medical images by such models are still reasonable in quality when evaluated based on traditional metrics such as FID score, precision, and recall.","However, these metrics fail to capture the medical/biological plausibility of the generated images.","Human expert feedback has been used to get biological plausibility which demonstrates that these generated images have very low plausibility.","Recently, the research community has further integrated this human feedback through Reinforcement Learning from Human Feedback(RLHF), which generates more medically plausible images.","However, incorporating human feedback is a costly and slow process.","In this work, we propose a novel approach to improve the medical plausibility of generated images without the need for human feedback.","We introduce IMPROVE:Improving Medical Plausibility without Reliance on Human Validation - An Enhanced Prototype-Guided Diffusion Framework, a prototype-guided diffusion process for medical image generation and show that it substantially enhances the biological plausibility of the generated medical images without the need for any human feedback.","We perform experiments on Bone Marrow and HAM10000 datasets and show that medical accuracy can be substantially increased without human feedback."],"url":"http://arxiv.org/abs/2411.17535v1"}
{"created":"2024-11-26 15:49:39","title":"Dynamic Trajectory Adaptation for Efficient UAV Inspections of Wind Energy Units","abstract":"The research presents an automated method for determining the trajectory of an unmanned aerial vehicle (UAV) for wind turbine inspection. The proposed method enables efficient data collection from multiple wind installations using UAV optical sensors, considering the spatial positioning of blades and other components of the wind energy installation. It includes component segmentation of the wind energy unit (WEU), determination of the blade pitch angle, and generation of optimal flight trajectories, considering safe distances and optimal viewing angles. The results of computational experiments have demonstrated the advantage of the proposed method in monitoring WEU, achieving a 78% reduction in inspection time, a 17% decrease in total trajectory length, and a 6% increase in average blade surface coverage compared to traditional methods. Furthermore, the process minimizes the average deviation from the optimal trajectory by 68%, indicating its high accuracy and ability to compensate for external influences.","sentences":["The research presents an automated method for determining the trajectory of an unmanned aerial vehicle (UAV) for wind turbine inspection.","The proposed method enables efficient data collection from multiple wind installations using UAV optical sensors, considering the spatial positioning of blades and other components of the wind energy installation.","It includes component segmentation of the wind energy unit (WEU), determination of the blade pitch angle, and generation of optimal flight trajectories, considering safe distances and optimal viewing angles.","The results of computational experiments have demonstrated the advantage of the proposed method in monitoring WEU, achieving a 78% reduction in inspection time, a 17% decrease in total trajectory length, and a 6% increase in average blade surface coverage compared to traditional methods.","Furthermore, the process minimizes the average deviation from the optimal trajectory by 68%, indicating its high accuracy and ability to compensate for external influences."],"url":"http://arxiv.org/abs/2411.17534v1"}
{"created":"2024-11-26 15:48:12","title":"FTMoMamba: Motion Generation with Frequency and Text State Space Models","abstract":"Diffusion models achieve impressive performance in human motion generation. However, current approaches typically ignore the significance of frequency-domain information in capturing fine-grained motions within the latent space (e.g., low frequencies correlate with static poses, and high frequencies align with fine-grained motions). Additionally, there is a semantic discrepancy between text and motion, leading to inconsistency between the generated motions and the text descriptions. In this work, we propose a novel diffusion-based FTMoMamba framework equipped with a Frequency State Space Model (FreqSSM) and a Text State Space Model (TextSSM). Specifically, to learn fine-grained representation, FreqSSM decomposes sequences into low-frequency and high-frequency components, guiding the generation of static pose (e.g., sits, lay) and fine-grained motions (e.g., transition, stumble), respectively. To ensure the consistency between text and motion, TextSSM encodes text features at the sentence level, aligning textual semantics with sequential features. Extensive experiments show that FTMoMamba achieves superior performance on the text-to-motion generation task, especially gaining the lowest FID of 0.181 (rather lower than 0.421 of MLD) on the HumanML3D dataset.","sentences":["Diffusion models achieve impressive performance in human motion generation.","However, current approaches typically ignore the significance of frequency-domain information in capturing fine-grained motions within the latent space (e.g., low frequencies correlate with static poses, and high frequencies align with fine-grained motions).","Additionally, there is a semantic discrepancy between text and motion, leading to inconsistency between the generated motions and the text descriptions.","In this work, we propose a novel diffusion-based FTMoMamba framework equipped with a Frequency State Space Model (FreqSSM) and a Text State Space Model (TextSSM).","Specifically, to learn fine-grained representation, FreqSSM decomposes sequences into low-frequency and high-frequency components, guiding the generation of static pose (e.g., sits, lay) and fine-grained motions (e.g., transition, stumble), respectively.","To ensure the consistency between text and motion, TextSSM encodes text features at the sentence level, aligning textual semantics with sequential features.","Extensive experiments show that FTMoMamba achieves superior performance on the text-to-motion generation task, especially gaining the lowest FID of 0.181 (rather lower than 0.421 of MLD) on the HumanML3D dataset."],"url":"http://arxiv.org/abs/2411.17532v1"}
{"created":"2024-11-26 15:45:59","title":"HSI-Drive v2.0: More Data for New Challenges in Scene Understanding for Autonomous Driving","abstract":"We present the updated version of the HSI-Drive dataset aimed at developing automated driving systems (ADS) using hyperspectral imaging (HSI). The v2.0 version includes new annotated images from videos recorded during winter and fall in real driving scenarios. Added to the spring and summer images included in the previous v1.1 version, the new dataset contains 752 images covering the four seasons. In this paper, we show the improvements achieved over previously published results obtained on the v1.1 dataset, showcasing the enhanced performance of models trained on the new v2.0 dataset. We also show the progress made in comprehensive scene understanding by experimenting with more capable image segmentation models. These models include new segmentation categories aimed at the identification of essential road safety objects such as the presence of vehicles and road signs, as well as highly vulnerable groups like pedestrians and cyclists. In addition, we provide evidence of the performance and robustness of the models when applied to segmenting HSI video sequences captured in various environments and conditions. Finally, for a correct assessment of the results described in this work, the constraints imposed by the processing platforms that can sensibly be deployed in vehicles for ADS must be taken into account. Thus, and although implementation details are out of the scope of this paper, we focus our research on the development of computationally efficient, lightweight ML models that can eventually operate at high throughput rates. The dataset and some examples of segmented videos are available in https://ipaccess.ehu.eus/HSI-Drive/.","sentences":["We present the updated version of the HSI-Drive dataset aimed at developing automated driving systems (ADS) using hyperspectral imaging (HSI).","The v2.0 version includes new annotated images from videos recorded during winter and fall in real driving scenarios.","Added to the spring and summer images included in the previous v1.1 version, the new dataset contains 752 images covering the four seasons.","In this paper, we show the improvements achieved over previously published results obtained on the v1.1 dataset, showcasing the enhanced performance of models trained on the new v2.0 dataset.","We also show the progress made in comprehensive scene understanding by experimenting with more capable image segmentation models.","These models include new segmentation categories aimed at the identification of essential road safety objects such as the presence of vehicles and road signs, as well as highly vulnerable groups like pedestrians and cyclists.","In addition, we provide evidence of the performance and robustness of the models when applied to segmenting HSI video sequences captured in various environments and conditions.","Finally, for a correct assessment of the results described in this work, the constraints imposed by the processing platforms that can sensibly be deployed in vehicles for ADS must be taken into account.","Thus, and although implementation details are out of the scope of this paper, we focus our research on the development of computationally efficient, lightweight ML models that can eventually operate at high throughput rates.","The dataset and some examples of segmented videos are available in https://ipaccess.ehu.eus/HSI-Drive/."],"url":"http://arxiv.org/abs/2411.17530v1"}
{"created":"2024-11-26 15:42:21","title":"Evolving Markov Chains: Unsupervised Mode Discovery and Recognition from Data Streams","abstract":"Markov chains are simple yet powerful mathematical structures to model temporally dependent processes. They generally assume stationary data, i.e., fixed transition probabilities between observations/states. However, live, real-world processes, like in the context of activity tracking, biological time series, or industrial monitoring, often switch behavior over time. Such behavior switches can be modeled as transitions between higher-level \\emph{modes} (e.g., running, walking, etc.). Yet all modes are usually not previously known, often exhibit vastly differing transition probabilities, and can switch unpredictably. Thus, to track behavior changes of live, real-world processes, this study proposes an online and efficient method to construct Evolving Markov chains (EMCs). EMCs adaptively track transition probabilities, automatically discover modes, and detect mode switches in an online manner. In contrast to previous work, EMCs are of arbitrary order, the proposed update scheme does not rely on tracking windows, only updates the relevant region of the probability tensor, and enjoys geometric convergence of the expected estimates. Our evaluation of synthetic data and real-world applications on human activity recognition, electric motor condition monitoring, and eye-state recognition from electroencephalography (EEG) measurements illustrates the versatility of the approach and points to the potential of EMCs to efficiently track, model, and understand live, real-world processes.","sentences":["Markov chains are simple yet powerful mathematical structures to model temporally dependent processes.","They generally assume stationary data, i.e., fixed transition probabilities between observations/states.","However, live, real-world processes, like in the context of activity tracking, biological time series, or industrial monitoring, often switch behavior over time.","Such behavior switches can be modeled as transitions between higher-level \\emph{modes} (e.g., running, walking, etc.).","Yet all modes are usually not previously known, often exhibit vastly differing transition probabilities, and can switch unpredictably.","Thus, to track behavior changes of live, real-world processes, this study proposes an online and efficient method to construct Evolving Markov chains (EMCs).","EMCs adaptively track transition probabilities, automatically discover modes, and detect mode switches in an online manner.","In contrast to previous work, EMCs are of arbitrary order, the proposed update scheme does not rely on tracking windows, only updates the relevant region of the probability tensor, and enjoys geometric convergence of the expected estimates.","Our evaluation of synthetic data and real-world applications on human activity recognition, electric motor condition monitoring, and eye-state recognition from electroencephalography (EEG) measurements illustrates the versatility of the approach and points to the potential of EMCs to efficiently track, model, and understand live, real-world processes."],"url":"http://arxiv.org/abs/2411.17528v1"}
{"created":"2024-11-26 15:35:44","title":"Pushing the Limits of Large Language Model Quantization via the Linearity Theorem","abstract":"Quantizing large language models has become a standard way to reduce their memory and computational costs. Typically, existing methods focus on breaking down the problem into individual layer-wise sub-problems, and minimizing per-layer error, measured via various metrics. Yet, this approach currently lacks theoretical justification and the metrics employed may be sub-optimal. In this paper, we present a \"linearity theorem\" establishing a direct relationship between the layer-wise $\\ell_2$ reconstruction error and the model perplexity increase due to quantization. This insight enables two novel applications: (1) a simple data-free LLM quantization method using Hadamard rotations and MSE-optimal grids, dubbed HIGGS, which outperforms all prior data-free approaches such as the extremely popular NF4 quantized format, and (2) an optimal solution to the problem of finding non-uniform per-layer quantization levels which match a given compression constraint in the medium-bitwidth regime, obtained by reduction to dynamic programming. On the practical side, we demonstrate improved accuracy-compression trade-offs on Llama-3.1 and 3.2-family models, as well as on Qwen-family models. Further, we show that our method can be efficiently supported in terms of GPU kernels at various batch sizes, advancing both data-free and non-uniform quantization for LLMs.","sentences":["Quantizing large language models has become a standard way to reduce their memory and computational costs.","Typically, existing methods focus on breaking down the problem into individual layer-wise sub-problems, and minimizing per-layer error, measured via various metrics.","Yet, this approach currently lacks theoretical justification and the metrics employed may be sub-optimal.","In this paper, we present a \"linearity theorem\" establishing a direct relationship between the layer-wise $\\ell_2$ reconstruction error and the model perplexity increase due to quantization.","This insight enables two novel applications: (1) a simple data-free LLM quantization method using Hadamard rotations and MSE-optimal grids, dubbed HIGGS, which outperforms all prior data-free approaches such as the extremely popular NF4 quantized format, and (2) an optimal solution to the problem of finding non-uniform per-layer quantization levels which match a given compression constraint in the medium-bitwidth regime, obtained by reduction to dynamic programming.","On the practical side, we demonstrate improved accuracy-compression trade-offs on Llama-3.1 and 3.2-family models, as well as on Qwen-family models.","Further, we show that our method can be efficiently supported in terms of GPU kernels at various batch sizes, advancing both data-free and non-uniform quantization for LLMs."],"url":"http://arxiv.org/abs/2411.17525v1"}
{"created":"2024-11-26 15:30:20","title":"BESTAnP: Bi-Step Efficient and Statistically Optimal Estimator for Acoustic-n-Point Problem","abstract":"We consider the acoustic-n-point (AnP) problem, which estimates the pose of a 2D forward-looking sonar (FLS) according to n 3D-2D point correspondences. We explore the nature of the measured partial spherical coordinates and reveal their inherent relationships to translation and orientation. Based on this, we propose a bi-step efficient and statistically optimal AnP (BESTAnP) algorithm that decouples the estimation of translation and orientation. Specifically, in the first step, the translation estimation is formulated as the range-based localization problem based on distance-only measurements. In the second step, the rotation is estimated via eigendecomposition based on azimuth-only measurements and the estimated translation. BESTAnP is the first AnP algorithm that gives a closed-form solution for the full six-degree pose. In addition, we conduct bias elimination for BESTAnP such that it owns the statistical property of consistency. Through simulation and real-world experiments, we demonstrate that compared with the state-of-the-art (SOTA) methods, BESTAnP is over ten times faster and features real-time capacity in resource-constrained platforms while exhibiting comparable accuracy. Moreover, for the first time, we embed BESTAnP into a sonar-based odometry which shows its effectiveness for trajectory estimation.","sentences":["We consider the acoustic-n-point (AnP) problem, which estimates the pose of a 2D forward-looking sonar (FLS) according to n 3D-2D point correspondences.","We explore the nature of the measured partial spherical coordinates and reveal their inherent relationships to translation and orientation.","Based on this, we propose a bi-step efficient and statistically optimal AnP (BESTAnP) algorithm that decouples the estimation of translation and orientation.","Specifically, in the first step, the translation estimation is formulated as the range-based localization problem based on distance-only measurements.","In the second step, the rotation is estimated via eigendecomposition based on azimuth-only measurements and the estimated translation.","BESTAnP is the first AnP algorithm that gives a closed-form solution for the full six-degree pose.","In addition, we conduct bias elimination for BESTAnP such that it owns the statistical property of consistency.","Through simulation and real-world experiments, we demonstrate that compared with the state-of-the-art (SOTA) methods, BESTAnP is over ten times faster and features real-time capacity in resource-constrained platforms while exhibiting comparable accuracy.","Moreover, for the first time, we embed BESTAnP into a sonar-based odometry which shows its effectiveness for trajectory estimation."],"url":"http://arxiv.org/abs/2411.17521v1"}
{"created":"2024-11-26 15:26:06","title":"SuperMat: Physically Consistent PBR Material Estimation at Interactive Rates","abstract":"Decomposing physically-based materials from images into their constituent properties remains challenging, particularly when maintaining both computational efficiency and physical consistency. While recent diffusion-based approaches have shown promise, they face substantial computational overhead due to multiple denoising steps and separate models for different material properties. We present SuperMat, a single-step framework that achieves high-quality material decomposition with one-step inference. This enables end-to-end training with perceptual and re-render losses while decomposing albedo, metallic, and roughness maps at millisecond-scale speeds. We further extend our framework to 3D objects through a UV refinement network, enabling consistent material estimation across viewpoints while maintaining efficiency. Experiments demonstrate that SuperMat achieves state-of-the-art PBR material decomposition quality while reducing inference time from seconds to milliseconds per image, and completes PBR material estimation for 3D objects in approximately 3 seconds.","sentences":["Decomposing physically-based materials from images into their constituent properties remains challenging, particularly when maintaining both computational efficiency and physical consistency.","While recent diffusion-based approaches have shown promise, they face substantial computational overhead due to multiple denoising steps and separate models for different material properties.","We present SuperMat, a single-step framework that achieves high-quality material decomposition with one-step inference.","This enables end-to-end training with perceptual and re-render losses while decomposing albedo, metallic, and roughness maps at millisecond-scale speeds.","We further extend our framework to 3D objects through a UV refinement network, enabling consistent material estimation across viewpoints while maintaining efficiency.","Experiments demonstrate that SuperMat achieves state-of-the-art PBR material decomposition quality while reducing inference time from seconds to milliseconds per image, and completes PBR material estimation for 3D objects in approximately 3 seconds."],"url":"http://arxiv.org/abs/2411.17515v1"}
{"created":"2024-11-26 15:24:45","title":"Perceptually Optimized Super Resolution","abstract":"Modern deep-learning based super-resolution techniques process images and videos independently of the underlying content and viewing conditions. However, the sensitivity of the human visual system to image details changes depending on the underlying content characteristics, such as spatial frequency, luminance, color, contrast, or motion. This observation hints that computational resources spent on up-sampling visual content may be wasted whenever a viewer cannot resolve the results. Motivated by this observation, we propose a perceptually inspired and architecture-agnostic approach for controlling the visual quality and efficiency of super-resolution techniques. The core is a perceptual model that dynamically guides super-resolution methods according to the human's sensitivity to image details. Our technique leverages the limitations of the human visual system to improve the efficiency of super-resolution techniques by focusing computational resources on perceptually important regions; judged on the basis of factors such as adapting luminance, contrast, spatial frequency, motion, and viewing conditions. We demonstrate the application of our proposed model in combination with network branching, and network complexity reduction to improve the computational efficiency of super-resolution methods without visible quality loss. Quantitative and qualitative evaluations, including user studies, demonstrate the effectiveness of our approach in reducing FLOPS by factors of 2$\\mathbf{x}$ and greater, without sacrificing perceived quality.","sentences":["Modern deep-learning based super-resolution techniques process images and videos independently of the underlying content and viewing conditions.","However, the sensitivity of the human visual system to image details changes depending on the underlying content characteristics, such as spatial frequency, luminance, color, contrast, or motion.","This observation hints that computational resources spent on up-sampling visual content may be wasted whenever a viewer cannot resolve the results.","Motivated by this observation, we propose a perceptually inspired and architecture-agnostic approach for controlling the visual quality and efficiency of super-resolution techniques.","The core is a perceptual model that dynamically guides super-resolution methods according to the human's sensitivity to image details.","Our technique leverages the limitations of the human visual system to improve the efficiency of super-resolution techniques by focusing computational resources on perceptually important regions; judged on the basis of factors such as adapting luminance, contrast, spatial frequency, motion, and viewing conditions.","We demonstrate the application of our proposed model in combination with network branching, and network complexity reduction to improve the computational efficiency of super-resolution methods without visible quality loss.","Quantitative and qualitative evaluations, including user studies, demonstrate the effectiveness of our approach in reducing FLOPS by factors of 2$\\mathbf{x}$ and greater, without sacrificing perceived quality."],"url":"http://arxiv.org/abs/2411.17513v1"}
{"created":"2024-11-26 15:22:30","title":"Training Hamiltonian neural networks without backpropagation","abstract":"Neural networks that synergistically integrate data and physical laws offer great promise in modeling dynamical systems. However, iterative gradient-based optimization of network parameters is often computationally expensive and suffers from slow convergence. In this work, we present a backpropagation-free algorithm to accelerate the training of neural networks for approximating Hamiltonian systems through data-agnostic and data-driven algorithms. We empirically show that data-driven sampling of the network parameters outperforms data-agnostic sampling or the traditional gradient-based iterative optimization of the network parameters when approximating functions with steep gradients or wide input domains. We demonstrate that our approach is more than 100 times faster with CPUs than the traditionally trained Hamiltonian Neural Networks using gradient-based iterative optimization and is more than four orders of magnitude accurate in chaotic examples, including the H\\'enon-Heiles system.","sentences":["Neural networks that synergistically integrate data and physical laws offer great promise in modeling dynamical systems.","However, iterative gradient-based optimization of network parameters is often computationally expensive and suffers from slow convergence.","In this work, we present a backpropagation-free algorithm to accelerate the training of neural networks for approximating Hamiltonian systems through data-agnostic and data-driven algorithms.","We empirically show that data-driven sampling of the network parameters outperforms data-agnostic sampling or the traditional gradient-based iterative optimization of the network parameters when approximating functions with steep gradients or wide input domains.","We demonstrate that our approach is more than 100 times faster with CPUs than the traditionally trained Hamiltonian Neural Networks using gradient-based iterative optimization and is more than four orders of magnitude accurate in chaotic examples, including the H\\'enon-Heiles system."],"url":"http://arxiv.org/abs/2411.17511v1"}
{"created":"2024-11-26 15:17:57","title":"Learning-Based On-Track System Identification for Scaled Autonomous Racing in Under a Minute","abstract":"Accurate tire modeling is crucial for optimizing autonomous racing vehicles, as state-of-the-art (SotA) model-based techniques rely on precise knowledge of the vehicle's parameters. Yet, system identification in dynamic racing conditions is challenging due to varying track and tire conditions. Traditional methods require extensive operational ranges, often impractical in racing scenarios. Machine learning (ML)-based methods, while improving performance, struggle with generalization and depend on accurate initialization. This paper introduces a novel on-track system identification algorithm, incorporating a neural network (NN) for error correction, which is then employed for traditional system identification with virtually generated data. Crucially, the process is iteratively reapplied, with tire parameters updated at each cycle, leading to notable improvements in accuracy in tests on a scaled vehicle. Experiments show that it is possible to learn a tire model without prior knowledge with only 30 seconds of driving data and 3 seconds of training time. This method demonstrates greater one-step prediction accuracy than the baseline nonlinear least squares (NLS) method under noisy conditions, achieving a 3.3x lower root mean square error (RMSE), and yields tire models with comparable accuracy to traditional steady-state system identification. Furthermore, unlike steady-state methods requiring large spaces and specific experimental setups, the proposed approach identifies tire parameters directly on a race track in dynamic racing environments.","sentences":["Accurate tire modeling is crucial for optimizing autonomous racing vehicles, as state-of-the-art (SotA) model-based techniques rely on precise knowledge of the vehicle's parameters.","Yet, system identification in dynamic racing conditions is challenging due to varying track and tire conditions.","Traditional methods require extensive operational ranges, often impractical in racing scenarios.","Machine learning (ML)-based methods, while improving performance, struggle with generalization and depend on accurate initialization.","This paper introduces a novel on-track system identification algorithm, incorporating a neural network (NN) for error correction, which is then employed for traditional system identification with virtually generated data.","Crucially, the process is iteratively reapplied, with tire parameters updated at each cycle, leading to notable improvements in accuracy in tests on a scaled vehicle.","Experiments show that it is possible to learn a tire model without prior knowledge with only 30 seconds of driving data and 3 seconds of training time.","This method demonstrates greater one-step prediction accuracy than the baseline nonlinear least squares (NLS) method under noisy conditions, achieving a 3.3x lower root mean square error (RMSE), and yields tire models with comparable accuracy to traditional steady-state system identification.","Furthermore, unlike steady-state methods requiring large spaces and specific experimental setups, the proposed approach identifies tire parameters directly on a race track in dynamic racing environments."],"url":"http://arxiv.org/abs/2411.17508v1"}
{"created":"2024-11-26 15:17:24","title":"Neural network modelling of kinematic and dynamic features for signature verification","abstract":"Online signature parameters, which are based on human characteristics, broaden the applicability of an automatic signature verifier. Although kinematic and dynamic features have previously been suggested, accurately measuring features such as arm and forearm torques remains challenging. We present two approaches for estimating angular velocities, angular positions, and force torques. The first approach involves using a physical UR5e robotic arm to reproduce a signature while capturing those parameters over time. The second method, a cost effective approach, uses a neural network to estimate the same parameters. Our findings demonstrate that a simple neural network model can extract effective parameters for signature verification. Training the neural network with the MCYT300 dataset and cross validating with other databases, namely, BiosecurID, Visual, Blind, OnOffSigDevanagari 75 and OnOffSigBengali 75 confirm the models generalization capability.","sentences":["Online signature parameters, which are based on human characteristics, broaden the applicability of an automatic signature verifier.","Although kinematic and dynamic features have previously been suggested, accurately measuring features such as arm and forearm torques remains challenging.","We present two approaches for estimating angular velocities, angular positions, and force torques.","The first approach involves using a physical UR5e robotic arm to reproduce a signature while capturing those parameters over time.","The second method, a cost effective approach, uses a neural network to estimate the same parameters.","Our findings demonstrate that a simple neural network model can extract effective parameters for signature verification.","Training the neural network with the MCYT300 dataset and cross validating with other databases, namely, BiosecurID, Visual, Blind, OnOffSigDevanagari 75 and OnOffSigBengali 75 confirm the models generalization capability."],"url":"http://arxiv.org/abs/2411.17506v1"}
{"created":"2024-11-26 15:17:03","title":"Resonant Inductive Coupling Power Transfer for Mid-Sized Inspection Robot","abstract":"This paper presents a wireless power transfer (WPT) for a mid-sized inspection mobile robot. The objective is to transmit 100 W of power over 1 meter of distance, achieved through lightweight Litz wire coils weighing 320 g held together with a coil structure of 3.54 kg. The Wireless Power Transfer System (WPTS) is mounted onto an unmanned ground vehicle (UGV). The study addresses an investigation of coil design, accounting for misalignment and tolerance issues in resonance-coupled coils. In experimental validation, the system effectively transmits 109.7 W of power over a 1-meter distance, with obstacles present. This achievement yields a system efficiency of 47.14%, a value that is remarkably close to the maximum power transfer point (50%) when the WPTS utilises the full voltage allowance of the capacitor. The paper shows the WPTS charging speed of 5 minutes for 12 V, 0.8 Ah lead acid batteries.","sentences":["This paper presents a wireless power transfer (WPT) for a mid-sized inspection mobile robot.","The objective is to transmit 100 W of power over 1 meter of distance, achieved through lightweight Litz wire coils weighing 320 g held together with a coil structure of 3.54 kg.","The Wireless Power Transfer System (WPTS) is mounted onto an unmanned ground vehicle (UGV).","The study addresses an investigation of coil design, accounting for misalignment and tolerance issues in resonance-coupled coils.","In experimental validation, the system effectively transmits 109.7 W of power over a 1-meter distance, with obstacles present.","This achievement yields a system efficiency of 47.14%, a value that is remarkably close to the maximum power transfer point (50%) when the WPTS utilises the full voltage allowance of the capacitor.","The paper shows the WPTS charging speed of 5 minutes for 12 V, 0.8 Ah lead acid batteries."],"url":"http://arxiv.org/abs/2411.17505v1"}
{"created":"2024-11-26 15:13:13","title":"Confidence-Aware Deep Learning for Load Plan Adjustments in the Parcel Service Industry","abstract":"This study develops a deep learning-based approach to automate inbound load plan adjustments for a large transportation and logistics company. It addresses a critical challenge for the efficient and resilient planning of E-commerce operations in presence of increasing uncertainties. The paper introduces an innovative data-driven approach to inbound load planning. Leveraging extensive historical data, the paper presents a two-stage decision-making process using deep learning and conformal prediction to provide scalable, accurate, and confidence-aware solutions. The first stage of the prediction is dedicated to tactical load-planning, while the second stage is dedicated to the operational planning, incorporating the latest available data to refine the decisions at the finest granularity. Extensive experiments compare traditional machine learning models and deep learning methods. They highlight the importance and effectiveness of the embedding layers for enhancing the performance of deep learning models. Furthermore, the results emphasize the efficacy of conformal prediction to provide confidence-aware prediction sets. The findings suggest that data-driven methods can substantially improve decision making in inbound load planning, offering planners a comprehensive, trustworthy, and real-time framework to make decisions. The initial deployment in the industry setting indicates a high accuracy of the proposed framework.","sentences":["This study develops a deep learning-based approach to automate inbound load plan adjustments for a large transportation and logistics company.","It addresses a critical challenge for the efficient and resilient planning of E-commerce operations in presence of increasing uncertainties.","The paper introduces an innovative data-driven approach to inbound load planning.","Leveraging extensive historical data, the paper presents a two-stage decision-making process using deep learning and conformal prediction to provide scalable, accurate, and confidence-aware solutions.","The first stage of the prediction is dedicated to tactical load-planning, while the second stage is dedicated to the operational planning, incorporating the latest available data to refine the decisions at the finest granularity.","Extensive experiments compare traditional machine learning models and deep learning methods.","They highlight the importance and effectiveness of the embedding layers for enhancing the performance of deep learning models.","Furthermore, the results emphasize the efficacy of conformal prediction to provide confidence-aware prediction sets.","The findings suggest that data-driven methods can substantially improve decision making in inbound load planning, offering planners a comprehensive, trustworthy, and real-time framework to make decisions.","The initial deployment in the industry setting indicates a high accuracy of the proposed framework."],"url":"http://arxiv.org/abs/2411.17502v1"}
{"created":"2024-11-26 15:13:06","title":"Inference Scaling $\\scriptsize\\mathtt{F}$Laws: The Limits of LLM Resampling with Imperfect Verifiers","abstract":"Recent research has generated hope that inference scaling could allow weaker language models to match or exceed the accuracy of stronger models, such as by repeatedly sampling solutions to a coding problem until it passes unit tests. The central thesis of this paper is that there is no free lunch for inference scaling: indefinite accuracy improvement through resampling can only be realized if the \"verifier\" (in this case, a set of unit tests) is perfect. When the verifier is imperfect, as it almost always is in domains such as reasoning or coding (for example, unit tests have imperfect coverage), there is a nonzero probability of false positives: incorrect solutions that pass the verifier. Resampling cannot decrease this probability, so it imposes an upper bound to the accuracy of resampling-based inference scaling even with an infinite compute budget. We find that there is a very strong correlation between the model's single-sample accuracy (i.e. accuracy without unit tests) and its false positive rate on coding benchmarks HumanEval and MBPP, whose unit tests have limited coverage. Therefore, no amount of inference scaling of weaker models can enable them to match the single-sample accuracy of a sufficiently strong model (Fig. 1a). When we consider that false positives have a negative utility compared to abstaining from producing a solution, it bends the inference scaling curve further downward. Empirically, we find that the optimal number of samples can be less than 10 under realistic assumptions (Fig. 1b). Finally, we show that beyond accuracy, false positives may have other undesirable qualities, such as poor adherence to coding style conventions.","sentences":["Recent research has generated hope that inference scaling could allow weaker language models to match or exceed the accuracy of stronger models, such as by repeatedly sampling solutions to a coding problem until it passes unit tests.","The central thesis of this paper is that there is no free lunch for inference scaling: indefinite accuracy improvement through resampling can only be realized if the \"verifier\" (in this case, a set of unit tests) is perfect.","When the verifier is imperfect, as it almost always is in domains such as reasoning or coding (for example, unit tests have imperfect coverage), there is a nonzero probability of false positives: incorrect solutions that pass the verifier.","Resampling cannot decrease this probability, so it imposes an upper bound to the accuracy of resampling-based inference scaling even with an infinite compute budget.","We find that there is a very strong correlation between the model's single-sample accuracy (i.e. accuracy without unit tests) and its false positive rate on coding benchmarks HumanEval and MBPP, whose unit tests have limited coverage.","Therefore, no amount of inference scaling of weaker models can enable them to match the single-sample accuracy of a sufficiently strong model (Fig. 1a).","When we consider that false positives have a negative utility compared to abstaining from producing a solution, it bends the inference scaling curve further downward.","Empirically, we find that the optimal number of samples can be less than 10 under realistic assumptions (Fig. 1b).","Finally, we show that beyond accuracy, false positives may have other undesirable qualities, such as poor adherence to coding style conventions."],"url":"http://arxiv.org/abs/2411.17501v1"}
{"created":"2024-11-26 15:10:31","title":"Time-Series Forecasting in Smart Manufacturing Systems: An Experimental Evaluation of the State-of-the-art Algorithms","abstract":"TSF is growing in various domains including manufacturing. Although numerous TSF algorithms have been developed recently, the validation and evaluation of algorithms hold substantial value for researchers and practitioners and are missing. This study aims to fill this gap by evaluating the SoTA TSF algorithms on thirteen manufacturing datasets, focusing on their applicability in manufacturing. Each algorithm was selected based on its TSF category to ensure a representative set of algorithms. The evaluation includes different scenarios to evaluate the models using two problem categories and two forecasting horizons. To evaluate the performance, the WAPE was calculated, and additional post hoc analyses were conducted to assess the significance of observed differences. Only algorithms with codes from open-source libraries were utilized, and no hyperparameter tuning was done. This allowed us to evaluate the algorithms as \"out-of-the-box\" solutions that can be easily implemented, ensuring their usability within the manufacturing by practitioners with limited technical knowledge. This aligns to facilitate the adoption of these techniques in smart manufacturing systems. Based on the results, transformer and MLP-based architectures demonstrated the best performance with MLP-based architecture winning the most scenarios. For univariate TSF, PatchTST emerged as the most robust, particularly for long-term horizons, while for multivariate problems, MLP-based architectures like N-HITS and TiDE showed superior results. The study revealed that simpler algorithms like XGBoost could outperform complex algorithms in certain tasks. These findings challenge the assumption that more sophisticated models produce better results. Additionally, the research highlighted the importance of computational resource considerations, showing variations in runtime and memory usage across different algorithms.","sentences":["TSF is growing in various domains including manufacturing.","Although numerous TSF algorithms have been developed recently, the validation and evaluation of algorithms hold substantial value for researchers and practitioners and are missing.","This study aims to fill this gap by evaluating the SoTA TSF algorithms on thirteen manufacturing datasets, focusing on their applicability in manufacturing.","Each algorithm was selected based on its TSF category to ensure a representative set of algorithms.","The evaluation includes different scenarios to evaluate the models using two problem categories and two forecasting horizons.","To evaluate the performance, the WAPE was calculated, and additional post hoc analyses were conducted to assess the significance of observed differences.","Only algorithms with codes from open-source libraries were utilized, and no hyperparameter tuning was done.","This allowed us to evaluate the algorithms as \"out-of-the-box\" solutions that can be easily implemented, ensuring their usability within the manufacturing by practitioners with limited technical knowledge.","This aligns to facilitate the adoption of these techniques in smart manufacturing systems.","Based on the results, transformer and MLP-based architectures demonstrated the best performance with MLP-based architecture winning the most scenarios.","For univariate TSF, PatchTST emerged as the most robust, particularly for long-term horizons, while for multivariate problems, MLP-based architectures like N-HITS and TiDE showed superior results.","The study revealed that simpler algorithms like XGBoost could outperform complex algorithms in certain tasks.","These findings challenge the assumption that more sophisticated models produce better results.","Additionally, the research highlighted the importance of computational resource considerations, showing variations in runtime and memory usage across different algorithms."],"url":"http://arxiv.org/abs/2411.17499v1"}
{"created":"2024-11-26 15:10:30","title":"Simplification of Polyhedral Reductions in Practice","abstract":"Reductions combine collections of inputs with an associative (and here, also commutative) operator to produce collections of outputs. When the same value contributes to multiple outputs, there is an opportunity to reuse partial results, enabling reduction simplification. We provide the first complete push-button implementation of reduction simplification in a compiler. We evaluate its effectiveness on a range of real-world applications, and show that simplification rediscovers several key results in algorithmic improvement across multiple domains, previously only obtained through clever manual human analysis and effort. We also discover alternate, previously unknown algorithms, albeit without improving the asymptotic complexity.","sentences":["Reductions combine collections of inputs with an associative (and here, also commutative) operator to produce collections of outputs.","When the same value contributes to multiple outputs, there is an opportunity to reuse partial results, enabling reduction simplification.","We provide the first complete push-button implementation of reduction simplification in a compiler.","We evaluate its effectiveness on a range of real-world applications, and show that simplification rediscovers several key results in algorithmic improvement across multiple domains, previously only obtained through clever manual human analysis and effort.","We also discover alternate, previously unknown algorithms, albeit without improving the asymptotic complexity."],"url":"http://arxiv.org/abs/2411.17498v1"}
