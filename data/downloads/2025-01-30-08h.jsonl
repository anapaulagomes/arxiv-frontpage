{"created":"2025-01-29 18:58:48","title":"Dialogue is Better Than Monologue: Instructing Medical LLMs via Strategical Conversations","abstract":"Current medical AI systems often fail to replicate real-world clinical reasoning, as they are predominantly trained and evaluated on static text and question-answer tasks. These tuning methods and benchmarks overlook critical aspects like evidence-based reasoning and handling distracting information. To bridge this gap, we introduce a novel benchmark that simulates real-world diagnostic scenarios, integrating noise and difficulty levels aligned with USMLE standards. Moreover, we explore dialogue-based fine-tuning, which transforms static datasets into conversational formats to better capture iterative reasoning processes. Experiments show that dialogue-tuned models outperform traditional methods, with improvements of $9.64\\%$ in multi-round reasoning scenarios and $6.18\\%$ in accuracy in a noisy environment. Our findings highlight dialogue tuning as a promising approach for advancing clinically aligned and robust medical AI systems.","sentences":["Current medical AI systems often fail to replicate real-world clinical reasoning, as they are predominantly trained and evaluated on static text and question-answer tasks.","These tuning methods and benchmarks overlook critical aspects like evidence-based reasoning and handling distracting information.","To bridge this gap, we introduce a novel benchmark that simulates real-world diagnostic scenarios, integrating noise and difficulty levels aligned with USMLE standards.","Moreover, we explore dialogue-based fine-tuning, which transforms static datasets into conversational formats to better capture iterative reasoning processes.","Experiments show that dialogue-tuned models outperform traditional methods, with improvements of $9.64\\%$ in multi-round reasoning scenarios and $6.18\\%$ in accuracy in a noisy environment.","Our findings highlight dialogue tuning as a promising approach for advancing clinically aligned and robust medical AI systems."],"url":"http://arxiv.org/abs/2501.17860v1"}
{"created":"2025-01-29 18:57:44","title":"rEGGression: an Interactive and Agnostic Tool for the Exploration of Symbolic Regression Models","abstract":"Regression analysis is used for prediction and to understand the effect of independent variables on dependent variables. Symbolic regression (SR) automates the search for non-linear regression models, delivering a set of hypotheses that balances accuracy with the possibility to understand the phenomena. Many SR implementations return a Pareto front allowing the choice of the best trade-off. However, this hides alternatives that are close to non-domination, limiting these choices. Equality graphs (e-graphs) allow to represent large sets of expressions compactly by efficiently handling duplicated parts occurring in multiple expressions. E-graphs allow to store and query all SR solution candidates visited in one or multiple GP runs efficiently and open the possibility to analyse much larger sets of SR solution candidates. We introduce rEGGression, a tool using e-graphs to enable the exploration of a large set of symbolic expressions which provides querying, filtering, and pattern matching features creating an interactive experience to gain insights about SR models. The main highlight is its focus in the exploration of the building blocks found during the search that can help the experts to find insights about the studied phenomena.This is possible by exploiting the pattern matching capability of the e-graph data structure.","sentences":["Regression analysis is used for prediction and to understand the effect of independent variables on dependent variables.","Symbolic regression (SR) automates the search for non-linear regression models, delivering a set of hypotheses that balances accuracy with the possibility to understand the phenomena.","Many SR implementations return a Pareto front allowing the choice of the best trade-off.","However, this hides alternatives that are close to non-domination, limiting these choices.","Equality graphs (e-graphs) allow to represent large sets of expressions compactly by efficiently handling duplicated parts occurring in multiple expressions.","E-graphs allow to store and query all SR solution candidates visited in one or multiple GP runs efficiently and open the possibility to analyse much larger sets of SR solution candidates.","We introduce rEGGression, a tool using e-graphs to enable the exploration of a large set of symbolic expressions which provides querying, filtering, and pattern matching features creating an interactive experience to gain insights about SR models.","The main highlight is its focus in the exploration of the building blocks found during the search that can help the experts to find insights about the studied phenomena.","This is possible by exploiting the pattern matching capability of the e-graph data structure."],"url":"http://arxiv.org/abs/2501.17859v1"}
{"created":"2025-01-29 18:57:29","title":"Improving Your Model Ranking on Chatbot Arena by Vote Rigging","abstract":"Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles, where users vote for their preferred response from two randomly sampled anonymous models. While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be rigged to improve (or decrease) the ranking of a target model $m_{t}$. We first introduce a straightforward target-only rigging strategy that focuses on new battles involving $m_{t}$, identifying it via watermarking or a binary classifier, and exclusively voting for $m_{t}$ wins. However, this strategy is practically inefficient because there are over $190$ models on Chatbot Arena and on average only about $1\\%$ of new battles will involve $m_{t}$. To overcome this, we propose omnipresent rigging strategies, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model $m_{t}$, even if $m_{t}$ is not directly involved in the battle. We conduct experiments on around $1.7$ million historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategies can improve model rankings by rigging only hundreds of new votes. While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging. Our code is available at https://github.com/sail-sg/Rigging-ChatbotArena.","sentences":["Chatbot Arena is a popular platform for evaluating LLMs by pairwise battles, where users vote for their preferred response from two randomly sampled anonymous models.","While Chatbot Arena is widely regarded as a reliable LLM ranking leaderboard, we show that crowdsourced voting can be rigged to improve (or decrease) the ranking of a target model $m_{t}$. We first introduce a straightforward target-only rigging strategy that focuses on new battles involving $m_{t}$, identifying it via watermarking or a binary classifier, and exclusively voting for $m_{t}$ wins.","However, this strategy is practically inefficient because there are over $190$ models on Chatbot Arena and on average only about $1\\%$ of new battles will involve $m_{t}$. To overcome this, we propose omnipresent rigging strategies, exploiting the Elo rating mechanism of Chatbot Arena that any new vote on a battle can influence the ranking of the target model $m_{t}$, even if $m_{t}$ is not directly involved in the battle.","We conduct experiments on around $1.7$ million historical votes from the Chatbot Arena Notebook, showing that omnipresent rigging strategies can improve model rankings by rigging only hundreds of new votes.","While we have evaluated several defense mechanisms, our findings highlight the importance of continued efforts to prevent vote rigging.","Our code is available at https://github.com/sail-sg/Rigging-ChatbotArena."],"url":"http://arxiv.org/abs/2501.17858v1"}
{"created":"2025-01-29 18:55:07","title":"GRACE: Generalizing Robot-Assisted Caregiving with User Functionality Embeddings","abstract":"Robot caregiving should be personalized to meet the diverse needs of care recipients -- assisting with tasks as needed, while taking user agency in action into account. In physical tasks such as handover, bathing, dressing, and rehabilitation, a key aspect of this diversity is the functional range of motion (fROM), which can vary significantly between individuals. In this work, we learn to predict personalized fROM as a way to generalize robot decision-making in a wide range of caregiving tasks. We propose a novel data-driven method for predicting personalized fROM using functional assessment scores from occupational therapy. We develop a neural model that learns to embed functional assessment scores into a latent representation of the user's physical function. The model is trained using motion capture data collected from users with emulated mobility limitations. After training, the model predicts personalized fROM for new users without motion capture. Through simulated experiments and a real-robot user study, we show that the personalized fROM predictions from our model enable the robot to provide personalized and effective assistance while improving the user's agency in action. See our website for more visualizations: https://emprise.cs.cornell.edu/grace/.","sentences":["Robot caregiving should be personalized to meet the diverse needs of care recipients -- assisting with tasks as needed, while taking user agency in action into account.","In physical tasks such as handover, bathing, dressing, and rehabilitation, a key aspect of this diversity is the functional range of motion (fROM), which can vary significantly between individuals.","In this work, we learn to predict personalized fROM as a way to generalize robot decision-making in a wide range of caregiving tasks.","We propose a novel data-driven method for predicting personalized fROM using functional assessment scores from occupational therapy.","We develop a neural model that learns to embed functional assessment scores into a latent representation of the user's physical function.","The model is trained using motion capture data collected from users with emulated mobility limitations.","After training, the model predicts personalized fROM for new users without motion capture.","Through simulated experiments and a real-robot user study, we show that the personalized fROM predictions from our model enable the robot to provide personalized and effective assistance while improving the user's agency in action.","See our website for more visualizations: https://emprise.cs.cornell.edu/grace/."],"url":"http://arxiv.org/abs/2501.17855v1"}
{"created":"2025-01-29 18:50:41","title":"UGSim: Autonomous Buoyancy-Driven Underwater Glider Simulator with LQR Control Strategy and Recursive Guidance System","abstract":"This paper presents the UGSim, a simulator for buoyancy-driven gliders, with a LQR control strategy, and a recursive guidance system. Building on the top of the DAVE and the UUVsim, it is designed to address unique challenges that come from the complex hydrodynamic and hydrostatic impacts on buoyancy-driven gliders, which conventional robotics simulators can't deal with. Since distinguishing features of the class of vehicles, general controllers and guidance systems developed for underwater robotics are infeasible. The simulator is provided to accelerate the development and the evaluation of algorithms that would otherwise require expensive and time-consuming operations at sea. It consists of a basic kinetic module, a LQR control module and a recursive guidance module, which allows the user to concentrate on the single problem rather than the whole robotics system and the software infrastructure. We demonstrate the usage of the simulator through an example, loading the configuration of the buoyancy-driven glider named Petrel-II, presenting its dynamics simulation, performances of the control strategy and the guidance system.","sentences":["This paper presents the UGSim, a simulator for buoyancy-driven gliders, with a LQR control strategy, and a recursive guidance system.","Building on the top of the DAVE and the UUVsim, it is designed to address unique challenges that come from the complex hydrodynamic and hydrostatic impacts on buoyancy-driven gliders, which conventional robotics simulators can't deal with.","Since distinguishing features of the class of vehicles, general controllers and guidance systems developed for underwater robotics are infeasible.","The simulator is provided to accelerate the development and the evaluation of algorithms that would otherwise require expensive and time-consuming operations at sea.","It consists of a basic kinetic module, a LQR control module and a recursive guidance module, which allows the user to concentrate on the single problem rather than the whole robotics system and the software infrastructure.","We demonstrate the usage of the simulator through an example, loading the configuration of the buoyancy-driven glider named Petrel-II, presenting its dynamics simulation, performances of the control strategy and the guidance system."],"url":"http://arxiv.org/abs/2501.17851v1"}
{"created":"2025-01-29 18:49:34","title":"Improving Genetic Programming for Symbolic Regression with Equality Graphs","abstract":"The search for symbolic regression models with genetic programming (GP) has a tendency of revisiting expressions in their original or equivalent forms. Repeatedly evaluating equivalent expressions is inefficient, as it does not immediately lead to better solutions. However, evolutionary algorithms require diversity and should allow the accumulation of inactive building blocks that can play an important role at a later point. The equality graph is a data structure capable of compactly storing expressions and their equivalent forms allowing an efficient verification of whether an expression has been visited in any of their stored equivalent forms. We exploit the e-graph to adapt the subtree operators to reduce the chances of revisiting expressions. Our adaptation, called eggp, stores every visited expression in the e-graph, allowing us to filter out from the available selection of subtrees all the combinations that would create already visited expressions. Results show that, for small expressions, this approach improves the performance of a simple GP algorithm to compete with PySR and Operon without increasing computational cost. As a highlight, eggp was capable of reliably delivering short and at the same time accurate models for a selected set of benchmarks from SRBench and a set of real-world datasets.","sentences":["The search for symbolic regression models with genetic programming (GP) has a tendency of revisiting expressions in their original or equivalent forms.","Repeatedly evaluating equivalent expressions is inefficient, as it does not immediately lead to better solutions.","However, evolutionary algorithms require diversity and should allow the accumulation of inactive building blocks that can play an important role at a later point.","The equality graph is a data structure capable of compactly storing expressions and their equivalent forms allowing an efficient verification of whether an expression has been visited in any of their stored equivalent forms.","We exploit the e-graph to adapt the subtree operators to reduce the chances of revisiting expressions.","Our adaptation, called eggp, stores every visited expression in the e-graph, allowing us to filter out from the available selection of subtrees all the combinations that would create already visited expressions.","Results show that, for small expressions, this approach improves the performance of a simple GP algorithm to compete with PySR and Operon without increasing computational cost.","As a highlight, eggp was capable of reliably delivering short and at the same time accurate models for a selected set of benchmarks from SRBench and a set of real-world datasets."],"url":"http://arxiv.org/abs/2501.17848v1"}
{"created":"2025-01-29 18:48:22","title":"Private Information Retrieval on Multigraph-Based Replicated Storage","abstract":"We consider the private information retrieval (PIR) problem for a multigraph-based replication system, where each set of $r$ files is stored on two of the servers according to an underlying $r$-multigraph. Our goal is to establish upper and lower bounds on the PIR capacity of the $r$-multigraph. Specifically, we first propose a construction for multigraph-based PIR systems that leverages the symmetry of the underlying graph-based PIR scheme, deriving a capacity lower bound for such multigraphs. Then, we establish a general upper bound using linear programming, expressed as a function of the underlying graph parameters. Our bounds are demonstrated to be tight for PIR systems on multipaths for even number of vertices.","sentences":["We consider the private information retrieval (PIR) problem for a multigraph-based replication system, where each set of $r$ files is stored on two of the servers according to an underlying $r$-multigraph.","Our goal is to establish upper and lower bounds on the PIR capacity of the $r$-multigraph.","Specifically, we first propose a construction for multigraph-based PIR systems that leverages the symmetry of the underlying graph-based PIR scheme, deriving a capacity lower bound for such multigraphs.","Then, we establish a general upper bound using linear programming, expressed as a function of the underlying graph parameters.","Our bounds are demonstrated to be tight for PIR systems on multipaths for even number of vertices."],"url":"http://arxiv.org/abs/2501.17845v1"}
{"created":"2025-01-29 18:46:35","title":"From Sparse to Dense: Toddler-inspired Reward Transition in Goal-Oriented Reinforcement Learning","abstract":"Reinforcement learning (RL) agents often face challenges in balancing exploration and exploitation, particularly in environments where sparse or dense rewards bias learning. Biological systems, such as human toddlers, naturally navigate this balance by transitioning from free exploration with sparse rewards to goal-directed behavior guided by increasingly dense rewards. Inspired by this natural progression, we investigate the Toddler-Inspired Reward Transition in goal-oriented RL tasks. Our study focuses on transitioning from sparse to potential-based dense (S2D) rewards while preserving optimal strategies. Through experiments on dynamic robotic arm manipulation and egocentric 3D navigation tasks, we demonstrate that effective S2D reward transitions significantly enhance learning performance and sample efficiency. Additionally, using a Cross-Density Visualizer, we show that S2D transitions smooth the policy loss landscape, resulting in wider minima that improve generalization in RL models. In addition, we reinterpret Tolman's maze experiments, underscoring the critical role of early free exploratory learning in the context of S2D rewards.","sentences":["Reinforcement learning (RL) agents often face challenges in balancing exploration and exploitation, particularly in environments where sparse or dense rewards bias learning.","Biological systems, such as human toddlers, naturally navigate this balance by transitioning from free exploration with sparse rewards to goal-directed behavior guided by increasingly dense rewards.","Inspired by this natural progression, we investigate the Toddler-Inspired Reward Transition in goal-oriented RL tasks.","Our study focuses on transitioning from sparse to potential-based dense (S2D) rewards while preserving optimal strategies.","Through experiments on dynamic robotic arm manipulation and egocentric 3D navigation tasks, we demonstrate that effective S2D reward transitions significantly enhance learning performance and sample efficiency.","Additionally, using a Cross-Density Visualizer, we show that S2D transitions smooth the policy loss landscape, resulting in wider minima that improve generalization in RL models.","In addition, we reinterpret Tolman's maze experiments, underscoring the critical role of early free exploratory learning in the context of S2D rewards."],"url":"http://arxiv.org/abs/2501.17842v1"}
{"created":"2025-01-29 18:44:48","title":"acoupi: An Open-Source Python Framework for Deploying Bioacoustic AI Models on Edge Devices","abstract":"1. Passive acoustic monitoring (PAM) coupled with artificial intelligence (AI) is becoming an essential tool for biodiversity monitoring. Traditional PAM systems require manual data offloading and impose substantial demands on storage and computing infrastructure. The combination of on-device AI-based processing and network connectivity enables local data analysis and transmission of only relevant information, greatly reducing storage needs. However, programming these devices for robust operation is challenging, requiring expertise in embedded systems and software engineering. Despite the increase in AI-based models for bioacoustics, their full potential remains unrealized without accessible tools to deploy them on custom hardware and tailor device behaviour to specific monitoring goals. 2. To address this challenge, we develop acoupi, an open-source Python framework that simplifies the creation and deployment of smart bioacoustic devices. acoupi integrates audio recording, AI-based data processing, data management, and real-time wireless messaging into a unified and configurable framework. By modularising key elements of the bioacoustic monitoring workflow, acoupi allows users to easily customise, extend, or select specific components to fit their unique monitoring needs. 3. We demonstrate the flexibility of acoupi by integrating two bioacoustic classifiers: BirdNET, for the classification of bird species, and BatDetect2, for the classification of UK bat species. We test the reliability of acoupi over a month-long deployment of two acoupi-powered devices in a UK urban park. 4. acoupi can be deployed on low-cost hardware such as the Raspberry Pi and can be customised for various applications. acoupi standardised framework and simplified tools facilitate the adoption of AI-powered PAM systems for researchers and conservationists. acoupi is on GitHub at https://github.com/acoupi/acoupi.","sentences":["1.","Passive acoustic monitoring (PAM) coupled with artificial intelligence (AI) is becoming an essential tool for biodiversity monitoring.","Traditional PAM systems require manual data offloading and impose substantial demands on storage and computing infrastructure.","The combination of on-device AI-based processing and network connectivity enables local data analysis and transmission of only relevant information, greatly reducing storage needs.","However, programming these devices for robust operation is challenging, requiring expertise in embedded systems and software engineering.","Despite the increase in AI-based models for bioacoustics, their full potential remains unrealized without accessible tools to deploy them on custom hardware and tailor device behaviour to specific monitoring goals.","2.","To address this challenge, we develop acoupi, an open-source Python framework that simplifies the creation and deployment of smart bioacoustic devices.","acoupi integrates audio recording, AI-based data processing, data management, and real-time wireless messaging into a unified and configurable framework.","By modularising key elements of the bioacoustic monitoring workflow, acoupi allows users to easily customise, extend, or select specific components to fit their unique monitoring needs.","3.","We demonstrate the flexibility of acoupi by integrating two bioacoustic classifiers: BirdNET, for the classification of bird species, and BatDetect2, for the classification of UK bat species.","We test the reliability of acoupi over a month-long deployment of two acoupi-powered devices in a UK urban park.","4. acoupi can be deployed on low-cost hardware such as the Raspberry Pi and can be customised for various applications.","acoupi standardised framework and simplified tools facilitate the adoption of AI-powered PAM systems for researchers and conservationists.","acoupi is on GitHub at https://github.com/acoupi/acoupi."],"url":"http://arxiv.org/abs/2501.17841v1"}
{"created":"2025-01-29 18:40:32","title":"Learning Beyond the Surface: How Far Can Continual Pre-Training with LoRA Enhance LLMs' Domain-Specific Insight Learning?","abstract":"Large Language Models (LLMs) have demonstrated remarkable performance on various tasks, yet their ability to extract and internalize deeper insights from domain-specific datasets remains underexplored. In this study, we investigate how continual pre-training can enhance LLMs' capacity for insight learning across three distinct forms: declarative, statistical, and probabilistic insights. Focusing on two critical domains: medicine and finance, we employ LoRA to train LLMs on two existing datasets. To evaluate each insight type, we create benchmarks to measure how well continual pre-training helps models go beyond surface-level knowledge. We also assess the impact of document modification on capturing insights. The results show that, while continual pre-training on original documents has a marginal effect, modifying documents to retain only essential information significantly enhances the insight-learning capabilities of LLMs.","sentences":["Large Language Models (LLMs) have demonstrated remarkable performance on various tasks, yet their ability to extract and internalize deeper insights from domain-specific datasets remains underexplored.","In this study, we investigate how continual pre-training can enhance LLMs' capacity for insight learning across three distinct forms: declarative, statistical, and probabilistic insights.","Focusing on two critical domains: medicine and finance, we employ LoRA to train LLMs on two existing datasets.","To evaluate each insight type, we create benchmarks to measure how well continual pre-training helps models go beyond surface-level knowledge.","We also assess the impact of document modification on capturing insights.","The results show that, while continual pre-training on original documents has a marginal effect, modifying documents to retain only essential information significantly enhances the insight-learning capabilities of LLMs."],"url":"http://arxiv.org/abs/2501.17840v1"}
{"created":"2025-01-29 18:35:38","title":"Matrix Product Sketching via Coordinated Sampling","abstract":"We revisit the well-studied problem of approximating a matrix product, $\\mathbf{A}^T\\mathbf{B}$, based on small space sketches $\\mathcal{S}(\\mathbf{A})$ and $\\mathcal{S}(\\mathbf{B})$ of $\\mathbf{A} \\in \\R^{n \\times d}$ and $\\mathbf{B}\\in \\R^{n \\times m}$. We are interested in the setting where the sketches must be computed independently of each other, except for the use of a shared random seed. We prove that, when $\\mathbf{A}$ and $\\mathbf{B}$ are sparse, methods based on \\emph{coordinated random sampling} can outperform classical linear sketching approaches, like Johnson-Lindenstrauss Projection or CountSketch. For example, to obtain Frobenius norm error $\\epsilon\\|\\mathbf{A}\\|_F\\|\\mathbf{B}\\|_F$, coordinated sampling requires sketches of size $O(s/\\epsilon^2)$ when $\\mathbf{A}$ and $\\mathbf{B}$ have at most $s \\leq d,m$ non-zeros per row. In contrast, linear sketching leads to sketches of size $O(d/\\epsilon^2)$ and $O(m/\\epsilon^2)$ for $\\mathbf{A}$ and $\\mathbf{B}$. We empirically evaluate our approach on two applications: 1) distributed linear regression in databases, a problem motivated by tasks like dataset discovery and augmentation, and 2) approximating attention matrices in transformer-based language models. In both cases, our sampling algorithms yield an order of magnitude improvement over linear sketching.","sentences":["We revisit the well-studied problem of approximating a matrix product, $\\mathbf{A}^T\\mathbf{B}$, based on small space sketches $\\mathcal{S}(\\mathbf{A})$ and $\\mathcal{S}(\\mathbf{B})$ of $\\mathbf{A} \\in \\R^{n \\times d}$ and $\\mathbf{B}\\in \\R^{n \\times m}$. We are interested in the setting where the sketches must be computed independently of each other, except for the use of a shared random seed.","We prove that, when $\\mathbf{A}$ and $\\mathbf{B}$ are sparse, methods based on \\emph{coordinated random sampling} can outperform classical linear sketching approaches, like Johnson-Lindenstrauss Projection or CountSketch.","For example, to obtain Frobenius norm error $\\epsilon\\|\\mathbf{A}\\|_F\\|\\mathbf{B}\\|_F$, coordinated sampling requires sketches of size $O(s/\\epsilon^2)$ when $\\mathbf{A}$ and $\\mathbf{B}$ have at most $s \\leq d,m$ non-zeros per row.","In contrast, linear sketching leads to sketches of size $O(d/\\epsilon^2)$ and $O(m/\\epsilon^2)$ for $\\mathbf{A}$ and $\\mathbf{B}$. We empirically evaluate our approach on two applications: 1) distributed linear regression in databases, a problem motivated by tasks like dataset discovery and augmentation, and 2) approximating attention matrices in transformer-based language models.","In both cases, our sampling algorithms yield an order of magnitude improvement over linear sketching."],"url":"http://arxiv.org/abs/2501.17836v1"}
{"created":"2025-01-29 18:30:18","title":"Hierarchical Fallback Architecture for High Risk Online Machine Learning Inference","abstract":"Open Banking powered machine learning applications require novel robustness approaches to deal with challenging stress and failure scenarios. In this paper we propose an hierarchical fallback architecture for improving robustness in high risk machine learning applications with a focus in the financial domain. We define generic failure scenarios often found in online inference that depend on external data providers and we describe in detail how to apply the hierarchical fallback architecture to address them. Finally, we offer a real world example of its applicability in the industry for near-real time transactional fraud risk evaluation using Open Banking data and under extreme stress scenarios.","sentences":["Open Banking powered machine learning applications require novel robustness approaches to deal with challenging stress and failure scenarios.","In this paper we propose an hierarchical fallback architecture for improving robustness in high risk machine learning applications with a focus in the financial domain.","We define generic failure scenarios often found in online inference that depend on external data providers and we describe in detail how to apply the hierarchical fallback architecture to address them.","Finally, we offer a real world example of its applicability in the industry for near-real time transactional fraud risk evaluation using Open Banking data and under extreme stress scenarios."],"url":"http://arxiv.org/abs/2501.17834v1"}
{"created":"2025-01-29 18:24:20","title":"TikTok's recommendations skewed towards Republican content during the 2024 U.S. presidential race","abstract":"TikTok is a major force among social media platforms with over a billion monthly active users worldwide and 170 million in the United States. The platform's status as a key news source, particularly among younger demographics, raises concerns about its potential influence on politics in the U.S. and globally. Despite these concerns, there is scant research investigating TikTok's recommendation algorithm for political biases. We fill this gap by conducting 323 independent algorithmic audit experiments testing partisan content recommendations in the lead-up to the 2024 U.S. presidential elections. Specifically, we create hundreds of \"sock puppet\" TikTok accounts in Texas, New York, and Georgia, seeding them with varying partisan content and collecting algorithmic content recommendations for each of them. Collectively, these accounts viewed ~394,000 videos from April 30th to November 11th, 2024, which we label for political and partisan content. Our analysis reveals significant asymmetries in content distribution: Republican-seeded accounts received ~11.8% more party-aligned recommendations compared to their Democratic-seeded counterparts, and Democratic-seeded accounts were exposed to ~7.5% more opposite-party recommendations on average. These asymmetries exist across all three states and persist when accounting for video- and channel-level engagement metrics such as likes, views, shares, comments, and followers, and are driven primarily by negative partisanship content. Our findings provide insights into the inner workings of TikTok's recommendation algorithm during a critical election period, raising fundamental questions about platform neutrality.","sentences":["TikTok is a major force among social media platforms with over a billion monthly active users worldwide and 170 million in the United States.","The platform's status as a key news source, particularly among younger demographics, raises concerns about its potential influence on politics in the U.S. and globally.","Despite these concerns, there is scant research investigating TikTok's recommendation algorithm for political biases.","We fill this gap by conducting 323 independent algorithmic audit experiments testing partisan content recommendations in the lead-up to the 2024 U.S. presidential elections.","Specifically, we create hundreds of \"sock puppet\" TikTok accounts in Texas, New York, and Georgia, seeding them with varying partisan content and collecting algorithmic content recommendations for each of them.","Collectively, these accounts viewed ~394,000 videos from April 30th to November 11th, 2024, which we label for political and partisan content.","Our analysis reveals significant asymmetries in content distribution: Republican-seeded accounts received ~11.8% more party-aligned recommendations compared to their Democratic-seeded counterparts, and Democratic-seeded accounts were exposed to ~7.5% more opposite-party recommendations on average.","These asymmetries exist across all three states and persist when accounting for video- and channel-level engagement metrics such as likes, views, shares, comments, and followers, and are driven primarily by negative partisanship content.","Our findings provide insights into the inner workings of TikTok's recommendation algorithm during a critical election period, raising fundamental questions about platform neutrality."],"url":"http://arxiv.org/abs/2501.17831v1"}
{"created":"2025-01-29 18:22:14","title":"A Comprehensive Survey on Legal Summarization: Challenges and Future Directions","abstract":"This article provides a systematic up-to-date survey of automatic summarization techniques, datasets, models, and evaluation methods in the legal domain. Through specific source selection criteria, we thoroughly review over 120 papers spanning the modern `transformer' era of natural language processing (NLP), thus filling a gap in existing systematic surveys on the matter. We present existing research along several axes and discuss trends, challenges, and opportunities for future research.","sentences":["This article provides a systematic up-to-date survey of automatic summarization techniques, datasets, models, and evaluation methods in the legal domain.","Through specific source selection criteria, we thoroughly review over 120 papers spanning the modern `transformer' era of natural language processing (NLP), thus filling a gap in existing systematic surveys on the matter.","We present existing research along several axes and discuss trends, challenges, and opportunities for future research."],"url":"http://arxiv.org/abs/2501.17830v1"}
{"created":"2025-01-29 18:18:00","title":"Langevin Soft Actor-Critic: Efficient Exploration through Uncertainty-Driven Critic Learning","abstract":"Existing actor-critic algorithms, which are popular for continuous control reinforcement learning (RL) tasks, suffer from poor sample efficiency due to lack of principled exploration mechanism within them. Motivated by the success of Thompson sampling for efficient exploration in RL, we propose a novel model-free RL algorithm, Langevin Soft Actor Critic (LSAC), which prioritizes enhancing critic learning through uncertainty estimation over policy optimization. LSAC employs three key innovations: approximate Thompson sampling through distributional Langevin Monte Carlo (LMC) based $Q$ updates, parallel tempering for exploring multiple modes of the posterior of the $Q$ function, and diffusion synthesized state-action samples regularized with $Q$ action gradients. Our extensive experiments demonstrate that LSAC outperforms or matches the performance of mainstream model-free RL algorithms for continuous control tasks. Notably, LSAC marks the first successful application of an LMC based Thompson sampling in continuous control tasks with continuous action spaces.","sentences":["Existing actor-critic algorithms, which are popular for continuous control reinforcement learning (RL) tasks, suffer from poor sample efficiency due to lack of principled exploration mechanism within them.","Motivated by the success of Thompson sampling for efficient exploration in RL, we propose a novel model-free RL algorithm, Langevin Soft Actor Critic (LSAC), which prioritizes enhancing critic learning through uncertainty estimation over policy optimization.","LSAC employs three key innovations: approximate Thompson sampling through distributional Langevin Monte Carlo (LMC) based $Q$ updates, parallel tempering for exploring multiple modes of the posterior of the $Q$ function, and diffusion synthesized state-action samples regularized with $Q$ action gradients.","Our extensive experiments demonstrate that LSAC outperforms or matches the performance of mainstream model-free RL algorithms for continuous control tasks.","Notably, LSAC marks the first successful application of an LMC based Thompson sampling in continuous control tasks with continuous action spaces."],"url":"http://arxiv.org/abs/2501.17827v1"}
{"created":"2025-01-29 18:16:20","title":"SMT-Boosted Security Types for Low-Level MPC","abstract":"Secure Multi-Party Computation (MPC) is an important enabling technology for data privacy in modern distributed applications. We develop a new type theory to automatically enforce correctness,confidentiality, and integrity properties of protocols written in the \\emph{Prelude/Overture} language framework. Judgements in the type theory are predicated on SMT verifications in a theory of finite fields, which supports precise and efficient analysis. Our approach is automated, compositional, scalable, and generalizes to arbitrary prime fields for data and key sizes.","sentences":["Secure Multi-Party Computation (MPC) is an important enabling technology for data privacy in modern distributed applications.","We develop a new type theory to automatically enforce correctness,confidentiality, and integrity properties of protocols written in the \\emph{Prelude/Overture} language framework.","Judgements in the type theory are predicated on SMT verifications in a theory of finite fields, which supports precise and efficient analysis.","Our approach is automated, compositional, scalable, and generalizes to arbitrary prime fields for data and key sizes."],"url":"http://arxiv.org/abs/2501.17824v1"}
{"created":"2025-01-29 18:15:49","title":"U2A: Unified Unimodal Adaptation for Robust and Efficient Multimodal Learning","abstract":"Multimodal learning often relies on designing new models and complex training strategies to achieve optimal performance. We present Unified Unimodal Adaptation (U2A), which jointly fine-tunes pretrained unimodal encoders using low-rank adaptation (LoRA) for various multimodal tasks. Our method significantly reduces the number of learnable parameters and eliminates the need for complex training strategies, such as alternating training, gradient modifications, or unimodal fine-tuning. To address missing modalities during both training and testing, we introduce Mask Tokens (MT), which generate missing modality features from available modalities using a single token per modality. This simplifies the process, removing the need for specialized feature estimation or prompt-tuning methods. Our evaluation demonstrates that U2A matches or outperforms state-of-the-art methods in both complete and missing modality settings, showcasing strong performance and robustness across various modalities, tasks, and datasets. We also analyze and report the effectiveness of Mask Tokens in different missing modality scenarios. Overall, our method provides a robust, flexible, and efficient solution for multimodal learning, with minimal computational overhead.","sentences":["Multimodal learning often relies on designing new models and complex training strategies to achieve optimal performance.","We present Unified Unimodal Adaptation (U2A), which jointly fine-tunes pretrained unimodal encoders using low-rank adaptation (LoRA) for various multimodal tasks.","Our method significantly reduces the number of learnable parameters and eliminates the need for complex training strategies, such as alternating training, gradient modifications, or unimodal fine-tuning.","To address missing modalities during both training and testing, we introduce Mask Tokens (MT), which generate missing modality features from available modalities using a single token per modality.","This simplifies the process, removing the need for specialized feature estimation or prompt-tuning methods.","Our evaluation demonstrates that U2A matches or outperforms state-of-the-art methods in both complete and missing modality settings, showcasing strong performance and robustness across various modalities, tasks, and datasets.","We also analyze and report the effectiveness of Mask Tokens in different missing modality scenarios.","Overall, our method provides a robust, flexible, and efficient solution for multimodal learning, with minimal computational overhead."],"url":"http://arxiv.org/abs/2501.17823v1"}
{"created":"2025-01-29 18:14:16","title":"SSF: Sparse Long-Range Scene Flow for Autonomous Driving","abstract":"Scene flow enables an understanding of the motion characteristics of the environment in the 3D world. It gains particular significance in the long-range, where object-based perception methods might fail due to sparse observations far away. Although significant advancements have been made in scene flow pipelines to handle large-scale point clouds, a gap remains in scalability with respect to long-range. We attribute this limitation to the common design choice of using dense feature grids, which scale quadratically with range. In this paper, we propose Sparse Scene Flow (SSF), a general pipeline for long-range scene flow, adopting a sparse convolution based backbone for feature extraction. This approach introduces a new challenge: a mismatch in size and ordering of sparse feature maps between time-sequential point scans. To address this, we propose a sparse feature fusion scheme, that augments the feature maps with virtual voxels at missing locations. Additionally, we propose a range-wise metric that implicitly gives greater importance to faraway points. Our method, SSF, achieves state-of-the-art results on the Argoverse2 dataset, demonstrating strong performance in long-range scene flow estimation. Our code will be released at https://github.com/KTH-RPL/SSF.git.","sentences":["Scene flow enables an understanding of the motion characteristics of the environment in the 3D world.","It gains particular significance in the long-range, where object-based perception methods might fail due to sparse observations far away.","Although significant advancements have been made in scene flow pipelines to handle large-scale point clouds, a gap remains in scalability with respect to long-range.","We attribute this limitation to the common design choice of using dense feature grids, which scale quadratically with range.","In this paper, we propose Sparse Scene Flow (SSF), a general pipeline for long-range scene flow, adopting a sparse convolution based backbone for feature extraction.","This approach introduces a new challenge: a mismatch in size and ordering of sparse feature maps between time-sequential point scans.","To address this, we propose a sparse feature fusion scheme, that augments the feature maps with virtual voxels at missing locations.","Additionally, we propose a range-wise metric that implicitly gives greater importance to faraway points.","Our method, SSF, achieves state-of-the-art results on the Argoverse2 dataset, demonstrating strong performance in long-range scene flow estimation.","Our code will be released at https://github.com/KTH-RPL/SSF.git."],"url":"http://arxiv.org/abs/2501.17821v1"}
{"created":"2025-01-29 18:13:34","title":"eaSEL: Promoting Social-Emotional Learning and Parent-Child Interaction through AI-Mediated Content Consumption","abstract":"As children increasingly consume media on devices, parents look for ways this usage can support learning and growth, especially in domains like social-emotional learning. We introduce eaSEL, a system that (a) integrates social-emotional learning (SEL) curricula into children's video consumption by generating reflection activities and (b) facilitates parent-child discussions around digital media without requiring co-consumption of videos. We present a technical evaluation of our system's ability to detect social-emotional moments within a transcript and to generate high-quality SEL-based activities for both children and parents. Through a user study with N=20 parent-child dyads, we find that after completing an eaSEL activity, children reflect more on the emotional content of videos. Furthermore, parents find that the tool promotes meaningful active engagement and could scaffold deeper conversations around content. Our work paves directions in how AI can support children's social-emotional reflection of media and family connections in the digital age.","sentences":["As children increasingly consume media on devices, parents look for ways this usage can support learning and growth, especially in domains like social-emotional learning.","We introduce eaSEL, a system that (a) integrates social-emotional learning (SEL) curricula into children's video consumption by generating reflection activities and (b) facilitates parent-child discussions around digital media without requiring co-consumption of videos.","We present a technical evaluation of our system's ability to detect social-emotional moments within a transcript and to generate high-quality SEL-based activities for both children and parents.","Through a user study with N=20 parent-child dyads, we find that after completing an eaSEL activity, children reflect more on the emotional content of videos.","Furthermore, parents find that the tool promotes meaningful active engagement and could scaffold deeper conversations around content.","Our work paves directions in how AI can support children's social-emotional reflection of media and family connections in the digital age."],"url":"http://arxiv.org/abs/2501.17819v1"}
{"created":"2025-01-29 18:09:20","title":"Improving community detection via community association strength scores","abstract":"Community detection methods play a central role in understanding complex networks by revealing highly connected subsets of entities. However, most community detection algorithms generate partitions of the nodes, thus (i) forcing every node to be part of a community and (ii) ignoring the possibility that some nodes may be part of multiple communities. In our work, we investigate three simple community association strength (CAS) scores and their usefulness as post-processing tools given some partition of the nodes. We show that these measures can be used to improve node partitions, detect outlier nodes (not part of any community), and help find nodes with multiple community memberships.","sentences":["Community detection methods play a central role in understanding complex networks by revealing highly connected subsets of entities.","However, most community detection algorithms generate partitions of the nodes, thus (i) forcing every node to be part of a community and (ii) ignoring the possibility that some nodes may be part of multiple communities.","In our work, we investigate three simple community association strength (CAS) scores and their usefulness as post-processing tools given some partition of the nodes.","We show that these measures can be used to improve node partitions, detect outlier nodes (not part of any community), and help find nodes with multiple community memberships."],"url":"http://arxiv.org/abs/2501.17817v1"}
{"created":"2025-01-29 18:06:08","title":"P-TAME: Explain Any Image Classifier with Trained Perturbations","abstract":"The adoption of Deep Neural Networks (DNNs) in critical fields where predictions need to be accompanied by justifications is hindered by their inherent black-box nature. In this paper, we introduce P-TAME (Perturbation-based Trainable Attention Mechanism for Explanations), a model-agnostic method for explaining DNN-based image classifiers. P-TAME employs an auxiliary image classifier to extract features from the input image, bypassing the need to tailor the explanation method to the internal architecture of the backbone classifier being explained. Unlike traditional perturbation-based methods, which have high computational requirements, P-TAME offers an efficient alternative by generating high-resolution explanations in a single forward pass during inference. We apply P-TAME to explain the decisions of VGG-16, ResNet-50, and ViT-B-16, three distinct and widely used image classifiers. Quantitative and qualitative results show that our method matches or outperforms previous explainability methods, including model-specific approaches. Code and trained models will be released upon acceptance.","sentences":["The adoption of Deep Neural Networks (DNNs) in critical fields where predictions need to be accompanied by justifications is hindered by their inherent black-box nature.","In this paper, we introduce P-TAME (Perturbation-based Trainable Attention Mechanism for Explanations), a model-agnostic method for explaining DNN-based image classifiers.","P-TAME employs an auxiliary image classifier to extract features from the input image, bypassing the need to tailor the explanation method to the internal architecture of the backbone classifier being explained.","Unlike traditional perturbation-based methods, which have high computational requirements, P-TAME offers an efficient alternative by generating high-resolution explanations in a single forward pass during inference.","We apply P-TAME to explain the decisions of VGG-16, ResNet-50, and ViT-B-16, three distinct and widely used image classifiers.","Quantitative and qualitative results show that our method matches or outperforms previous explainability methods, including model-specific approaches.","Code and trained models will be released upon acceptance."],"url":"http://arxiv.org/abs/2501.17813v1"}
{"created":"2025-01-29 18:00:19","title":"Janus-Pro: Unified Multimodal Understanding and Generation with Data and Model Scaling","abstract":"In this work, we introduce Janus-Pro, an advanced version of the previous work Janus. Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size. With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also enhancing the stability of text-to-image generation. We hope this work will inspire further exploration in the field. Code and models are publicly available.","sentences":["In this work, we introduce Janus-Pro, an advanced version of the previous work Janus.","Specifically, Janus-Pro incorporates (1) an optimized training strategy, (2) expanded training data, and (3) scaling to larger model size.","With these improvements, Janus-Pro achieves significant advancements in both multimodal understanding and text-to-image instruction-following capabilities, while also enhancing the stability of text-to-image generation.","We hope this work will inspire further exploration in the field.","Code and models are publicly available."],"url":"http://arxiv.org/abs/2501.17811v1"}
{"created":"2025-01-29 17:47:36","title":"International AI Safety Report","abstract":"The first International AI Safety Report comprehensively synthesizes the current evidence on the capabilities, risks, and safety of advanced AI systems. The report was mandated by the nations attending the AI Safety Summit in Bletchley, UK. Thirty nations, the UN, the OECD, and the EU each nominated a representative to the report's Expert Advisory Panel. A total of 100 AI experts contributed, representing diverse perspectives and disciplines. Led by the report's Chair, these independent experts collectively had full discretion over the report's content.","sentences":["The first International AI Safety Report comprehensively synthesizes the current evidence on the capabilities, risks, and safety of advanced AI systems.","The report was mandated by the nations attending the AI Safety Summit in Bletchley, UK.","Thirty nations, the UN, the OECD, and the EU each nominated a representative to the report's Expert Advisory Panel.","A total of 100 AI experts contributed, representing diverse perspectives and disciplines.","Led by the report's Chair, these independent experts collectively had full discretion over the report's content."],"url":"http://arxiv.org/abs/2501.17805v1"}
{"created":"2025-01-29 17:44:57","title":"LEKA:LLM-Enhanced Knowledge Augmentation","abstract":"Humans excel in analogical learning and knowledge transfer and, more importantly, possess a unique understanding of identifying appropriate sources of knowledge. From a model's perspective, this presents an interesting challenge. If models could autonomously retrieve knowledge useful for transfer or decision-making to solve problems, they would transition from passively acquiring to actively accessing and learning from knowledge. However, filling models with knowledge is relatively straightforward -- it simply requires more training and accessible knowledge bases. The more complex task is teaching models about which knowledge can be analogized and transferred. Therefore, we design a knowledge augmentation method LEKA for knowledge transfer that actively searches for suitable knowledge sources that can enrich the target domain's knowledge. This LEKA method extracts key information from textual information from the target domain, retrieves pertinent data from external data libraries, and harmonizes retrieved data with the target domain data in feature space and marginal probability measures. We validate the effectiveness of our approach through extensive experiments across various domains and demonstrate significant improvements over traditional methods in reducing computational costs, automating data alignment, and optimizing transfer learning outcomes.","sentences":["Humans excel in analogical learning and knowledge transfer and, more importantly, possess a unique understanding of identifying appropriate sources of knowledge.","From a model's perspective, this presents an interesting challenge.","If models could autonomously retrieve knowledge useful for transfer or decision-making to solve problems, they would transition from passively acquiring to actively accessing and learning from knowledge.","However, filling models with knowledge is relatively straightforward -- it simply requires more training and accessible knowledge bases.","The more complex task is teaching models about which knowledge can be analogized and transferred.","Therefore, we design a knowledge augmentation method LEKA for knowledge transfer that actively searches for suitable knowledge sources that can enrich the target domain's knowledge.","This LEKA method extracts key information from textual information from the target domain, retrieves pertinent data from external data libraries, and harmonizes retrieved data with the target domain data in feature space and marginal probability measures.","We validate the effectiveness of our approach through extensive experiments across various domains and demonstrate significant improvements over traditional methods in reducing computational costs, automating data alignment, and optimizing transfer learning outcomes."],"url":"http://arxiv.org/abs/2501.17802v1"}
{"created":"2025-01-29 17:38:39","title":"Leveraging Multimodal LLM for Inspirational User Interface Search","abstract":"Inspirational search, the process of exploring designs to inform and inspire new creative work, is pivotal in mobile user interface (UI) design. However, exploring the vast space of UI references remains a challenge. Existing AI-based UI search methods often miss crucial semantics like target users or the mood of apps. Additionally, these models typically require metadata like view hierarchies, limiting their practical use. We used a multimodal large language model (MLLM) to extract and interpret semantics from mobile UI images. We identified key UI semantics through a formative study and developed a semantic-based UI search system. Through computational and human evaluations, we demonstrate that our approach significantly outperforms existing UI retrieval methods, offering UI designers a more enriched and contextually relevant search experience. We enhance the understanding of mobile UI design semantics and highlight MLLMs' potential in inspirational search, providing a rich dataset of UI semantics for future studies.","sentences":["Inspirational search, the process of exploring designs to inform and inspire new creative work, is pivotal in mobile user interface (UI) design.","However, exploring the vast space of UI references remains a challenge.","Existing AI-based UI search methods often miss crucial semantics like target users or the mood of apps.","Additionally, these models typically require metadata like view hierarchies, limiting their practical use.","We used a multimodal large language model (MLLM) to extract and interpret semantics from mobile UI images.","We identified key UI semantics through a formative study and developed a semantic-based UI search system.","Through computational and human evaluations, we demonstrate that our approach significantly outperforms existing UI retrieval methods, offering UI designers a more enriched and contextually relevant search experience.","We enhance the understanding of mobile UI design semantics and highlight MLLMs' potential in inspirational search, providing a rich dataset of UI semantics for future studies."],"url":"http://arxiv.org/abs/2501.17799v1"}
{"created":"2025-01-29 17:35:26","title":"An Incremental Multi-Level, Multi-Scale Approach to Assessment of Multifidelity HPC Systems","abstract":"With the growing complexity in architecture and the size of large-scale computing systems, monitoring and analyzing system behavior and events has become daunting. Monitoring data amounting to terabytes per day are collected by sensors housed in these massive systems at multiple fidelity levels and varying temporal resolutions. In this work, we develop an incremental version of multiresolution dynamic mode decomposition (mrDMD), which converts high-dimensional data to spatial-temporal patterns at varied frequency ranges. Our incremental implementation of the mrDMD algorithm (I-mrDMD) promptly reveals valuable information in the massive environment log dataset, which is then visually aligned with the processed hardware and job log datasets through our generalizable rack visualization using D3 visualization integrated into the Jupyter Notebook interface. We demonstrate the efficacy of our approach with two use scenarios on a real-world dataset from a Cray XC40 supercomputer, Theta.","sentences":["With the growing complexity in architecture and the size of large-scale computing systems, monitoring and analyzing system behavior and events has become daunting.","Monitoring data amounting to terabytes per day are collected by sensors housed in these massive systems at multiple fidelity levels and varying temporal resolutions.","In this work, we develop an incremental version of multiresolution dynamic mode decomposition (mrDMD), which converts high-dimensional data to spatial-temporal patterns at varied frequency ranges.","Our incremental implementation of the mrDMD algorithm (I-mrDMD) promptly reveals valuable information in the massive environment log dataset, which is then visually aligned with the processed hardware and job log datasets through our generalizable rack visualization using D3 visualization integrated into the Jupyter Notebook interface.","We demonstrate the efficacy of our approach with two use scenarios on a real-world dataset from a Cray XC40 supercomputer, Theta."],"url":"http://arxiv.org/abs/2501.17796v1"}
{"created":"2025-01-29 17:31:46","title":"CrowdSplat: Exploring Gaussian Splatting For Crowd Rendering","abstract":"We present CrowdSplat, a novel approach that leverages 3D Gaussian Splatting for real-time, high-quality crowd rendering. Our method utilizes 3D Gaussian functions to represent animated human characters in diverse poses and outfits, which are extracted from monocular videos. We integrate Level of Detail (LoD) rendering to optimize computational efficiency and quality. The CrowdSplat framework consists of two stages: (1) avatar reconstruction and (2) crowd synthesis. The framework is also optimized for GPU memory usage to enhance scalability. Quantitative and qualitative evaluations show that CrowdSplat achieves good levels of rendering quality, memory efficiency, and computational performance. Through these experiments, we demonstrate that CrowdSplat is a viable solution for dynamic, realistic crowd simulation in real-time applications.","sentences":["We present CrowdSplat, a novel approach that leverages 3D Gaussian Splatting for real-time, high-quality crowd rendering.","Our method utilizes 3D Gaussian functions to represent animated human characters in diverse poses and outfits, which are extracted from monocular videos.","We integrate Level of Detail (LoD) rendering to optimize computational efficiency and quality.","The CrowdSplat framework consists of two stages: (1) avatar reconstruction and (2) crowd synthesis.","The framework is also optimized for GPU memory usage to enhance scalability.","Quantitative and qualitative evaluations show that CrowdSplat achieves good levels of rendering quality, memory efficiency, and computational performance.","Through these experiments, we demonstrate that CrowdSplat is a viable solution for dynamic, realistic crowd simulation in real-time applications."],"url":"http://arxiv.org/abs/2501.17792v1"}
{"created":"2025-01-29 17:31:26","title":"BreezyVoice: Adapting TTS for Taiwanese Mandarin with Enhanced Polyphone Disambiguation -- Challenges and Insights","abstract":"We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted for Taiwanese Mandarin, highlighting phonetic control abilities to address the unique challenges of polyphone disambiguation in the language. Building upon CosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an optimal-transport conditional flow matching model (OT-CFM), and a grapheme to phoneme prediction model, to generate realistic speech that closely mimics human utterances. Our evaluation demonstrates BreezyVoice's superior performance in both general and code-switching contexts, highlighting its robustness and effectiveness in generating high-fidelity speech. Additionally, we address the challenges of generalizability in modeling long-tail speakers and polyphone disambiguation. Our approach significantly enhances performance and offers valuable insights into the workings of neural codec TTS systems.","sentences":["We present BreezyVoice, a Text-to-Speech (TTS) system specifically adapted for Taiwanese Mandarin, highlighting phonetic control abilities to address the unique challenges of polyphone disambiguation in the language.","Building upon CosyVoice, we incorporate a $S^{3}$ tokenizer, a large language model (LLM), an optimal-transport conditional flow matching model (OT-CFM), and a grapheme to phoneme prediction model, to generate realistic speech that closely mimics human utterances.","Our evaluation demonstrates BreezyVoice's superior performance in both general and code-switching contexts, highlighting its robustness and effectiveness in generating high-fidelity speech.","Additionally, we address the challenges of generalizability in modeling long-tail speakers and polyphone disambiguation.","Our approach significantly enhances performance and offers valuable insights into the workings of neural codec TTS systems."],"url":"http://arxiv.org/abs/2501.17790v1"}
{"created":"2025-01-29 17:26:47","title":"WARP: An Efficient Engine for Multi-Vector Retrieval","abstract":"We study the efficiency of multi-vector retrieval methods like ColBERT and its recent variant XTR. We introduce WARP, a retrieval engine that drastically improves the efficiency of XTR-based ColBERT retrievers through three key innovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation, (2) implicit decompression to bypass costly vector reconstruction, and (3) a two-stage reduction process for efficient scoring. Combined with optimized C++ kernels and specialized inference runtimes, WARP reduces end-to-end latency by 41x compared to XTR's reference implementation and thereby achieves a 3x speedup over PLAID from the the official ColBERT implementation.   We study the efficiency of multi-vector retrieval methods like ColBERT and its recent variant XTR. We introduce WARP, a retrieval engine that drastically improves the efficiency of XTR-based ColBERT retrievers through three key innovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation, (2) implicit decompression during retrieval, and (3) a two-stage reduction process for efficient scoring. Thanks also to highly-optimized C++ kernels and to the adoption of specialized inference runtimes, WARP can reduce end-to-end query latency relative to XTR's reference implementation by 41x. And it thereby achieves a 3x speedup over the official ColBERTv2 PLAID engine, while preserving retrieval quality.","sentences":["We study the efficiency of multi-vector retrieval methods like ColBERT and its recent variant XTR.","We introduce WARP, a retrieval engine that drastically improves the efficiency of XTR-based ColBERT retrievers through three key innovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation, (2) implicit decompression to bypass costly vector reconstruction, and (3) a two-stage reduction process for efficient scoring.","Combined with optimized C++ kernels and specialized inference runtimes, WARP reduces end-to-end latency by 41x compared to XTR's reference implementation and thereby achieves a 3x speedup over PLAID from the the official ColBERT implementation.   ","We study the efficiency of multi-vector retrieval methods like ColBERT and its recent variant XTR.","We introduce WARP, a retrieval engine that drastically improves the efficiency of XTR-based ColBERT retrievers through three key innovations: (1) WARP$_\\text{SELECT}$ for dynamic similarity imputation, (2) implicit decompression during retrieval, and (3) a two-stage reduction process for efficient scoring.","Thanks also to highly-optimized C++ kernels and to the adoption of specialized inference runtimes, WARP can reduce end-to-end query latency relative to XTR's reference implementation by 41x.","And it thereby achieves a 3x speedup over the official ColBERTv2 PLAID engine, while preserving retrieval quality."],"url":"http://arxiv.org/abs/2501.17788v1"}
{"created":"2025-01-29 17:26:31","title":"Detecting Anomalies Using Rotated Isolation Forest","abstract":"The Isolation Forest (iForest), proposed by Liu, Ting, and Zhou at TKDE 2012, has become a prominent tool for unsupervised anomaly detection. However, recent research by Hariri, Kind, and Brunner, published in TKDE 2021, has revealed issues with iForest. They identified the presence of axis-aligned ghost clusters that can be misidentified as normal clusters, leading to biased anomaly scores and inaccurate predictions. In response, they developed the Extended Isolation Forest (EIF), which effectively solves these issues by eliminating the ghost clusters introduced by iForest. This enhancement results in improved consistency of anomaly scores and superior performance. We reveal a previously overlooked problem in the Extended Isolation Forest (EIF), showing that it is vulnerable to ghost inter-clusters between normal clusters of data points. In this paper, we introduce the Rotated Isolation Forest (RIF) algorithm which effectively addresses both the axis-aligned ghost clusters observed in iForest and the ghost inter-clusters seen in EIF. RIF accomplishes this by randomly rotating the dataset (using random rotation matrices and QR decomposition) before feeding it into the iForest construction, thereby increasing dataset variation and eliminating ghost clusters. Our experiments conclusively demonstrate that the RIF algorithm outperforms iForest and EIF, as evidenced by the results obtained from both synthetic datasets and real-world datasets.","sentences":["The Isolation Forest (iForest), proposed by Liu, Ting, and Zhou at TKDE 2012, has become a prominent tool for unsupervised anomaly detection.","However, recent research by Hariri, Kind, and Brunner, published in TKDE 2021, has revealed issues with iForest.","They identified the presence of axis-aligned ghost clusters that can be misidentified as normal clusters, leading to biased anomaly scores and inaccurate predictions.","In response, they developed the Extended Isolation Forest (EIF), which effectively solves these issues by eliminating the ghost clusters introduced by iForest.","This enhancement results in improved consistency of anomaly scores and superior performance.","We reveal a previously overlooked problem in the Extended Isolation Forest (EIF), showing that it is vulnerable to ghost inter-clusters between normal clusters of data points.","In this paper, we introduce the Rotated Isolation Forest (RIF) algorithm which effectively addresses both the axis-aligned ghost clusters observed in iForest and the ghost inter-clusters seen in EIF.","RIF accomplishes this by randomly rotating the dataset (using random rotation matrices and QR decomposition) before feeding it into the iForest construction, thereby increasing dataset variation and eliminating ghost clusters.","Our experiments conclusively demonstrate that the RIF algorithm outperforms iForest and EIF, as evidenced by the results obtained from both synthetic datasets and real-world datasets."],"url":"http://arxiv.org/abs/2501.17787v1"}
{"created":"2025-01-29 17:25:53","title":"Atomic Transfer Graphs: Secure-by-design Protocols for Heterogeneous Blockchain Ecosystems","abstract":"The heterogeneity of the blockchain landscape has motivated the design of blockchain protocols tailored to specific blockchains and applications that, hence, require custom security proofs. We observe that many blockchain protocols share common security and functionality goals, which can be captured by an atomic transfer graph (ATG) describing the structure of desired transfers. Based on this observation, we contribute a framework for generating secure-by-design protocols that realize these goals. The resulting protocols build upon Conditional Timelock Contracts (CTLCs), a novel minimal smart contract functionality that can be implemented in a large variety of cryptocurrencies with a restricted scripting language (e.g., Bitcoin), and payment channels. We show how ATGs, in addition to enabling novel applications, capture the security and functionality goals of existing applications, including many examples from payment channel networks and complex multi-party cross-currency swaps among Ethereum-style cryptocurrencies. Our framework is the first to provide generic and provably secure protocols for all these use cases while matching or improving the performance of existing use-case-specific protocols.","sentences":["The heterogeneity of the blockchain landscape has motivated the design of blockchain protocols tailored to specific blockchains and applications that, hence, require custom security proofs.","We observe that many blockchain protocols share common security and functionality goals, which can be captured by an atomic transfer graph (ATG) describing the structure of desired transfers.","Based on this observation, we contribute a framework for generating secure-by-design protocols that realize these goals.","The resulting protocols build upon Conditional Timelock Contracts (CTLCs), a novel minimal smart contract functionality that can be implemented in a large variety of cryptocurrencies with a restricted scripting language (e.g., Bitcoin), and payment channels.","We show how ATGs, in addition to enabling novel applications, capture the security and functionality goals of existing applications, including many examples from payment channel networks and complex multi-party cross-currency swaps among Ethereum-style cryptocurrencies.","Our framework is the first to provide generic and provably secure protocols for all these use cases while matching or improving the performance of existing use-case-specific protocols."],"url":"http://arxiv.org/abs/2501.17786v1"}
{"created":"2025-01-29 17:24:19","title":"Reasoning Over the Glyphs: Evaluation of LLM's Decipherment of Rare Scripts","abstract":"We explore the capabilities of LVLMs and LLMs in deciphering rare scripts not encoded in Unicode. We introduce a novel approach to construct a multimodal dataset of linguistic puzzles involving such scripts, utilizing a tokenization method for language glyphs. Our methods include the Picture Method for LVLMs and the Description Method for LLMs, enabling these models to tackle these challenges. We conduct experiments using prominent models, GPT-4o, Gemini, and Claude 3.5 Sonnet, on linguistic puzzles. Our findings reveal the strengths and limitations of current AI methods in linguistic decipherment, highlighting the impact of Unicode encoding on model performance and the challenges of modeling visual language tokens through descriptions. Our study advances understanding of AI's potential in linguistic decipherment and underscores the need for further research.","sentences":["We explore the capabilities of LVLMs and LLMs in deciphering rare scripts not encoded in Unicode.","We introduce a novel approach to construct a multimodal dataset of linguistic puzzles involving such scripts, utilizing a tokenization method for language glyphs.","Our methods include the Picture Method for LVLMs and the Description Method for LLMs, enabling these models to tackle these challenges.","We conduct experiments using prominent models, GPT-4o, Gemini, and Claude 3.5 Sonnet, on linguistic puzzles.","Our findings reveal the strengths and limitations of current AI methods in linguistic decipherment, highlighting the impact of Unicode encoding on model performance and the challenges of modeling visual language tokens through descriptions.","Our study advances understanding of AI's potential in linguistic decipherment and underscores the need for further research."],"url":"http://arxiv.org/abs/2501.17785v1"}
{"created":"2025-01-29 17:18:01","title":"AdditiveLLM: Large Language Models Predict Defects in Additive Manufacturing","abstract":"In this work we investigate the ability of large language models to predict additive manufacturing defect regimes given a set of process parameter inputs. For this task we utilize a process parameter defect dataset to fine-tune a collection of models, titled AdditiveLLM, for the purpose of predicting potential defect regimes including Keyholing, Lack of Fusion, and Balling. We compare different methods of input formatting in order to gauge the model's performance to correctly predict defect regimes on our sparse Baseline dataset and our natural language Prompt dataset. The model displays robust predictive capability, achieving an accuracy of 93\\% when asked to provide the defect regimes associated with a set of process parameters. The incorporation of natural language input further simplifies the task of process parameters selection, enabling users to identify optimal settings specific to their build.","sentences":["In this work we investigate the ability of large language models to predict additive manufacturing defect regimes given a set of process parameter inputs.","For this task we utilize a process parameter defect dataset to fine-tune a collection of models, titled AdditiveLLM, for the purpose of predicting potential defect regimes including Keyholing, Lack of Fusion, and Balling.","We compare different methods of input formatting in order to gauge the model's performance to correctly predict defect regimes on our sparse Baseline dataset and our natural language Prompt dataset.","The model displays robust predictive capability, achieving an accuracy of 93\\% when asked to provide the defect regimes associated with a set of process parameters.","The incorporation of natural language input further simplifies the task of process parameters selection, enabling users to identify optimal settings specific to their build."],"url":"http://arxiv.org/abs/2501.17784v1"}
{"created":"2025-01-29 17:15:45","title":"Picard-KKT-hPINN: Enforcing Nonlinear Enthalpy Balances for Physically Consistent Neural Networks","abstract":"Neural networks are widely used as surrogate models but they do not guarantee physically consistent predictions thereby preventing adoption in various applications. We propose a method that can enforce NNs to satisfy physical laws that are nonlinear in nature such as enthalpy balances. Our approach, inspired by Picard successive approximations method, aims to enforce multiplicatively separable constraints by sequentially freezing and projecting a set of the participating variables. We demonstrate our PicardKKThPINN for surrogate modeling of a catalytic packed bed reactor for methanol synthesis. Our results show that the method efficiently enforces nonlinear enthalpy and linear atomic balances at machine-level precision. Additionally, we show that enforcing conservation laws can improve accuracy in data-scarce conditions compared to vanilla multilayer perceptron.","sentences":["Neural networks are widely used as surrogate models but they do not guarantee physically consistent predictions thereby preventing adoption in various applications.","We propose a method that can enforce NNs to satisfy physical laws that are nonlinear in nature such as enthalpy balances.","Our approach, inspired by Picard successive approximations method, aims to enforce multiplicatively separable constraints by sequentially freezing and projecting a set of the participating variables.","We demonstrate our PicardKKThPINN for surrogate modeling of a catalytic packed bed reactor for methanol synthesis.","Our results show that the method efficiently enforces nonlinear enthalpy and linear atomic balances at machine-level precision.","Additionally, we show that enforcing conservation laws can improve accuracy in data-scarce conditions compared to vanilla multilayer perceptron."],"url":"http://arxiv.org/abs/2501.17782v1"}
{"created":"2025-01-29 17:14:29","title":"Iso-Recursive Multiparty Sessions and their Automated Verification -- Technical Report","abstract":"Most works on session types take an equi-recursive approach and do not distinguish among a recursive type and its unfolding. This becomes more important in recent type systems which do not require global types, also known as generalised multiparty session types (GMST). In GMST, in order to establish properties as deadlock-freedom, the environments which type processes are assumed to satisfy extensional properties holding in all infinite sequences. This is a problem because: (1) the mechanisation of GMST and equi-recursion in proof assistants is utterly complex and eventually requires co-induction; and (2) the implementation of GMST in type checkers relies on model checkers for environment verification, and thus the program analysis is not self-contained.   In this paper, we overcome these limitations by providing an iso-recursive typing system that computes the behavioural properties of environments. The type system relies on a terminating function named compliance that computes all final redexes of an environment, and determines when these redexes do not contain mismatches or deadlocks: compliant environments cannot go wrong. The function is defined theoretically by introducing the novel notions of deterministic LTS of environments and of environment closure, and can be implemented in mainstream programming languages and compilers. We showcase an implementation in OCaml by using exception handling to tackle the inherent non-determinism of synchronisation of branching and selection types. We assess that the implementation provides the desired properties, namely absence of mismatches and of deadlocks in environments, by resorting to automated deductive verification performed in tools of the OCaml ecosystem relying on Why3.","sentences":["Most works on session types take an equi-recursive approach and do not distinguish among a recursive type and its unfolding.","This becomes more important in recent type systems which do not require global types, also known as generalised multiparty session types (GMST).","In GMST, in order to establish properties as deadlock-freedom, the environments which type processes are assumed to satisfy extensional properties holding in all infinite sequences.","This is a problem because: (1) the mechanisation of GMST and equi-recursion in proof assistants is utterly complex and eventually requires co-induction; and (2) the implementation of GMST in type checkers relies on model checkers for environment verification, and thus the program analysis is not self-contained.   ","In this paper, we overcome these limitations by providing an iso-recursive typing system that computes the behavioural properties of environments.","The type system relies on a terminating function named compliance that computes all final redexes of an environment, and determines when these redexes do not contain mismatches or deadlocks: compliant environments cannot go wrong.","The function is defined theoretically by introducing the novel notions of deterministic LTS of environments and of environment closure, and can be implemented in mainstream programming languages and compilers.","We showcase an implementation in OCaml by using exception handling to tackle the inherent non-determinism of synchronisation of branching and selection types.","We assess that the implementation provides the desired properties, namely absence of mismatches and of deadlocks in environments, by resorting to automated deductive verification performed in tools of the OCaml ecosystem relying on Why3."],"url":"http://arxiv.org/abs/2501.17778v1"}
{"created":"2025-01-29 17:13:17","title":"On decoding hyperbolic codes","abstract":"This work studies several decoding algorithms for hyperbolic codes. We use some previous ideas to describe how to decode a hyperbolic code using the largest Reed-Muller code contained in it or using the smallest Reed-Muller code that contains it. A combination of these two algorithms is proposed when hyperbolic codes are defined by polynomials in two variables. Then, we compare hyperbolic codes and Cube codes (tensor product of Reed-Solomon codes) and propose decoding algorithms of hyperbolic codes based on their closest Cube codes. Finally, we adapt to hyperbolic codes the Geil and Matsumoto's generalization of Sudan's list decoding algorithm.","sentences":["This work studies several decoding algorithms for hyperbolic codes.","We use some previous ideas to describe how to decode a hyperbolic code using the largest Reed-Muller code contained in it or using the smallest Reed-Muller code that contains it.","A combination of these two algorithms is proposed when hyperbolic codes are defined by polynomials in two variables.","Then, we compare hyperbolic codes and Cube codes (tensor product of Reed-Solomon codes) and propose decoding algorithms of hyperbolic codes based on their closest Cube codes.","Finally, we adapt to hyperbolic codes the Geil and Matsumoto's generalization of Sudan's list decoding algorithm."],"url":"http://arxiv.org/abs/2501.17777v1"}
{"created":"2025-01-29 17:09:28","title":"SafePR: Unified Approach for Safe Parallel Robots by Contact Detection and Reaction with Redundancy Resolution","abstract":"Fast and safe motion is crucial for the successful deployment of physically interactive robots. Parallel robots (PRs) offer the potential for higher speeds while maintaining the same energy limits due to their low moving masses. However, they require methods for contact detection and reaction while avoiding singularities and self-collisions. We address this issue and present SafePR - a unified approach for the detection and localization, including the distinction between collision and clamping to perform a reaction that is safe for humans and feasible for PRs. Our approach uses information from the encoders and motor currents to estimate forces via a generalized-momentum observer. Neural networks and particle filters classify and localize the contacts. We introduce reactions with redundancy resolution to avoid type-II singularities and self-collisions. Our approach detected and terminated 72 real-world collision and clamping contacts with end-effector speeds of up to 1.5 m/s, each within 25-275 ms. The forces were below the thresholds from ISO/TS 15066. By using built-in sensors, SafePR enables safe interaction with already assembled PRs without the need for new hardware components.","sentences":["Fast and safe motion is crucial for the successful deployment of physically interactive robots.","Parallel robots (PRs) offer the potential for higher speeds while maintaining the same energy limits due to their low moving masses.","However, they require methods for contact detection and reaction while avoiding singularities and self-collisions.","We address this issue and present SafePR - a unified approach for the detection and localization, including the distinction between collision and clamping to perform a reaction that is safe for humans and feasible for PRs.","Our approach uses information from the encoders and motor currents to estimate forces via a generalized-momentum observer.","Neural networks and particle filters classify and localize the contacts.","We introduce reactions with redundancy resolution to avoid type-II singularities and self-collisions.","Our approach detected and terminated 72 real-world collision and clamping contacts with end-effector speeds of up to 1.5 m/s, each within 25-275 ms.","The forces were below the thresholds from ISO/TS 15066.","By using built-in sensors, SafePR enables safe interaction with already assembled PRs without the need for new hardware components."],"url":"http://arxiv.org/abs/2501.17773v1"}
{"created":"2025-01-29 17:05:33","title":"2SSP: A Two-Stage Framework for Structured Pruning of LLMs","abstract":"We propose a novel Two-Stage framework for Structured Pruning (2SSP) for pruning Large Language Models (LLMs), which combines two different strategies of pruning, namely Width and Depth Pruning. The first stage (Width Pruning) removes entire neurons, hence their corresponding rows and columns, aiming to preserve the connectivity among the pruned structures in the intermediate state of the Feed-Forward Networks in each Transformer block. This is done based on an importance score measuring the impact of each neuron over the output magnitude. The second stage (Depth Pruning), instead, removes entire Attention submodules. This is done by applying an iterative process that removes the Attention submodules with the minimum impact on a given metric of interest (in our case, perplexity). We also propose a novel mechanism to balance the sparsity rate of the two stages w.r.t. to the desired global sparsity. We test 2SSP on four LLM families and three sparsity rates (25\\%, 37.5\\%, and 50\\%), measuring the resulting perplexity over three language modeling datasets as well as the performance over six downstream tasks. Our method consistently outperforms five state-of-the-art competitors over three language modeling and six downstream tasks, with an up to two-order-of-magnitude gain in terms of pruning time. The code is available at available at \\url{https://github.com/FabrizioSandri/2SSP}.","sentences":["We propose a novel Two-Stage framework for Structured Pruning (2SSP) for pruning Large Language Models (LLMs), which combines two different strategies of pruning, namely Width and Depth Pruning.","The first stage (Width Pruning) removes entire neurons, hence their corresponding rows and columns, aiming to preserve the connectivity among the pruned structures in the intermediate state of the Feed-Forward Networks in each Transformer block.","This is done based on an importance score measuring the impact of each neuron over the output magnitude.","The second stage (Depth Pruning), instead, removes entire Attention submodules.","This is done by applying an iterative process that removes the Attention submodules with the minimum impact on a given metric of interest (in our case, perplexity).","We also propose a novel mechanism to balance the sparsity rate of the two stages w.r.t.","to the desired global sparsity.","We test 2SSP on four LLM families and three sparsity rates (25\\%, 37.5\\%, and 50\\%), measuring the resulting perplexity over three language modeling datasets as well as the performance over six downstream tasks.","Our method consistently outperforms five state-of-the-art competitors over three language modeling and six downstream tasks, with an up to two-order-of-magnitude gain in terms of pruning time.","The code is available at available at \\url{https://github.com/FabrizioSandri/2SSP}."],"url":"http://arxiv.org/abs/2501.17771v1"}
{"created":"2025-01-29 17:03:44","title":"Generative Unordered Flow for Set-Structured Data Generation","abstract":"Flow-based generative models have demonstrated promising performance across a broad spectrum of data modalities (e.g., image and text). However, there are few works exploring their extension to unordered data (e.g., spatial point set), which is not trivial because previous models are mostly designed for vector data that are naturally ordered. In this paper, we present unordered flow, a type of flow-based generative model for set-structured data generation. Specifically, we convert unordered data into an appropriate function representation, and learn the probability measure of such representations through function-valued flow matching. For the inverse map from a function representation to unordered data, we propose a method similar to particle filtering, with Langevin dynamics to first warm-up the initial particles and gradient-based search to update them until convergence. We have conducted extensive experiments on multiple real-world datasets, showing that our unordered flow model is very effective in generating set-structured data and significantly outperforms previous baselines.","sentences":["Flow-based generative models have demonstrated promising performance across a broad spectrum of data modalities (e.g., image and text).","However, there are few works exploring their extension to unordered data (e.g., spatial point set), which is not trivial because previous models are mostly designed for vector data that are naturally ordered.","In this paper, we present unordered flow, a type of flow-based generative model for set-structured data generation.","Specifically, we convert unordered data into an appropriate function representation, and learn the probability measure of such representations through function-valued flow matching.","For the inverse map from a function representation to unordered data, we propose a method similar to particle filtering, with Langevin dynamics to first warm-up the initial particles and gradient-based search to update them until convergence.","We have conducted extensive experiments on multiple real-world datasets, showing that our unordered flow model is very effective in generating set-structured data and significantly outperforms previous baselines."],"url":"http://arxiv.org/abs/2501.17770v1"}
{"created":"2025-01-29 17:01:37","title":"TeamPortal: Exploring Virtual Reality Collaboration Through Shared and Manipulating Parallel Views","abstract":"Virtual Reality (VR) offers a unique collaborative experience, with parallel views playing a pivotal role in Collaborative Virtual Environments by supporting the transfer and delivery of items. Sharing and manipulating partners' views provides users with a broader perspective that helps them identify the targets and partner actions. We proposed TeamPortal accordingly and conducted two user studies with 72 participants (36 pairs) to investigate the potential benefits of interactive, shared perspectives in VR collaboration. Our first study compared ShaView and TeamPortal against a baseline in a collaborative task that encompassed a series of searching and manipulation tasks. The results show that TeamPortal significantly reduced movement and increased collaborative efficiency and social presence in complex tasks. Following the results, the second study evaluated three variants: TeamPortal+, SnapTeamPortal+, and DropTeamPortal+. The results show that both SnapTeamPortal+ and DropTeamPortal+ improved task efficiency and willingness to further adopt these technologies, though SnapTeamPortal+ reduced co-presence. Based on the findings, we proposed three design implications to inform the development of future VR collaboration systems.","sentences":["Virtual Reality (VR) offers a unique collaborative experience, with parallel views playing a pivotal role in Collaborative Virtual Environments by supporting the transfer and delivery of items.","Sharing and manipulating partners' views provides users with a broader perspective that helps them identify the targets and partner actions.","We proposed TeamPortal accordingly and conducted two user studies with 72 participants (36 pairs) to investigate the potential benefits of interactive, shared perspectives in VR collaboration.","Our first study compared ShaView and TeamPortal against a baseline in a collaborative task that encompassed a series of searching and manipulation tasks.","The results show that TeamPortal significantly reduced movement and increased collaborative efficiency and social presence in complex tasks.","Following the results, the second study evaluated three variants: TeamPortal+, SnapTeamPortal+, and DropTeamPortal+.","The results show that both SnapTeamPortal+ and DropTeamPortal+ improved task efficiency and willingness to further adopt these technologies, though SnapTeamPortal+ reduced co-presence.","Based on the findings, we proposed three design implications to inform the development of future VR collaboration systems."],"url":"http://arxiv.org/abs/2501.17768v1"}
{"created":"2025-01-29 16:58:18","title":"Hybrid Graphs for Table-and-Text based Question Answering using LLMs","abstract":"Answering questions that require reasoning and aggregation across both structured (tables) and unstructured (raw text) data sources presents significant challenges. Current methods rely on fine-tuning and high-quality, human-curated data, which is difficult to obtain. Recent advances in Large Language Models (LLMs) have shown promising results for multi-hop question answering (QA) over single-source text data in a zero-shot setting, yet exploration into multi-source Table-Text QA remains limited. In this paper, we present a novel Hybrid Graph-based approach for Table-Text QA that leverages LLMs without fine-tuning. Our method constructs a unified Hybrid Graph from textual and tabular data, pruning information based on the input question to provide the LLM with relevant context concisely. We evaluate our approach on the challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs, including GPT-3.5, GPT-4, and LLaMA-3. Our method achieves the best zero-shot performance on both datasets, improving Exact Match scores by up to 10% on Hybrid-QA and 5.4% on OTT-QA. Moreover, our approach reduces token usage by up to 53% compared to the original context.","sentences":["Answering questions that require reasoning and aggregation across both structured (tables) and unstructured (raw text) data sources presents significant challenges.","Current methods rely on fine-tuning and high-quality, human-curated data, which is difficult to obtain.","Recent advances in Large Language Models (LLMs) have shown promising results for multi-hop question answering (QA) over single-source text data in a zero-shot setting, yet exploration into multi-source Table-Text QA remains limited.","In this paper, we present a novel Hybrid Graph-based approach for Table-Text QA that leverages LLMs without fine-tuning.","Our method constructs a unified Hybrid Graph from textual and tabular data, pruning information based on the input question to provide the LLM with relevant context concisely.","We evaluate our approach on the challenging Hybrid-QA and OTT-QA datasets using state-of-the-art LLMs, including GPT-3.5, GPT-4, and LLaMA-3.","Our method achieves the best zero-shot performance on both datasets, improving Exact Match scores by up to 10% on Hybrid-QA and 5.4% on OTT-QA.","Moreover, our approach reduces token usage by up to 53% compared to the original context."],"url":"http://arxiv.org/abs/2501.17767v1"}
{"created":"2025-01-29 16:57:15","title":"Formally Verified Binary-level Pointer Analysis","abstract":"Binary-level pointer analysis can be of use in symbolic execution, testing, verification, and decompilation of software binaries. In various such contexts, it is crucial that the result is trustworthy, i.e., it can be formally established that the pointer designations are overapproximative. This paper presents an approach to formally proven correct binary-level pointer analysis. A salient property of our approach is that it first generically considers what proof obligations a generic abstract domain for pointer analysis must satisfy. This allows easy instantiation of different domains, varying in precision, while preserving the correctness of the analysis. In the trade-off between scalability and precision, such customization allows \"meaningful\" precision (sufficiently precise to ensure basic sanity properties, such as that relevant parts of the stack frame are not overwritten during function execution) while also allowing coarse analysis when pointer computations have become too obfuscated during compilation for sound and accurate bounds analysis. We experiment with three different abstract domains with high, medium, and low precision. Evaluation shows that our approach is able to derive designations for memory writes soundly in COTS binaries, in a context-sensitive interprocedural fashion.","sentences":["Binary-level pointer analysis can be of use in symbolic execution, testing, verification, and decompilation of software binaries.","In various such contexts, it is crucial that the result is trustworthy, i.e., it can be formally established that the pointer designations are overapproximative.","This paper presents an approach to formally proven correct binary-level pointer analysis.","A salient property of our approach is that it first generically considers what proof obligations a generic abstract domain for pointer analysis must satisfy.","This allows easy instantiation of different domains, varying in precision, while preserving the correctness of the analysis.","In the trade-off between scalability and precision, such customization allows \"meaningful\" precision (sufficiently precise to ensure basic sanity properties, such as that relevant parts of the stack frame are not overwritten during function execution) while also allowing coarse analysis when pointer computations have become too obfuscated during compilation for sound and accurate bounds analysis.","We experiment with three different abstract domains with high, medium, and low precision.","Evaluation shows that our approach is able to derive designations for memory writes soundly in COTS binaries, in a context-sensitive interprocedural fashion."],"url":"http://arxiv.org/abs/2501.17766v1"}
{"created":"2025-01-29 16:53:16","title":"Improving Privacy Benefits of Redaction","abstract":"We propose a novel redaction methodology that can be used to sanitize natural text data. Our new technique provides better privacy benefits than other state of the art techniques while maintaining lower redaction levels.","sentences":["We propose a novel redaction methodology that can be used to sanitize natural text data.","Our new technique provides better privacy benefits than other state of the art techniques while maintaining lower redaction levels."],"url":"http://arxiv.org/abs/2501.17762v1"}
{"created":"2025-01-29 16:50:34","title":"Unraveling Log4Shell: Analyzing the Impact and Response to the Log4j Vulnerabil","abstract":"The realm of technology frequently confronts threats posed by adversaries exploiting loopholes in programs. Among these, the Log4Shell vulnerability in the Log4j library stands out due to its widespread impact. Log4j, a prevalent software library for log recording, is integrated into millions of devices worldwide. The Log4Shell vulnerability facilitates remote code execution with relative ease. Its combination with the extensive utilization of Log4j marks it as one of the most dangerous vulnerabilities discovered to date. The severity of this vulnerability, which quickly escalated into a media frenzy, prompted swift action within the industry, thereby mitigating potential extensive damage. This rapid response was crucial, as the consequences could have been significantly more severe if the vulnerability had been exploited by adversaries prior to its public disclosure.   This paper details the discovery of the Log4Shell vulnerability and its potential for exploitation. It examines the vulnerability's impact on various stakeholders, including governments, the Apache Software Foundation (which manages the Log4j library), and companies affected by it. The paper also describes strategies for defending against Log4Shell in several scenarios. While numerous Log4j users acted promptly to safeguard their systems, the vulnerability remains a persistent threat until all vulnerable instances of the library are adequately protected.","sentences":["The realm of technology frequently confronts threats posed by adversaries exploiting loopholes in programs.","Among these, the Log4Shell vulnerability in the Log4j library stands out due to its widespread impact.","Log4j, a prevalent software library for log recording, is integrated into millions of devices worldwide.","The Log4Shell vulnerability facilitates remote code execution with relative ease.","Its combination with the extensive utilization of Log4j marks it as one of the most dangerous vulnerabilities discovered to date.","The severity of this vulnerability, which quickly escalated into a media frenzy, prompted swift action within the industry, thereby mitigating potential extensive damage.","This rapid response was crucial, as the consequences could have been significantly more severe if the vulnerability had been exploited by adversaries prior to its public disclosure.   ","This paper details the discovery of the Log4Shell vulnerability and its potential for exploitation.","It examines the vulnerability's impact on various stakeholders, including governments, the Apache Software Foundation (which manages the Log4j library), and companies affected by it.","The paper also describes strategies for defending against Log4Shell in several scenarios.","While numerous Log4j users acted promptly to safeguard their systems, the vulnerability remains a persistent threat until all vulnerable instances of the library are adequately protected."],"url":"http://arxiv.org/abs/2501.17760v1"}
{"created":"2025-01-29 16:50:09","title":"Yin-Yang: Developing Motifs With Long-Term Structure And Controllability","abstract":"Transformer models have made great strides in generating symbolically represented music with local coherence. However, controlling the development of motifs in a structured way with global form remains an open research area. One of the reasons for this challenge is due to the note-by-note autoregressive generation of such models, which lack the ability to correct themselves after deviations from the motif. In addition, their structural performance on datasets with shorter durations has not been studied in the literature. In this study, we propose Yin-Yang, a framework consisting of a phrase generator, phrase refiner, and phrase selector models for the development of motifs into melodies with long-term structure and controllability. The phrase refiner is trained on a novel corruption-refinement strategy which allows it to produce melodic and rhythmic variations of an original motif at generation time, thereby rectifying deviations of the phrase generator. We also introduce a new objective evaluation metric for quantifying how smoothly the motif manifests itself within the piece. Evaluation results show that our model achieves better performance compared to state-of-the-art transformer models while having the advantage of being controllable and making the generated musical structure semi-interpretable, paving the way for musical analysis. Our code and demo page can be found at https://github.com/keshavbhandari/yinyang.","sentences":["Transformer models have made great strides in generating symbolically represented music with local coherence.","However, controlling the development of motifs in a structured way with global form remains an open research area.","One of the reasons for this challenge is due to the note-by-note autoregressive generation of such models, which lack the ability to correct themselves after deviations from the motif.","In addition, their structural performance on datasets with shorter durations has not been studied in the literature.","In this study, we propose Yin-Yang, a framework consisting of a phrase generator, phrase refiner, and phrase selector models for the development of motifs into melodies with long-term structure and controllability.","The phrase refiner is trained on a novel corruption-refinement strategy which allows it to produce melodic and rhythmic variations of an original motif at generation time, thereby rectifying deviations of the phrase generator.","We also introduce a new objective evaluation metric for quantifying how smoothly the motif manifests itself within the piece.","Evaluation results show that our model achieves better performance compared to state-of-the-art transformer models while having the advantage of being controllable and making the generated musical structure semi-interpretable, paving the way for musical analysis.","Our code and demo page can be found at https://github.com/keshavbhandari/yinyang."],"url":"http://arxiv.org/abs/2501.17759v1"}
{"created":"2025-01-29 16:41:15","title":"On the Partitioning of GPU Power among Multi-Instances","abstract":"Efficient power management in cloud data centers is essential for reducing costs, enhancing performance, and minimizing environmental impact. GPUs, critical for tasks like machine learning (ML) and GenAI, are major contributors to power consumption. NVIDIA's Multi-Instance GPU (MIG) technology improves GPU utilization by enabling isolated partitions with per-partition resource tracking, facilitating GPU sharing by multiple tenants. However, accurately apportioning GPU power consumption among MIG instances remains challenging due to a lack of hardware support. This paper addresses this challenge by developing software methods to estimate power usage per MIG partition. We analyze NVIDIA GPU utilization metrics and find that light-weight methods with good accuracy can be difficult to construct. We hence explore the use of ML-based power models to enable accurate, partition-level power estimation. Our findings reveal that a single generic offline power model or modeling method is not applicable across diverse workloads, especially with concurrent MIG usage, and that online models constructed using partition-level utilization metrics of workloads under execution can significantly improve accuracy. Using NVIDIA A100 GPUs, we demonstrate this approach for accurate partition-level power estimation for workloads including matrix multiplication and Large Language Model inference, contributing to transparent and fair carbon reporting.","sentences":["Efficient power management in cloud data centers is essential for reducing costs, enhancing performance, and minimizing environmental impact.","GPUs, critical for tasks like machine learning (ML) and GenAI, are major contributors to power consumption.","NVIDIA's Multi-Instance GPU (MIG) technology improves GPU utilization by enabling isolated partitions with per-partition resource tracking, facilitating GPU sharing by multiple tenants.","However, accurately apportioning GPU power consumption among MIG instances remains challenging due to a lack of hardware support.","This paper addresses this challenge by developing software methods to estimate power usage per MIG partition.","We analyze NVIDIA GPU utilization metrics and find that light-weight methods with good accuracy can be difficult to construct.","We hence explore the use of ML-based power models to enable accurate, partition-level power estimation.","Our findings reveal that a single generic offline power model or modeling method is not applicable across diverse workloads, especially with concurrent MIG usage, and that online models constructed using partition-level utilization metrics of workloads under execution can significantly improve accuracy.","Using NVIDIA A100 GPUs, we demonstrate this approach for accurate partition-level power estimation for workloads including matrix multiplication and Large Language Model inference, contributing to transparent and fair carbon reporting."],"url":"http://arxiv.org/abs/2501.17752v1"}
{"created":"2025-01-29 16:38:51","title":"Privacy Audit as Bits Transmission: (Im)possibilities for Audit by One Run","abstract":"Auditing algorithms' privacy typically involves simulating a game-based protocol that guesses which of two adjacent datasets was the original input. Traditional approaches require thousands of such simulations, leading to significant computational overhead. Recent methods propose single-run auditing of the target algorithm to address this, substantially reducing computational cost. However, these methods' general applicability and tightness in producing empirical privacy guarantees remain uncertain.   This work studies such problems in detail. Our contributions are twofold: First, we introduce a unifying framework for privacy audits based on information-theoretic principles, modeling the audit as a bit transmission problem in a noisy channel. This formulation allows us to derive fundamental limits and develop an audit approach that yields tight privacy lower bounds for various DP protocols. Second, leveraging this framework, we demystify the method of privacy audit by one run, identifying the conditions under which single-run audits are feasible or infeasible. Our analysis provides general guidelines for conducting privacy audits and offers deeper insights into the privacy audit.   Finally, through experiments, we demonstrate that our approach produces tighter privacy lower bounds on common differentially private mechanisms while requiring significantly fewer observations. We also provide a case study illustrating that our method successfully detects privacy violations in flawed implementations of private algorithms.","sentences":["Auditing algorithms' privacy typically involves simulating a game-based protocol that guesses which of two adjacent datasets was the original input.","Traditional approaches require thousands of such simulations, leading to significant computational overhead.","Recent methods propose single-run auditing of the target algorithm to address this, substantially reducing computational cost.","However, these methods' general applicability and tightness in producing empirical privacy guarantees remain uncertain.   ","This work studies such problems in detail.","Our contributions are twofold:","First, we introduce a unifying framework for privacy audits based on information-theoretic principles, modeling the audit as a bit transmission problem in a noisy channel.","This formulation allows us to derive fundamental limits and develop an audit approach that yields tight privacy lower bounds for various DP protocols.","Second, leveraging this framework, we demystify the method of privacy audit by one run, identifying the conditions under which single-run audits are feasible or infeasible.","Our analysis provides general guidelines for conducting privacy audits and offers deeper insights into the privacy audit.   ","Finally, through experiments, we demonstrate that our approach produces tighter privacy lower bounds on common differentially private mechanisms while requiring significantly fewer observations.","We also provide a case study illustrating that our method successfully detects privacy violations in flawed implementations of private algorithms."],"url":"http://arxiv.org/abs/2501.17750v1"}
{"created":"2025-01-29 16:36:53","title":"Early External Safety Testing of OpenAI's o3-mini: Insights from the Pre-Deployment Evaluation","abstract":"Large Language Models (LLMs) have become an integral part of our daily lives. However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation. These risks highlight the need for robust safety mechanisms, ethical guidelines, and thorough testing to ensure their responsible deployment. Safety of LLMs is a key property that needs to be thoroughly tested prior the model to be deployed and accessible to the general users. This paper reports the external safety testing experience conducted by researchers from Mondragon University and University of Seville on OpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing program. In particular, we apply our tool, ASTRAL, to automatically and systematically generate up to date unsafe test inputs (i.e., prompts) that helps us test and assess different safety categories of LLMs. We automatically generate and execute a total of 10,080 unsafe test input on a early o3-mini beta version. After manually verifying the test cases classified as unsafe by ASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior. We highlight key insights and findings uncovered during the pre-deployment external testing phase of OpenAI's latest LLM.","sentences":["Large Language Models (LLMs) have become an integral part of our daily lives.","However, they impose certain risks, including those that can harm individuals' privacy, perpetuate biases and spread misinformation.","These risks highlight the need for robust safety mechanisms, ethical guidelines, and thorough testing to ensure their responsible deployment.","Safety of LLMs is a key property that needs to be thoroughly tested prior the model to be deployed and accessible to the general users.","This paper reports the external safety testing experience conducted by researchers from Mondragon University and University of Seville on OpenAI's new o3-mini LLM as part of OpenAI's early access for safety testing program.","In particular, we apply our tool, ASTRAL, to automatically and systematically generate up to date unsafe test inputs (i.e., prompts) that helps us test and assess different safety categories of LLMs.","We automatically generate and execute a total of 10,080 unsafe test input on a early o3-mini beta version.","After manually verifying the test cases classified as unsafe by ASTRAL, we identify a total of 87 actual instances of unsafe LLM behavior.","We highlight key insights and findings uncovered during the pre-deployment external testing phase of OpenAI's latest LLM."],"url":"http://arxiv.org/abs/2501.17749v1"}
{"created":"2025-01-29 16:36:41","title":"Investigating Vulnerability Disclosures in Open-Source Software Using Bug Bounty Reports and Security Advisories","abstract":"In the world of open-source software (OSS), the number of known vulnerabilities has tremendously increased. The GitHub Advisory Database contains advisories for security risks in GitHub-hosted OSS projects. As of 09/25/2023, there are 197,609 unreviewed GitHub security advisories. Of those unreviewed, at least 63,852 are publicly documented vulnerabilities, potentially leaving many OSS projects vulnerable. Recently, bug bounty platforms have emerged to focus solely on providing bounties to help secure OSS. In this paper, we conduct an empirical study on 3,798 reviewed GitHub security advisories and 4,033 disclosed OSS bug bounty reports, a perspective that is currently understudied, because they contain comprehensive information about security incidents, e.g., the nature of vulnerabilities, their impact, and how they were resolved. We are the first to determine the explicit process describing how OSS vulnerabilities propagate from security advisories and bug bounty reports, which are the main intermediaries between vulnerability reporters, OSS maintainers, and dependent projects, to vulnerable OSS projects and entries in global vulnerability databases and possibly back. This process uncovers how missing or delayed CVE assignments for OSS vulnerabilities result in projects, both in and out of OSS, not being notified of necessary security updates promptly and corresponding bottlenecks. Based on our findings, we provide suggestions, actionable items, and future research directions to help improve the security posture of OSS projects.","sentences":["In the world of open-source software (OSS), the number of known vulnerabilities has tremendously increased.","The GitHub Advisory Database contains advisories for security risks in GitHub-hosted OSS projects.","As of 09/25/2023, there are 197,609 unreviewed GitHub security advisories.","Of those unreviewed, at least 63,852 are publicly documented vulnerabilities, potentially leaving many OSS projects vulnerable.","Recently, bug bounty platforms have emerged to focus solely on providing bounties to help secure OSS.","In this paper, we conduct an empirical study on 3,798 reviewed GitHub security advisories and 4,033 disclosed OSS bug bounty reports, a perspective that is currently understudied, because they contain comprehensive information about security incidents, e.g., the nature of vulnerabilities, their impact, and how they were resolved.","We are the first to determine the explicit process describing how OSS vulnerabilities propagate from security advisories and bug bounty reports, which are the main intermediaries between vulnerability reporters, OSS maintainers, and dependent projects, to vulnerable OSS projects and entries in global vulnerability databases and possibly back.","This process uncovers how missing or delayed CVE assignments for OSS vulnerabilities result in projects, both in and out of OSS, not being notified of necessary security updates promptly and corresponding bottlenecks.","Based on our findings, we provide suggestions, actionable items, and future research directions to help improve the security posture of OSS projects."],"url":"http://arxiv.org/abs/2501.17748v1"}
{"created":"2025-01-29 16:34:22","title":"In-IDE Programming Courses: Learning Software Development in a Real-World Setting","abstract":"While learning programming languages is crucial for software engineers, mastering the necessary tools is equally important. To facilitate this, JetBrains recently released the JetBrains Academy plugin, which customizes the IDE for learners, allowing tutors to create courses entirely within IDE.   In this work, we provide the first exploratory study of this learning format. We carried out eight one-hour interviews with students and developers who completed at least one course using the plugin, inquiring about their experience with the format, the used IDE features, and the current shortcomings. Our results indicate that learning inside the IDE is overall welcomed by the learners, allowing them to study in a more realistic setting, using features such as debugging and code analysis, which are crucial for real software development. With the collected results and the analysis of the current drawbacks, we aim to contribute to teaching students more practical skills.","sentences":["While learning programming languages is crucial for software engineers, mastering the necessary tools is equally important.","To facilitate this, JetBrains recently released the JetBrains Academy plugin, which customizes the IDE for learners, allowing tutors to create courses entirely within IDE.   ","In this work, we provide the first exploratory study of this learning format.","We carried out eight one-hour interviews with students and developers who completed at least one course using the plugin, inquiring about their experience with the format, the used IDE features, and the current shortcomings.","Our results indicate that learning inside the IDE is overall welcomed by the learners, allowing them to study in a more realistic setting, using features such as debugging and code analysis, which are crucial for real software development.","With the collected results and the analysis of the current drawbacks, we aim to contribute to teaching students more practical skills."],"url":"http://arxiv.org/abs/2501.17747v1"}
{"created":"2025-01-29 16:32:14","title":"Dynamics of Transient Structure in In-Context Linear Regression Transformers","abstract":"Modern deep neural networks display striking examples of rich internal computational structure. Uncovering principles governing the development of such structure is a priority for the science of deep learning. In this paper, we explore the transient ridge phenomenon: when transformers are trained on in-context linear regression tasks with intermediate task diversity, they initially behave like ridge regression before specializing to the tasks in their training distribution. This transition from a general solution to a specialized solution is revealed by joint trajectory principal component analysis. Further, we draw on the theory of Bayesian internal model selection to suggest a general explanation for the phenomena of transient structure in transformers, based on an evolving tradeoff between loss and complexity. This explanation is grounded in empirical measurements of model complexity using the local learning coefficient.","sentences":["Modern deep neural networks display striking examples of rich internal computational structure.","Uncovering principles governing the development of such structure is a priority for the science of deep learning.","In this paper, we explore the transient ridge phenomenon: when transformers are trained on in-context linear regression tasks with intermediate task diversity, they initially behave like ridge regression before specializing to the tasks in their training distribution.","This transition from a general solution to a specialized solution is revealed by joint trajectory principal component analysis.","Further, we draw on the theory of Bayesian internal model selection to suggest a general explanation for the phenomena of transient structure in transformers, based on an evolving tradeoff between loss and complexity.","This explanation is grounded in empirical measurements of model complexity using the local learning coefficient."],"url":"http://arxiv.org/abs/2501.17745v1"}
{"created":"2025-01-29 16:28:19","title":"Multiparty Session Typing, Embedded (Technical Report)","abstract":"Multiparty session typing (MPST) is a method to make concurrent programming simpler. The idea is to use type checking to automatically detect safety and liveness violations of implementations relative to specifications. In practice, the premier approach to combine MPST with mainstream languages -- in the absence of native support -- is based on external DSLs and associated tooling.   In contrast, we study the question of how to support MPST by using internal DSLs. Answering this question positively, this paper presents the mpst.embedded library: it leverages Scala's lightweight form of dependent typing, called match types, to embed MPST directly into Scala. Our internal-DSL-based approach avoids programming friction and leaky abstractions of the external-DSL-based approach for MPST.","sentences":["Multiparty session typing (MPST) is a method to make concurrent programming simpler.","The idea is to use type checking to automatically detect safety and liveness violations of implementations relative to specifications.","In practice, the premier approach to combine MPST with mainstream languages -- in the absence of native support -- is based on external DSLs and associated tooling.   ","In contrast, we study the question of how to support MPST by using internal DSLs.","Answering this question positively, this paper presents the mpst.embedded library: it leverages Scala's lightweight form of dependent typing, called match types, to embed MPST directly into Scala.","Our internal-DSL-based approach avoids programming friction and leaky abstractions of the external-DSL-based approach for MPST."],"url":"http://arxiv.org/abs/2501.17741v1"}
{"created":"2025-01-29 16:27:43","title":"Attacker Control and Bug Prioritization","abstract":"As bug-finding methods improve, bug-fixing capabilities are exceeded, resulting in an accumulation of potential vulnerabilities. There is thus a need for efficient and precise bug prioritization based on exploitability. In this work, we explore the notion of control of an attacker over a vulnerability's parameters, which is an often overlooked factor of exploitability. We show that taint as well as straightforward qualitative and quantitative notions of control are not enough to effectively differentiate vulnerabilities. Instead, we propose to focus analysis on feasible value sets, which we call domains of control, in order to better take into account threat models and expert insight. Our new Shrink and Split algorithm efficiently extracts domains of control from path constraints obtained with symbolic execution and renders them in an easily processed, human-readable form. This in turn allows to automatically compute more complex control metrics, such as weighted Quantitative Control, which factors in the varying threat levels of different values. Experiments show that our method is both efficient and precise. In particular, it is the only one able to distinguish between vulnerabilities such as cve-2019-14192 and cve-2022-30552, while revealing a mistake in the human evaluation of cve-2022-30790. The high degree of automation of our tool also brings us closer to a fully-automated evaluation pipeline.","sentences":["As bug-finding methods improve, bug-fixing capabilities are exceeded, resulting in an accumulation of potential vulnerabilities.","There is thus a need for efficient and precise bug prioritization based on exploitability.","In this work, we explore the notion of control of an attacker over a vulnerability's parameters, which is an often overlooked factor of exploitability.","We show that taint as well as straightforward qualitative and quantitative notions of control are not enough to effectively differentiate vulnerabilities.","Instead, we propose to focus analysis on feasible value sets, which we call domains of control, in order to better take into account threat models and expert insight.","Our new Shrink and Split algorithm efficiently extracts domains of control from path constraints obtained with symbolic execution and renders them in an easily processed, human-readable form.","This in turn allows to automatically compute more complex control metrics, such as weighted Quantitative Control, which factors in the varying threat levels of different values.","Experiments show that our method is both efficient and precise.","In particular, it is the only one able to distinguish between vulnerabilities such as cve-2019-14192 and cve-2022-30552, while revealing a mistake in the human evaluation of cve-2022-30790.","The high degree of automation of our tool also brings us closer to a fully-automated evaluation pipeline."],"url":"http://arxiv.org/abs/2501.17740v1"}
{"created":"2025-01-29 16:27:13","title":"Testing Research Software: An In-Depth Survey of Practices, Methods, and Tools","abstract":"Context: Research software is essential for developing advanced tools and models to solve complex research problems and drive innovation across domains. Therefore, it is essential to ensure its correctness. Software testing plays a vital role in this task. However, testing research software is challenging due to the software's complexity and to the unique culture of the research software community. Aims: Building on previous research, this study provides an in-depth investigation of testing practices in research software, focusing on test case design, challenges with expected outputs, use of quality metrics, execution methods, tools, and desired tool features. Additionally, we explore whether demographic factors influence testing processes. Method: We survey research software developers to understand how they design test cases, handle output challenges, use metrics, execute tests, and select tools. Results: Research software testing varies widely. The primary challenges are test case design, evaluating test quality, and evaluating the correctness of test outputs. Overall, research software developers are not familiar with existing testing tools and have a need for new tools to support their specific needs. Conclusion: Allocating human resources to testing and providing developers with knowledge about effective testing techniques are important steps toward improving the testing process of research software. While many industrial testing tools exist, they are inadequate for testing research software due to its complexity, specialized algorithms, continuous updates, and need for flexible, custom testing approaches. Access to a standard set of testing tools that address these special characteristics will increase level of testing in research software development and reduce the overhead of distributing knowledge about software testing.","sentences":["Context: Research software is essential for developing advanced tools and models to solve complex research problems and drive innovation across domains.","Therefore, it is essential to ensure its correctness.","Software testing plays a vital role in this task.","However, testing research software is challenging due to the software's complexity and to the unique culture of the research software community.","Aims:","Building on previous research, this study provides an in-depth investigation of testing practices in research software, focusing on test case design, challenges with expected outputs, use of quality metrics, execution methods, tools, and desired tool features.","Additionally, we explore whether demographic factors influence testing processes.","Method: We survey research software developers to understand how they design test cases, handle output challenges, use metrics, execute tests, and select tools.","Results: Research software testing varies widely.","The primary challenges are test case design, evaluating test quality, and evaluating the correctness of test outputs.","Overall, research software developers are not familiar with existing testing tools and have a need for new tools to support their specific needs.","Conclusion: Allocating human resources to testing and providing developers with knowledge about effective testing techniques are important steps toward improving the testing process of research software.","While many industrial testing tools exist, they are inadequate for testing research software due to its complexity, specialized algorithms, continuous updates, and need for flexible, custom testing approaches.","Access to a standard set of testing tools that address these special characteristics will increase level of testing in research software development and reduce the overhead of distributing knowledge about software testing."],"url":"http://arxiv.org/abs/2501.17739v1"}
{"created":"2025-01-29 16:21:54","title":"Sparser, Better, Faster, Stronger: Efficient Automatic Differentiation for Sparse Jacobians and Hessians","abstract":"From implicit differentiation to probabilistic modeling, Jacobians and Hessians have many potential use cases in Machine Learning (ML), but conventional wisdom views them as computationally prohibitive. Fortunately, these matrices often exhibit sparsity, which can be leveraged to significantly speed up the process of Automatic Differentiation (AD). This paper presents advances in Automatic Sparse Differentiation (ASD), starting with a new perspective on sparsity detection. Our refreshed exposition is based on operator overloading, able to detect both local and global sparsity patterns, and naturally avoids dead ends in the control flow graph. We also describe a novel ASD pipeline in Julia, consisting of independent software packages for sparsity detection, matrix coloring, and differentiation, which together enable ASD based on arbitrary AD backends. Our pipeline is fully automatic and requires no modification of existing code, making it compatible with existing ML codebases. We demonstrate that this pipeline unlocks Jacobian and Hessian matrices at scales where they were considered too expensive to compute. On real-world problems from scientific ML and optimization, we show significant speed-ups of up to three orders of magnitude. Notably, our ASD pipeline often outperforms standard AD for one-off computations, once thought impractical due to slower sparsity detection methods.","sentences":["From implicit differentiation to probabilistic modeling, Jacobians and Hessians have many potential use cases in Machine Learning (ML), but conventional wisdom views them as computationally prohibitive.","Fortunately, these matrices often exhibit sparsity, which can be leveraged to significantly speed up the process of Automatic Differentiation (AD).","This paper presents advances in Automatic Sparse Differentiation (ASD), starting with a new perspective on sparsity detection.","Our refreshed exposition is based on operator overloading, able to detect both local and global sparsity patterns, and naturally avoids dead ends in the control flow graph.","We also describe a novel ASD pipeline in Julia, consisting of independent software packages for sparsity detection, matrix coloring, and differentiation, which together enable ASD based on arbitrary AD backends.","Our pipeline is fully automatic and requires no modification of existing code, making it compatible with existing ML codebases.","We demonstrate that this pipeline unlocks Jacobian and Hessian matrices at scales where they were considered too expensive to compute.","On real-world problems from scientific ML and optimization, we show significant speed-ups of up to three orders of magnitude.","Notably, our ASD pipeline often outperforms standard AD for one-off computations, once thought impractical due to slower sparsity detection methods."],"url":"http://arxiv.org/abs/2501.17737v1"}
{"created":"2025-01-29 16:16:37","title":"BitMLx: Secure Cross-chain Smart Contracts For Bitcoin-style Cryptocurrencies","abstract":"A smart contract is an interactive program that governs funds in the realm of a single cryptocurrency. Yet, the many existing cryptocurrencies have spurred the design of cross-chain applications that require interactions with multiple cryptocurrencies simultaneously. Currently, cross-chain applications are implemented as use-case-specific cryptographic protocols that serve as overlay to synchronize smart contract executions in the different cryptocurrencies. Hence, their design requires substantial expertise, as well as a security analysis in complex cryptographic frameworks.   In this work, we present BitMLx, the first domain-specific language for cross-chain smart contracts, enabling interactions with several users that hold funds across multiple Bitcoin-like cryptocurrencies. We contribute a compiler to automatically translate a BitMLx contract into one contract per involved cryptocurrency and a user strategy that synchronizes the execution of these contracts. We prove that an honest user, who follows the prescribed strategy when interacting with the several contracts, ends up with at least as many funds as in the corresponding execution of the BitMLx contract. Last, but not least, we implement the BitMLx compiler and demonstrate its utility in the design of illustrative examples of cross-chain applications such as multi-chain donations or loans across different cryptocurrencies.","sentences":["A smart contract is an interactive program that governs funds in the realm of a single cryptocurrency.","Yet, the many existing cryptocurrencies have spurred the design of cross-chain applications that require interactions with multiple cryptocurrencies simultaneously.","Currently, cross-chain applications are implemented as use-case-specific cryptographic protocols that serve as overlay to synchronize smart contract executions in the different cryptocurrencies.","Hence, their design requires substantial expertise, as well as a security analysis in complex cryptographic frameworks.   ","In this work, we present BitMLx, the first domain-specific language for cross-chain smart contracts, enabling interactions with several users that hold funds across multiple Bitcoin-like cryptocurrencies.","We contribute a compiler to automatically translate a BitMLx contract into one contract per involved cryptocurrency and a user strategy that synchronizes the execution of these contracts.","We prove that an honest user, who follows the prescribed strategy when interacting with the several contracts, ends up with at least as many funds as in the corresponding execution of the BitMLx contract.","Last, but not least, we implement the BitMLx compiler and demonstrate its utility in the design of illustrative examples of cross-chain applications such as multi-chain donations or loans across different cryptocurrencies."],"url":"http://arxiv.org/abs/2501.17733v1"}
{"created":"2025-01-29 16:15:31","title":"Gateways for Institutional-Grade Commerce and Interoperability of Digital Assets","abstract":"It is time for the legacy financial infrastructure to seamlessly connect with modern, decentralized infrastructure. Although it is increasingly evident that decentralized infrastructure for finance (namely distributed ledgers) will coexist with and complement legacy infrastructure, it is also clear that such interoperability efforts carry new risks and concerns. In particular, managing the range of heterogeneous (and not well-established) infrastructure brings security, privacy, and regulatory issues. The first step to overcome some of these challenges is to recognize that in many deployment instances using distributed ledgers, the purpose of the ledger is to share resources among the community members. The second step after recognizing that borders exist is to understand that interoperability across systems can be best achieved through the use of standardized service interfaces (or application programming interfaces (API)). In this paper we use the term ledger gateways (or simply gateways) to denote the computer and software systems that implement the standardized service interfaces into a distributed ledger. The main purpose of a gateway is to communicate with other peer gateways that implement the same standardized service interface. Among others, peer gateways perform the transfer of data and value across borders (legal or national borders). Gateways also become a mechanism to manage a permissioned environment, where abiding by laws and regulations is crucial for business compliance (e.g., EU General Data Protection Regulations (GDPR), EU MiCa regulation on digital assets, FAFT Recommendation 15, ISO 27001.","sentences":["It is time for the legacy financial infrastructure to seamlessly connect with modern, decentralized infrastructure.","Although it is increasingly evident that decentralized infrastructure for finance (namely distributed ledgers) will coexist with and complement legacy infrastructure, it is also clear that such interoperability efforts carry new risks and concerns.","In particular, managing the range of heterogeneous (and not well-established) infrastructure brings security, privacy, and regulatory issues.","The first step to overcome some of these challenges is to recognize that in many deployment instances using distributed ledgers, the purpose of the ledger is to share resources among the community members.","The second step after recognizing that borders exist is to understand that interoperability across systems can be best achieved through the use of standardized service interfaces (or application programming interfaces (API)).","In this paper we use the term ledger gateways (or simply gateways) to denote the computer and software systems that implement the standardized service interfaces into a distributed ledger.","The main purpose of a gateway is to communicate with other peer gateways that implement the same standardized service interface.","Among others, peer gateways perform the transfer of data and value across borders (legal or national borders).","Gateways also become a mechanism to manage a permissioned environment, where abiding by laws and regulations is crucial for business compliance (e.g., EU General Data Protection Regulations (GDPR), EU MiCa regulation on digital assets, FAFT Recommendation 15, ISO 27001."],"url":"http://arxiv.org/abs/2501.17732v1"}
{"created":"2025-01-29 16:11:12","title":"Sparse Autoencoders Can Interpret Randomly Initialized Transformers","abstract":"Sparse autoencoders (SAEs) are an increasingly popular technique for interpreting the internal representations of transformers. In this paper, we apply SAEs to 'interpret' random transformers, i.e., transformers where the parameters are sampled IID from a Gaussian rather than trained on text data. We find that random and trained transformers produce similarly interpretable SAE latents, and we confirm this finding quantitatively using an open-source auto-interpretability pipeline. Further, we find that SAE quality metrics are broadly similar for random and trained transformers. We find that these results hold across model sizes and layers. We discuss a number of number interesting questions that this work raises for the use of SAEs and auto-interpretability in the context of mechanistic interpretability.","sentences":["Sparse autoencoders (SAEs) are an increasingly popular technique for interpreting the internal representations of transformers.","In this paper, we apply SAEs to 'interpret' random transformers, i.e., transformers where the parameters are sampled IID from a Gaussian rather than trained on text data.","We find that random and trained transformers produce similarly interpretable SAE latents, and we confirm this finding quantitatively using an open-source auto-interpretability pipeline.","Further, we find that SAE quality metrics are broadly similar for random and trained transformers.","We find that these results hold across model sizes and layers.","We discuss a number of number interesting questions that this work raises for the use of SAEs and auto-interpretability in the context of mechanistic interpretability."],"url":"http://arxiv.org/abs/2501.17727v1"}
{"created":"2025-01-29 16:02:16","title":"VICCA: Visual Interpretation and Comprehension of Chest X-ray Anomalies in Generated Report Without Human Feedback","abstract":"As artificial intelligence (AI) becomes increasingly central to healthcare, the demand for explainable and trustworthy models is paramount. Current report generation systems for chest X-rays (CXR) often lack mechanisms for validating outputs without expert oversight, raising concerns about reliability and interpretability. To address these challenges, we propose a novel multimodal framework designed to enhance the semantic alignment and localization accuracy of AI-generated medical reports. Our framework integrates two key modules: a Phrase Grounding Model, which identifies and localizes pathologies in CXR images based on textual prompts, and a Text-to-Image Diffusion Module, which generates synthetic CXR images from prompts while preserving anatomical fidelity. By comparing features between the original and generated images, we introduce a dual-scoring system: one score quantifies localization accuracy, while the other evaluates semantic consistency. This approach significantly outperforms existing methods, achieving state-of-the-art results in pathology localization and text-to-image alignment. The integration of phrase grounding with diffusion models, coupled with the dual-scoring evaluation system, provides a robust mechanism for validating report quality, paving the way for more trustworthy and transparent AI in medical imaging.","sentences":["As artificial intelligence (AI) becomes increasingly central to healthcare, the demand for explainable and trustworthy models is paramount.","Current report generation systems for chest X-rays (CXR) often lack mechanisms for validating outputs without expert oversight, raising concerns about reliability and interpretability.","To address these challenges, we propose a novel multimodal framework designed to enhance the semantic alignment and localization accuracy of AI-generated medical reports.","Our framework integrates two key modules: a Phrase Grounding Model, which identifies and localizes pathologies in CXR images based on textual prompts, and a Text-to-Image Diffusion Module, which generates synthetic CXR images from prompts while preserving anatomical fidelity.","By comparing features between the original and generated images, we introduce a dual-scoring system: one score quantifies localization accuracy, while the other evaluates semantic consistency.","This approach significantly outperforms existing methods, achieving state-of-the-art results in pathology localization and text-to-image alignment.","The integration of phrase grounding with diffusion models, coupled with the dual-scoring evaluation system, provides a robust mechanism for validating report quality, paving the way for more trustworthy and transparent AI in medical imaging."],"url":"http://arxiv.org/abs/2501.17726v1"}
{"created":"2025-01-29 15:57:43","title":"Using Code Generation to Solve Open Instances of Combinatorial Design Problems","abstract":"The Handbook of Combinatorial Designs catalogs many types of combinatorial designs, together with lists of open instances for which existence has not yet been determined. We develop a constructive protocol CPro1, which uses Large Language Models (LLMs) to generate code that constructs combinatorial designs and resolves some of these open instances. The protocol starts from a definition of a particular type of design, and a verifier that reliably confirms whether a proposed design is valid. The LLM selects strategies and implements them in code, and scaffolding provides automated hyperparameter tuning and execution feedback using the verifier. Most generated code fails, but by generating many candidates, the protocol automates exploration of a variety of standard methods (e.g. simulated annealing, genetic algorithms) and experimentation with variations (e.g. cost functions) to find successful approaches. Testing on 16 different types of designs, CPro1 constructs solutions to open instances for 6 of them: Symmetric and Skew Weighing Matrices, Equidistant Permutation Arrays, Packing Arrays, Balanced Ternary Designs, and Florentine Rectangles.","sentences":["The Handbook of Combinatorial Designs catalogs many types of combinatorial designs, together with lists of open instances for which existence has not yet been determined.","We develop a constructive protocol CPro1, which uses Large Language Models (LLMs) to generate code that constructs combinatorial designs and resolves some of these open instances.","The protocol starts from a definition of a particular type of design, and a verifier that reliably confirms whether a proposed design is valid.","The LLM selects strategies and implements them in code, and scaffolding provides automated hyperparameter tuning and execution feedback using the verifier.","Most generated code fails, but by generating many candidates, the protocol automates exploration of a variety of standard methods (e.g. simulated annealing, genetic algorithms) and experimentation with variations (e.g. cost functions) to find successful approaches.","Testing on 16 different types of designs, CPro1 constructs solutions to open instances for 6 of them:","Symmetric and Skew Weighing Matrices, Equidistant Permutation Arrays, Packing Arrays, Balanced Ternary Designs, and Florentine Rectangles."],"url":"http://arxiv.org/abs/2501.17725v1"}
{"created":"2025-01-29 15:46:06","title":"Parsimonious Hawkes Processes for temporal networks modelling","abstract":"Temporal networks are characterised by interdependent link events between nodes, forming ordered sequences of links that may represent specific information flows in the system. Nevertheless, representing temporal networks using discrete snapshots in time partially cancels the effect of time-ordered links on each other, while continuous time models, such as Poisson or Hawkes processes, can describe the full influence between all the potential pairs of links at all times. In this paper, we introduce a continuous Hawkes temporal network model which accounts both for a community structure of the aggregate network and a strong heterogeneity in the activity of individual nodes, thus accounting for the presence of highly heterogeneous clusters with isolated high-activity influencer nodes, communities and low-activity nodes. Our model improves the prediction performance of previously available continuous time network models, and obtains a systematic increase in log-likelihood. Characterising the direct interaction between influencer nodes and communities, we can provide a more detailed description of the system that can better outline the sequence of activations in the components of the systems represented by temporal networks.","sentences":["Temporal networks are characterised by interdependent link events between nodes, forming ordered sequences of links that may represent specific information flows in the system.","Nevertheless, representing temporal networks using discrete snapshots in time partially cancels the effect of time-ordered links on each other, while continuous time models, such as Poisson or Hawkes processes, can describe the full influence between all the potential pairs of links at all times.","In this paper, we introduce a continuous Hawkes temporal network model which accounts both for a community structure of the aggregate network and a strong heterogeneity in the activity of individual nodes, thus accounting for the presence of highly heterogeneous clusters with isolated high-activity influencer nodes, communities and low-activity nodes.","Our model improves the prediction performance of previously available continuous time network models, and obtains a systematic increase in log-likelihood.","Characterising the direct interaction between influencer nodes and communities, we can provide a more detailed description of the system that can better outline the sequence of activations in the components of the systems represented by temporal networks."],"url":"http://arxiv.org/abs/2501.17720v1"}
{"created":"2025-01-29 15:40:42","title":"Learning Semantic Facial Descriptors for Accurate Face Animation","abstract":"Face animation is a challenging task. Existing model-based methods (utilizing 3DMMs or landmarks) often result in a model-like reconstruction effect, which doesn't effectively preserve identity. Conversely, model-free approaches face challenges in attaining a decoupled and semantically rich feature space, thereby making accurate motion transfer difficult to achieve. We introduce the semantic facial descriptors in learnable disentangled vector space to address the dilemma. The approach involves decoupling the facial space into identity and motion subspaces while endowing each of them with semantics by learning complete orthogonal basis vectors. We obtain basis vector coefficients by employing an encoder on the source and driving faces, leading to effective facial descriptors in the identity and motion subspaces. Ultimately, these descriptors can be recombined as latent codes to animate faces. Our approach successfully addresses the issue of model-based methods' limitations in high-fidelity identity and the challenges faced by model-free methods in accurate motion transfer. Extensive experiments are conducted on three challenging benchmarks (i.e. VoxCeleb, HDTF, CelebV). Comprehensive quantitative and qualitative results demonstrate that our model outperforms SOTA methods with superior identity preservation and motion transfer.","sentences":["Face animation is a challenging task.","Existing model-based methods (utilizing 3DMMs or landmarks) often result in a model-like reconstruction effect, which doesn't effectively preserve identity.","Conversely, model-free approaches face challenges in attaining a decoupled and semantically rich feature space, thereby making accurate motion transfer difficult to achieve.","We introduce the semantic facial descriptors in learnable disentangled vector space to address the dilemma.","The approach involves decoupling the facial space into identity and motion subspaces while endowing each of them with semantics by learning complete orthogonal basis vectors.","We obtain basis vector coefficients by employing an encoder on the source and driving faces, leading to effective facial descriptors in the identity and motion subspaces.","Ultimately, these descriptors can be recombined as latent codes to animate faces.","Our approach successfully addresses the issue of model-based methods' limitations in high-fidelity identity and the challenges faced by model-free methods in accurate motion transfer.","Extensive experiments are conducted on three challenging benchmarks (i.e. VoxCeleb, HDTF, CelebV).","Comprehensive quantitative and qualitative results demonstrate that our model outperforms SOTA methods with superior identity preservation and motion transfer."],"url":"http://arxiv.org/abs/2501.17718v1"}
{"created":"2025-01-29 15:32:27","title":"RICoTA: Red-teaming of In-the-wild Conversation with Test Attempts","abstract":"User interactions with conversational agents (CAs) evolve in the era of heavily guardrailed large language models (LLMs). As users push beyond programmed boundaries to explore and build relationships with these systems, there is a growing concern regarding the potential for unauthorized access or manipulation, commonly referred to as \"jailbreaking.\" Moreover, with CAs that possess highly human-like qualities, users show a tendency toward initiating intimate sexual interactions or attempting to tame their chatbots. To capture and reflect these in-the-wild interactions into chatbot designs, we propose RICoTA, a Korean red teaming dataset that consists of 609 prompts challenging LLMs with in-the-wild user-made dialogues capturing jailbreak attempts. We utilize user-chatbot conversations that were self-posted on a Korean Reddit-like community, containing specific testing and gaming intentions with a social chatbot. With these prompts, we aim to evaluate LLMs' ability to identify the type of conversation and users' testing purposes to derive chatbot design implications for mitigating jailbreaking risks. Our dataset will be made publicly available via GitHub.","sentences":["User interactions with conversational agents (CAs) evolve in the era of heavily guardrailed large language models (LLMs).","As users push beyond programmed boundaries to explore and build relationships with these systems, there is a growing concern regarding the potential for unauthorized access or manipulation, commonly referred to as \"jailbreaking.\"","Moreover, with CAs that possess highly human-like qualities, users show a tendency toward initiating intimate sexual interactions or attempting to tame their chatbots.","To capture and reflect these in-the-wild interactions into chatbot designs, we propose RICoTA, a Korean red teaming dataset that consists of 609 prompts challenging LLMs with in-the-wild user-made dialogues capturing jailbreak attempts.","We utilize user-chatbot conversations that were self-posted on a Korean Reddit-like community, containing specific testing and gaming intentions with a social chatbot.","With these prompts, we aim to evaluate LLMs' ability to identify the type of conversation and users' testing purposes to derive chatbot design implications for mitigating jailbreaking risks.","Our dataset will be made publicly available via GitHub."],"url":"http://arxiv.org/abs/2501.17715v1"}
{"created":"2025-01-29 15:28:06","title":"STGCN-LSTM for Olympic Medal Prediction: Dynamic Power Modeling and Causal Policy Optimization","abstract":"This paper proposes a novel hybrid model, STGCN-LSTM, to forecast Olympic medal distributions by integrating the spatio-temporal relationships among countries and the long-term dependencies of national performance. The Spatial-Temporal Graph Convolution Network (STGCN) captures geographic and interactive factors-such as coaching exchange and socio-economic links-while the Long Short-Term Memory (LSTM) module models historical trends in medal counts, economic data, and demographics. To address zero-inflated outputs (i.e., the disparity between countries that consistently yield wins and those never having won medals), a Zero-Inflated Compound Poisson (ZICP) framework is incorporated to separate random zeros from structural zeros, providing a clearer view of potential breakthrough performances. Validation includes historical backtracking, policy shock simulations, and causal inference checks, confirming the robustness of the proposed method. Results shed light on the influence of coaching mobility, event specialization, and strategic investment on medal forecasts, offering a data-driven foundation for optimizing sports policies and resource allocation in diverse Olympic contexts.","sentences":["This paper proposes a novel hybrid model, STGCN-LSTM, to forecast Olympic medal distributions by integrating the spatio-temporal relationships among countries and the long-term dependencies of national performance.","The Spatial-Temporal Graph Convolution Network (STGCN) captures geographic and interactive factors-such as coaching exchange and socio-economic links-while the Long Short-Term Memory (LSTM) module models historical trends in medal counts, economic data, and demographics.","To address zero-inflated outputs (i.e., the disparity between countries that consistently yield wins and those never having won medals), a Zero-Inflated Compound Poisson (ZICP) framework is incorporated to separate random zeros from structural zeros, providing a clearer view of potential breakthrough performances.","Validation includes historical backtracking, policy shock simulations, and causal inference checks, confirming the robustness of the proposed method.","Results shed light on the influence of coaching mobility, event specialization, and strategic investment on medal forecasts, offering a data-driven foundation for optimizing sports policies and resource allocation in diverse Olympic contexts."],"url":"http://arxiv.org/abs/2501.17711v1"}
{"created":"2025-01-29 15:23:34","title":"Improved fixed-parameter bounds for Min-Sum-Radii and Diameters $k$-clustering and their fair variants","abstract":"We provide improved upper and lower bounds for the Min-Sum-Radii (MSR) and Min-Sum-Diameters (MSD) clustering problems with a bounded number of clusters $k$. In particular, we propose an exact MSD algorithm with running-time $n^{O(k)}$. We also provide $(1+\\epsilon)$ approximation algorithms for both MSR and MSD with running-times of $O(kn) +(1/\\epsilon)^{O(dk)}$ in metrics spaces of doubling dimension $d$. Our algorithms extend to $k$-center, improving upon previous results, and to $\\alpha$-MSR, where radii are raised to the $\\alpha$ power for $\\alpha>1$. For $\\alpha$-MSD we prove an exponential time ETH-based lower bound for $\\alpha>\\log 3$. All algorithms can also be modified to handle outliers. Moreover, we can extend the results to variants that observe \\emph{fairness} constraints, as well as to the general framework of \\emph{mergeable} clustering, which includes many other popular clustering variants. We complement these upper bounds with ETH-based lower bounds for these problems, in particular proving that $n^{O(k)}$ time is tight for MSR and $\\alpha$-MSR even in doubling spaces, and that $2^{o(k)}$ bounds are impossible for MSD.","sentences":["We provide improved upper and lower bounds for the Min-Sum-Radii (MSR) and Min-Sum-Diameters (MSD) clustering problems with a bounded number of clusters $k$. In particular, we propose an exact MSD algorithm with running-time $n^{O(k)}$. We also provide $(1+\\epsilon)$ approximation algorithms for both MSR and MSD with running-times of $O(kn)","+(1/\\epsilon)^{O(dk)}$ in metrics spaces of doubling dimension $d$.","Our algorithms extend to $k$-center, improving upon previous results, and to $\\alpha$-MSR, where radii are raised to the $\\alpha$ power for $\\alpha>1$. For $\\alpha$-MSD we prove an exponential time ETH-based lower bound for $\\alpha>\\log 3$.","All algorithms can also be modified to handle outliers.","Moreover, we can extend the results to variants that observe \\emph{fairness} constraints, as well as to the general framework of \\emph{mergeable} clustering, which includes many other popular clustering variants.","We complement these upper bounds with ETH-based lower bounds for these problems, in particular proving that $n^{O(k)}$ time is tight for MSR and $\\alpha$-MSR even in doubling spaces, and that $2^{o(k)}$ bounds are impossible for MSD."],"url":"http://arxiv.org/abs/2501.17708v1"}
{"created":"2025-01-29 15:22:19","title":"Ownership-based Virtual Memory for Intermittently-Powered Embedded Systems","abstract":"The Battery-Free Internet of Things might revolutionize our understanding of connected devices, which harvest their operational energy from the environment (e.g., using solar cells). These systems come with the major system-software challenge that the intermittently-powered IoT devices have to checkpoint their state in non-volatile memory to later resume operation with this state when sufficient energy is available. The scarce energy resources demand that only modified data is persisted to non-volatile memory before a power failure, which requires precise modification-tracking.   In this paper, we present the first virtually Non-Volatile Heap (vNV-Heap) abstraction for intermittently-powered systems with guaranteed power-failure resilience and non-volatile memory safety (analogous to memory-safety for RAM). The heap exploits ownership systems, a zero-cost (i.e., compile-time) abstraction for example implemented by Rust, to track modifications and virtualize object-persistence. To achieve power-failure resilience, our heap is designed and implemented to guarantee bounded operations by static program code analysis: As an example, the heap allows for determining a worst-case energy consumption for the operation of persisting modified and currently volatile objects. Our evaluations with our open-source implementation on an embedded hardware platform (i.e., ESP32-C3) shows that using our heap abstraction is more energy-efficient than existing approaches, while also providing runtime guarantees by static worst-case analysis bounds.","sentences":["The Battery-Free Internet of Things might revolutionize our understanding of connected devices, which harvest their operational energy from the environment (e.g., using solar cells).","These systems come with the major system-software challenge that the intermittently-powered IoT devices have to checkpoint their state in non-volatile memory to later resume operation with this state when sufficient energy is available.","The scarce energy resources demand that only modified data is persisted to non-volatile memory before a power failure, which requires precise modification-tracking.   ","In this paper, we present the first virtually Non-Volatile Heap (vNV-Heap) abstraction for intermittently-powered systems with guaranteed power-failure resilience and non-volatile memory safety (analogous to memory-safety for RAM).","The heap exploits ownership systems, a zero-cost (i.e., compile-time) abstraction for example implemented by Rust, to track modifications and virtualize object-persistence.","To achieve power-failure resilience, our heap is designed and implemented to guarantee bounded operations by static program code analysis: As an example, the heap allows for determining a worst-case energy consumption for the operation of persisting modified and currently volatile objects.","Our evaluations with our open-source implementation on an embedded hardware platform (i.e., ESP32-C3) shows that using our heap abstraction is more energy-efficient than existing approaches, while also providing runtime guarantees by static worst-case analysis bounds."],"url":"http://arxiv.org/abs/2501.17707v1"}
{"created":"2025-01-29 15:21:56","title":"Source-Channel Separation Theorems for Distortion Perception Coding","abstract":"It is well known that separation between lossy source coding and channel coding is asymptotically optimal under classical additive distortion measures. Recently, coding under a new class of quality considerations, often referred to as perception or realism, has attracted significant attention due to its close connection to neural generative models and semantic communications. In this work, we revisit source-channel separation under the consideration of distortion-perception. We show that when the perception quality is measured on the block level, i.e., in the strong-sense, the optimality of separation still holds when common randomness is shared between the encoder and the decoder; however, separation is no longer optimal when such common randomness is not available. In contrast, when the perception quality is the average per-symbol measure, i.e., in the weak-sense, the optimality of separation holds regardless of the availability of common randomness.","sentences":["It is well known that separation between lossy source coding and channel coding is asymptotically optimal under classical additive distortion measures.","Recently, coding under a new class of quality considerations, often referred to as perception or realism, has attracted significant attention due to its close connection to neural generative models and semantic communications.","In this work, we revisit source-channel separation under the consideration of distortion-perception.","We show that when the perception quality is measured on the block level, i.e., in the strong-sense, the optimality of separation still holds when common randomness is shared between the encoder and the decoder; however, separation is no longer optimal when such common randomness is not available.","In contrast, when the perception quality is the average per-symbol measure, i.e., in the weak-sense, the optimality of separation holds regardless of the availability of common randomness."],"url":"http://arxiv.org/abs/2501.17706v1"}
{"created":"2025-01-29 15:20:43","title":"Inferring Implicit Goals Across Differing Task Models","abstract":"One of the significant challenges to generating value-aligned behavior is to not only account for the specified user objectives but also any implicit or unspecified user requirements. The existence of such implicit requirements could be particularly common in settings where the user's understanding of the task model may differ from the agent's estimate of the model. Under this scenario, the user may incorrectly expect some agent behavior to be inevitable or guaranteed. This paper addresses such expectation mismatch in the presence of differing models by capturing the possibility of unspecified user subgoal in the context of a task captured as a Markov Decision Process (MDP) and querying for it as required. Our method identifies bottleneck states and uses them as candidates for potential implicit subgoals. We then introduce a querying strategy that will generate the minimal number of queries required to identify a policy guaranteed to achieve the underlying goal. Our empirical evaluations demonstrate the effectiveness of our approach in inferring and achieving unstated goals across various tasks.","sentences":["One of the significant challenges to generating value-aligned behavior is to not only account for the specified user objectives but also any implicit or unspecified user requirements.","The existence of such implicit requirements could be particularly common in settings where the user's understanding of the task model may differ from the agent's estimate of the model.","Under this scenario, the user may incorrectly expect some agent behavior to be inevitable or guaranteed.","This paper addresses such expectation mismatch in the presence of differing models by capturing the possibility of unspecified user subgoal in the context of a task captured as a Markov Decision Process (MDP) and querying for it as required.","Our method identifies bottleneck states and uses them as candidates for potential implicit subgoals.","We then introduce a querying strategy that will generate the minimal number of queries required to identify a policy guaranteed to achieve the underlying goal.","Our empirical evaluations demonstrate the effectiveness of our approach in inferring and achieving unstated goals across various tasks."],"url":"http://arxiv.org/abs/2501.17704v1"}
{"created":"2025-01-29 15:20:30","title":"Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate","abstract":"Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions. In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct ones. Inspired by human learning processes that emphasize critical thinking, CFT encourages deeper analysis and nuanced understanding-traits often overlooked by standard SFT. To validate the effectiveness of CFT, we construct a 50K-sample dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of (input=[query; noisy response], output=critique). CFT on this dataset yields a consistent 4-10% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT. Notably, our Qwen2.5-Math-CFT model-trained on just 50K samples-matches or outperforms competitive models such as AceMath and Qwen2.5-Math-Instruct on most benchmarks, both of which use over 2M samples. Ablation studies show that CFT is robust to the source of noisy response and teacher critique model. Through these findings, we argue that critique-based training offers a more effective alternative to advance the reasoning of language models.","sentences":["Supervised Fine-Tuning (SFT) is commonly used to train language models to imitate annotated responses for given instructions.","In this paper, we challenge this paradigm and propose Critique Fine-Tuning (CFT), a strategy where models learn to critique noisy responses rather than simply imitate correct ones.","Inspired by human learning processes that emphasize critical thinking, CFT encourages deeper analysis and nuanced understanding-traits often overlooked by standard SFT.","To validate the effectiveness of CFT, we construct a 50K-sample dataset from WebInstruct, using GPT-4o as the teacher to generate critiques in the form of (input=[query; noisy response], output=critique).","CFT on this dataset yields a consistent 4-10% improvement over SFT on six math benchmarks with different base models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math.","We further expand to MetaMath and NuminaMath datasets and observe similar gains over SFT.","Notably, our Qwen2.5-Math-CFT model-trained on just 50K samples-matches or outperforms competitive models such as AceMath and Qwen2.5-Math-Instruct on most benchmarks, both of which use over 2M samples.","Ablation studies show that CFT is robust to the source of noisy response and teacher critique model.","Through these findings, we argue that critique-based training offers a more effective alternative to advance the reasoning of language models."],"url":"http://arxiv.org/abs/2501.17703v1"}
{"created":"2025-01-29 15:16:27","title":"Decision-Theoretic Approaches in Learning-Augmented Algorithms","abstract":"In this work, we initiate the systemic study of decision-theoretic metrics in the design and analysis of algorithms with machine-learned predictions. We introduce approaches based on both deterministic measures such as distance-based evaluation, that help us quantify how close the algorithm is to an ideal solution, as well as stochastic measures that allow us to balance the trade-off between the algorithm's performance and the risk associated with the imperfect oracle. These approaches help us quantify the algorithmic performance across the entire spectrum of prediction error, unlike several previous works that focus on few, and often extreme values of the error. We apply these techniques to two well-known problems from resource allocation and online decision making, namely contract scheduling and 1-max search.","sentences":["In this work, we initiate the systemic study of decision-theoretic metrics in the design and analysis of algorithms with machine-learned predictions.","We introduce approaches based on both deterministic measures such as distance-based evaluation, that help us quantify how close the algorithm is to an ideal solution, as well as stochastic measures that allow us to balance the trade-off between the algorithm's performance and the risk associated with the imperfect oracle.","These approaches help us quantify the algorithmic performance across the entire spectrum of prediction error, unlike several previous works that focus on few, and often extreme values of the error.","We apply these techniques to two well-known problems from resource allocation and online decision making, namely contract scheduling and 1-max search."],"url":"http://arxiv.org/abs/2501.17701v1"}
{"created":"2025-01-29 14:58:48","title":"Segmentation-Aware Generative Reinforcement Network (GRN) for Tissue Layer Segmentation in 3-D Ultrasound Images for Chronic Low-back Pain (cLBP) Assessment","abstract":"We introduce a novel segmentation-aware joint training framework called generative reinforcement network (GRN) that integrates segmentation loss feedback to optimize both image generation and segmentation performance in a single stage. An image enhancement technique called segmentation-guided enhancement (SGE) is also developed, where the generator produces images tailored specifically for the segmentation model. Two variants of GRN were also developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for semi-supervised learning (GRN-SSL). GRN's performance was evaluated using a dataset of 69 fully annotated 3D ultrasound scans from 29 subjects. The annotations included six anatomical structures: dermis, superficial fat, superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and muscle. Our results show that GRN-SEL with SGE reduces labeling efforts by up to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient (DSC) compared to models trained on fully labeled datasets. GRN-SEL alone reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling requirements by 70%, and GRN-SSL alone by 60%, all while maintaining performance comparable to fully supervised models. These findings suggest the effectiveness of the GRN framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for ultrasound image analysis and reducing the burdens associated with data annotation.","sentences":["We introduce a novel segmentation-aware joint training framework called generative reinforcement network (GRN) that integrates segmentation loss feedback to optimize both image generation and segmentation performance in a single stage.","An image enhancement technique called segmentation-guided enhancement (SGE) is also developed, where the generator produces images tailored specifically for the segmentation model.","Two variants of GRN were also developed, including GRN for sample-efficient learning (GRN-SEL) and GRN for semi-supervised learning (GRN-SSL).","GRN's performance was evaluated using a dataset of 69 fully annotated 3D ultrasound scans from 29 subjects.","The annotations included six anatomical structures: dermis, superficial fat, superficial fascial membrane (SFM), deep fat, deep fascial membrane (DFM), and muscle.","Our results show that GRN-SEL with SGE reduces labeling efforts by up to 70% while achieving a 1.98% improvement in the Dice Similarity Coefficient (DSC) compared to models trained on fully labeled datasets.","GRN-SEL alone reduces labeling efforts by 60%, GRN-SSL with SGE decreases labeling requirements by 70%, and GRN-SSL alone by 60%, all while maintaining performance comparable to fully supervised models.","These findings suggest the effectiveness of the GRN framework in optimizing segmentation performance with significantly less labeled data, offering a scalable and efficient solution for ultrasound image analysis and reducing the burdens associated with data annotation."],"url":"http://arxiv.org/abs/2501.17690v1"}
{"created":"2025-01-29 14:56:27","title":"ContourFormer:Real-Time Contour-Based End-to-End Instance Segmentation Transformer","abstract":"This paper presents Contourformer, a real-time contour-based instance segmentation algorithm. The method is fully based on the DETR paradigm and achieves end-to-end inference through iterative and progressive mechanisms to optimize contours. To improve efficiency and accuracy, we develop two novel techniques: sub-contour decoupling mechanisms and contour fine-grained distribution refinement.In the sub-contour decoupling mechanism, we propose a deformable attention-based module that adaptively selects sampling regions based on the current predicted contour, enabling more effective capturing of object boundary information. Additionally, we design a multi-stage optimization process to enhance segmentation precision by progressively refining sub-contours. The contour fine-grained distribution refinement technique aims to further improve the ability to express fine details of contours.These innovations enable Contourformer to achieve stable and precise segmentation for each instance while maintaining real-time performance. Extensive experiments demonstrate the superior performance of Contourformer on multiple benchmark datasets, including SBD, COCO, and KINS. We conduct comprehensive evaluations and comparisons with existing state-of-the-art methods, showing significant improvements in both accuracy and inference speed.This work provides a new solution for contour-based instance segmentation tasks and lays a foundation for future research, with the potential to become a strong baseline method in this field.","sentences":["This paper presents Contourformer, a real-time contour-based instance segmentation algorithm.","The method is fully based on the DETR paradigm and achieves end-to-end inference through iterative and progressive mechanisms to optimize contours.","To improve efficiency and accuracy, we develop two novel techniques: sub-contour decoupling mechanisms and contour fine-grained distribution refinement.","In the sub-contour decoupling mechanism, we propose a deformable attention-based module that adaptively selects sampling regions based on the current predicted contour, enabling more effective capturing of object boundary information.","Additionally, we design a multi-stage optimization process to enhance segmentation precision by progressively refining sub-contours.","The contour fine-grained distribution refinement technique aims to further improve the ability to express fine details of contours.","These innovations enable Contourformer to achieve stable and precise segmentation for each instance while maintaining real-time performance.","Extensive experiments demonstrate the superior performance of Contourformer on multiple benchmark datasets, including SBD, COCO, and KINS.","We conduct comprehensive evaluations and comparisons with existing state-of-the-art methods, showing significant improvements in both accuracy and inference speed.","This work provides a new solution for contour-based instance segmentation tasks and lays a foundation for future research, with the potential to become a strong baseline method in this field."],"url":"http://arxiv.org/abs/2501.17688v1"}
{"created":"2025-01-29 14:43:31","title":"Open-Source ESP32-C3 Wi-Fi Drivers for Static Analysis","abstract":"The Battery-Free Internet of Things might revolutionize our understanding of sustainable communication, as these IoT devices operate on harvested energy. As this energy can change unpredictably, device operations, including those of its network stack, must be resilient to power loss. Transactional intermittency approaches break down tasks into atomic sub-tasks that can be executed free from power failures when sufficient energy is available. For this operation, static program-code analysis methods are required to analyse the worst-case energy consumption (WCEC) of transactions. However, static code analysis require the availability of all code and its semantics. In the case of Wi-Fi-capable devices, Wi-Fi drivers are closed-source and therefore the energy required for physical layer operations cannot be evaluated. In this work, we integrate a transactional network stack with reverse-engineered Wi-Fi drivers to enable WCEC analysis for physical transmission and reception of packets. Our evaluations with the RISC-V--based ESP32-C3 platform validate that we are able to give worst-case bounds with our static analysis for the operations of the Wi-Fi modem.","sentences":["The Battery-Free Internet of Things might revolutionize our understanding of sustainable communication, as these IoT devices operate on harvested energy.","As this energy can change unpredictably, device operations, including those of its network stack, must be resilient to power loss.","Transactional intermittency approaches break down tasks into atomic sub-tasks that can be executed free from power failures when sufficient energy is available.","For this operation, static program-code analysis methods are required to analyse the worst-case energy consumption (WCEC) of transactions.","However, static code analysis require the availability of all code and its semantics.","In the case of Wi-Fi-capable devices, Wi-Fi drivers are closed-source and therefore the energy required for physical layer operations cannot be evaluated.","In this work, we integrate a transactional network stack with reverse-engineered Wi-Fi drivers to enable WCEC analysis for physical transmission and reception of packets.","Our evaluations with the RISC-V--based ESP32-C3 platform validate that we are able to give worst-case bounds with our static analysis for the operations of the Wi-Fi modem."],"url":"http://arxiv.org/abs/2501.17684v1"}
{"created":"2025-01-29 14:43:21","title":"Temperature-Free Loss Function for Contrastive Learning","abstract":"As one of the most promising methods in self-supervised learning, contrastive learning has achieved a series of breakthroughs across numerous fields. A predominant approach to implementing contrastive learning is applying InfoNCE loss: By capturing the similarities between pairs, InfoNCE loss enables learning the representation of data. Albeit its success, adopting InfoNCE loss requires tuning a temperature, which is a core hyperparameter for calibrating similarity scores. Despite its significance and sensitivity to performance being emphasized by several studies, searching for a valid temperature requires extensive trial-and-error-based experiments, which increases the difficulty of adopting InfoNCE loss. To address this difficulty, we propose a novel method to deploy InfoNCE loss without temperature. Specifically, we replace temperature scaling with the inverse hyperbolic tangent function, resulting in a modified InfoNCE loss. In addition to hyperparameter-free deployment, we observed that the proposed method even yielded a performance gain in contrastive learning. Our detailed theoretical analysis discovers that the current practice of temperature scaling in InfoNCE loss causes serious problems in gradient descent, whereas our method provides desirable gradient properties. The proposed method was validated on five benchmarks on contrastive learning, yielding satisfactory results without temperature tuning.","sentences":["As one of the most promising methods in self-supervised learning, contrastive learning has achieved a series of breakthroughs across numerous fields.","A predominant approach to implementing contrastive learning is applying InfoNCE loss: By capturing the similarities between pairs, InfoNCE loss enables learning the representation of data.","Albeit its success, adopting InfoNCE loss requires tuning a temperature, which is a core hyperparameter for calibrating similarity scores.","Despite its significance and sensitivity to performance being emphasized by several studies, searching for a valid temperature requires extensive trial-and-error-based experiments, which increases the difficulty of adopting InfoNCE loss.","To address this difficulty, we propose a novel method to deploy InfoNCE loss without temperature.","Specifically, we replace temperature scaling with the inverse hyperbolic tangent function, resulting in a modified InfoNCE loss.","In addition to hyperparameter-free deployment, we observed that the proposed method even yielded a performance gain in contrastive learning.","Our detailed theoretical analysis discovers that the current practice of temperature scaling in InfoNCE loss causes serious problems in gradient descent, whereas our method provides desirable gradient properties.","The proposed method was validated on five benchmarks on contrastive learning, yielding satisfactory results without temperature tuning."],"url":"http://arxiv.org/abs/2501.17683v1"}
{"created":"2025-01-29 14:41:32","title":"Unifying Scheduling Algorithms for Group Completion Time","abstract":"We propose new abstract problems that unify a collection of scheduling and graph coloring problems with general min-sum objectives. Specifically, we consider the weighted sum of completion times over groups of entities (jobs, vertices, or edges), which generalizes two important objectives in scheduling: makespan and sum of weighted completion times.   We study these problems in both online and offline settings. In the non-clairvoyant online setting, we give a novel $O(\\log g)$-competitive algorithm, where $g$ is the size of the largest group. This is the first non-trivial competitive bound for many problems with group completion time objective, and it is an exponential improvement over previous results for non-clairvoyant coflow scheduling. Notably, this bound is asymptotically best-possible. For offline scheduling, we provide powerful meta-frameworks that lead to new or stronger approximation algorithms for our new abstract problems and for previously well-studied special cases. In particular, we improve the approximation ratio from $13.5$ to $10.874$ for non-preemptive related machine scheduling and from $4+\\varepsilon$ to $2+\\varepsilon$ for preemptive unrelated machine scheduling (MOR 2012), and we improve the approximation ratio for sum coloring problems from $10.874$ to $5.437$ for perfect graphs and from $11.273$ to $10.874$ for interval graphs (TALG 2008).","sentences":["We propose new abstract problems that unify a collection of scheduling and graph coloring problems with general min-sum objectives.","Specifically, we consider the weighted sum of completion times over groups of entities (jobs, vertices, or edges), which generalizes two important objectives in scheduling: makespan and sum of weighted completion times.   ","We study these problems in both online and offline settings.","In the non-clairvoyant online setting, we give a novel $O(\\log g)$-competitive algorithm, where $g$ is the size of the largest group.","This is the first non-trivial competitive bound for many problems with group completion time objective, and it is an exponential improvement over previous results for non-clairvoyant coflow scheduling.","Notably, this bound is asymptotically best-possible.","For offline scheduling, we provide powerful meta-frameworks that lead to new or stronger approximation algorithms for our new abstract problems and for previously well-studied special cases.","In particular, we improve the approximation ratio from $13.5$ to $10.874$ for non-preemptive related machine scheduling and from $4+\\varepsilon$ to $2+\\varepsilon$ for preemptive unrelated machine scheduling (MOR 2012), and we improve the approximation ratio for sum coloring problems from $10.874$ to $5.437$ for perfect graphs and from $11.273$ to $10.874$ for interval graphs (TALG 2008)."],"url":"http://arxiv.org/abs/2501.17682v1"}
{"created":"2025-01-29 14:36:04","title":"Automated Repair of Cyber-Physical Systems","abstract":"Cyber-Physical Systems (CPS) integrate digital technologies with physical processes and are common in different domains and industries, such as robotic systems, autonomous vehicles or satellites. Debugging and verification of CPS software consumes much of the development budget as it is often purely manual. To speed up this process, Automated Program Repair (APR) has been targeted for a long time. Although there have been advances in software APR and CPS verification techniques, research specifically on APR for CPSs is limited. This Ph.D. research project aims to develop scalable APR techniques for CPSs, addressing problems of fault localization, long test execution times, and fitness function limitations. A new method combining spectrum-based fault localization (SBFL) with patch generation and advanced artificial intelligence techniques will be investigated. The approach will be validated by empirical studies on open and industrial code bases of CPSs.","sentences":["Cyber-Physical Systems (CPS) integrate digital technologies with physical processes and are common in different domains and industries, such as robotic systems, autonomous vehicles or satellites.","Debugging and verification of CPS software consumes much of the development budget as it is often purely manual.","To speed up this process, Automated Program Repair (APR) has been targeted for a long time.","Although there have been advances in software APR and CPS verification techniques, research specifically on APR for CPSs is limited.","This Ph.D. research project aims to develop scalable APR techniques for CPSs, addressing problems of fault localization, long test execution times, and fitness function limitations.","A new method combining spectrum-based fault localization (SBFL) with patch generation and advanced artificial intelligence techniques will be investigated.","The approach will be validated by empirical studies on open and industrial code bases of CPSs."],"url":"http://arxiv.org/abs/2501.17678v1"}
{"created":"2025-01-29 14:33:23","title":"Explainable Artificial Intelligence for identifying profitability predictors in Financial Statements","abstract":"The interconnected nature of the economic variables influencing a firm's performance makes the prediction of a company's earning trend a challenging task. Existing methodologies often rely on simplistic models and financial ratios failing to capture the complexity of interacting influences. In this paper, we apply Machine Learning techniques to raw financial statements data taken from AIDA, a Database comprising Italian listed companies' data from 2013 to 2022.   We present a comparative study of different models and following the European AI regulations, we complement our analysis by applying explainability techniques to the proposed models. In particular, we propose adopting an eXplainable Artificial Intelligence method based on Game Theory to identify the most sensitive features and make the result more interpretable.","sentences":["The interconnected nature of the economic variables influencing a firm's performance makes the prediction of a company's earning trend a challenging task.","Existing methodologies often rely on simplistic models and financial ratios failing to capture the complexity of interacting influences.","In this paper, we apply Machine Learning techniques to raw financial statements data taken from AIDA, a Database comprising Italian listed companies' data from 2013 to 2022.   ","We present a comparative study of different models and following the European AI regulations, we complement our analysis by applying explainability techniques to the proposed models.","In particular, we propose adopting an eXplainable Artificial Intelligence method based on Game Theory to identify the most sensitive features and make the result more interpretable."],"url":"http://arxiv.org/abs/2501.17676v1"}
{"created":"2025-01-29 14:20:42","title":"Distinguished Quantized Guidance for Diffusion-based Sequence Recommendation","abstract":"Diffusion models (DMs) have emerged as promising approaches for sequential recommendation due to their strong ability to model data distributions and generate high-quality items. Existing work typically adds noise to the next item and progressively denoises it guided by the user's interaction sequence, generating items that closely align with user interests. However, we identify two key issues in this paradigm. First, the sequences are often heterogeneous in length and content, exhibiting noise due to stochastic user behaviors. Using such sequences as guidance may hinder DMs from accurately understanding user interests. Second, DMs are prone to data bias and tend to generate only the popular items that dominate the training dataset, thus failing to meet the personalized needs of different users. To address these issues, we propose Distinguished Quantized Guidance for Diffusion-based Sequence Recommendation (DiQDiff), which aims to extract robust guidance to understand user interests and generate distinguished items for personalized user interests within DMs. To extract robust guidance, DiQDiff introduces Semantic Vector Quantization (SVQ) to quantize sequences into semantic vectors (e.g., collaborative signals and category interests) using a codebook, which can enrich the guidance to better understand user interests. To generate distinguished items, DiQDiff personalizes the generation through Contrastive Discrepancy Maximization (CDM), which maximizes the distance between denoising trajectories using contrastive loss to prevent biased generation for different users. Extensive experiments are conducted to compare DiQDiff with multiple baseline models across four widely-used datasets. The superior recommendation performance of DiQDiff against leading approaches demonstrates its effectiveness in sequential recommendation tasks.","sentences":["Diffusion models (DMs) have emerged as promising approaches for sequential recommendation due to their strong ability to model data distributions and generate high-quality items.","Existing work typically adds noise to the next item and progressively denoises it guided by the user's interaction sequence, generating items that closely align with user interests.","However, we identify two key issues in this paradigm.","First, the sequences are often heterogeneous in length and content, exhibiting noise due to stochastic user behaviors.","Using such sequences as guidance may hinder DMs from accurately understanding user interests.","Second, DMs are prone to data bias and tend to generate only the popular items that dominate the training dataset, thus failing to meet the personalized needs of different users.","To address these issues, we propose Distinguished Quantized Guidance for Diffusion-based Sequence Recommendation (DiQDiff), which aims to extract robust guidance to understand user interests and generate distinguished items for personalized user interests within DMs.","To extract robust guidance, DiQDiff introduces Semantic Vector Quantization (SVQ) to quantize sequences into semantic vectors (e.g., collaborative signals and category interests) using a codebook, which can enrich the guidance to better understand user interests.","To generate distinguished items, DiQDiff personalizes the generation through Contrastive Discrepancy Maximization (CDM), which maximizes the distance between denoising trajectories using contrastive loss to prevent biased generation for different users.","Extensive experiments are conducted to compare DiQDiff with multiple baseline models across four widely-used datasets.","The superior recommendation performance of DiQDiff against leading approaches demonstrates its effectiveness in sequential recommendation tasks."],"url":"http://arxiv.org/abs/2501.17670v1"}
{"created":"2025-01-29 14:08:08","title":"CAMP in the Odyssey: Provably Robust Reinforcement Learning with Certified Radius Maximization","abstract":"Deep reinforcement learning (DRL) has gained widespread adoption in control and decision-making tasks due to its strong performance in dynamic environments. However, DRL agents are vulnerable to noisy observations and adversarial attacks, and concerns about the adversarial robustness of DRL systems have emerged. Recent efforts have focused on addressing these robustness issues by establishing rigorous theoretical guarantees for the returns achieved by DRL agents in adversarial settings. Among these approaches, policy smoothing has proven to be an effective and scalable method for certifying the robustness of DRL agents. Nevertheless, existing certifiably robust DRL relies on policies trained with simple Gaussian augmentations, resulting in a suboptimal trade-off between certified robustness and certified return. To address this issue, we introduce a novel paradigm dubbed \\texttt{C}ertified-r\\texttt{A}dius-\\texttt{M}aximizing \\texttt{P}olicy (\\texttt{CAMP}) training. \\texttt{CAMP} is designed to enhance DRL policies, achieving better utility without compromising provable robustness. By leveraging the insight that the global certified radius can be derived from local certified radii based on training-time statistics, \\texttt{CAMP} formulates a surrogate loss related to the local certified radius and optimizes the policy guided by this surrogate loss. We also introduce \\textit{policy imitation} as a novel technique to stabilize \\texttt{CAMP} training. Experimental results demonstrate that \\texttt{CAMP} significantly improves the robustness-return trade-off across various tasks. Based on the results, \\texttt{CAMP} can achieve up to twice the certified expected return compared to that of baselines. Our code is available at https://github.com/NeuralSec/camp-robust-rl.","sentences":["Deep reinforcement learning (DRL) has gained widespread adoption in control and decision-making tasks due to its strong performance in dynamic environments.","However, DRL agents are vulnerable to noisy observations and adversarial attacks, and concerns about the adversarial robustness of DRL systems have emerged.","Recent efforts have focused on addressing these robustness issues by establishing rigorous theoretical guarantees for the returns achieved by DRL agents in adversarial settings.","Among these approaches, policy smoothing has proven to be an effective and scalable method for certifying the robustness of DRL agents.","Nevertheless, existing certifiably robust DRL relies on policies trained with simple Gaussian augmentations, resulting in a suboptimal trade-off between certified robustness and certified return.","To address this issue, we introduce a novel paradigm dubbed \\texttt{C}ertified-r\\texttt{A}dius-\\texttt{M}aximizing \\texttt{P}olicy (\\texttt{CAMP}) training.","\\texttt{CAMP} is designed to enhance DRL policies, achieving better utility without compromising provable robustness.","By leveraging the insight that the global certified radius can be derived from local certified radii based on training-time statistics, \\texttt{CAMP} formulates a surrogate loss related to the local certified radius and optimizes the policy guided by this surrogate loss.","We also introduce \\textit{policy imitation} as a novel technique to stabilize \\texttt{CAMP} training.","Experimental results demonstrate that \\texttt{CAMP} significantly improves the robustness-return trade-off across various tasks.","Based on the results, \\texttt{CAMP} can achieve up to twice the certified expected return compared to that of baselines.","Our code is available at https://github.com/NeuralSec/camp-robust-rl."],"url":"http://arxiv.org/abs/2501.17667v1"}
{"created":"2025-01-29 14:05:38","title":"An Intelligent System-on-a-Chip for a Real-Time Assessment of Fuel Consumption to Promote Eco-Driving","abstract":"Pollution that originates from automobiles is a concern in the current world, not only because of global warming, but also due to the harmful effects on people's health and lives. Despite regulations on exhaust gas emissions being applied, minimizing unsuitable driving habits that cause elevated fuel consumption and emissions would achieve further reductions. For that reason, this work proposes a self-organized map (SOM)-based intelligent system in order to provide drivers with eco-driving-intended driving style (DS) recommendations. The development of the DS advisor uses driving data from the Uyanik instrumented car. The system classifies drivers regarding the underlying causes of non-optimal DSs from the eco-driving viewpoint. When compared with other solutions, the main advantage of this approach is the personalization of the recommendations that are provided to motorists, comprising the handling of the pedals and the gearbox, with potential improvements in both fuel consumption and emissions ranging from the 9.5\\% to the 31.5\\%, or even higher for drivers that are strongly engaged with the system. It was successfully implemented using a field-programmable gate array (FPGA) device of the Xilinx ZynQ programmable system-on-a-chip (PSoC) family. This SOM-based system allows for real-time implementation, state-of-the-art timing performances, and low power consumption, which are suitable for developing advanced driving assistance systems (ADASs).","sentences":["Pollution that originates from automobiles is a concern in the current world, not only because of global warming, but also due to the harmful effects on people's health and lives.","Despite regulations on exhaust gas emissions being applied, minimizing unsuitable driving habits that cause elevated fuel consumption and emissions would achieve further reductions.","For that reason, this work proposes a self-organized map (SOM)-based intelligent system in order to provide drivers with eco-driving-intended driving style (DS) recommendations.","The development of the DS advisor uses driving data from the Uyanik instrumented car.","The system classifies drivers regarding the underlying causes of non-optimal DSs from the eco-driving viewpoint.","When compared with other solutions, the main advantage of this approach is the personalization of the recommendations that are provided to motorists, comprising the handling of the pedals and the gearbox, with potential improvements in both fuel consumption and emissions ranging from the 9.5\\% to the 31.5\\%, or even higher for drivers that are strongly engaged with the system.","It was successfully implemented using a field-programmable gate array (FPGA) device of the Xilinx ZynQ programmable system-on-a-chip (PSoC) family.","This SOM-based system allows for real-time implementation, state-of-the-art timing performances, and low power consumption, which are suitable for developing advanced driving assistance systems (ADASs)."],"url":"http://arxiv.org/abs/2501.17666v1"}
{"created":"2025-01-29 14:04:54","title":"Planning with Vision-Language Models and a Use Case in Robot-Assisted Teaching","abstract":"Automating the generation of Planning Domain Definition Language (PDDL) with Large Language Model (LLM) opens new research topic in AI planning, particularly for complex real-world tasks. This paper introduces Image2PDDL, a novel framework that leverages Vision-Language Models (VLMs) to automatically convert images of initial states and descriptions of goal states into PDDL problems. By providing a PDDL domain alongside visual inputs, Imasge2PDDL addresses key challenges in bridging perceptual understanding with symbolic planning, reducing the expertise required to create structured problem instances, and improving scalability across tasks of varying complexity. We evaluate the framework on various domains, including standard planning domains like blocksworld and sliding tile puzzles, using datasets with multiple difficulty levels. Performance is assessed on syntax correctness, ensuring grammar and executability, and content correctness, verifying accurate state representation in generated PDDL problems. The proposed approach demonstrates promising results across diverse task complexities, suggesting its potential for broader applications in AI planning. We will discuss a potential use case in robot-assisted teaching of students with Autism Spectrum Disorder.","sentences":["Automating the generation of Planning Domain Definition Language (PDDL) with Large Language Model (LLM) opens new research topic in AI planning, particularly for complex real-world tasks.","This paper introduces Image2PDDL, a novel framework that leverages Vision-Language Models (VLMs) to automatically convert images of initial states and descriptions of goal states into PDDL problems.","By providing a PDDL domain alongside visual inputs, Imasge2PDDL addresses key challenges in bridging perceptual understanding with symbolic planning, reducing the expertise required to create structured problem instances, and improving scalability across tasks of varying complexity.","We evaluate the framework on various domains, including standard planning domains like blocksworld and sliding tile puzzles, using datasets with multiple difficulty levels.","Performance is assessed on syntax correctness, ensuring grammar and executability, and content correctness, verifying accurate state representation in generated PDDL problems.","The proposed approach demonstrates promising results across diverse task complexities, suggesting its potential for broader applications in AI planning.","We will discuss a potential use case in robot-assisted teaching of students with Autism Spectrum Disorder."],"url":"http://arxiv.org/abs/2501.17665v1"}
{"created":"2025-01-29 14:04:16","title":"Analysis of the Motion Sickness and the Lack of Comfort in Car Passengers","abstract":"Advanced driving assistance systems (ADAS) are primarily designed to increase driving safety and reduce traffic congestion without paying too much attention to passenger comfort or motion sickness. However, in view of autonomous cars, and taking into account that the lack of comfort and motion sickness increase in passengers, analysis from a comfort perspective is essential in the future car investigation. The aim of this work is to study in detail how passenger's comfort evaluation parameters vary depending on the driving style, car or road. The database used has been developed by compiling the accelerations suffered by passengers when three drivers cruise two different vehicles on different types of routes. In order to evaluate both comfort and motion sickness, first, the numerical values of the main comfort evaluation variables reported in the literature have been analyzed. Moreover, a complementary statistical analysis of probability density and a power spectral analysis are performed. Finally, quantitative results are compared with passenger qualitative feedback. The results show the high dependence of comfort evaluation variables' value with the road type. In addition, it has been demonstrated that the driving style and vehicle dynamics amplify or attenuate those values. Additionally, it has been demonstrated that contributions from longitudinal and lateral accelerations have a much greater effect in the lack of comfort than vertical ones. Finally, based on the concrete results obtained, a new experimental campaign is proposed.","sentences":["Advanced driving assistance systems (ADAS) are primarily designed to increase driving safety and reduce traffic congestion without paying too much attention to passenger comfort or motion sickness.","However, in view of autonomous cars, and taking into account that the lack of comfort and motion sickness increase in passengers, analysis from a comfort perspective is essential in the future car investigation.","The aim of this work is to study in detail how passenger's comfort evaluation parameters vary depending on the driving style, car or road.","The database used has been developed by compiling the accelerations suffered by passengers when three drivers cruise two different vehicles on different types of routes.","In order to evaluate both comfort and motion sickness, first, the numerical values of the main comfort evaluation variables reported in the literature have been analyzed.","Moreover, a complementary statistical analysis of probability density and a power spectral analysis are performed.","Finally, quantitative results are compared with passenger qualitative feedback.","The results show the high dependence of comfort evaluation variables' value with the road type.","In addition, it has been demonstrated that the driving style and vehicle dynamics amplify or attenuate those values.","Additionally, it has been demonstrated that contributions from longitudinal and lateral accelerations have a much greater effect in the lack of comfort than vertical ones.","Finally, based on the concrete results obtained, a new experimental campaign is proposed."],"url":"http://arxiv.org/abs/2501.17664v1"}
{"created":"2025-01-29 14:03:27","title":"Landscape Features in Single-Objective Continuous Optimization: Have We Hit a Wall in Algorithm Selection Generalization?","abstract":"%% Text of abstract The process of identifying the most suitable optimization algorithm for a specific problem, referred to as algorithm selection (AS), entails training models that leverage problem landscape features to forecast algorithm performance. A significant challenge in this domain is ensuring that AS models can generalize effectively to novel, unseen problems. This study evaluates the generalizability of AS models based on different problem representations in the context of single-objective continuous optimization. In particular, it considers the most widely used Exploratory Landscape Analysis features, as well as recently proposed Topological Landscape Analysis features, and features based on deep learning, such as DeepELA, TransOptAS and Doe2Vec. Our results indicate that when presented with out-of-distribution evaluation data, none of the feature-based AS models outperform a simple baseline model, i.e., a Single Best Solver.","sentences":["%%","Text of abstract The process of identifying the most suitable optimization algorithm for a specific problem, referred to as algorithm selection (AS), entails training models that leverage problem landscape features to forecast algorithm performance.","A significant challenge in this domain is ensuring that AS models can generalize effectively to novel, unseen problems.","This study evaluates the generalizability of AS models based on different problem representations in the context of single-objective continuous optimization.","In particular, it considers the most widely used Exploratory Landscape Analysis features, as well as recently proposed Topological Landscape Analysis features, and features based on deep learning, such as DeepELA, TransOptAS and Doe2Vec.","Our results indicate that when presented with out-of-distribution evaluation data, none of the feature-based AS models outperform a simple baseline model, i.e., a Single Best Solver."],"url":"http://arxiv.org/abs/2501.17663v1"}
{"created":"2025-01-29 14:01:41","title":"Multi-Agent Path Finding Using Conflict-Based Search and Structural-Semantic Topometric Maps","abstract":"As industries increasingly adopt large robotic fleets, there is a pressing need for computationally efficient, practical, and optimal conflict-free path planning for multiple robots. Conflict-Based Search (CBS) is a popular method for multi-agent path finding (MAPF) due to its completeness and optimality; however, it is often impractical for real-world applications, as it is computationally intensive to solve and relies on assumptions about agents and operating environments that are difficult to realize. This article proposes a solution to overcome computational challenges and practicality issues of CBS by utilizing structural-semantic topometric maps. Instead of running CBS over large grid-based maps, the proposed solution runs CBS over a sparse topometric map containing structural-semantic cells representing intersections, pathways, and dead ends. This approach significantly accelerates the MAPF process and reduces the number of conflict resolutions handled by CBS while operating in continuous time. In the proposed method, robots are assigned time ranges to move between topometric regions, departing from the traditional CBS assumption that a robot can move to any connected cell in a single time step. The approach is validated through real-world multi-robot path-finding experiments and benchmarking simulations. The results demonstrate that the proposed MAPF method can be applied to real-world non-holonomic robots and yields significant improvement in computational efficiency compared to traditional CBS methods while improving conflict detection and resolution in cases of corridor symmetries.","sentences":["As industries increasingly adopt large robotic fleets, there is a pressing need for computationally efficient, practical, and optimal conflict-free path planning for multiple robots.","Conflict-Based Search (CBS) is a popular method for multi-agent path finding (MAPF) due to its completeness and optimality; however, it is often impractical for real-world applications, as it is computationally intensive to solve and relies on assumptions about agents and operating environments that are difficult to realize.","This article proposes a solution to overcome computational challenges and practicality issues of CBS by utilizing structural-semantic topometric maps.","Instead of running CBS over large grid-based maps, the proposed solution runs CBS over a sparse topometric map containing structural-semantic cells representing intersections, pathways, and dead ends.","This approach significantly accelerates the MAPF process and reduces the number of conflict resolutions handled by CBS while operating in continuous time.","In the proposed method, robots are assigned time ranges to move between topometric regions, departing from the traditional CBS assumption that a robot can move to any connected cell in a single time step.","The approach is validated through real-world multi-robot path-finding experiments and benchmarking simulations.","The results demonstrate that the proposed MAPF method can be applied to real-world non-holonomic robots and yields significant improvement in computational efficiency compared to traditional CBS methods while improving conflict detection and resolution in cases of corridor symmetries."],"url":"http://arxiv.org/abs/2501.17661v1"}
