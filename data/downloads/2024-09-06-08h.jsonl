{"created":"2024-09-05 17:59:56","title":"Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding","abstract":"Complex 3D scene understanding has gained increasing attention, with scene encoding strategies playing a crucial role in this success. However, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts. To address this issue, we present a comprehensive study that probes various visual encoding models for 3D scene understanding, identifying the strengths and limitations of each model across different scenarios. Our evaluation spans seven vision foundation encoders, including image-based, video-based, and 3D foundation models. We evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding. Our evaluations yield key findings: DINOv2 demonstrates superior performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, and language-pretrained models show unexpected limitations in language-related tasks. These insights challenge some conventional understandings, provide novel perspectives on leveraging visual foundation models, and highlight the need for more flexible encoder selection in future vision-language and scene-understanding tasks.","sentences":["Complex 3D scene understanding has gained increasing attention, with scene encoding strategies playing a crucial role in this success.","However, the optimal scene encoding strategies for various scenarios remain unclear, particularly compared to their image-based counterparts.","To address this issue, we present a comprehensive study that probes various visual encoding models for 3D scene understanding, identifying the strengths and limitations of each model across different scenarios.","Our evaluation spans seven vision foundation encoders, including image-based, video-based, and 3D foundation models.","We evaluate these models in four tasks: Vision-Language Scene Reasoning, Visual Grounding, Segmentation, and Registration, each focusing on different aspects of scene understanding.","Our evaluations yield key findings: DINOv2 demonstrates superior performance, video models excel in object-level tasks, diffusion models benefit geometric tasks, and language-pretrained models show unexpected limitations in language-related tasks.","These insights challenge some conventional understandings, provide novel perspectives on leveraging visual foundation models, and highlight the need for more flexible encoder selection in future vision-language and scene-understanding tasks."],"url":"http://arxiv.org/abs/2409.03757v1"}
{"created":"2024-09-05 17:59:46","title":"DC-Solver: Improving Predictor-Corrector Diffusion Sampler via Dynamic Compensation","abstract":"Diffusion probabilistic models (DPMs) have shown remarkable performance in visual synthesis but are computationally expensive due to the need for multiple evaluations during the sampling. Recent predictor-corrector diffusion samplers have significantly reduced the required number of function evaluations (NFE), but inherently suffer from a misalignment issue caused by the extra corrector step, especially with a large classifier-free guidance scale (CFG). In this paper, we introduce a new fast DPM sampler called DC-Solver, which leverages dynamic compensation (DC) to mitigate the misalignment of the predictor-corrector samplers. The dynamic compensation is controlled by compensation ratios that are adaptive to the sampling steps and can be optimized on only 10 datapoints by pushing the sampling trajectory toward a ground truth trajectory. We further propose a cascade polynomial regression (CPR) which can instantly predict the compensation ratios on unseen sampling configurations. Additionally, we find that the proposed dynamic compensation can also serve as a plug-and-play module to boost the performance of predictor-only samplers. Extensive experiments on both unconditional sampling and conditional sampling demonstrate that our DC-Solver can consistently improve the sampling quality over previous methods on different DPMs with a wide range of resolutions up to 1024$\\times$1024. Notably, we achieve 10.38 FID (NFE=5) on unconditional FFHQ and 0.394 MSE (NFE=5, CFG=7.5) on Stable-Diffusion-2.1. Code is available at https://github.com/wl-zhao/DC-Solver","sentences":["Diffusion probabilistic models (DPMs) have shown remarkable performance in visual synthesis but are computationally expensive due to the need for multiple evaluations during the sampling.","Recent predictor-corrector diffusion samplers have significantly reduced the required number of function evaluations (NFE), but inherently suffer from a misalignment issue caused by the extra corrector step, especially with a large classifier-free guidance scale (CFG).","In this paper, we introduce a new fast DPM sampler called DC-Solver, which leverages dynamic compensation (DC) to mitigate the misalignment of the predictor-corrector samplers.","The dynamic compensation is controlled by compensation ratios that are adaptive to the sampling steps and can be optimized on only 10 datapoints by pushing the sampling trajectory toward a ground truth trajectory.","We further propose a cascade polynomial regression (CPR) which can instantly predict the compensation ratios on unseen sampling configurations.","Additionally, we find that the proposed dynamic compensation can also serve as a plug-and-play module to boost the performance of predictor-only samplers.","Extensive experiments on both unconditional sampling and conditional sampling demonstrate that our DC-Solver can consistently improve the sampling quality over previous methods on different DPMs with a wide range of resolutions up to 1024$\\times$1024.","Notably, we achieve 10.38 FID (NFE=5) on unconditional FFHQ and 0.394 MSE (NFE=5, CFG=7.5) on Stable-Diffusion-2.1.","Code is available at https://github.com/wl-zhao/DC-Solver"],"url":"http://arxiv.org/abs/2409.03755v1"}
{"created":"2024-09-05 17:59:32","title":"Foundation Model or Finetune? Evaluation of few-shot semantic segmentation for river pollution","abstract":"Foundation models (FMs) are a popular topic of research in AI. Their ability to generalize to new tasks and datasets without retraining or needing an abundance of data makes them an appealing candidate for applications on specialist datasets. In this work, we compare the performance of FMs to finetuned pre-trained supervised models in the task of semantic segmentation on an entirely new dataset. We see that finetuned models consistently outperform the FMs tested, even in cases were data is scarce. We release the code and dataset for this work on GitHub.","sentences":["Foundation models (FMs) are a popular topic of research in AI.","Their ability to generalize to new tasks and datasets without retraining or needing an abundance of data makes them an appealing candidate for applications on specialist datasets.","In this work, we compare the performance of FMs to finetuned pre-trained supervised models in the task of semantic segmentation on an entirely new dataset.","We see that finetuned models consistently outperform the FMs tested, even in cases were data is scarce.","We release the code and dataset for this work on GitHub."],"url":"http://arxiv.org/abs/2409.03754v1"}
{"created":"2024-09-05 17:59:15","title":"WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild","abstract":"The increasing availability of real-world conversation data offers exciting opportunities for researchers to study user-chatbot interactions. However, the sheer volume of this data makes manually examining individual conversations impractical. To overcome this challenge, we introduce WildVis, an interactive tool that enables fast, versatile, and large-scale conversation analysis. WildVis provides search and visualization capabilities in the text and embedding spaces based on a list of criteria. To manage million-scale datasets, we implemented optimizations including search index construction, embedding precomputation and compression, and caching to ensure responsive user interactions within seconds. We demonstrate WildVis's utility through three case studies: facilitating chatbot misuse research, visualizing and comparing topic distributions across datasets, and characterizing user-specific conversation patterns. WildVis is open-source and designed to be extendable, supporting additional datasets and customized search and visualization functionalities.","sentences":["The increasing availability of real-world conversation data offers exciting opportunities for researchers to study user-chatbot interactions.","However, the sheer volume of this data makes manually examining individual conversations impractical.","To overcome this challenge, we introduce WildVis, an interactive tool that enables fast, versatile, and large-scale conversation analysis.","WildVis provides search and visualization capabilities in the text and embedding spaces based on a list of criteria.","To manage million-scale datasets, we implemented optimizations including search index construction, embedding precomputation and compression, and caching to ensure responsive user interactions within seconds.","We demonstrate WildVis's utility through three case studies: facilitating chatbot misuse research, visualizing and comparing topic distributions across datasets, and characterizing user-specific conversation patterns.","WildVis is open-source and designed to be extendable, supporting additional datasets and customized search and visualization functionalities."],"url":"http://arxiv.org/abs/2409.03753v1"}
{"created":"2024-09-05 17:59:12","title":"Attention Heads of Large Language Models: A Survey","abstract":"Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in various tasks but remain largely as black-box systems. Consequently, their development relies heavily on data-driven approaches, limiting performance enhancement through changes in internal architecture and reasoning pathways. As a result, many researchers have begun exploring the potential internal mechanisms of LLMs, aiming to identify the essence of their reasoning bottlenecks, with most studies focusing on attention heads. Our survey aims to shed light on the internal reasoning processes of LLMs by concentrating on the interpretability and underlying mechanisms of attention heads. We first distill the human thought process into a four-stage framework: Knowledge Recalling, In-Context Identification, Latent Reasoning, and Expression Preparation. Using this framework, we systematically review existing research to identify and categorize the functions of specific attention heads. Furthermore, we summarize the experimental methodologies used to discover these special heads, dividing them into two categories: Modeling-Free methods and Modeling-Required methods. Also, we outline relevant evaluation methods and benchmarks. Finally, we discuss the limitations of current research and propose several potential future directions. Our reference list is open-sourced at \\url{https://github.com/IAAR-Shanghai/Awesome-Attention-Heads}.","sentences":["Since the advent of ChatGPT, Large Language Models (LLMs) have excelled in various tasks but remain largely as black-box systems.","Consequently, their development relies heavily on data-driven approaches, limiting performance enhancement through changes in internal architecture and reasoning pathways.","As a result, many researchers have begun exploring the potential internal mechanisms of LLMs, aiming to identify the essence of their reasoning bottlenecks, with most studies focusing on attention heads.","Our survey aims to shed light on the internal reasoning processes of LLMs by concentrating on the interpretability and underlying mechanisms of attention heads.","We first distill the human thought process into a four-stage framework: Knowledge Recalling, In-Context Identification, Latent Reasoning, and Expression Preparation.","Using this framework, we systematically review existing research to identify and categorize the functions of specific attention heads.","Furthermore, we summarize the experimental methodologies used to discover these special heads, dividing them into two categories: Modeling-Free methods and Modeling-Required methods.","Also, we outline relevant evaluation methods and benchmarks.","Finally, we discuss the limitations of current research and propose several potential future directions.","Our reference list is open-sourced at \\url{https://github.com/IAAR-Shanghai/Awesome-Attention-Heads}."],"url":"http://arxiv.org/abs/2409.03752v1"}
{"created":"2024-09-05 17:59:08","title":"Randomized Lower Bounds for Tarski Fixed Points in High Dimensions","abstract":"The Knaster-Tarski theorem, also known as Tarski's theorem, guarantees that every monotone function defined on a complete lattice has a fixed point. We analyze the query complexity of finding such a fixed point on the $k$-dimensional grid of side length $n$ under the $\\leq$ relation. Specifically, there is an unknown monotone function $f: \\{0,1,\\ldots, n-1\\}^k \\to \\{0,1,\\ldots, n-1\\}^k$ and an algorithm must query a vertex $v$ to learn $f(v)$.   Our main result is a randomized lower bound of $\\Omega\\left( k + \\frac{k \\cdot \\log{n}}{\\log{k}} \\right)$ for the $k$-dimensional grid of side length $n$, which is nearly optimal in high dimensions when $k$ is large relative to $n$. As a corollary, we characterize the randomized and deterministic query complexity on the Boolean hypercube $\\{0,1\\}^k$ as $\\Theta(k)$.","sentences":["The Knaster-Tarski theorem, also known as Tarski's theorem, guarantees that every monotone function defined on a complete lattice has a fixed point.","We analyze the query complexity of finding such a fixed point on the $k$-dimensional grid of side length $n$ under the $\\leq$ relation.","Specifically, there is an unknown monotone function $f: \\{0,1,\\ldots, n-1\\}^k \\to \\{0,1,\\ldots, n-1\\}^k$ and an algorithm must query a vertex $v$ to learn $f(v)$.   Our main result is a randomized lower bound of $\\Omega\\left( k + \\frac{k \\cdot \\log{n}}{\\log{k}} \\right)$ for the $k$-dimensional grid of side length $n$, which is nearly optimal in high dimensions when $k$ is large relative to $n$. As a corollary, we characterize the randomized and deterministic query complexity on the Boolean hypercube $\\{0,1\\}^k$ as $\\Theta(k)$."],"url":"http://arxiv.org/abs/2409.03751v1"}
{"created":"2024-09-05 17:58:28","title":"Dynamics of Supervised and Reinforcement Learning in the Non-Linear Perceptron","abstract":"The ability of a brain or a neural network to efficiently learn depends crucially on both the task structure and the learning rule. Previous works have analyzed the dynamical equations describing learning in the relatively simplified context of the perceptron under assumptions of a student-teacher framework or a linearized output. While these assumptions have facilitated theoretical understanding, they have precluded a detailed understanding of the roles of the nonlinearity and input-data distribution in determining the learning dynamics, limiting the applicability of the theories to real biological or artificial neural networks. Here, we use a stochastic-process approach to derive flow equations describing learning, applying this framework to the case of a nonlinear perceptron performing binary classification. We characterize the effects of the learning rule (supervised or reinforcement learning, SL/RL) and input-data distribution on the perceptron's learning curve and the forgetting curve as subsequent tasks are learned. In particular, we find that the input-data noise differently affects the learning speed under SL vs. RL, as well as determines how quickly learning of a task is overwritten by subsequent learning. Additionally, we verify our approach with real data using the MNIST dataset. This approach points a way toward analyzing learning dynamics for more-complex circuit architectures.","sentences":["The ability of a brain or a neural network to efficiently learn depends crucially on both the task structure and the learning rule.","Previous works have analyzed the dynamical equations describing learning in the relatively simplified context of the perceptron under assumptions of a student-teacher framework or a linearized output.","While these assumptions have facilitated theoretical understanding, they have precluded a detailed understanding of the roles of the nonlinearity and input-data distribution in determining the learning dynamics, limiting the applicability of the theories to real biological or artificial neural networks.","Here, we use a stochastic-process approach to derive flow equations describing learning, applying this framework to the case of a nonlinear perceptron performing binary classification.","We characterize the effects of the learning rule (supervised or reinforcement learning, SL/RL) and input-data distribution on the perceptron's learning curve and the forgetting curve as subsequent tasks are learned.","In particular, we find that the input-data noise differently affects the learning speed under SL vs. RL, as well as determines how quickly learning of a task is overwritten by subsequent learning.","Additionally, we verify our approach with real data using the MNIST dataset.","This approach points a way toward analyzing learning dynamics for more-complex circuit architectures."],"url":"http://arxiv.org/abs/2409.03749v1"}
{"created":"2024-09-05 17:57:59","title":"ArtiFade: Learning to Generate High-quality Subject from Blemished Images","abstract":"Subject-driven text-to-image generation has witnessed remarkable advancements in its ability to learn and capture characteristics of a subject using only a limited number of images. However, existing methods commonly rely on high-quality images for training and may struggle to generate reasonable images when the input images are blemished by artifacts. This is primarily attributed to the inadequate capability of current techniques in distinguishing subject-related features from disruptive artifacts. In this paper, we introduce ArtiFade to tackle this issue and successfully generate high-quality artifact-free images from blemished datasets. Specifically, ArtiFade exploits fine-tuning of a pre-trained text-to-image model, aiming to remove artifacts. The elimination of artifacts is achieved by utilizing a specialized dataset that encompasses both unblemished images and their corresponding blemished counterparts during fine-tuning. ArtiFade also ensures the preservation of the original generative capabilities inherent within the diffusion model, thereby enhancing the overall performance of subject-driven methods in generating high-quality and artifact-free images. We further devise evaluation benchmarks tailored for this task. Through extensive qualitative and quantitative experiments, we demonstrate the generalizability of ArtiFade in effective artifact removal under both in-distribution and out-of-distribution scenarios.","sentences":["Subject-driven text-to-image generation has witnessed remarkable advancements in its ability to learn and capture characteristics of a subject using only a limited number of images.","However, existing methods commonly rely on high-quality images for training and may struggle to generate reasonable images when the input images are blemished by artifacts.","This is primarily attributed to the inadequate capability of current techniques in distinguishing subject-related features from disruptive artifacts.","In this paper, we introduce ArtiFade to tackle this issue and successfully generate high-quality artifact-free images from blemished datasets.","Specifically, ArtiFade exploits fine-tuning of a pre-trained text-to-image model, aiming to remove artifacts.","The elimination of artifacts is achieved by utilizing a specialized dataset that encompasses both unblemished images and their corresponding blemished counterparts during fine-tuning.","ArtiFade also ensures the preservation of the original generative capabilities inherent within the diffusion model, thereby enhancing the overall performance of subject-driven methods in generating high-quality and artifact-free images.","We further devise evaluation benchmarks tailored for this task.","Through extensive qualitative and quantitative experiments, we demonstrate the generalizability of ArtiFade in effective artifact removal under both in-distribution and out-of-distribution scenarios."],"url":"http://arxiv.org/abs/2409.03745v1"}
{"created":"2024-09-05 17:56:19","title":"Libra: Architectural Support For Principled, Secure And Efficient Balanced Execution On High-End Processors (Extended Version)","abstract":"Control-flow leakage (CFL) attacks enable an attacker to expose control-flow decisions of a victim program via side-channel observations. Linearization (i.e., elimination) of secret-dependent control flow is the main countermeasure against these attacks, yet it comes at a non-negligible cost. Conversely, balancing secret-dependent branches often incurs a smaller overhead, but is notoriously insecure on high-end processors. Hence, linearization has been widely believed to be the only effective countermeasure against CFL attacks. In this paper, we challenge this belief and investigate an unexplored alternative: how to securely balance secret-dependent branches on higher-end processors?   We propose Libra, a generic and principled hardware-software codesign to efficiently address CFL on high-end processors. We perform a systematic classification of hardware primitives leaking control flow from the literature, and provide guidelines to handle them with our design. Importantly, Libra enables secure control-flow balancing without the need to disable performance-critical hardware such as the instruction cache and the prefetcher. We formalize the semantics of Libra and propose a code transformation algorithm for securing programs, which we prove correct and secure. Finally, we implement and evaluate Libra on an out-of-order RISC-V processor, showing performance overhead on par with insecure balanced code, and outperforming state-of-the-art linearized code by 19.3%.","sentences":["Control-flow leakage (CFL) attacks enable an attacker to expose control-flow decisions of a victim program via side-channel observations.","Linearization (i.e., elimination) of secret-dependent control flow is the main countermeasure against these attacks, yet it comes at a non-negligible cost.","Conversely, balancing secret-dependent branches often incurs a smaller overhead, but is notoriously insecure on high-end processors.","Hence, linearization has been widely believed to be the only effective countermeasure against CFL attacks.","In this paper, we challenge this belief and investigate an unexplored alternative: how to securely balance secret-dependent branches on higher-end processors?   ","We propose Libra, a generic and principled hardware-software codesign to efficiently address CFL on high-end processors.","We perform a systematic classification of hardware primitives leaking control flow from the literature, and provide guidelines to handle them with our design.","Importantly, Libra enables secure control-flow balancing without the need to disable performance-critical hardware such as the instruction cache and the prefetcher.","We formalize the semantics of Libra and propose a code transformation algorithm for securing programs, which we prove correct and secure.","Finally, we implement and evaluate Libra on an out-of-order RISC-V processor, showing performance overhead on par with insecure balanced code, and outperforming state-of-the-art linearized code by 19.3%."],"url":"http://arxiv.org/abs/2409.03743v1"}
{"created":"2024-09-05 17:54:26","title":"Understanding Data Importance in Machine Learning Attacks: Does Valuable Data Pose Greater Harm?","abstract":"Machine learning has revolutionized numerous domains, playing a crucial role in driving advancements and enabling data-centric processes. The significance of data in training models and shaping their performance cannot be overstated. Recent research has highlighted the heterogeneous impact of individual data samples, particularly the presence of valuable data that significantly contributes to the utility and effectiveness of machine learning models. However, a critical question remains unanswered: are these valuable data samples more vulnerable to machine learning attacks? In this work, we investigate the relationship between data importance and machine learning attacks by analyzing five distinct attack types. Our findings reveal notable insights. For example, we observe that high importance data samples exhibit increased vulnerability in certain attacks, such as membership inference and model stealing. By analyzing the linkage between membership inference vulnerability and data importance, we demonstrate that sample characteristics can be integrated into membership metrics by introducing sample-specific criteria, therefore enhancing the membership inference performance. These findings emphasize the urgent need for innovative defense mechanisms that strike a balance between maximizing utility and safeguarding valuable data against potential exploitation.","sentences":["Machine learning has revolutionized numerous domains, playing a crucial role in driving advancements and enabling data-centric processes.","The significance of data in training models and shaping their performance cannot be overstated.","Recent research has highlighted the heterogeneous impact of individual data samples, particularly the presence of valuable data that significantly contributes to the utility and effectiveness of machine learning models.","However, a critical question remains unanswered: are these valuable data samples more vulnerable to machine learning attacks?","In this work, we investigate the relationship between data importance and machine learning attacks by analyzing five distinct attack types.","Our findings reveal notable insights.","For example, we observe that high importance data samples exhibit increased vulnerability in certain attacks, such as membership inference and model stealing.","By analyzing the linkage between membership inference vulnerability and data importance, we demonstrate that sample characteristics can be integrated into membership metrics by introducing sample-specific criteria, therefore enhancing the membership inference performance.","These findings emphasize the urgent need for innovative defense mechanisms that strike a balance between maximizing utility and safeguarding valuable data against potential exploitation."],"url":"http://arxiv.org/abs/2409.03741v1"}
{"created":"2024-09-05 17:53:54","title":"Differentiable Discrete Event Simulation for Queuing Network Control","abstract":"Queuing network control is essential for managing congestion in job-processing systems such as service systems, communication networks, and manufacturing processes. Despite growing interest in applying reinforcement learning (RL) techniques, queueing network control poses distinct challenges, including high stochasticity, large state and action spaces, and lack of stability. To tackle these challenges, we propose a scalable framework for policy optimization based on differentiable discrete event simulation. Our main insight is that by implementing a well-designed smoothing technique for discrete event dynamics, we can compute pathwise policy gradients for large-scale queueing networks using auto-differentiation software (e.g., Tensorflow, PyTorch) and GPU parallelization. Through extensive empirical experiments, we observe that our policy gradient estimators are several orders of magnitude more accurate than typical REINFORCE-based estimators. In addition, We propose a new policy architecture, which drastically improves stability while maintaining the flexibility of neural-network policies. In a wide variety of scheduling and admission control tasks, we demonstrate that training control policies with pathwise gradients leads to a 50-1000x improvement in sample efficiency over state-of-the-art RL methods. Unlike prior tailored approaches to queueing, our methods can flexibly handle realistic scenarios, including systems operating in non-stationary environments and those with non-exponential interarrival/service times.","sentences":["Queuing network control is essential for managing congestion in job-processing systems such as service systems, communication networks, and manufacturing processes.","Despite growing interest in applying reinforcement learning (RL) techniques, queueing network control poses distinct challenges, including high stochasticity, large state and action spaces, and lack of stability.","To tackle these challenges, we propose a scalable framework for policy optimization based on differentiable discrete event simulation.","Our main insight is that by implementing a well-designed smoothing technique for discrete event dynamics, we can compute pathwise policy gradients for large-scale queueing networks using auto-differentiation software (e.g., Tensorflow, PyTorch) and GPU parallelization.","Through extensive empirical experiments, we observe that our policy gradient estimators are several orders of magnitude more accurate than typical REINFORCE-based estimators.","In addition, We propose a new policy architecture, which drastically improves stability while maintaining the flexibility of neural-network policies.","In a wide variety of scheduling and admission control tasks, we demonstrate that training control policies with pathwise gradients leads to a 50-1000x improvement in sample efficiency over state-of-the-art RL methods.","Unlike prior tailored approaches to queueing, our methods can flexibly handle realistic scenarios, including systems operating in non-stationary environments and those with non-exponential interarrival/service times."],"url":"http://arxiv.org/abs/2409.03740v1"}
{"created":"2024-09-05 17:50:57","title":"Reprogrammable sequencing for physically intelligent under-actuated robots","abstract":"Programming physical intelligence into mechanisms holds great promise for machines that can accomplish tasks such as navigation of unstructured environments while utilizing a minimal amount of computational resources and electronic components. In this study, we introduce a novel design approach for physically intelligent under-actuated mechanisms capable of autonomously adjusting their motion in response to environmental interactions. Specifically, multistability is harnessed to sequence the motion of different degrees of freedom in a programmed order. A key aspect of this approach is that these sequences can be passively reprogrammed through mechanical stimuli that arise from interactions with the environment. To showcase our approach, we construct a four degree of freedom robot capable of autonomously navigating mazes and moving away from obstacles. Remarkably, this robot operates without relying on traditional computational architectures and utilizes only a single linear actuator.","sentences":["Programming physical intelligence into mechanisms holds great promise for machines that can accomplish tasks such as navigation of unstructured environments while utilizing a minimal amount of computational resources and electronic components.","In this study, we introduce a novel design approach for physically intelligent under-actuated mechanisms capable of autonomously adjusting their motion in response to environmental interactions.","Specifically, multistability is harnessed to sequence the motion of different degrees of freedom in a programmed order.","A key aspect of this approach is that these sequences can be passively reprogrammed through mechanical stimuli that arise from interactions with the environment.","To showcase our approach, we construct a four degree of freedom robot capable of autonomously navigating mazes and moving away from obstacles.","Remarkably, this robot operates without relying on traditional computational architectures and utilizes only a single linear actuator."],"url":"http://arxiv.org/abs/2409.03737v1"}
{"created":"2024-09-05 17:50:31","title":"LLM-CI: Assessing Contextual Integrity Norms in Language Models","abstract":"Large language models (LLMs), while memorizing parts of their training data scraped from the Internet, may also inadvertently encode societal preferences and norms. As these models are integrated into sociotechnical systems, it is crucial that the norms they encode align with societal expectations. These norms could vary across models, hyperparameters, optimization techniques, and datasets. This is especially challenging due to prompt sensitivity$-$small variations in prompts yield different responses, rendering existing assessment methodologies unreliable. There is a need for a comprehensive framework covering various models, optimization, and datasets, along with a reliable methodology to assess encoded norms.   We present LLM-CI, the first open-sourced framework to assess privacy norms encoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette methodology to assess the encoded norms across different contexts and LLMs. We propose the multi-prompt assessment methodology to address prompt sensitivity by assessing the norms from only the prompts that yield consistent responses across multiple variants. Using LLM-CI and our proposed methodology, we comprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior work, examining the impact of model properties (e.g., hyperparameters, capacity) and optimization strategies (e.g., alignment, quantization).","sentences":["Large language models (LLMs), while memorizing parts of their training data scraped from the Internet, may also inadvertently encode societal preferences and norms.","As these models are integrated into sociotechnical systems, it is crucial that the norms they encode align with societal expectations.","These norms could vary across models, hyperparameters, optimization techniques, and datasets.","This is especially challenging due to prompt sensitivity$-$small variations in prompts yield different responses, rendering existing assessment methodologies unreliable.","There is a need for a comprehensive framework covering various models, optimization, and datasets, along with a reliable methodology to assess encoded norms.   ","We present LLM-CI, the first open-sourced framework to assess privacy norms encoded in LLMs.","LLM-CI uses a Contextual Integrity-based factorial vignette methodology to assess the encoded norms across different contexts and LLMs.","We propose the multi-prompt assessment methodology to address prompt sensitivity by assessing the norms from only the prompts that yield consistent responses across multiple variants.","Using LLM-CI and our proposed methodology, we comprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior work, examining the impact of model properties (e.g., hyperparameters, capacity) and optimization strategies (e.g., alignment, quantization)."],"url":"http://arxiv.org/abs/2409.03735v1"}
{"created":"2024-09-05 17:45:01","title":"Safety vs. Performance: How Multi-Objective Learning Reduces Barriers to Market Entry","abstract":"Emerging marketplaces for large language models and other large-scale machine learning (ML) models appear to exhibit market concentration, which has raised concerns about whether there are insurmountable barriers to entry in such markets. In this work, we study this issue from both an economic and an algorithmic point of view, focusing on a phenomenon that reduces barriers to entry. Specifically, an incumbent company risks reputational damage unless its model is sufficiently aligned with safety objectives, whereas a new company can more easily avoid reputational damage. To study this issue formally, we define a multi-objective high-dimensional regression framework that captures reputational damage, and we characterize the number of data points that a new company needs to enter the market. Our results demonstrate how multi-objective considerations can fundamentally reduce barriers to entry -- the required number of data points can be significantly smaller than the incumbent company's dataset size. En route to proving these results, we develop scaling laws for high-dimensional linear regression in multi-objective environments, showing that the scaling rate becomes slower when the dataset size is large, which could be of independent interest.","sentences":["Emerging marketplaces for large language models and other large-scale machine learning (ML) models appear to exhibit market concentration, which has raised concerns about whether there are insurmountable barriers to entry in such markets.","In this work, we study this issue from both an economic and an algorithmic point of view, focusing on a phenomenon that reduces barriers to entry.","Specifically, an incumbent company risks reputational damage unless its model is sufficiently aligned with safety objectives, whereas a new company can more easily avoid reputational damage.","To study this issue formally, we define a multi-objective high-dimensional regression framework that captures reputational damage, and we characterize the number of data points that a new company needs to enter the market.","Our results demonstrate how multi-objective considerations can fundamentally reduce barriers to entry -- the required number of data points can be significantly smaller than the incumbent company's dataset size.","En route to proving these results, we develop scaling laws for high-dimensional linear regression in multi-objective environments, showing that the scaling rate becomes slower when the dataset size is large, which could be of independent interest."],"url":"http://arxiv.org/abs/2409.03734v1"}
{"created":"2024-09-05 17:44:49","title":"Planning In Natural Language Improves LLM Search For Code Generation","abstract":"While scaling training compute has led to remarkable improvements in large language models (LLMs), scaling inference compute has not yet yielded analogous gains. We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations. We empirically demonstrate that this lack of diversity can be mitigated by searching over candidate plans for solving a problem in natural language. Based on this insight, we propose PLANSEARCH, a novel search algorithm which shows strong results across HumanEval+, MBPP+, and LiveCodeBench (a contamination-free benchmark for competitive coding). PLANSEARCH generates a diverse set of observations about the problem and then uses these observations to construct plans for solving the problem. By searching over plans in natural language rather than directly over code solutions, PLANSEARCH explores a significantly more diverse range of potential solutions compared to baseline search methods. Using PLANSEARCH on top of Claude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on LiveCodeBench, outperforming both the best score achieved without search (pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%). Finally, we show that, across all models, search algorithms, and benchmarks analyzed, we can accurately predict performance gains due to search as a direct function of the diversity over generated ideas.","sentences":["While scaling training compute has led to remarkable improvements in large language models (LLMs), scaling inference compute has not yet yielded analogous gains.","We hypothesize that a core missing component is a lack of diverse LLM outputs, leading to inefficient search due to models repeatedly sampling highly similar, yet incorrect generations.","We empirically demonstrate that this lack of diversity can be mitigated by searching over candidate plans for solving a problem in natural language.","Based on this insight, we propose PLANSEARCH, a novel search algorithm which shows strong results across HumanEval+, MBPP+, and LiveCodeBench (a contamination-free benchmark for competitive coding).","PLANSEARCH generates a diverse set of observations about the problem and then uses these observations to construct plans for solving the problem.","By searching over plans in natural language rather than directly over code solutions, PLANSEARCH explores a significantly more diverse range of potential solutions compared to baseline search methods.","Using PLANSEARCH on top of Claude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on LiveCodeBench, outperforming both the best score achieved without search (pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).","Finally, we show that, across all models, search algorithms, and benchmarks analyzed, we can accurately predict performance gains due to search as a direct function of the diversity over generated ideas."],"url":"http://arxiv.org/abs/2409.03733v1"}
{"created":"2024-09-05 17:43:17","title":"A Logarithmic Decomposition and a Signed Measure Space for Entropy","abstract":"The Shannon entropy of a random variable X has much behaviour analogous to a signed measure. Previous work has explored this connection by defining a signed measure on abstract sets, which are taken to represent the information that different random variables contain. This construction is sufficient to derive many measure-theoretical counterparts to information quantities such as the mutual information $I(X; Y) = \\mu(\\tilde{X} \\cap \\tilde{Y})$, the joint entropy $H(X,Y) = \\mu(\\tilde{X} \\cup \\tilde{Y})$, and the conditional entropy $H(X|Y) = \\mu(\\tilde{X} \\setminus \\tilde{Y})$. Here we provide concrete characterisations of these abstract sets and a corresponding signed measure, and in doing so we demonstrate that there exists a much finer decomposition with intuitive properties which we call the logarithmic decomposition (LD). We show that this signed measure space has the useful property that its logarithmic atoms are easily characterised with negative or positive entropy, while also being consistent with Yeung's I-measure. We present the usability of our approach by re-examining the G\\'acs-K\\\"orner common information and the Wyner common information from this new geometric perspective and characterising it in terms of our logarithmic atoms - a property we call logarithmic decomposability. We present possible extensions of this construction to continuous probability distributions before discussing implications for quality-led information theory. Lastly, we apply our new decomposition to examine the Dyadic and Triadic systems of James and Crutchfield and show that, in contrast to the I-measure alone, our decomposition is able to qualitatively distinguish between them.","sentences":["The Shannon entropy of a random variable X has much behaviour analogous to a signed measure.","Previous work has explored this connection by defining a signed measure on abstract sets, which are taken to represent the information that different random variables contain.","This construction is sufficient to derive many measure-theoretical counterparts to information quantities such as the mutual information $I(X; Y) = \\mu(\\tilde{X} \\cap \\tilde{Y})$, the joint entropy $H(X,Y) = \\mu(\\tilde{X} \\cup \\tilde{Y})$, and the conditional entropy $H(X|Y) = \\mu(\\tilde{X} \\setminus \\tilde{Y})$.","Here we provide concrete characterisations of these abstract sets and a corresponding signed measure, and in doing so we demonstrate that there exists a much finer decomposition with intuitive properties which we call the logarithmic decomposition (LD).","We show that this signed measure space has the useful property that its logarithmic atoms are easily characterised with negative or positive entropy, while also being consistent with Yeung's I-measure.","We present the usability of our approach by re-examining the G\\'acs-K\\\"orner common information and the Wyner common information from this new geometric perspective and characterising it in terms of our logarithmic atoms - a property we call logarithmic decomposability.","We present possible extensions of this construction to continuous probability distributions before discussing implications for quality-led information theory.","Lastly, we apply our new decomposition to examine the Dyadic and Triadic systems of James and Crutchfield and show that, in contrast to the I-measure alone, our decomposition is able to qualitatively distinguish between them."],"url":"http://arxiv.org/abs/2409.03732v1"}
{"created":"2024-09-05 17:24:05","title":"Confidential Computing Transparency","abstract":"Confidential Computing is a security paradigm designed to protect data in-use by leveraging hardware-based Trusted Execution Environments (TEEs). While TEEs offer significant security benefits, the need for user trust remains a challenge, as attestation alone cannot guarantee the absence of vulnerabilities or backdoors. To address this, we propose a Confidential Computing Transparency framework with progressive levels of transparency. This framework goes beyond current measures like open-source code and audits by incorporating accountability for reviewers and robust technical safeguards, creating a comprehensive trust chain. Our tiered approach provides a practical pathway to achieving transparency in complex, real-world systems. Through a user study with 400 participants, we demonstrate that higher levels of transparency are associated with increased user comfort, particularly for sensitive data types.","sentences":["Confidential Computing is a security paradigm designed to protect data in-use by leveraging hardware-based Trusted Execution Environments (TEEs).","While TEEs offer significant security benefits, the need for user trust remains a challenge, as attestation alone cannot guarantee the absence of vulnerabilities or backdoors.","To address this, we propose a Confidential Computing Transparency framework with progressive levels of transparency.","This framework goes beyond current measures like open-source code and audits by incorporating accountability for reviewers and robust technical safeguards, creating a comprehensive trust chain.","Our tiered approach provides a practical pathway to achieving transparency in complex, real-world systems.","Through a user study with 400 participants, we demonstrate that higher levels of transparency are associated with increased user comfort, particularly for sensitive data types."],"url":"http://arxiv.org/abs/2409.03720v1"}
{"created":"2024-09-05 17:21:54","title":"Geometry Image Diffusion: Fast and Data-Efficient Text-to-3D with Image-Based Surface Representation","abstract":"Generating high-quality 3D objects from textual descriptions remains a challenging problem due to computational cost, the scarcity of 3D data, and complex 3D representations. We introduce Geometry Image Diffusion (GIMDiffusion), a novel Text-to-3D model that utilizes geometry images to efficiently represent 3D shapes using 2D images, thereby avoiding the need for complex 3D-aware architectures. By integrating a Collaborative Control mechanism, we exploit the rich 2D priors of existing Text-to-Image models such as Stable Diffusion. This enables strong generalization even with limited 3D training data (allowing us to use only high-quality training data) as well as retaining compatibility with guidance techniques such as IPAdapter. In short, GIMDiffusion enables the generation of 3D assets at speeds comparable to current Text-to-Image models. The generated objects consist of semantically meaningful, separate parts and include internal structures, enhancing both usability and versatility.","sentences":["Generating high-quality 3D objects from textual descriptions remains a challenging problem due to computational cost, the scarcity of 3D data, and complex 3D representations.","We introduce Geometry Image Diffusion (GIMDiffusion), a novel Text-to-3D model that utilizes geometry images to efficiently represent 3D shapes using 2D images, thereby avoiding the need for complex 3D-aware architectures.","By integrating a Collaborative Control mechanism, we exploit the rich 2D priors of existing Text-to-Image models such as Stable Diffusion.","This enables strong generalization even with limited 3D training data (allowing us to use only high-quality training data) as well as retaining compatibility with guidance techniques such as IPAdapter.","In short, GIMDiffusion enables the generation of 3D assets at speeds comparable to current Text-to-Image models.","The generated objects consist of semantically meaningful, separate parts and include internal structures, enhancing both usability and versatility."],"url":"http://arxiv.org/abs/2409.03718v1"}
{"created":"2024-09-05 17:14:23","title":"RAG based Question-Answering for Contextual Response Prediction System","abstract":"Large Language Models (LLMs) have shown versatility in various Natural Language Processing (NLP) tasks, including their potential as effective question-answering systems. However, to provide precise and relevant information in response to specific customer queries in industry settings, LLMs require access to a comprehensive knowledge base to avoid hallucinations. Retrieval Augmented Generation (RAG) emerges as a promising technique to address this challenge. Yet, developing an accurate question-answering framework for real-world applications using RAG entails several challenges: 1) data availability issues, 2) evaluating the quality of generated content, and 3) the costly nature of human evaluation. In this paper, we introduce an end-to-end framework that employs LLMs with RAG capabilities for industry use cases. Given a customer query, the proposed system retrieves relevant knowledge documents and leverages them, along with previous chat history, to generate response suggestions for customer service agents in the contact centers of a major retail company. Through comprehensive automated and human evaluations, we show that this solution outperforms the current BERT-based algorithms in accuracy and relevance. Our findings suggest that RAG-based LLMs can be an excellent support to human customer service representatives by lightening their workload.","sentences":["Large Language Models (LLMs) have shown versatility in various Natural Language Processing (NLP) tasks, including their potential as effective question-answering systems.","However, to provide precise and relevant information in response to specific customer queries in industry settings, LLMs require access to a comprehensive knowledge base to avoid hallucinations.","Retrieval Augmented Generation (RAG) emerges as a promising technique to address this challenge.","Yet, developing an accurate question-answering framework for real-world applications using RAG entails several challenges: 1) data availability issues, 2) evaluating the quality of generated content, and 3) the costly nature of human evaluation.","In this paper, we introduce an end-to-end framework that employs LLMs with RAG capabilities for industry use cases.","Given a customer query, the proposed system retrieves relevant knowledge documents and leverages them, along with previous chat history, to generate response suggestions for customer service agents in the contact centers of a major retail company.","Through comprehensive automated and human evaluations, we show that this solution outperforms the current BERT-based algorithms in accuracy and relevance.","Our findings suggest that RAG-based LLMs can be an excellent support to human customer service representatives by lightening their workload."],"url":"http://arxiv.org/abs/2409.03708v1"}
{"created":"2024-09-05 17:13:38","title":"A Different Level Text Protection Mechanism With Differential Privacy","abstract":"The article introduces a method for extracting words of different degrees of importance based on the BERT pre-training model and proves the effectiveness of this method. The article also discusses the impact of maintaining the same perturbation results for words of different importance on the overall text utility. This method can be applied to long text protection.","sentences":["The article introduces a method for extracting words of different degrees of importance based on the BERT pre-training model and proves the effectiveness of this method.","The article also discusses the impact of maintaining the same perturbation results for words of different importance on the overall text utility.","This method can be applied to long text protection."],"url":"http://arxiv.org/abs/2409.03707v1"}
{"created":"2024-09-05 16:57:39","title":"LAST: Language Model Aware Speech Tokenization","abstract":"Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc. Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods. Following such an approach may create a mismatch between the tokenization process and its usage afterward. In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs. We advocate for the integration of this objective into the process of learning discrete speech representations. Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs. We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size. Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text. More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches.","sentences":["Speech tokenization serves as the foundation of speech language model (LM), enabling them to perform various tasks such as spoken language modeling, text-to-speech, speech-to-text, etc.","Most speech tokenizers are trained independently of the LM training process, relying on separate acoustic models and quantization methods.","Following such an approach may create a mismatch between the tokenization process and its usage afterward.","In this study, we propose a novel approach to training a speech tokenizer by leveraging objectives from pre-trained textual LMs.","We advocate for the integration of this objective into the process of learning discrete speech representations.","Our aim is to transform features from a pre-trained speech model into a new feature space that enables better clustering for speech LMs.","We empirically investigate the impact of various model design choices, including speech vocabulary size and text LM size.","Our results demonstrate the proposed tokenization method outperforms the evaluated baselines considering both spoken language modeling and speech-to-text.","More importantly, unlike prior work, the proposed method allows the utilization of a single pre-trained LM for processing both speech and text inputs, setting it apart from conventional tokenization approaches."],"url":"http://arxiv.org/abs/2409.03701v1"}
{"created":"2024-09-05 16:56:58","title":"Constituent automorphism decoding of Reed-Muller codes","abstract":"Automorphism-ensemble decoding is applied to the Plotkin constituents of Reed-Muller codes, resulting in a new soft-decision decoding algorithm with state-of-the-art performance versus complexity trade-offs.","sentences":["Automorphism-ensemble decoding is applied to the Plotkin constituents of Reed-Muller codes, resulting in a new soft-decision decoding algorithm with state-of-the-art performance versus complexity trade-offs."],"url":"http://arxiv.org/abs/2409.03700v1"}
{"created":"2024-09-05 16:52:20","title":"Classification and Prediction of Heart Diseases using Machine Learning Algorithms","abstract":"Heart disease is a serious worldwide health issue because it claims the lives of many people who might have been treated if the disease had been identified earlier. The leading cause of death in the world is cardiovascular disease, usually referred to as heart disease. Creating reliable, effective, and precise predictions for these diseases is one of the biggest issues facing the medical world today. Although there are tools for predicting heart diseases, they are either expensive or challenging to apply for determining a patient's risk. The best classifier for foretelling and spotting heart disease was the aim of this research. This experiment examined a range of machine learning approaches, including Logistic Regression, K-Nearest Neighbor, Support Vector Machine, and Artificial Neural Networks, to determine which machine learning algorithm was most effective at predicting heart diseases. One of the most often utilized data sets for this purpose, the UCI heart disease repository provided the data set for this study. The K-Nearest Neighbor technique was shown to be the most effective machine learning algorithm for determining whether a patient has heart disease. It will be beneficial to conduct further studies on the application of additional machine learning algorithms for heart disease prediction.","sentences":["Heart disease is a serious worldwide health issue because it claims the lives of many people who might have been treated if the disease had been identified earlier.","The leading cause of death in the world is cardiovascular disease, usually referred to as heart disease.","Creating reliable, effective, and precise predictions for these diseases is one of the biggest issues facing the medical world today.","Although there are tools for predicting heart diseases, they are either expensive or challenging to apply for determining a patient's risk.","The best classifier for foretelling and spotting heart disease was the aim of this research.","This experiment examined a range of machine learning approaches, including Logistic Regression, K-Nearest Neighbor, Support Vector Machine, and Artificial Neural Networks, to determine which machine learning algorithm was most effective at predicting heart diseases.","One of the most often utilized data sets for this purpose, the UCI heart disease repository provided the data set for this study.","The K-Nearest Neighbor technique was shown to be the most effective machine learning algorithm for determining whether a patient has heart disease.","It will be beneficial to conduct further studies on the application of additional machine learning algorithms for heart disease prediction."],"url":"http://arxiv.org/abs/2409.03697v1"}
{"created":"2024-09-05 16:44:33","title":"Gathering Information about a Graph by Counting Walks from a Single Vertex","abstract":"We say that a vertex $v$ in a connected graph $G$ is decisive if the numbers of walks from $v$ of each length determine the graph $G$ rooted at $v$ up to isomorphism among all connected rooted graphs with the same number of vertices. On the other hand, $v$ is called ambivalent if it has the same walk counts as a vertex in a non-isomorphic connected graph with the same number of vertices as $G$. Using the classical constructions of cospectral trees, we first observe that ambivalent vertices exist in almost all trees. If a graph $G$ is determined by spectrum and its characteristic polynomial is irreducible, then we prove that all vertices of $G$ are decisive. Note that both assumptions are conjectured to be true for almost all graphs. Without using any assumption, we are able to prove that the vertices of a random graph are with high probability distinguishable from each other by the numbers of closed walks of length at most 4. As a consequence, the closed walk counts for lengths 2, 3, and 4 provide a canonical labeling of a random graph. Answering a question posed in chemical graph theory, we finally show that all walk counts for a vertex in an $n$-vertex graph are determined by the counts for the $2n$ shortest lengths, and the bound $2n$ is here asymptotically tight.","sentences":["We say that a vertex $v$ in a connected graph $G$ is decisive if the numbers of walks from $v$ of each length determine the graph $G$ rooted at $v$ up to isomorphism among all connected rooted graphs with the same number of vertices.","On the other hand, $v$ is called ambivalent if it has the same walk counts as a vertex in a non-isomorphic connected graph with the same number of vertices as $G$. Using the classical constructions of cospectral trees, we first observe that ambivalent vertices exist in almost all trees.","If a graph $G$ is determined by spectrum and its characteristic polynomial is irreducible, then we prove that all vertices of $G$ are decisive.","Note that both assumptions are conjectured to be true for almost all graphs.","Without using any assumption, we are able to prove that the vertices of a random graph are with high probability distinguishable from each other by the numbers of closed walks of length at most 4.","As a consequence, the closed walk counts for lengths 2, 3, and 4 provide a canonical labeling of a random graph.","Answering a question posed in chemical graph theory, we finally show that all walk counts for a vertex in an $n$-vertex graph are determined by the counts for the $2n$ shortest lengths, and the bound $2n$ is here asymptotically tight."],"url":"http://arxiv.org/abs/2409.03690v1"}
{"created":"2024-09-05 16:39:21","title":"View-Invariant Policy Learning via Zero-Shot Novel View Synthesis","abstract":"Large-scale visuomotor policy learning is a promising approach toward developing generalizable manipulation systems. Yet, policies that can be deployed on diverse embodiments, environments, and observational modalities remain elusive. In this work, we investigate how knowledge from large-scale visual data of the world may be used to address one axis of variation for generalizable manipulation: observational viewpoint. Specifically, we study single-image novel view synthesis models, which learn 3D-aware scene-level priors by rendering images of the same scene from alternate camera viewpoints given a single input image. For practical application to diverse robotic data, these models must operate zero-shot, performing view synthesis on unseen tasks and environments. We empirically analyze view synthesis models within a simple data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to understand their capabilities for learning viewpoint-invariant policies from single-viewpoint demonstration data. Upon evaluating the robustness of policies trained with our method to out-of-distribution camera viewpoints, we find that they outperform baselines in both simulated and real-world manipulation tasks. Videos and additional visualizations are available at https://s-tian.github.io/projects/vista.","sentences":["Large-scale visuomotor policy learning is a promising approach toward developing generalizable manipulation systems.","Yet, policies that can be deployed on diverse embodiments, environments, and observational modalities remain elusive.","In this work, we investigate how knowledge from large-scale visual data of the world may be used to address one axis of variation for generalizable manipulation: observational viewpoint.","Specifically, we study single-image novel view synthesis models, which learn 3D-aware scene-level priors by rendering images of the same scene from alternate camera viewpoints given a single input image.","For practical application to diverse robotic data, these models must operate zero-shot, performing view synthesis on unseen tasks and environments.","We empirically analyze view synthesis models within a simple data-augmentation scheme that we call View Synthesis Augmentation (VISTA) to understand their capabilities for learning viewpoint-invariant policies from single-viewpoint demonstration data.","Upon evaluating the robustness of policies trained with our method to out-of-distribution camera viewpoints, we find that they outperform baselines in both simulated and real-world manipulation tasks.","Videos and additional visualizations are available at https://s-tian.github.io/projects/vista."],"url":"http://arxiv.org/abs/2409.03685v1"}
{"created":"2024-09-05 16:37:26","title":"A New First-Order Meta-Learning Algorithm with Convergence Guarantees","abstract":"Learning new tasks by drawing on prior experience gathered from other (related) tasks is a core property of any intelligent system. Gradient-based meta-learning, especially MAML and its variants, has emerged as a viable solution to accomplish this goal. One problem MAML encounters is its computational and memory burdens needed to compute the meta-gradients. We propose a new first-order variant of MAML that we prove converges to a stationary point of the MAML objective, unlike other first-order variants. We also show that the MAML objective does not satisfy the smoothness assumption assumed in previous works; we show instead that its smoothness constant grows with the norm of the meta-gradient, which theoretically suggests the use of normalized or clipped-gradient methods compared to the plain gradient method used in previous works. We validate our theory on a synthetic experiment.","sentences":["Learning new tasks by drawing on prior experience gathered from other (related) tasks is a core property of any intelligent system.","Gradient-based meta-learning, especially MAML and its variants, has emerged as a viable solution to accomplish this goal.","One problem MAML encounters is its computational and memory burdens needed to compute the meta-gradients.","We propose a new first-order variant of MAML that we prove converges to a stationary point of the MAML objective, unlike other first-order variants.","We also show that the MAML objective does not satisfy the smoothness assumption assumed in previous works; we show instead that its smoothness constant grows with the norm of the meta-gradient, which theoretically suggests the use of normalized or clipped-gradient methods compared to the plain gradient method used in previous works.","We validate our theory on a synthetic experiment."],"url":"http://arxiv.org/abs/2409.03682v1"}
{"created":"2024-09-05 16:33:53","title":"Space-Efficient Algorithm for Integer Programming with Few Constraints","abstract":"Integer linear programs $\\min\\{c^T x : A x = b, x \\in \\mathbb{Z}^n_{\\ge 0}\\}$, where $A \\in \\mathbb{Z}^{m \\times n}$, $b \\in \\mathbb{Z}^m$, and $c \\in \\mathbb{Z}^n$, can be solved in pseudopolynomial time for any fixed number of constraints $m = O(1)$. More precisely, in time $(m\\Delta)^{O(m)} \\text{poly}(I)$, where $\\Delta$ is the maximum absolute value of an entry in $A$ and $I$ the input size.   Known algorithms rely heavily on dynamic programming, which leads to a space complexity of similar order of magnitude as the running time. In this paper, we present a polynomial space algorithm that solves integer linear programs in $(m\\Delta)^{O(m (\\log m + \\log\\log\\Delta))} \\text{poly}(I)$ time, that is, in almost the same time as previous dynamic programming algorithms.","sentences":["Integer linear programs $\\min\\{c^T x : A x = b, x \\in \\mathbb{Z}^n_{\\ge 0}\\}$, where $A \\in \\mathbb{Z}^{m \\times n}$, $b \\in \\mathbb{Z}^m$, and $c \\in \\mathbb{Z}^n$, can be solved in pseudopolynomial time for any fixed number of constraints $m","= O(1)$. More precisely, in time $(m\\Delta)^{O(m)} \\text{poly}(I)$, where $\\Delta$ is the maximum absolute value of an entry in $A$ and $I$ the input size.   ","Known algorithms rely heavily on dynamic programming, which leads to a space complexity of similar order of magnitude as the running time.","In this paper, we present a polynomial space algorithm that solves integer linear programs in $(m\\Delta)^{O(m (\\log m + \\log\\log\\Delta))} \\text{poly}(I)$ time, that is, in almost the same time as previous dynamic programming algorithms."],"url":"http://arxiv.org/abs/2409.03681v1"}
{"created":"2024-09-05 16:28:00","title":"Fine-Grained Equivalence for Problems Related to Integer Linear Programming","abstract":"Integer Linear Programming with $n$ binary variables and $m$ many $0/1$-constraints can be solved in time $2^{\\tilde O(m^2)} \\text{poly}(n)$ and it is open whether the dependence on $m$ is optimal. Several seemingly unrelated problems, which include variants of Closest String, Discrepancy Minimization, Set Cover, and Set Packing, can be modelled as Integer Linear Programming with $0/1$ constraints to obtain algorithms with the same running time for a natural parameter $m$ in each of the problems. Our main result establishes through fine-grained reductions that these problems are equivalent, meaning that a $2^{O(m^{2-\\varepsilon})} \\text{poly}(n)$ algorithm with $\\varepsilon > 0$ for one of them implies such an algorithm for all of them.   In the setting above, one can alternatively obtain an $n^{O(m)}$ time algorithm for Integer Linear Programming using a straightforward dynamic programming approach, which can be more efficient if $n$ is relatively small (e.g., subexponential in $m$). We show that this can be improved to ${n'}^{O(m)} + O(nm)$, where $n'$ is the number of distinct (i.e., non-symmetric) variables. This dominates both of the aforementioned running times.","sentences":["Integer Linear Programming with $n$ binary variables and $m$ many $0/1$-constraints can be solved in time $2^{\\tilde O(m^2)}","\\text{poly}(n)$ and it is open whether the dependence on $m$ is optimal.","Several seemingly unrelated problems, which include variants of Closest String, Discrepancy Minimization, Set Cover, and Set Packing, can be modelled as Integer Linear Programming with $0/1$ constraints to obtain algorithms with the same running time for a natural parameter $m$ in each of the problems.","Our main result establishes through fine-grained reductions that these problems are equivalent, meaning that a $2^{O(m^{2-\\varepsilon})} \\text{poly}(n)$ algorithm with $\\varepsilon > 0$ for one of them implies such an algorithm for all of them.   ","In the setting above, one can alternatively obtain an $n^{O(m)}$ time algorithm for Integer Linear Programming using a straightforward dynamic programming approach, which can be more efficient if $n$ is relatively small (e.g., subexponential in $m$).","We show that this can be improved to ${n'}^{O(m)}","+ O(nm)$, where $n'$ is the number of distinct (i.e., non-symmetric) variables.","This dominates both of the aforementioned running times."],"url":"http://arxiv.org/abs/2409.03675v1"}
{"created":"2024-09-05 16:27:16","title":"Practical Forecasting of Cryptocoins Timeseries using Correlation Patterns","abstract":"Cryptocoins (i.e., Bitcoin, Ether, Litecoin) are tradable digital assets. Ownerships of cryptocoins are registered on distributed ledgers (i.e., blockchains). Secure encryption techniques guarantee the security of the transactions (transfers of coins among owners), registered into the ledger. Cryptocoins are exchanged for specific trading prices. The extreme volatility of such trading prices across all different sets of crypto-assets remains undisputed. However, the relations between the trading prices across different cryptocoins remains largely unexplored. Major coin exchanges indicate trend correlation to advise for sells or buys. However, price correlations remain largely unexplored. We shed some light on the trend correlations across a large variety of cryptocoins, by investigating their coin/price correlation trends over the past two years. We study the causality between the trends, and exploit the derived correlations to understand the accuracy of state-of-the-art forecasting techniques for time series modeling (e.g., GBMs, LSTM and GRU) of correlated cryptocoins. Our evaluation shows (i) strong correlation patterns between the most traded coins (e.g., Bitcoin and Ether) and other types of cryptocurrencies, and (ii) state-of-the-art time series forecasting algorithms can be used to forecast cryptocoins price trends. We released datasets and code to reproduce our analysis to the research community.","sentences":["Cryptocoins (i.e., Bitcoin, Ether, Litecoin) are tradable digital assets.","Ownerships of cryptocoins are registered on distributed ledgers (i.e., blockchains).","Secure encryption techniques guarantee the security of the transactions (transfers of coins among owners), registered into the ledger.","Cryptocoins are exchanged for specific trading prices.","The extreme volatility of such trading prices across all different sets of crypto-assets remains undisputed.","However, the relations between the trading prices across different cryptocoins remains largely unexplored.","Major coin exchanges indicate trend correlation to advise for sells or buys.","However, price correlations remain largely unexplored.","We shed some light on the trend correlations across a large variety of cryptocoins, by investigating their coin/price correlation trends over the past two years.","We study the causality between the trends, and exploit the derived correlations to understand the accuracy of state-of-the-art forecasting techniques for time series modeling (e.g., GBMs, LSTM and GRU) of correlated cryptocoins.","Our evaluation shows (i) strong correlation patterns between the most traded coins (e.g., Bitcoin and Ether) and other types of cryptocurrencies, and (ii) state-of-the-art time series forecasting algorithms can be used to forecast cryptocoins price trends.","We released datasets and code to reproduce our analysis to the research community."],"url":"http://arxiv.org/abs/2409.03674v1"}
{"created":"2024-09-05 16:25:30","title":"Wind turbine condition monitoring based on intra- and inter-farm federated learning","abstract":"As wind energy adoption is growing, ensuring the efficient operation and maintenance of wind turbines becomes essential for maximizing energy production and minimizing costs and downtime. Many AI applications in wind energy, such as in condition monitoring and power forecasting, may benefit from using operational data not only from individual wind turbines but from multiple turbines and multiple wind farms. Collaborative distributed AI which preserves data privacy holds a strong potential for these applications. Federated learning has emerged as a privacy-preserving distributed machine learning approach in this context. We explore federated learning in wind turbine condition monitoring, specifically for fault detection using normal behaviour models. We investigate various federated learning strategies, including collaboration across different wind farms and turbine models, as well as collaboration restricted to the same wind farm and turbine model. Our case study results indicate that federated learning across multiple wind turbines consistently outperforms models trained on a single turbine, especially when training data is scarce. Moreover, the amount of historical data necessary to train an effective model can be significantly reduced by employing a collaborative federated learning strategy. Finally, our findings show that extending the collaboration to multiple wind farms may result in inferior performance compared to restricting learning within a farm, specifically when faced with statistical heterogeneity and imbalanced datasets.","sentences":["As wind energy adoption is growing, ensuring the efficient operation and maintenance of wind turbines becomes essential for maximizing energy production and minimizing costs and downtime.","Many AI applications in wind energy, such as in condition monitoring and power forecasting, may benefit from using operational data not only from individual wind turbines but from multiple turbines and multiple wind farms.","Collaborative distributed AI which preserves data privacy holds a strong potential for these applications.","Federated learning has emerged as a privacy-preserving distributed machine learning approach in this context.","We explore federated learning in wind turbine condition monitoring, specifically for fault detection using normal behaviour models.","We investigate various federated learning strategies, including collaboration across different wind farms and turbine models, as well as collaboration restricted to the same wind farm and turbine model.","Our case study results indicate that federated learning across multiple wind turbines consistently outperforms models trained on a single turbine, especially when training data is scarce.","Moreover, the amount of historical data necessary to train an effective model can be significantly reduced by employing a collaborative federated learning strategy.","Finally, our findings show that extending the collaboration to multiple wind farms may result in inferior performance compared to restricting learning within a farm, specifically when faced with statistical heterogeneity and imbalanced datasets."],"url":"http://arxiv.org/abs/2409.03672v1"}
{"created":"2024-09-05 16:24:42","title":"TRACE-cs: Trustworthy Reasoning for Contrastive Explanations in Course Scheduling Problems","abstract":"We present TRACE-cs, a novel hybrid system that combines symbolic reasoning with large language models (LLMs) to address contrastive queries in scheduling problems. TRACE-cs leverages SAT solving techniques to encode scheduling constraints and generate explanations for user queries, while utilizing an LLM to process the user queries into logical clauses as well as refine the explanations generated by the symbolic solver to natural language sentences. By integrating these components, our approach demonstrates the potential of combining symbolic methods with LLMs to create explainable AI agents with correctness guarantees.","sentences":["We present TRACE-cs, a novel hybrid system that combines symbolic reasoning with large language models (LLMs) to address contrastive queries in scheduling problems.","TRACE-cs leverages SAT solving techniques to encode scheduling constraints and generate explanations for user queries, while utilizing an LLM to process the user queries into logical clauses as well as refine the explanations generated by the symbolic solver to natural language sentences.","By integrating these components, our approach demonstrates the potential of combining symbolic methods with LLMs to create explainable AI agents with correctness guarantees."],"url":"http://arxiv.org/abs/2409.03671v1"}
{"created":"2024-09-05 16:22:31","title":"A Fused Large Language Model for Predicting Startup Success","abstract":"Investors are continuously seeking profitable investment opportunities in startups and, hence, for effective decision-making, need to predict a startup's probability of success. Nowadays, investors can use not only various fundamental information about a startup (e.g., the age of the startup, the number of founders, and the business sector) but also textual description of a startup's innovation and business model, which is widely available through online venture capital (VC) platforms such as Crunchbase. To support the decision-making of investors, we develop a machine learning approach with the aim of locating successful startups on VC platforms. Specifically, we develop, train, and evaluate a tailored, fused large language model to predict startup success. Thereby, we assess to what extent self-descriptions on VC platforms are predictive of startup success. Using 20,172 online profiles from Crunchbase, we find that our fused large language model can predict startup success, with textual self-descriptions being responsible for a significant part of the predictive power. Our work provides a decision support tool for investors to find profitable investment opportunities.","sentences":["Investors are continuously seeking profitable investment opportunities in startups and, hence, for effective decision-making, need to predict a startup's probability of success.","Nowadays, investors can use not only various fundamental information about a startup (e.g., the age of the startup, the number of founders, and the business sector) but also textual description of a startup's innovation and business model, which is widely available through online venture capital (VC) platforms such as Crunchbase.","To support the decision-making of investors, we develop a machine learning approach with the aim of locating successful startups on VC platforms.","Specifically, we develop, train, and evaluate a tailored, fused large language model to predict startup success.","Thereby, we assess to what extent self-descriptions on VC platforms are predictive of startup success.","Using 20,172 online profiles from Crunchbase, we find that our fused large language model can predict startup success, with textual self-descriptions being responsible for a significant part of the predictive power.","Our work provides a decision support tool for investors to find profitable investment opportunities."],"url":"http://arxiv.org/abs/2409.03668v1"}
{"created":"2024-09-05 16:21:20","title":"Threat Classification on Deployed Optical Networks Using MIMO Digital Fiber Sensing, Wavelets, and Machine Learning","abstract":"We demonstrate mechanical threats classification including jackhammers and excavators, leveraging wavelet transform of MIMO-DFS output data across a 57-km operational network link. Our machine learning framework incorporates transfer learning and shows 93% classification accuracy from field data, with benefits for optical network supervision.","sentences":["We demonstrate mechanical threats classification including jackhammers and excavators, leveraging wavelet transform of MIMO-DFS output data across a 57-km operational network link.","Our machine learning framework incorporates transfer learning and shows 93% classification accuracy from field data, with benefits for optical network supervision."],"url":"http://arxiv.org/abs/2409.03667v1"}
{"created":"2024-09-05 16:15:52","title":"Weather-Adaptive Multi-Step Forecasting of State of Polarization Changes in Aerial Fibers Using Wavelet Neural Networks","abstract":"We introduce a novel weather-adaptive approach for multi-step forecasting of multi-scale SOP changes in aerial fiber links. By harnessing the discrete wavelet transform and incorporating weather data, our approach improves forecasting accuracy by over 65% in RMSE and 63% in MAPE compared to baselines.","sentences":["We introduce a novel weather-adaptive approach for multi-step forecasting of multi-scale SOP changes in aerial fiber links.","By harnessing the discrete wavelet transform and incorporating weather data, our approach improves forecasting accuracy by over 65% in RMSE and 63% in MAPE compared to baselines."],"url":"http://arxiv.org/abs/2409.03663v1"}
{"created":"2024-09-05 16:15:12","title":"The representation landscape of few-shot learning and fine-tuning in large language models","abstract":"In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks. Despite their different natures, these strategies often lead to comparable performance gains. However, little is known about whether they induce similar representations inside LLMs. We approach this problem by analyzing the probability landscape of their hidden representations in the two cases. More specifically, we compare how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network. In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content. In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed. In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while the landscape of ICL representations is characterized by less defined peaks. Our approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing us to make a step towards designing optimal methods to extract information from language models.","sentences":["In-context learning (ICL) and supervised fine-tuning (SFT) are two common strategies for improving the performance of modern large language models (LLMs) on specific tasks.","Despite their different natures, these strategies often lead to comparable performance gains.","However, little is known about whether they induce similar representations inside LLMs.","We approach this problem by analyzing the probability landscape of their hidden representations in the two cases.","More specifically, we compare how LLMs solve the same question-answering task, finding that ICL and SFT create very different internal structures, in both cases undergoing a sharp transition in the middle of the network.","In the first half of the network, ICL shapes interpretable representations hierarchically organized according to their semantic content.","In contrast, the probability landscape obtained with SFT is fuzzier and semantically mixed.","In the second half of the model, the fine-tuned representations develop probability modes that better encode the identity of answers, while the landscape of ICL representations is characterized by less defined peaks.","Our approach reveals the diverse computational strategies developed inside LLMs to solve the same task across different conditions, allowing us to make a step towards designing optimal methods to extract information from language models."],"url":"http://arxiv.org/abs/2409.03662v1"}
{"created":"2024-09-05 16:12:29","title":"LLM-based multi-agent poetry generation in non-cooperative environments","abstract":"Despite substantial progress of large language models (LLMs) for automatic poetry generation, the generated poetry lacks diversity while the training process differs greatly from human learning. Under the rationale that the learning process of the poetry generation systems should be more human-like and their output more diverse and novel, we introduce a framework based on social learning where we emphasize non-cooperative interactions besides cooperative interactions to encourage diversity. Our experiments are the first attempt at LLM-based multi-agent systems in non-cooperative environments for poetry generation employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED agents (GPT-3 and GPT-4). Our evaluation based on 96k generated poems shows that our framework benefits the poetry generation process for TRAINING-BASED agents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity and a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams. The generated poetry from TRAINING-BASED agents also exhibits group divergence in terms of lexicons, styles and semantics. PROMPTING-BASED agents in our framework also benefit from non-cooperative environments and a more diverse ensemble of models with non-homogeneous agents has the potential to further enhance diversity, with an increase of 7.0-17.5 pp according to our experiments. However, PROMPTING-BASED agents show a decrease in lexical diversity over time and do not exhibit the group-based divergence intended in the social network. Our paper argues for a paradigm shift in creative tasks such as automatic poetry generation to include social learning processes (via LLM-based agent modeling) similar to human interaction.","sentences":["Despite substantial progress of large language models (LLMs) for automatic poetry generation, the generated poetry lacks diversity while the training process differs greatly from human learning.","Under the rationale that the learning process of the poetry generation systems should be more human-like and their output more diverse and novel, we introduce a framework based on social learning where we emphasize non-cooperative interactions besides cooperative interactions to encourage diversity.","Our experiments are the first attempt at LLM-based multi-agent systems in non-cooperative environments for poetry generation employing both TRAINING-BASED agents (GPT-2) and PROMPTING-BASED agents (GPT-3 and GPT-4).","Our evaluation based on 96k generated poems shows that our framework benefits the poetry generation process for TRAINING-BASED agents resulting in 1) a 3.0-3.7 percentage point (pp) increase in diversity and a 5.6-11.3 pp increase in novelty according to distinct and novel n-grams.","The generated poetry from TRAINING-BASED agents also exhibits group divergence in terms of lexicons, styles and semantics.","PROMPTING-BASED agents in our framework also benefit from non-cooperative environments and a more diverse ensemble of models with non-homogeneous agents has the potential to further enhance diversity, with an increase of 7.0-17.5 pp according to our experiments.","However, PROMPTING-BASED agents show a decrease in lexical diversity over time and do not exhibit the group-based divergence intended in the social network.","Our paper argues for a paradigm shift in creative tasks such as automatic poetry generation to include social learning processes (via LLM-based agent modeling) similar to human interaction."],"url":"http://arxiv.org/abs/2409.03659v1"}
{"created":"2024-09-05 16:11:40","title":"A DNN Biophysics Model with Topological and Electrostatic Features","abstract":"In this project, we provide a deep-learning neural network (DNN) based biophysics model to predict protein properties. The model uses multi-scale and uniform topological and electrostatic features generated with protein structural information and force field, which governs the molecular mechanics. The topological features are generated using the element specified persistent homology (ESPH) while the electrostatic features are fast computed using a Cartesian treecode. These features are uniform in number for proteins with various sizes thus the broadly available protein structure database can be used in training the network. These features are also multi-scale thus the resolution and computational cost can be balanced by the users. The machine learning simulation on over 4000 protein structures shows the efficiency and fidelity of these features in representing the protein structure and force field for the predication of their biophysical properties such as electrostatic solvation energy. Tests on topological or electrostatic features alone and the combination of both showed the optimal performance when both features are used. This model shows its potential as a general tool in assisting biophysical properties and function prediction for the broad biomolecules using data from both theoretical computing and experiments.","sentences":["In this project, we provide a deep-learning neural network (DNN) based biophysics model to predict protein properties.","The model uses multi-scale and uniform topological and electrostatic features generated with protein structural information and force field, which governs the molecular mechanics.","The topological features are generated using the element specified persistent homology (ESPH) while the electrostatic features are fast computed using a Cartesian treecode.","These features are uniform in number for proteins with various sizes thus the broadly available protein structure database can be used in training the network.","These features are also multi-scale thus the resolution and computational cost can be balanced by the users.","The machine learning simulation on over 4000 protein structures shows the efficiency and fidelity of these features in representing the protein structure and force field for the predication of their biophysical properties such as electrostatic solvation energy.","Tests on topological or electrostatic features alone and the combination of both showed the optimal performance when both features are used.","This model shows its potential as a general tool in assisting biophysical properties and function prediction for the broad biomolecules using data from both theoretical computing and experiments."],"url":"http://arxiv.org/abs/2409.03658v1"}
{"created":"2024-09-05 16:11:36","title":"Unsupervised Anomaly Detection and Localization with Generative Adversarial Networks","abstract":"We propose a novel unsupervised anomaly detection approach using generative adversarial networks and SOP-derived spectrograms. Demonstrating remarkable efficacy, our method achieves over 97% accuracy on SOP datasets from both submarine and terrestrial fiber links, all achieved without the need for labelled data.","sentences":["We propose a novel unsupervised anomaly detection approach using generative adversarial networks and SOP-derived spectrograms.","Demonstrating remarkable efficacy, our method achieves over 97% accuracy on SOP datasets from both submarine and terrestrial fiber links, all achieved without the need for labelled data."],"url":"http://arxiv.org/abs/2409.03657v1"}
{"created":"2024-09-05 16:08:19","title":"On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization","abstract":"Reinforcement Learning from Human Feedback (RLHF) is an effective approach for aligning language models to human preferences. Central to RLHF is learning a reward function for scoring human preferences. Two main approaches for learning a reward model are 1) training an EXplicit Reward Model (EXRM) as in RLHF, and 2) using an implicit reward learned from preference data through methods such as Direct Preference Optimization (DPO). Prior work has shown that the implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in the limit. DPORM's effectiveness directly implies the optimality of the learned policy, and also has practical implication for LLM alignment methods including iterative DPO. However, it is unclear how well DPORM empirically matches the performance of EXRM. This work studies the accuracy at distinguishing preferred and rejected answers for both DPORM and EXRM. Our findings indicate that even though DPORM fits the training dataset comparably, it generalizes less effectively than EXRM, especially when the validation datasets contain distribution shifts. Across five out-of-distribution settings, DPORM has a mean drop in accuracy of 3% and a maximum drop of 7%. These findings highlight that DPORM has limited generalization ability and substantiates the integration of an explicit reward model in iterative DPO approaches.","sentences":["Reinforcement Learning from Human Feedback (RLHF) is an effective approach for aligning language models to human preferences.","Central to RLHF is learning a reward function for scoring human preferences.","Two main approaches for learning a reward model are 1) training an EXplicit Reward Model (EXRM) as in RLHF, and 2) using an implicit reward learned from preference data through methods such as Direct Preference Optimization (DPO).","Prior work has shown that the implicit reward model of DPO (denoted as DPORM) can approximate an EXRM in the limit.","DPORM's effectiveness directly implies the optimality of the learned policy, and also has practical implication for LLM alignment methods including iterative DPO.","However, it is unclear how well DPORM empirically matches the performance of EXRM.","This work studies the accuracy at distinguishing preferred and rejected answers for both DPORM and EXRM.","Our findings indicate that even though DPORM fits the training dataset comparably, it generalizes less effectively than EXRM, especially when the validation datasets contain distribution shifts.","Across five out-of-distribution settings, DPORM has a mean drop in accuracy of 3% and a maximum drop of 7%.","These findings highlight that DPORM has limited generalization ability and substantiates the integration of an explicit reward model in iterative DPO approaches."],"url":"http://arxiv.org/abs/2409.03650v1"}
{"created":"2024-09-05 16:04:57","title":"Limited but consistent gains in adversarial robustness by co-training object recognition models with human EEG","abstract":"In contrast to human vision, artificial neural networks (ANNs) remain relatively susceptible to adversarial attacks. To address this vulnerability, efforts have been made to transfer inductive bias from human brains to ANNs, often by training the ANN representations to match their biological counterparts. Previous works relied on brain data acquired in rodents or primates using invasive techniques, from specific regions of the brain, under non-natural conditions (anesthetized animals), and with stimulus datasets lacking diversity and naturalness. In this work, we explored whether aligning model representations to human EEG responses to a rich set of real-world images increases robustness to ANNs. Specifically, we trained ResNet50-backbone models on a dual task of classification and EEG prediction; and evaluated their EEG prediction accuracy and robustness to adversarial attacks. We observed significant correlation between the networks' EEG prediction accuracy, often highest around 100 ms post stimulus onset, and their gains in adversarial robustness. Although effect size was limited, effects were consistent across different random initializations and robust for architectural variants. We further teased apart the data from individual EEG channels and observed strongest contribution from electrodes in the parieto-occipital regions. The demonstrated utility of human EEG for such tasks opens up avenues for future efforts that scale to larger datasets under diverse stimuli conditions with the promise of stronger effects.","sentences":["In contrast to human vision, artificial neural networks (ANNs) remain relatively susceptible to adversarial attacks.","To address this vulnerability, efforts have been made to transfer inductive bias from human brains to ANNs, often by training the ANN representations to match their biological counterparts.","Previous works relied on brain data acquired in rodents or primates using invasive techniques, from specific regions of the brain, under non-natural conditions (anesthetized animals), and with stimulus datasets lacking diversity and naturalness.","In this work, we explored whether aligning model representations to human EEG responses to a rich set of real-world images increases robustness to ANNs.","Specifically, we trained ResNet50-backbone models on a dual task of classification and EEG prediction; and evaluated their EEG prediction accuracy and robustness to adversarial attacks.","We observed significant correlation between the networks' EEG prediction accuracy, often highest around 100 ms post stimulus onset, and their gains in adversarial robustness.","Although effect size was limited, effects were consistent across different random initializations and robust for architectural variants.","We further teased apart the data from individual EEG channels and observed strongest contribution from electrodes in the parieto-occipital regions.","The demonstrated utility of human EEG for such tasks opens up avenues for future efforts that scale to larger datasets under diverse stimuli conditions with the promise of stronger effects."],"url":"http://arxiv.org/abs/2409.03646v1"}
{"created":"2024-09-05 16:02:11","title":"RealisHuman: A Two-Stage Approach for Refining Malformed Human Parts in Generated Images","abstract":"In recent years, diffusion models have revolutionized visual generation, outperforming traditional frameworks like Generative Adversarial Networks (GANs). However, generating images of humans with realistic semantic parts, such as hands and faces, remains a significant challenge due to their intricate structural complexity. To address this issue, we propose a novel post-processing solution named RealisHuman. The RealisHuman framework operates in two stages. First, it generates realistic human parts, such as hands or faces, using the original malformed parts as references, ensuring consistent details with the original image. Second, it seamlessly integrates the rectified human parts back into their corresponding positions by repainting the surrounding areas to ensure smooth and realistic blending. The RealisHuman framework significantly enhances the realism of human generation, as demonstrated by notable improvements in both qualitative and quantitative metrics. Code is available at https://github.com/Wangbenzhi/RealisHuman.","sentences":["In recent years, diffusion models have revolutionized visual generation, outperforming traditional frameworks like Generative Adversarial Networks (GANs).","However, generating images of humans with realistic semantic parts, such as hands and faces, remains a significant challenge due to their intricate structural complexity.","To address this issue, we propose a novel post-processing solution named RealisHuman.","The RealisHuman framework operates in two stages.","First, it generates realistic human parts, such as hands or faces, using the original malformed parts as references, ensuring consistent details with the original image.","Second, it seamlessly integrates the rectified human parts back into their corresponding positions by repainting the surrounding areas to ensure smooth and realistic blending.","The RealisHuman framework significantly enhances the realism of human generation, as demonstrated by notable improvements in both qualitative and quantitative metrics.","Code is available at https://github.com/Wangbenzhi/RealisHuman."],"url":"http://arxiv.org/abs/2409.03644v1"}
{"created":"2024-09-05 16:01:21","title":"CDM: A Reliable Metric for Fair and Accurate Formula Recognition Evaluation","abstract":"Formula recognition presents significant challenges due to the complicated structure and varied notation of mathematical expressions. Despite continuous advancements in formula recognition models, the evaluation metrics employed by these models, such as BLEU and Edit Distance, still exhibit notable limitations. They overlook the fact that the same formula has diverse representations and is highly sensitive to the distribution of training data, thereby causing the unfairness in formula recognition evaluation. To this end, we propose a Character Detection Matching (CDM) metric, ensuring the evaluation objectivity by designing a image-level rather than LaTex-level metric score. Specifically, CDM renders both the model-predicted LaTeX and the ground-truth LaTeX formulas into image-formatted formulas, then employs visual feature extraction and localization techniques for precise character-level matching, incorporating spatial position information. Such a spatially-aware and character-matching method offers a more accurate and equitable evaluation compared with previous BLEU and Edit Distance metrics that rely solely on text-based character matching. Experimentally, we evaluated various formula recognition models using CDM, BLEU, and ExpRate metrics. Their results demonstrate that the CDM aligns more closely with human evaluation standards and provides a fairer comparison across different models by eliminating discrepancies caused by diverse formula representations.","sentences":["Formula recognition presents significant challenges due to the complicated structure and varied notation of mathematical expressions.","Despite continuous advancements in formula recognition models, the evaluation metrics employed by these models, such as BLEU and Edit Distance, still exhibit notable limitations.","They overlook the fact that the same formula has diverse representations and is highly sensitive to the distribution of training data, thereby causing the unfairness in formula recognition evaluation.","To this end, we propose a Character Detection Matching (CDM) metric, ensuring the evaluation objectivity by designing a image-level rather than LaTex-level metric score.","Specifically, CDM renders both the model-predicted LaTeX and the ground-truth LaTeX formulas into image-formatted formulas, then employs visual feature extraction and localization techniques for precise character-level matching, incorporating spatial position information.","Such a spatially-aware and character-matching method offers a more accurate and equitable evaluation compared with previous BLEU and Edit Distance metrics that rely solely on text-based character matching.","Experimentally, we evaluated various formula recognition models using CDM, BLEU, and ExpRate metrics.","Their results demonstrate that the CDM aligns more closely with human evaluation standards and provides a fairer comparison across different models by eliminating discrepancies caused by diverse formula representations."],"url":"http://arxiv.org/abs/2409.03643v1"}
{"created":"2024-09-05 15:48:02","title":"Surface-Centric Modeling for High-Fidelity Generalizable Neural Surface Reconstruction","abstract":"Reconstructing the high-fidelity surface from multi-view images, especially sparse images, is a critical and practical task that has attracted widespread attention in recent years. However, existing methods are impeded by the memory constraint or the requirement of ground-truth depths and cannot recover satisfactory geometric details. To this end, we propose SuRF, a new Surface-centric framework that incorporates a new Region sparsification based on a matching Field, achieving good trade-offs between performance, efficiency and scalability. To our knowledge, this is the first unsupervised method achieving end-to-end sparsification powered by the introduced matching field, which leverages the weight distribution to efficiently locate the boundary regions containing surface. Instead of predicting an SDF value for each voxel, we present a new region sparsification approach to sparse the volume by judging whether the voxel is inside the surface region. In this way, our model can exploit higher frequency features around the surface with less memory and computational consumption. Extensive experiments on multiple benchmarks containing complex large-scale scenes show that our reconstructions exhibit high-quality details and achieve new state-of-the-art performance, i.e., 46% improvements with 80% less memory consumption. Code is available at https://github.com/prstrive/SuRF.","sentences":["Reconstructing the high-fidelity surface from multi-view images, especially sparse images, is a critical and practical task that has attracted widespread attention in recent years.","However, existing methods are impeded by the memory constraint or the requirement of ground-truth depths and cannot recover satisfactory geometric details.","To this end, we propose SuRF, a new Surface-centric framework that incorporates a new Region sparsification based on a matching Field, achieving good trade-offs between performance, efficiency and scalability.","To our knowledge, this is the first unsupervised method achieving end-to-end sparsification powered by the introduced matching field, which leverages the weight distribution to efficiently locate the boundary regions containing surface.","Instead of predicting an SDF value for each voxel, we present a new region sparsification approach to sparse the volume by judging whether the voxel is inside the surface region.","In this way, our model can exploit higher frequency features around the surface with less memory and computational consumption.","Extensive experiments on multiple benchmarks containing complex large-scale scenes show that our reconstructions exhibit high-quality details and achieve new state-of-the-art performance, i.e., 46% improvements with 80% less memory consumption.","Code is available at https://github.com/prstrive/SuRF."],"url":"http://arxiv.org/abs/2409.03634v1"}
{"created":"2024-09-05 15:47:04","title":"Beyond Model Interpretability: Socio-Structural Explanations in Machine Learning","abstract":"What is it to interpret the outputs of an opaque machine learning model. One approach is to develop interpretable machine learning techniques. These techniques aim to show how machine learning models function by providing either model centric local or global explanations, which can be based on mechanistic interpretations revealing the inner working mechanisms of models or nonmechanistic approximations showing input feature output data relationships. In this paper, we draw on social philosophy to argue that interpreting machine learning outputs in certain normatively salient domains could require appealing to a third type of explanation that we call sociostructural explanation. The relevance of this explanation type is motivated by the fact that machine learning models are not isolated entities but are embedded within and shaped by social structures. Sociostructural explanations aim to illustrate how social structures contribute to and partially explain the outputs of machine learning models. We demonstrate the importance of sociostructural explanations by examining a racially biased healthcare allocation algorithm. Our proposal highlights the need for transparency beyond model interpretability, understanding the outputs of machine learning systems could require a broader analysis that extends beyond the understanding of the machine learning model itself.","sentences":["What is it to interpret the outputs of an opaque machine learning model.","One approach is to develop interpretable machine learning techniques.","These techniques aim to show how machine learning models function by providing either model centric local or global explanations, which can be based on mechanistic interpretations revealing the inner working mechanisms of models or nonmechanistic approximations showing input feature output data relationships.","In this paper, we draw on social philosophy to argue that interpreting machine learning outputs in certain normatively salient domains could require appealing to a third type of explanation that we call sociostructural explanation.","The relevance of this explanation type is motivated by the fact that machine learning models are not isolated entities but are embedded within and shaped by social structures.","Sociostructural explanations aim to illustrate how social structures contribute to and partially explain the outputs of machine learning models.","We demonstrate the importance of sociostructural explanations by examining a racially biased healthcare allocation algorithm.","Our proposal highlights the need for transparency beyond model interpretability, understanding the outputs of machine learning systems could require a broader analysis that extends beyond the understanding of the machine learning model itself."],"url":"http://arxiv.org/abs/2409.03632v1"}
{"created":"2024-09-05 15:35:53","title":"On the Compliance of Self-Sovereign Identity with GDPR Principles: A Critical Review","abstract":"Identity Management Systems (IdMs) have complemented how users are identified, authenticated, and authorised on e-services. Among the methods used for this purpose are traditional IdMs (isolated, centralised and federated) that mostly rely on identity providers (IdPs) to broker trust between a user and service-providers (SPs). An IdP also identifies and authenticates a user on-behalf of the SP, who then determines the authorisation of the user. In these processes, both SP and IdP collect, process or store private users' data, which can be prone to breach. One approach to address the data breach is to relieve the IdP, and return control and storage of personal data to the owner. Self-sovereign identity (SSI) was introduced as an IdM model to reduce the possibility of data breaches by offering control of personal data to the owner. SSI is a decentralised IdM, where the data owner has sovereign control of personal data stored in their digital wallet. Since SSI is an emerging technology, its components and methods require careful evaluation. This paper provides an evolution to IdMs and reviews the state-of-the-art SSI frameworks. We explored articles in the literature that reviewed blockchain solutions for General Data Protection Regulation (GDPR). We systematically searched recent SSI and blockchain proposals, evaluated the compliance of the retrieved documents with the GDPR privacy principles, and discussed their potentials, constraints, and limitations. This work identifies potential research gaps and opportunities.","sentences":["Identity Management Systems (IdMs) have complemented how users are identified, authenticated, and authorised on e-services.","Among the methods used for this purpose are traditional IdMs (isolated, centralised and federated) that mostly rely on identity providers (IdPs) to broker trust between a user and service-providers (SPs).","An IdP also identifies and authenticates a user on-behalf of the SP, who then determines the authorisation of the user.","In these processes, both SP and IdP collect, process or store private users' data, which can be prone to breach.","One approach to address the data breach is to relieve the IdP, and return control and storage of personal data to the owner.","Self-sovereign identity (SSI) was introduced as an IdM model to reduce the possibility of data breaches by offering control of personal data to the owner.","SSI is a decentralised IdM, where the data owner has sovereign control of personal data stored in their digital wallet.","Since SSI is an emerging technology, its components and methods require careful evaluation.","This paper provides an evolution to IdMs and reviews the state-of-the-art SSI frameworks.","We explored articles in the literature that reviewed blockchain solutions for General Data Protection Regulation (GDPR).","We systematically searched recent SSI and blockchain proposals, evaluated the compliance of the retrieved documents with the GDPR privacy principles, and discussed their potentials, constraints, and limitations.","This work identifies potential research gaps and opportunities."],"url":"http://arxiv.org/abs/2409.03624v1"}
{"created":"2024-09-05 15:33:24","title":"Attend First, Consolidate Later: On the Importance of Attention in Different LLM Layers","abstract":"In decoder-based LLMs, the representation of a given layer serves two purposes: as input to the next layer during the computation of the current token; and as input to the attention mechanism of future tokens. In this work, we show that the importance of the latter role might be overestimated. To show that, we start by manipulating the representations of previous tokens; e.g. by replacing the hidden states at some layer k with random vectors. Our experimenting with four LLMs and four tasks show that this operation often leads to small to negligible drop in performance. Importantly, this happens if the manipulation occurs in the top part of the model-k is in the final 30-50% of the layers. In contrast, doing the same manipulation in earlier layers might lead to chance level performance. We continue by switching the hidden state of certain tokens with hidden states of other tokens from another prompt; e.g., replacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\". We find that when applying this switch in the top 1/3 of the model, the model ignores it (answering \"Rome\"). However if we apply it before, the model conforms to the switch (\"Paris\"). Our results hint at a two stage process in transformer-based LLMs: the first part gathers input from previous tokens, while the second mainly processes that information internally.","sentences":["In decoder-based LLMs, the representation of a given layer serves two purposes: as input to the next layer during the computation of the current token; and as input to the attention mechanism of future tokens.","In this work, we show that the importance of the latter role might be overestimated.","To show that, we start by manipulating the representations of previous tokens; e.g. by replacing the hidden states at some layer k with random vectors.","Our experimenting with four LLMs and four tasks show that this operation often leads to small to negligible drop in performance.","Importantly, this happens if the manipulation occurs in the top part of the model-k is in the final 30-50% of the layers.","In contrast, doing the same manipulation in earlier layers might lead to chance level performance.","We continue by switching the hidden state of certain tokens with hidden states of other tokens from another prompt; e.g., replacing the word \"Italy\" with \"France\" in \"What is the capital of Italy?\".","We find that when applying this switch in the top 1/3 of the model, the model ignores it (answering \"Rome\").","However if we apply it before, the model conforms to the switch (\"Paris\").","Our results hint at a two stage process in transformer-based LLMs: the first part gathers input from previous tokens, while the second mainly processes that information internally."],"url":"http://arxiv.org/abs/2409.03621v1"}
{"created":"2024-09-05 15:18:44","title":"1 Modular Parallel Manipulator for Long-Term Soft Robotic Data Collection","abstract":"Performing long-term experimentation or large-scale data collection for machine learning in the field of soft robotics is challenging, due to the hardware robustness and experimental flexibility required. In this work, we propose a modular parallel robotic manipulation platform suitable for such large-scale data collection and compatible with various soft-robotic fabrication methods. Considering the computational and theoretical difficulty of replicating the high-fidelity, faster-than-real-time simulations that enable large-scale data collection in rigid robotic systems, a robust soft-robotic hardware platform becomes a high priority development task for the field.   The platform's modules consist of a pair of off-the-shelf electrical motors which actuate a customizable finger consisting of a compliant parallel structure. The parallel mechanism of the finger can be as simple as a single 3D-printed urethane or molded silicone bulk structure, due to the motors being able to fully actuate a passive structure. This design flexibility allows experimentation with soft mechanism varied geometries, bulk properties and surface properties. Additionally, while the parallel mechanism does not require separate electronics or additional parts, these can be included, and it can be constructed using multi-functional soft materials to study compatible soft sensors and actuators in the learning process. In this work, we validate the platform's ability to be used for policy gradient reinforcement learning directly on hardware in a benchmark 2D manipulation task. We additionally demonstrate compatibility with multiple fingers and characterize the design constraints for compatible extensions.","sentences":["Performing long-term experimentation or large-scale data collection for machine learning in the field of soft robotics is challenging, due to the hardware robustness and experimental flexibility required.","In this work, we propose a modular parallel robotic manipulation platform suitable for such large-scale data collection and compatible with various soft-robotic fabrication methods.","Considering the computational and theoretical difficulty of replicating the high-fidelity, faster-than-real-time simulations that enable large-scale data collection in rigid robotic systems, a robust soft-robotic hardware platform becomes a high priority development task for the field.   ","The platform's modules consist of a pair of off-the-shelf electrical motors which actuate a customizable finger consisting of a compliant parallel structure.","The parallel mechanism of the finger can be as simple as a single 3D-printed urethane or molded silicone bulk structure, due to the motors being able to fully actuate a passive structure.","This design flexibility allows experimentation with soft mechanism varied geometries, bulk properties and surface properties.","Additionally, while the parallel mechanism does not require separate electronics or additional parts, these can be included, and it can be constructed using multi-functional soft materials to study compatible soft sensors and actuators in the learning process.","In this work, we validate the platform's ability to be used for policy gradient reinforcement learning directly on hardware in a benchmark 2D manipulation task.","We additionally demonstrate compatibility with multiple fingers and characterize the design constraints for compatible extensions."],"url":"http://arxiv.org/abs/2409.03614v1"}
{"created":"2024-09-05 15:17:26","title":"VFLGAN-TS: Vertical Federated Learning-based Generative Adversarial Networks for Publication of Vertically Partitioned Time-Series Data","abstract":"In the current artificial intelligence (AI) era, the scale and quality of the dataset play a crucial role in training a high-quality AI model. However, often original data cannot be shared due to privacy concerns and regulations. A potential solution is to release a synthetic dataset with a similar distribution to the private dataset. Nevertheless, in some scenarios, the attributes required to train an AI model are distributed among different parties, and the parties cannot share the local data for synthetic data construction due to privacy regulations. In PETS 2024, we recently introduced the first Vertical Federated Learning-based Generative Adversarial Network (VFLGAN) for publishing vertically partitioned static data. However, VFLGAN cannot effectively handle time-series data, presenting both temporal and attribute dimensions. In this article, we proposed VFLGAN-TS, which combines the ideas of attribute discriminator and vertical federated learning to generate synthetic time-series data in the vertically partitioned scenario. The performance of VFLGAN-TS is close to that of its counterpart, which is trained in a centralized manner and represents the upper limit for VFLGAN-TS. To further protect privacy, we apply a Gaussian mechanism to make VFLGAN-TS satisfy an $(\\epsilon,\\delta)$-differential privacy. Besides, we develop an enhanced privacy auditing scheme to evaluate the potential privacy breach through the framework of VFLGAN-TS and synthetic datasets.","sentences":["In the current artificial intelligence (AI) era, the scale and quality of the dataset play a crucial role in training a high-quality AI model.","However, often original data cannot be shared due to privacy concerns and regulations.","A potential solution is to release a synthetic dataset with a similar distribution to the private dataset.","Nevertheless, in some scenarios, the attributes required to train an AI model are distributed among different parties, and the parties cannot share the local data for synthetic data construction due to privacy regulations.","In PETS 2024, we recently introduced the first Vertical Federated Learning-based Generative Adversarial Network (VFLGAN) for publishing vertically partitioned static data.","However, VFLGAN cannot effectively handle time-series data, presenting both temporal and attribute dimensions.","In this article, we proposed VFLGAN-TS, which combines the ideas of attribute discriminator and vertical federated learning to generate synthetic time-series data in the vertically partitioned scenario.","The performance of VFLGAN-TS is close to that of its counterpart, which is trained in a centralized manner and represents the upper limit for VFLGAN-TS.","To further protect privacy, we apply a Gaussian mechanism to make VFLGAN-TS satisfy an $(\\epsilon,\\delta)$-differential privacy.","Besides, we develop an enhanced privacy auditing scheme to evaluate the potential privacy breach through the framework of VFLGAN-TS and synthetic datasets."],"url":"http://arxiv.org/abs/2409.03612v1"}
{"created":"2024-09-05 15:16:37","title":"Reimagining Data Visualization to Address Sustainability Goals","abstract":"Information visualization holds significant potential to support sustainability goals such as environmental stewardship, and climate resilience by transforming complex data into accessible visual formats that enhance public understanding of complex climate change data and drive actionable insights. While the field has predominantly focused on analytical orientation of visualization, challenging traditional visualization techniques and goals, through critical visualization research expands existing assumptions and conventions in the field. In this paper, I explore how reimagining overlooked aspects of data visualization, such as engagement, emotional resonance, communication, and community empowerment, can contribute to achieving sustainability objectives. I argue that by focusing on inclusive data visualization that promotes clarity, understandability, and public participation, we can make complex data more relatable and actionable, fostering broader connections and mobilizing collective action on critical issues like climate change. Moreover, I discuss the role of emotional receptivity in environmental data communication, stressing the need for visualizations that respect diverse cultural perspectives and emotional responses to achieve impactful outcomes. Drawing on insights from a decade of research in public participation and community engagement, I aim to highlight how data visualization can democratize data access and increase public involvement in order to contribute to a more sustainable and resilient future.","sentences":["Information visualization holds significant potential to support sustainability goals such as environmental stewardship, and climate resilience by transforming complex data into accessible visual formats that enhance public understanding of complex climate change data and drive actionable insights.","While the field has predominantly focused on analytical orientation of visualization, challenging traditional visualization techniques and goals, through critical visualization research expands existing assumptions and conventions in the field.","In this paper, I explore how reimagining overlooked aspects of data visualization, such as engagement, emotional resonance, communication, and community empowerment, can contribute to achieving sustainability objectives.","I argue that by focusing on inclusive data visualization that promotes clarity, understandability, and public participation, we can make complex data more relatable and actionable, fostering broader connections and mobilizing collective action on critical issues like climate change.","Moreover, I discuss the role of emotional receptivity in environmental data communication, stressing the need for visualizations that respect diverse cultural perspectives and emotional responses to achieve impactful outcomes.","Drawing on insights from a decade of research in public participation and community engagement, I aim to highlight how data visualization can democratize data access and increase public involvement in order to contribute to a more sustainable and resilient future."],"url":"http://arxiv.org/abs/2409.03611v1"}
{"created":"2024-09-05 15:11:40","title":"SegTalker: Segmentation-based Talking Face Generation with Mask-guided Local Editing","abstract":"Audio-driven talking face generation aims to synthesize video with lip movements synchronized to input audio. However, current generative techniques face challenges in preserving intricate regional textures (skin, teeth). To address the aforementioned challenges, we propose a novel framework called SegTalker to decouple lip movements and image textures by introducing segmentation as intermediate representation. Specifically, given the mask of image employed by a parsing network, we first leverage the speech to drive the mask and generate talking segmentation. Then we disentangle semantic regions of image into style codes using a mask-guided encoder. Ultimately, we inject the previously generated talking segmentation and style codes into a mask-guided StyleGAN to synthesize video frame. In this way, most of textures are fully preserved. Moreover, our approach can inherently achieve background separation and facilitate mask-guided facial local editing. In particular, by editing the mask and swapping the region textures from a given reference image (e.g. hair, lip, eyebrows), our approach enables facial editing seamlessly when generating talking face video. Experiments demonstrate that our proposed approach can effectively preserve texture details and generate temporally consistent video while remaining competitive in lip synchronization. Quantitative and qualitative results on the HDTF and MEAD datasets illustrate the superior performance of our method over existing methods.","sentences":["Audio-driven talking face generation aims to synthesize video with lip movements synchronized to input audio.","However, current generative techniques face challenges in preserving intricate regional textures (skin, teeth).","To address the aforementioned challenges, we propose a novel framework called SegTalker to decouple lip movements and image textures by introducing segmentation as intermediate representation.","Specifically, given the mask of image employed by a parsing network, we first leverage the speech to drive the mask and generate talking segmentation.","Then we disentangle semantic regions of image into style codes using a mask-guided encoder.","Ultimately, we inject the previously generated talking segmentation and style codes into a mask-guided StyleGAN to synthesize video frame.","In this way, most of textures are fully preserved.","Moreover, our approach can inherently achieve background separation and facilitate mask-guided facial local editing.","In particular, by editing the mask and swapping the region textures from a given reference image (e.g. hair, lip, eyebrows), our approach enables facial editing seamlessly when generating talking face video.","Experiments demonstrate that our proposed approach can effectively preserve texture details and generate temporally consistent video while remaining competitive in lip synchronization.","Quantitative and qualitative results on the HDTF and MEAD datasets illustrate the superior performance of our method over existing methods."],"url":"http://arxiv.org/abs/2409.03605v1"}
{"created":"2024-09-05 14:59:41","title":"TCDiff: Triple Condition Diffusion Model with 3D Constraints for Stylizing Synthetic Faces","abstract":"A robust face recognition model must be trained using datasets that include a large number of subjects and numerous samples per subject under varying conditions (such as pose, expression, age, noise, and occlusion). Due to ethical and privacy concerns, large-scale real face datasets have been discontinued, such as MS1MV3, and synthetic face generators have been proposed, utilizing GANs and Diffusion Models, such as SYNFace, SFace, DigiFace-1M, IDiff-Face, DCFace, and GANDiffFace, aiming to supply this demand. Some of these methods can produce high-fidelity realistic faces, but with low intra-class variance, while others generate high-variance faces with low identity consistency. In this paper, we propose a Triple Condition Diffusion Model (TCDiff) to improve face style transfer from real to synthetic faces through 2D and 3D facial constraints, enhancing face identity consistency while keeping the necessary high intra-class variance. Face recognition experiments using 1k, 2k, and 5k classes of our new dataset for training outperform state-of-the-art synthetic datasets in real face benchmarks such as LFW, CFP-FP, AgeDB, and BUPT. Our source code is available at: https://github.com/BOVIFOCR/tcdiff.","sentences":["A robust face recognition model must be trained using datasets that include a large number of subjects and numerous samples per subject under varying conditions (such as pose, expression, age, noise, and occlusion).","Due to ethical and privacy concerns, large-scale real face datasets have been discontinued, such as MS1MV3, and synthetic face generators have been proposed, utilizing GANs and Diffusion Models, such as SYNFace, SFace, DigiFace-1M, IDiff-Face, DCFace, and GANDiffFace, aiming to supply this demand.","Some of these methods can produce high-fidelity realistic faces, but with low intra-class variance, while others generate high-variance faces with low identity consistency.","In this paper, we propose a Triple Condition Diffusion Model (TCDiff) to improve face style transfer from real to synthetic faces through 2D and 3D facial constraints, enhancing face identity consistency while keeping the necessary high intra-class variance.","Face recognition experiments using 1k, 2k, and 5k classes of our new dataset for training outperform state-of-the-art synthetic datasets in real face benchmarks such as LFW, CFP-FP, AgeDB, and BUPT.","Our source code is available at: https://github.com/BOVIFOCR/tcdiff."],"url":"http://arxiv.org/abs/2409.03600v1"}
{"created":"2024-09-05 14:57:01","title":"A practical approach to evaluating the adversarial distance for machine learning classifiers","abstract":"Robustness is critical for machine learning (ML) classifiers to ensure consistent performance in real-world applications where models may encounter corrupted or adversarial inputs. In particular, assessing the robustness of classifiers to adversarial inputs is essential to protect systems from vulnerabilities and thus ensure safety in use. However, methods to accurately compute adversarial robustness have been challenging for complex ML models and high-dimensional data. Furthermore, evaluations typically measure adversarial accuracy on specific attack budgets, limiting the informative value of the resulting metrics. This paper investigates the estimation of the more informative adversarial distance using iterative adversarial attacks and a certification approach. Combined, the methods provide a comprehensive evaluation of adversarial robustness by computing estimates for the upper and lower bounds of the adversarial distance. We present visualisations and ablation studies that provide insights into how this evaluation method should be applied and parameterised. We find that our adversarial attack approach is effective compared to related implementations, while the certification method falls short of expectations. The approach in this paper should encourage a more informative way of evaluating the adversarial robustness of ML classifiers.","sentences":["Robustness is critical for machine learning (ML) classifiers to ensure consistent performance in real-world applications where models may encounter corrupted or adversarial inputs.","In particular, assessing the robustness of classifiers to adversarial inputs is essential to protect systems from vulnerabilities and thus ensure safety in use.","However, methods to accurately compute adversarial robustness have been challenging for complex ML models and high-dimensional data.","Furthermore, evaluations typically measure adversarial accuracy on specific attack budgets, limiting the informative value of the resulting metrics.","This paper investigates the estimation of the more informative adversarial distance using iterative adversarial attacks and a certification approach.","Combined, the methods provide a comprehensive evaluation of adversarial robustness by computing estimates for the upper and lower bounds of the adversarial distance.","We present visualisations and ablation studies that provide insights into how this evaluation method should be applied and parameterised.","We find that our adversarial attack approach is effective compared to related implementations, while the certification method falls short of expectations.","The approach in this paper should encourage a more informative way of evaluating the adversarial robustness of ML classifiers."],"url":"http://arxiv.org/abs/2409.03598v1"}
{"created":"2024-09-05 14:56:38","title":"Multimodal Laryngoscopic Video Analysis for Assisted Diagnosis of Vocal Cord Paralysis","abstract":"This paper presents the Multimodal Analyzing System for Laryngoscope (MASL), a system that combines audio and video data to automatically extract key segments and metrics from laryngeal videostroboscopic videos for clinical assessment. MASL integrates glottis detection with keyword spotting to analyze patient vocalizations and refine video highlights for better inspection of vocal cord movements. The system includes a strobing video extraction module that identifies frames by analyzing hue, saturation, and value fluctuations. MASL also provides effective metrics for vocal cord paralysis detection, employing a two-stage glottis segmentation process using U-Net followed by diffusion-based refinement to reduce false positives. Instead of glottal area waveforms, MASL estimates anterior glottic angle waveforms (AGAW) from glottis masks, evaluating both left and right vocal cords to detect unilateral vocal cord paralysis (UVFP). By comparing AGAW variances, MASL distinguishes between left and right paralysis. Ablation studies and experiments on public and real-world datasets validate MASL's segmentation module and demonstrate its ability to provide reliable metrics for UVFP diagnosis.","sentences":["This paper presents the Multimodal Analyzing System for Laryngoscope (MASL), a system that combines audio and video data to automatically extract key segments and metrics from laryngeal videostroboscopic videos for clinical assessment.","MASL integrates glottis detection with keyword spotting to analyze patient vocalizations and refine video highlights for better inspection of vocal cord movements.","The system includes a strobing video extraction module that identifies frames by analyzing hue, saturation, and value fluctuations.","MASL also provides effective metrics for vocal cord paralysis detection, employing a two-stage glottis segmentation process using U-Net followed by diffusion-based refinement to reduce false positives.","Instead of glottal area waveforms, MASL estimates anterior glottic angle waveforms (AGAW) from glottis masks, evaluating both left and right vocal cords to detect unilateral vocal cord paralysis (UVFP).","By comparing AGAW variances, MASL distinguishes between left and right paralysis.","Ablation studies and experiments on public and real-world datasets validate MASL's segmentation module and demonstrate its ability to provide reliable metrics for UVFP diagnosis."],"url":"http://arxiv.org/abs/2409.03597v1"}
{"created":"2024-09-05 14:54:06","title":"Constant Approximating Disjoint Paths on Acyclic Digraphs is W[1]-hard","abstract":"In the Disjoint Paths problem, one is given a graph with a set of $k$ vertex pairs $(s_i,t_i)$ and the task is to connect each $s_i$ to $t_i$ with a path, so that the $k$ paths are pairwise disjoint. In the optimization variant, Max Disjoint Paths, the goal is to maximize the number of vertex pairs to be connected. We study this problem on acyclic directed graphs, where Disjoint Paths is known to be W[1]-hard when parameterized by $k$. We show that in this setting Max Disjoint Paths is W[1]-hard to $c$-approximate for any constant $c$. To the best of our knowledge, this is the first non-trivial result regarding the parameterized approximation for Max Disjoint Paths with respect to the natural parameter $k$. Our proof is based on an elementary self-reduction that is guided by a certain combinatorial object constructed by the probabilistic method.","sentences":["In the Disjoint Paths problem, one is given a graph with a set of $k$ vertex pairs $(s_i,t_i)$ and the task is to connect each $s_i$ to $t_i$ with a path, so that the $k$ paths are pairwise disjoint.","In the optimization variant, Max Disjoint Paths, the goal is to maximize the number of vertex pairs to be connected.","We study this problem on acyclic directed graphs, where Disjoint Paths is known to be W[1]-hard when parameterized by $k$. We show that in this setting Max Disjoint Paths is W[1]-hard to $c$-approximate for any constant $c$. To the best of our knowledge, this is the first non-trivial result regarding the parameterized approximation for Max Disjoint Paths with respect to the natural parameter $k$.","Our proof is based on an elementary self-reduction that is guided by a certain combinatorial object constructed by the probabilistic method."],"url":"http://arxiv.org/abs/2409.03596v1"}
{"created":"2024-09-05 14:52:54","title":"A Complete Landscape of EFX Allocations of Mixed Manna on Graphs","abstract":"We study envy-free up to any item (EFX) allocations on graphs where vertices and edges represent agents and items respectively. An agent is only interested in items that are incident to her and all other items have zero marginal values to her. Christodoulou et al. [EC, 2023] first proposed this setting and studied the case of goods. We extend this setting to the case of mixed manna where an item may be liked or disliked by its endpoint agents. In our problem, an agent has an arbitrary valuation over her incident items such that the items she likes have non-negative marginal values to her and those she dislikes have non-positive marginal values. We provide a complete study of the four notions of EFX for mixed manna in the literature, which differ by whether the removed item can have zero marginal value. We prove that an allocation that satisfies the notion of EFX where the virtually-removed item could always have zero marginal value may not exist and determining its existence is NP-complete, while one that satisfies any of the other three notions always exists and can be computed in polynomial time. We also prove that an orientation (i.e., a special allocation where each edge must be allocated to one of its endpoint agents) that satisfies any of the four notions may not exist, and determining its existence is NP-complete.","sentences":["We study envy-free up to any item (EFX) allocations on graphs where vertices and edges represent agents and items respectively.","An agent is only interested in items that are incident to her and all other items have zero marginal values to her.","Christodoulou et al.","[EC, 2023] first proposed this setting and studied the case of goods.","We extend this setting to the case of mixed manna where an item may be liked or disliked by its endpoint agents.","In our problem, an agent has an arbitrary valuation over her incident items such that the items she likes have non-negative marginal values to her and those she dislikes have non-positive marginal values.","We provide a complete study of the four notions of EFX for mixed manna in the literature, which differ by whether the removed item can have zero marginal value.","We prove that an allocation that satisfies the notion of EFX where the virtually-removed item could always have zero marginal value may not exist and determining its existence is NP-complete, while one that satisfies any of the other three notions always exists and can be computed in polynomial time.","We also prove that an orientation (i.e., a special allocation where each edge must be allocated to one of its endpoint agents) that satisfies any of the four notions may not exist, and determining its existence is NP-complete."],"url":"http://arxiv.org/abs/2409.03594v1"}
{"created":"2024-09-05 14:43:11","title":"Costs Estimation in Unit Commitment Problems using Simulation-Based Inference","abstract":"The Unit Commitment (UC) problem is a key optimization task in power systems to forecast the generation schedules of power units over a finite time period by minimizing costs while meeting demand and technical constraints. However, many parameters required by the UC problem are unknown, such as the costs. In this work, we estimate these unknown costs using simulation-based inference on an illustrative UC problem, which provides an approximated posterior distribution of the parameters given observed generation schedules and demands. Our results highlight that the learned posterior distribution effectively captures the underlying distribution of the data, providing a range of possible values for the unknown parameters given a past observation. This posterior allows for the estimation of past costs using observed past generation schedules, enabling operators to better forecast future costs and make more robust generation scheduling forecasts. We present avenues for future research to address overconfidence in posterior estimation, enhance the scalability of the methodology and apply it to more complex UC problems modeling the network constraints and renewable energy sources.","sentences":["The Unit Commitment (UC) problem is a key optimization task in power systems to forecast the generation schedules of power units over a finite time period by minimizing costs while meeting demand and technical constraints.","However, many parameters required by the UC problem are unknown, such as the costs.","In this work, we estimate these unknown costs using simulation-based inference on an illustrative UC problem, which provides an approximated posterior distribution of the parameters given observed generation schedules and demands.","Our results highlight that the learned posterior distribution effectively captures the underlying distribution of the data, providing a range of possible values for the unknown parameters given a past observation.","This posterior allows for the estimation of past costs using observed past generation schedules, enabling operators to better forecast future costs and make more robust generation scheduling forecasts.","We present avenues for future research to address overconfidence in posterior estimation, enhance the scalability of the methodology and apply it to more complex UC problems modeling the network constraints and renewable energy sources."],"url":"http://arxiv.org/abs/2409.03588v1"}
{"created":"2024-09-05 14:37:43","title":"Text-Guided Mixup Towards Long-Tailed Image Categorization","abstract":"In many real-world applications, the frequency distribution of class labels for training data can exhibit a long-tailed distribution, which challenges traditional approaches of training deep neural networks that require heavy amounts of balanced data. Gathering and labeling data to balance out the class label distribution can be both costly and time-consuming. Many existing solutions that enable ensemble learning, re-balancing strategies, or fine-tuning applied to deep neural networks are limited by the inert problem of few class samples across a subset of classes. Recently, vision-language models like CLIP have been observed as effective solutions to zero-shot or few-shot learning by grasping a similarity between vision and language features for image and text pairs. Considering that large pre-trained vision-language models may contain valuable side textual information for minor classes, we propose to leverage text supervision to tackle the challenge of long-tailed learning. Concretely, we propose a novel text-guided mixup technique that takes advantage of the semantic relations between classes recognized by the pre-trained text encoder to help alleviate the long-tailed problem. Our empirical study on benchmark long-tailed tasks demonstrates the effectiveness of our proposal with a theoretical guarantee. Our code is available at https://github.com/rsamf/text-guided-mixup.","sentences":["In many real-world applications, the frequency distribution of class labels for training data can exhibit a long-tailed distribution, which challenges traditional approaches of training deep neural networks that require heavy amounts of balanced data.","Gathering and labeling data to balance out the class label distribution can be both costly and time-consuming.","Many existing solutions that enable ensemble learning, re-balancing strategies, or fine-tuning applied to deep neural networks are limited by the inert problem of few class samples across a subset of classes.","Recently, vision-language models like CLIP have been observed as effective solutions to zero-shot or few-shot learning by grasping a similarity between vision and language features for image and text pairs.","Considering that large pre-trained vision-language models may contain valuable side textual information for minor classes, we propose to leverage text supervision to tackle the challenge of long-tailed learning.","Concretely, we propose a novel text-guided mixup technique that takes advantage of the semantic relations between classes recognized by the pre-trained text encoder to help alleviate the long-tailed problem.","Our empirical study on benchmark long-tailed tasks demonstrates the effectiveness of our proposal with a theoretical guarantee.","Our code is available at https://github.com/rsamf/text-guided-mixup."],"url":"http://arxiv.org/abs/2409.03583v1"}
{"created":"2024-09-05 14:34:30","title":"Disjoint Compatibility via Graph Classes","abstract":"Two plane drawings of graphs on the same set of points are called disjoint compatible if their union is plane and they do not have an edge in common. Let $S$ be a convex point set of $2n \\geq 10$ points and let $\\mathcal{H}$ be a family of plane drawings on $S$. Two plane perfect matchings $M_1$ and $M_2$ on $S$ (which do not need to be disjoint nor compatible) are \\emph{disjoint $\\mathcal{H}$-compatible} if there exists a drawing in $\\mathcal{H}$ which is disjoint compatible to both $M_1$ and $M_2$ In this work, we consider the graph which has all plane perfect matchings as vertices and where two vertices are connected by an edge if the matchings are disjoint $\\mathcal{H}$-compatible. We study the diameter of this graph when $\\mathcal{H}$ is the family of all plane spanning trees, caterpillars or paths. We show that in the first two cases the graph is connected with constant and linear diameter, respectively, while in the third case it is disconnected.","sentences":["Two plane drawings of graphs on the same set of points are called disjoint compatible if their union is plane and they do not have an edge in common.","Let $S$ be a convex point set of $2n \\geq 10$ points and let $\\mathcal{H}$ be a family of plane drawings on $S$. Two plane perfect matchings $M_1$ and $M_2$ on $S$ (which do not need to be disjoint nor compatible) are \\emph{disjoint $\\mathcal{H}$-compatible} if there exists a drawing in $\\mathcal{H}$ which is disjoint compatible to both $M_1$ and $M_2$ In this work, we consider the graph which has all plane perfect matchings as vertices and where two vertices are connected by an edge if the matchings are disjoint $\\mathcal{H}$-compatible.","We study the diameter of this graph when $\\mathcal{H}$ is the family of all plane spanning trees, caterpillars or paths.","We show that in the first two cases the graph is connected with constant and linear diameter, respectively, while in the third case it is disconnected."],"url":"http://arxiv.org/abs/2409.03579v1"}
{"created":"2024-09-05 14:31:05","title":"CHIRPs: Change-Induced Regret Proxy metrics for Lifelong Reinforcement Learning","abstract":"Reinforcement learning agents can achieve superhuman performance in static tasks but are costly to train and fragile to task changes. This limits their deployment in real-world scenarios where training experience is expensive or the context changes through factors like sensor degradation, environmental processes or changing mission priorities. Lifelong reinforcement learning aims to improve sample efficiency and adaptability by studying how agents perform in evolving problems. The difficulty that these changes pose to an agent is rarely measured directly, however. Agent performances can be compared across a change, but this is often prohibitively expensive. We propose Change-Induced Regret Proxy (CHIRP) metrics, a class of metrics for approximating a change's difficulty while avoiding the high costs of using trained agents. A relationship between a CHIRP metric and agent performance is identified in two environments, a simple grid world and MetaWorld's suite of robotic arm tasks. We demonstrate two uses for these metrics: for learning, an agent that clusters MDPs based on a CHIRP metric achieves $17\\%$ higher average returns than three existing agents in a sequence of MetaWorld tasks. We also show how a CHIRP can be calibrated to compare the difficulty of changes across distinctly different environments.","sentences":["Reinforcement learning agents can achieve superhuman performance in static tasks but are costly to train and fragile to task changes.","This limits their deployment in real-world scenarios where training experience is expensive or the context changes through factors like sensor degradation, environmental processes or changing mission priorities.","Lifelong reinforcement learning aims to improve sample efficiency and adaptability by studying how agents perform in evolving problems.","The difficulty that these changes pose to an agent is rarely measured directly, however.","Agent performances can be compared across a change, but this is often prohibitively expensive.","We propose Change-Induced Regret Proxy (CHIRP) metrics, a class of metrics for approximating a change's difficulty while avoiding the high costs of using trained agents.","A relationship between a CHIRP metric and agent performance is identified in two environments, a simple grid world and MetaWorld's suite of robotic arm tasks.","We demonstrate two uses for these metrics: for learning, an agent that clusters MDPs based on a CHIRP metric achieves $17\\%$ higher average returns than three existing agents in a sequence of MetaWorld tasks.","We also show how a CHIRP can be calibrated to compare the difficulty of changes across distinctly different environments."],"url":"http://arxiv.org/abs/2409.03577v1"}
{"created":"2024-09-05 14:30:58","title":"Weight enumerators of self-dual quantum codes","abstract":"We use algebraic invariant theory to study three weight enumerators of self-dual quantum codes over finite fields. We show that the weight enumerators of self-dual quantum codes can be expressed algebraically by two polynomials and the double weight enumerators of self-dual quantum codes can be expressed algebraically by five polynomials. We also explicitly compute the complete weight enumerators of some special self-dual quantum codes. Our approach avoids applying the well-known Molien's formula and demonstrates the potential of employing invariant theory to compute weight enumerators of quantum codes.","sentences":["We use algebraic invariant theory to study three weight enumerators of self-dual quantum codes over finite fields.","We show that the weight enumerators of self-dual quantum codes can be expressed algebraically by two polynomials and the double weight enumerators of self-dual quantum codes can be expressed algebraically by five polynomials.","We also explicitly compute the complete weight enumerators of some special self-dual quantum codes.","Our approach avoids applying the well-known Molien's formula and demonstrates the potential of employing invariant theory to compute weight enumerators of quantum codes."],"url":"http://arxiv.org/abs/2409.03576v1"}
{"created":"2024-09-05 14:22:10","title":"QuAK: Quantitative Automata Kit","abstract":"System behaviors are traditionally evaluated through binary classifications of correctness, which do not suffice for properties involving quantitative aspects of systems and executions. Quantitative automata offer a more nuanced approach, mapping each execution to a real number by incorporating weighted transitions and value functions generalizing acceptance conditions. In this paper, we introduce QuAK, the first tool designed to automate the analysis of quantitative automata. QuAK currently supports a variety of quantitative automaton types, including Inf, Sup, LimInf, LimSup, LimInfAvg, and LimSupAvg automata, and implements decision procedures for problems such as emptiness, universality, inclusion, equivalence, as well as for checking whether an automaton is safe, live, or constant. Additionally, QuAK is able to compute extremal values when possible, construct safety-liveness decompositions, and monitor system behaviors. We demonstrate the effectiveness of QuAK through experiments focusing on the inclusion, constant-function check, and monitoring problems.","sentences":["System behaviors are traditionally evaluated through binary classifications of correctness, which do not suffice for properties involving quantitative aspects of systems and executions.","Quantitative automata offer a more nuanced approach, mapping each execution to a real number by incorporating weighted transitions and value functions generalizing acceptance conditions.","In this paper, we introduce QuAK, the first tool designed to automate the analysis of quantitative automata.","QuAK currently supports a variety of quantitative automaton types, including Inf, Sup, LimInf, LimSup, LimInfAvg, and LimSupAvg automata, and implements decision procedures for problems such as emptiness, universality, inclusion, equivalence, as well as for checking whether an automaton is safe, live, or constant.","Additionally, QuAK is able to compute extremal values when possible, construct safety-liveness decompositions, and monitor system behaviors.","We demonstrate the effectiveness of QuAK through experiments focusing on the inclusion, constant-function check, and monitoring problems."],"url":"http://arxiv.org/abs/2409.03569v1"}
{"created":"2024-09-05 14:22:02","title":"Enabling Practical and Privacy-Preserving Image Processing","abstract":"Fully Homomorphic Encryption (FHE) enables computations on encrypted data, preserving confidentiality without the need for decryption. However, FHE is often hindered by significant performance overhead, particularly for high-precision and complex data like images. Due to serious efficiency issues, traditional FHE methods often encrypt images by monolithic data blocks (such as pixel rows), instead of pixels. However, this strategy compromises the advantages of homomorphic operations and disables pixel-level image processing. In this study, we address these challenges by proposing and implementing a pixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS scheme. To enhance computational efficiency, we introduce three novel caching mechanisms to pre-encrypt radix values or frequently occurring pixel values, substantially reducing redundant encryption operations. Extensive experiments demonstrate that our approach achieves up to a 19-fold improvement in encryption speed compared to the original CKKS, while maintaining high image quality. Additionally, real-world image applications such as mean filtering, brightness enhancement, image matching and watermarking are tested based on FHE, showcasing up to a 91.53% speed improvement. We also proved that our method is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure, providing strong encryption security. These results underscore the practicality and efficiency of iCHEETAH, marking a significant advancement in privacy-preserving image processing at scale.","sentences":["Fully Homomorphic Encryption (FHE) enables computations on encrypted data, preserving confidentiality without the need for decryption.","However, FHE is often hindered by significant performance overhead, particularly for high-precision and complex data like images.","Due to serious efficiency issues, traditional FHE methods often encrypt images by monolithic data blocks (such as pixel rows), instead of pixels.","However, this strategy compromises the advantages of homomorphic operations and disables pixel-level image processing.","In this study, we address these challenges by proposing and implementing a pixel-level homomorphic encryption approach, iCHEETAH, based on the CKKS scheme.","To enhance computational efficiency, we introduce three novel caching mechanisms to pre-encrypt radix values or frequently occurring pixel values, substantially reducing redundant encryption operations.","Extensive experiments demonstrate that our approach achieves up to a 19-fold improvement in encryption speed compared to the original CKKS, while maintaining high image quality.","Additionally, real-world image applications such as mean filtering, brightness enhancement, image matching and watermarking are tested based on FHE, showcasing up to a 91.53% speed improvement.","We also proved that our method is IND-CPA (Indistinguishability under Chosen Plaintext Attack) secure, providing strong encryption security.","These results underscore the practicality and efficiency of iCHEETAH, marking a significant advancement in privacy-preserving image processing at scale."],"url":"http://arxiv.org/abs/2409.03568v1"}
{"created":"2024-09-05 14:19:45","title":"100 instances is all you need: predicting the success of a new LLM on unseen data by testing on a few instances","abstract":"Predicting the performance of LLMs on individual task instances is essential to ensure their reliability in high-stakes applications. To do so, a possibility is to evaluate the considered LLM on a set of task instances and train an assessor to predict its performance based on features of the instances. However, this approach requires evaluating each new LLM on a sufficiently large set of task instances to train an assessor specific to it. In this work, we leverage the evaluation results of previously tested LLMs to reduce the number of evaluations required to predict the performance of a new LLM. In practice, we propose to test the new LLM on a small set of reference instances and train a generic assessor which predicts the performance of the LLM on an instance based on the performance of the former on the reference set and features of the instance of interest. We conduct empirical studies on HELM-Lite and KindsOfReasoning, a collection of existing reasoning datasets that we introduce, where we evaluate all instruction-fine-tuned OpenAI models until the January 2024 version of GPT4. When predicting performance on instances with the same distribution as those used to train the generic assessor, we find this achieves performance comparable to the LLM-specific assessors trained on the full set of instances. Additionally, we find that randomly selecting the reference instances performs as well as some advanced selection methods we tested. For out of distribution, however, no clear winner emerges and the overall performance is worse, suggesting that the inherent predictability of LLMs is low.","sentences":["Predicting the performance of LLMs on individual task instances is essential to ensure their reliability in high-stakes applications.","To do so, a possibility is to evaluate the considered LLM on a set of task instances and train an assessor to predict its performance based on features of the instances.","However, this approach requires evaluating each new LLM on a sufficiently large set of task instances to train an assessor specific to it.","In this work, we leverage the evaluation results of previously tested LLMs to reduce the number of evaluations required to predict the performance of a new LLM.","In practice, we propose to test the new LLM on a small set of reference instances and train a generic assessor which predicts the performance of the LLM on an instance based on the performance of the former on the reference set and features of the instance of interest.","We conduct empirical studies on HELM-Lite and KindsOfReasoning, a collection of existing reasoning datasets that we introduce, where we evaluate all instruction-fine-tuned OpenAI models until the January 2024 version of GPT4.","When predicting performance on instances with the same distribution as those used to train the generic assessor, we find this achieves performance comparable to the LLM-specific assessors trained on the full set of instances.","Additionally, we find that randomly selecting the reference instances performs as well as some advanced selection methods we tested.","For out of distribution, however, no clear winner emerges and the overall performance is worse, suggesting that the inherent predictability of LLMs is low."],"url":"http://arxiv.org/abs/2409.03563v1"}
{"created":"2024-09-05 14:19:21","title":"Communication-Assisted Sensing Systems: Fundamental Limits and ISAC Waveform Design","abstract":"The communication-assisted sensing (CAS) systems are expected to endow the users with beyond-line-of-sight sensing capabilities without the aid of additional sensors. In this paper, we study the dual-functional signaling strategy, focusing on three primary aspects, namely, the information-theoretic framework, the optimal distribution of channel input, and the optimal waveform design for Gaussian signals. First, we establish the information-theoretic framework and develop a modified source-channel separation theorem (MSST) tailored for CAS systems. The proposed MSST elucidates the relationship between achievable distortion, coding rate, and communication channel capacity in cases where the distortion metric is separable for sensing and communication (S\\&C) processes. Second, we present an optimal channel input design for dual-functional signaling, which aims to minimize total distortion under the constraints of the MSST and resource budget. We then conceive a two-step Blahut-Arimoto (BA)-based optimal search algorithm to numerically solve the functional optimization problem. Third, in light of the current signaling strategy, we further propose an optimal waveform design for Gaussian signaling in multi-input multi-output (MIMO) CAS systems. The associated covariance matrix optimization problem is addressed using a successive convex approximation (SCA)-based waveform design algorithm. Finally, we provide numerical simulation results to demonstrate the effectiveness of the proposed algorithms and to show the unique performance tradeoff between S\\&C processes.","sentences":["The communication-assisted sensing (CAS) systems are expected to endow the users with beyond-line-of-sight sensing capabilities without the aid of additional sensors.","In this paper, we study the dual-functional signaling strategy, focusing on three primary aspects, namely, the information-theoretic framework, the optimal distribution of channel input, and the optimal waveform design for Gaussian signals.","First, we establish the information-theoretic framework and develop a modified source-channel separation theorem (MSST) tailored for CAS systems.","The proposed MSST elucidates the relationship between achievable distortion, coding rate, and communication channel capacity in cases where the distortion metric is separable for sensing and communication (S\\&C) processes.","Second, we present an optimal channel input design for dual-functional signaling, which aims to minimize total distortion under the constraints of the MSST and resource budget.","We then conceive a two-step Blahut-Arimoto (BA)-based optimal search algorithm to numerically solve the functional optimization problem.","Third, in light of the current signaling strategy, we further propose an optimal waveform design for Gaussian signaling in multi-input multi-output (MIMO) CAS systems.","The associated covariance matrix optimization problem is addressed using a successive convex approximation (SCA)-based waveform design algorithm.","Finally, we provide numerical simulation results to demonstrate the effectiveness of the proposed algorithms and to show the unique performance tradeoff between S\\&C processes."],"url":"http://arxiv.org/abs/2409.03561v1"}
{"created":"2024-09-05 14:17:01","title":"MaskVal: Simple but Effective Uncertainty Quantification for 6D Pose Estimation","abstract":"For the use of 6D pose estimation in robotic applications, reliable poses are of utmost importance to ensure a safe, reliable and predictable operational performance. Despite these requirements, state-of-the-art 6D pose estimators often do not provide any uncertainty quantification for their pose estimates at all, or if they do, it has been shown that the uncertainty provided is only weakly correlated with the actual true error. To address this issue, we investigate a simple but effective uncertainty quantification, that we call MaskVal, which compares the pose estimates with their corresponding instance segmentations by rendering and does not require any modification of the pose estimator itself. Despite its simplicity, MaskVal significantly outperforms a state-of-the-art ensemble method on both a dataset and a robotic setup. We show that by using MaskVal, the performance of a state-of-the-art 6D pose estimator is significantly improved towards a safe and reliable operation. In addition, we propose a new and specific approach to compare and evaluate uncertainty quantification methods for 6D pose estimation in the context of robotic manipulation.","sentences":["For the use of 6D pose estimation in robotic applications, reliable poses are of utmost importance to ensure a safe, reliable and predictable operational performance.","Despite these requirements, state-of-the-art 6D pose estimators often do not provide any uncertainty quantification for their pose estimates at all, or if they do, it has been shown that the uncertainty provided is only weakly correlated with the actual true error.","To address this issue, we investigate a simple but effective uncertainty quantification, that we call MaskVal, which compares the pose estimates with their corresponding instance segmentations by rendering and does not require any modification of the pose estimator itself.","Despite its simplicity, MaskVal significantly outperforms a state-of-the-art ensemble method on both a dataset and a robotic setup.","We show that by using MaskVal, the performance of a state-of-the-art 6D pose estimator is significantly improved towards a safe and reliable operation.","In addition, we propose a new and specific approach to compare and evaluate uncertainty quantification methods for 6D pose estimation in the context of robotic manipulation."],"url":"http://arxiv.org/abs/2409.03556v1"}
{"created":"2024-09-05 14:15:54","title":"Unified Framework for Neural Network Compression via Decomposition and Optimal Rank Selection","abstract":"Despite their high accuracy, complex neural networks demand significant computational resources, posing challenges for deployment on resource-constrained devices such as mobile phones and embedded systems. Compression algorithms have been developed to address these challenges by reducing model size and computational demands while maintaining accuracy. Among these approaches, factorization methods based on tensor decomposition are theoretically sound and effective. However, they face difficulties in selecting the appropriate rank for decomposition. This paper tackles this issue by presenting a unified framework that simultaneously applies decomposition and optimal rank selection, employing a composite compression loss within defined rank constraints. Our approach includes an automatic rank search in a continuous space, efficiently identifying optimal rank configurations without the use of training data, making it computationally efficient. Combined with a subsequent fine-tuning step, our approach maintains the performance of highly compressed models on par with their original counterparts. Using various benchmark datasets, we demonstrate the efficacy of our method through a comprehensive analysis.","sentences":["Despite their high accuracy, complex neural networks demand significant computational resources, posing challenges for deployment on resource-constrained devices such as mobile phones and embedded systems.","Compression algorithms have been developed to address these challenges by reducing model size and computational demands while maintaining accuracy.","Among these approaches, factorization methods based on tensor decomposition are theoretically sound and effective.","However, they face difficulties in selecting the appropriate rank for decomposition.","This paper tackles this issue by presenting a unified framework that simultaneously applies decomposition and optimal rank selection, employing a composite compression loss within defined rank constraints.","Our approach includes an automatic rank search in a continuous space, efficiently identifying optimal rank configurations without the use of training data, making it computationally efficient.","Combined with a subsequent fine-tuning step, our approach maintains the performance of highly compressed models on par with their original counterparts.","Using various benchmark datasets, we demonstrate the efficacy of our method through a comprehensive analysis."],"url":"http://arxiv.org/abs/2409.03555v1"}
{"created":"2024-09-05 14:13:05","title":"Organized Grouped Discrete Representation for Object-Centric Learning","abstract":"Object-Centric Learning (OCL) represents dense image or video pixels as sparse object features. Representative methods utilize discrete representation composed of Variational Autoencoder (VAE) template features to suppress pixel-level information redundancy and guide object-level feature aggregation. The most recent advancement, Grouped Discrete Representation (GDR), further decomposes these template features into attributes. However, its naive channel grouping as decomposition may erroneously group channels belonging to different attributes together and discretize them as sub-optimal template attributes, which losses information and harms expressivity. We propose Organized GDR (OGDR) to organize channels belonging to the same attributes together for correct decomposition from features into attributes. In unsupervised segmentation experiments, OGDR is fully superior to GDR in augmentating classical transformer-based OCL methods; it even improves state-of-the-art diffusion-based ones. Codebook PCA and representation similarity analyses show that compared with GDR, our OGDR eliminates redundancy and preserves information better for guiding object representation learning. The source code is available in the supplementary material.","sentences":["Object-Centric Learning (OCL) represents dense image or video pixels as sparse object features.","Representative methods utilize discrete representation composed of Variational Autoencoder (VAE) template features to suppress pixel-level information redundancy and guide object-level feature aggregation.","The most recent advancement, Grouped Discrete Representation (GDR), further decomposes these template features into attributes.","However, its naive channel grouping as decomposition may erroneously group channels belonging to different attributes together and discretize them as sub-optimal template attributes, which losses information and harms expressivity.","We propose Organized GDR (OGDR) to organize channels belonging to the same attributes together for correct decomposition from features into attributes.","In unsupervised segmentation experiments, OGDR is fully superior to GDR in augmentating classical transformer-based OCL methods; it even improves state-of-the-art diffusion-based ones.","Codebook PCA and representation similarity analyses show that compared with GDR, our OGDR eliminates redundancy and preserves information better for guiding object representation learning.","The source code is available in the supplementary material."],"url":"http://arxiv.org/abs/2409.03553v1"}
{"created":"2024-09-05 14:12:51","title":"On the Optimal Performance of Distributed Cell-Free Massive MIMO with LoS Propagation","abstract":"In this study, we revisit the performance analysis of distributed beamforming architectures in dense user-centric cell-free massive multiple-input multiple-output (mMIMO) systems in line-of-sight (LoS) scenarios. By incorporating a recently developed optimal distributed beamforming technique, called the team minimum mean square error (TMMSE) technique, we depart from previous studies that rely on suboptimal distributed beamforming approaches for LoS scenarios. Supported by extensive numerical simulations that follow 3GPP guidelines, we show that such suboptimal approaches may often lead to significant underestimation of the capabilities of distributed architectures, particularly in the presence of strong LoS paths. Considering the anticipated ultra-dense nature of cell-free mMIMO networks and the consequential high likelihood of strong LoS paths, our findings reveal that the team MMSE technique may significantly contribute in narrowing the performance gap between centralized and distributed architectures.","sentences":["In this study, we revisit the performance analysis of distributed beamforming architectures in dense user-centric cell-free massive multiple-input multiple-output (mMIMO) systems in line-of-sight (LoS) scenarios.","By incorporating a recently developed optimal distributed beamforming technique, called the team minimum mean square error (TMMSE) technique, we depart from previous studies that rely on suboptimal distributed beamforming approaches for LoS scenarios.","Supported by extensive numerical simulations that follow 3GPP guidelines, we show that such suboptimal approaches may often lead to significant underestimation of the capabilities of distributed architectures, particularly in the presence of strong LoS paths.","Considering the anticipated ultra-dense nature of cell-free mMIMO networks and the consequential high likelihood of strong LoS paths, our findings reveal that the team MMSE technique may significantly contribute in narrowing the performance gap between centralized and distributed architectures."],"url":"http://arxiv.org/abs/2409.03551v1"}
{"created":"2024-09-05 14:12:22","title":"DKDM: Data-Free Knowledge Distillation for Diffusion Models with Any Architecture","abstract":"Diffusion models (DMs) have demonstrated exceptional generative capabilities across various areas, while they are hindered by slow inference speeds and high computational demands during deployment. The most common way to accelerate DMs involves reducing the number of denoising steps during generation, achieved through faster sampling solvers or knowledge distillation (KD). In contrast to prior approaches, we propose a novel method that transfers the capability of large pretrained DMs to faster architectures. Specifically, we employ KD in a distinct manner to compress DMs by distilling their generative ability into more rapid variants. Furthermore, considering that the source data is either unaccessible or too enormous to store for current generative models, we introduce a new paradigm for their distillation without source data, termed Data-Free Knowledge Distillation for Diffusion Models (DKDM). Generally, our established DKDM framework comprises two main components: 1) a DKDM objective that uses synthetic denoising data produced by pretrained DMs to optimize faster DMs without source data, and 2) a dynamic iterative distillation method that flexibly organizes the synthesis of denoising data, preventing it from slowing down the optimization process as the generation is slow. To our knowledge, this is the first attempt at using KD to distill DMs into any architecture in a data-free manner. Importantly, our DKDM is orthogonal to most existing acceleration methods, such as denoising step reduction, quantization and pruning. Experiments show that our DKDM is capable of deriving 2x faster DMs with performance remaining on par with the baseline. Notably, our DKDM enables pretrained DMs to function as \"datasets\" for training new DMs.","sentences":["Diffusion models (DMs) have demonstrated exceptional generative capabilities across various areas, while they are hindered by slow inference speeds and high computational demands during deployment.","The most common way to accelerate DMs involves reducing the number of denoising steps during generation, achieved through faster sampling solvers or knowledge distillation (KD).","In contrast to prior approaches, we propose a novel method that transfers the capability of large pretrained DMs to faster architectures.","Specifically, we employ KD in a distinct manner to compress DMs by distilling their generative ability into more rapid variants.","Furthermore, considering that the source data is either unaccessible or too enormous to store for current generative models, we introduce a new paradigm for their distillation without source data, termed Data-Free Knowledge Distillation for Diffusion Models (DKDM).","Generally, our established DKDM framework comprises two main components: 1) a DKDM objective that uses synthetic denoising data produced by pretrained DMs to optimize faster DMs without source data, and 2) a dynamic iterative distillation method that flexibly organizes the synthesis of denoising data, preventing it from slowing down the optimization process as the generation is slow.","To our knowledge, this is the first attempt at using KD to distill DMs into any architecture in a data-free manner.","Importantly, our DKDM is orthogonal to most existing acceleration methods, such as denoising step reduction, quantization and pruning.","Experiments show that our DKDM is capable of deriving 2x faster DMs with performance remaining on par with the baseline.","Notably, our DKDM enables pretrained DMs to function as \"datasets\" for training new DMs."],"url":"http://arxiv.org/abs/2409.03550v1"}
{"created":"2024-09-05 14:09:29","title":"A Silicon Photonic Neural Network for Chromatic Dispersion Compensation in 20 Gbps PAM4 Signal at 125 km and Its Scalability up to 100 Gbps","abstract":"A feed-forward photonic neural network (PNN) is tested for chromatic dispersion compensation in Intensity Modulation/Direct Detection optical links. The PNN is based on a sequence of linear and nonlinear transformations. The linear stage is constituted by an 8-tap time-delayed complex perceptron implemented on a Silicon-On-insulator platform and acting as a tunable optical filter. The nonlinear stage is provided by the square modulus of the electrical field applied at the end-of-line photodetector. The training maximizes the separation between the optical levels (i.e. the eye diagram aperture), with consequent reduction of the Bit Error Rate. Effective equalization is experimentally demonstrated for 20 Gbps 4-level Pulse Amplitude Modulated signal up to 125 km. An evolutionary algorithm and a gradient-based approach are tested for the training and then compared in terms of repeatability and convergence time. The optimal weights resulting from the training are interpreted in light of the theoretical transfer function of the optical fiber. Finally, a simulative study proves the scalability of the layout to larger bandwidths, up to 100 Gbps.","sentences":["A feed-forward photonic neural network (PNN) is tested for chromatic dispersion compensation in Intensity Modulation/Direct Detection optical links.","The PNN is based on a sequence of linear and nonlinear transformations.","The linear stage is constituted by an 8-tap time-delayed complex perceptron implemented on a Silicon-On-insulator platform and acting as a tunable optical filter.","The nonlinear stage is provided by the square modulus of the electrical field applied at the end-of-line photodetector.","The training maximizes the separation between the optical levels (i.e. the eye diagram aperture), with consequent reduction of the Bit Error Rate.","Effective equalization is experimentally demonstrated for 20 Gbps 4-level Pulse Amplitude Modulated signal up to 125 km.","An evolutionary algorithm and a gradient-based approach are tested for the training and then compared in terms of repeatability and convergence time.","The optimal weights resulting from the training are interpreted in light of the theoretical transfer function of the optical fiber.","Finally, a simulative study proves the scalability of the layout to larger bandwidths, up to 100 Gbps."],"url":"http://arxiv.org/abs/2409.03547v1"}
