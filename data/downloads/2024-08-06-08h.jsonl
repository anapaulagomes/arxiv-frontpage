{"created":"2024-08-05 17:59:51","title":"Latent-INR: A Flexible Framework for Implicit Representations of Videos with Discriminative Semantics","abstract":"Implicit Neural Networks (INRs) have emerged as powerful representations to encode all forms of data, including images, videos, audios, and scenes. With video, many INRs for video have been proposed for the compression task, and recent methods feature significant improvements with respect to encoding time, storage, and reconstruction quality. However, these encoded representations lack semantic meaning, so they cannot be used for any downstream tasks that require such properties, such as retrieval. This can act as a barrier for adoption of video INRs over traditional codecs as they do not offer any significant edge apart from compression. To alleviate this, we propose a flexible framework that decouples the spatial and temporal aspects of the video INR. We accomplish this with a dictionary of per-frame latents that are learned jointly with a set of video specific hypernetworks, such that given a latent, these hypernetworks can predict the INR weights to reconstruct the given frame. This framework not only retains the compression efficiency, but the learned latents can be aligned with features from large vision models, which grants them discriminative properties. We align these latents with CLIP and show good performance for both compression and video retrieval tasks. By aligning with VideoLlama, we are able to perform open-ended chat with our learned latents as the visual inputs. Additionally, the learned latents serve as a proxy for the underlying weights, allowing us perform tasks like video interpolation. These semantic properties and applications, existing simultaneously with ability to perform compression, interpolation, and superresolution properties, are a first in this field of work.","sentences":["Implicit Neural Networks (INRs) have emerged as powerful representations to encode all forms of data, including images, videos, audios, and scenes.","With video, many INRs for video have been proposed for the compression task, and recent methods feature significant improvements with respect to encoding time, storage, and reconstruction quality.","However, these encoded representations lack semantic meaning, so they cannot be used for any downstream tasks that require such properties, such as retrieval.","This can act as a barrier for adoption of video INRs over traditional codecs as they do not offer any significant edge apart from compression.","To alleviate this, we propose a flexible framework that decouples the spatial and temporal aspects of the video INR.","We accomplish this with a dictionary of per-frame latents that are learned jointly with a set of video specific hypernetworks, such that given a latent, these hypernetworks can predict the INR weights to reconstruct the given frame.","This framework not only retains the compression efficiency, but the learned latents can be aligned with features from large vision models, which grants them discriminative properties.","We align these latents with CLIP and show good performance for both compression and video retrieval tasks.","By aligning with VideoLlama, we are able to perform open-ended chat with our learned latents as the visual inputs.","Additionally, the learned latents serve as a proxy for the underlying weights, allowing us perform tasks like video interpolation.","These semantic properties and applications, existing simultaneously with ability to perform compression, interpolation, and superresolution properties, are a first in this field of work."],"url":"http://arxiv.org/abs/2408.02672v1"}
{"created":"2024-08-05 17:57:02","title":"Self-Taught Evaluators","abstract":"Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation. To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve. In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only. Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions. Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples.","sentences":["Model-based evaluation is at the heart of successful model development -- as a reward model for training, and as a replacement for human evaluation.","To train such evaluators, the standard approach is to collect a large amount of human preference judgments over model responses, which is costly and the data becomes stale as models improve.","In this work, we present an approach that aims to im-prove evaluators without human annotations, using synthetic training data only.","Starting from unlabeled instructions, our iterative self-improvement scheme generates contrasting model outputs and trains an LLM-as-a-Judge to produce reasoning traces and final judgments, repeating this training at each new iteration using the improved predictions.","Without any labeled preference data, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct) from 75.4 to 88.3 (88.7 with majority vote) on RewardBench.","This outperforms commonly used LLM judges such as GPT-4 and matches the performance of the top-performing reward models trained with labeled examples."],"url":"http://arxiv.org/abs/2408.02666v1"}
{"created":"2024-08-05 17:55:23","title":"Integrating Model-Based Footstep Planning with Model-Free Reinforcement Learning for Dynamic Legged Locomotion","abstract":"In this work, we introduce a control framework that combines model-based footstep planning with Reinforcement Learning (RL), leveraging desired footstep patterns derived from the Linear Inverted Pendulum (LIP) dynamics. Utilizing the LIP model, our method forward predicts robot states and determines the desired foot placement given the velocity commands. We then train an RL policy to track the foot placements without following the full reference motions derived from the LIP model. This partial guidance from the physics model allows the RL policy to integrate the predictive capabilities of the physics-informed dynamics and the adaptability characteristics of the RL controller without overfitting the policy to the template model. Our approach is validated on the MIT Humanoid, demonstrating that our policy can achieve stable yet dynamic locomotion for walking and turning. We further validate the adaptability and generalizability of our policy by extending the locomotion task to unseen, uneven terrain. During the hardware deployment, we have achieved forward walking speeds of up to 1.5 m/s on a treadmill and have successfully performed dynamic locomotion maneuvers such as 90-degree and 180-degree turns.","sentences":["In this work, we introduce a control framework that combines model-based footstep planning with Reinforcement Learning (RL), leveraging desired footstep patterns derived from the Linear Inverted Pendulum (LIP) dynamics.","Utilizing the LIP model, our method forward predicts robot states and determines the desired foot placement given the velocity commands.","We then train an RL policy to track the foot placements without following the full reference motions derived from the LIP model.","This partial guidance from the physics model allows the RL policy to integrate the predictive capabilities of the physics-informed dynamics and the adaptability characteristics of the RL controller without overfitting the policy to the template model.","Our approach is validated on the MIT Humanoid, demonstrating that our policy can achieve stable yet dynamic locomotion for walking and turning.","We further validate the adaptability and generalizability of our policy by extending the locomotion task to unseen, uneven terrain.","During the hardware deployment, we have achieved forward walking speeds of up to 1.5 m/s on a treadmill and have successfully performed dynamic locomotion maneuvers such as 90-degree and 180-degree turns."],"url":"http://arxiv.org/abs/2408.02662v1"}
{"created":"2024-08-05 17:55:20","title":"Context-aware Mamba-based Reinforcement Learning for social robot navigation","abstract":"Social robot navigation (SRN) is a relevant problem that involves navigating a pedestrian-rich environment in a socially acceptable manner. It is an essential part of making social robots effective in pedestrian-rich settings. The use cases of such robots could vary from companion robots to warehouse robots to autonomous wheelchairs. In recent years, deep reinforcement learning has been increasingly used in research on social robot navigation. Our work introduces CAMRL (Context-Aware Mamba-based Reinforcement Learning). Mamba is a new deep learning-based State Space Model (SSM) that has achieved results comparable to transformers in sequencing tasks. CAMRL uses Mamba to determine the robot's next action, which maximizes the value of the next state predicted by the neural network, enabling the robot to navigate effectively based on the rewards assigned. We evaluate CAMRL alongside existing solutions (CADRL, LSTM-RL, SARL) using a rigorous testing dataset which involves a variety of densities and environment behaviors based on ORCA and SFM, thus, demonstrating that CAMRL achieves higher success rates, minimizes collisions, and maintains safer distances from pedestrians. This work introduces a new SRN planner, showcasing the potential for deep-state space models for robot navigation.","sentences":["Social robot navigation (SRN) is a relevant problem that involves navigating a pedestrian-rich environment in a socially acceptable manner.","It is an essential part of making social robots effective in pedestrian-rich settings.","The use cases of such robots could vary from companion robots to warehouse robots to autonomous wheelchairs.","In recent years, deep reinforcement learning has been increasingly used in research on social robot navigation.","Our work introduces CAMRL (Context-Aware Mamba-based Reinforcement Learning).","Mamba is a new deep learning-based State Space Model (SSM) that has achieved results comparable to transformers in sequencing tasks.","CAMRL uses Mamba to determine the robot's next action, which maximizes the value of the next state predicted by the neural network, enabling the robot to navigate effectively based on the rewards assigned.","We evaluate CAMRL alongside existing solutions (CADRL, LSTM-RL, SARL) using a rigorous testing dataset which involves a variety of densities and environment behaviors based on ORCA and SFM, thus, demonstrating that CAMRL achieves higher success rates, minimizes collisions, and maintains safer distances from pedestrians.","This work introduces a new SRN planner, showcasing the potential for deep-state space models for robot navigation."],"url":"http://arxiv.org/abs/2408.02661v1"}
{"created":"2024-08-05 17:46:53","title":"Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation with Multimodal Generative Pretraining","abstract":"We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions. Unlike existing autoregressive image generation approaches, Lumina-mGPT employs a pretrained decoder-only transformer as a unified framework for modeling multimodal token sequences. Our key insight is that a simple decoder-only transformer with multimodal Generative PreTraining (mGPT), utilizing the next-token prediction objective on massive interleaved text-image sequences, can learn broad and general multimodal capabilities, thereby illuminating photorealistic text-to-image generation. Building on these pretrained models, we propose Flexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text pairs to fully unlock their potential for high-aesthetic image synthesis at any resolution while maintaining their general multimodal capabilities. Furthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT), transforming Lumina-mGPT into a foundation model that seamlessly achieves omnipotent task unification. The resulting model demonstrates versatile multimodal capabilities, including visual generation tasks like flexible text-to-image generation and controllable generation, visual recognition tasks like segmentation and depth estimation, and vision-language tasks like multiturn visual question answering. Additionally, we analyze the differences and similarities between diffusion-based and autoregressive methods in a direct comparison.","sentences":["We present Lumina-mGPT, a family of multimodal autoregressive models capable of various vision and language tasks, particularly excelling in generating flexible photorealistic images from text descriptions.","Unlike existing autoregressive image generation approaches, Lumina-mGPT employs a pretrained decoder-only transformer as a unified framework for modeling multimodal token sequences.","Our key insight is that a simple decoder-only transformer with multimodal Generative PreTraining (mGPT), utilizing the next-token prediction objective on massive interleaved text-image sequences, can learn broad and general multimodal capabilities, thereby illuminating photorealistic text-to-image generation.","Building on these pretrained models, we propose Flexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text pairs to fully unlock their potential for high-aesthetic image synthesis at any resolution while maintaining their general multimodal capabilities.","Furthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT), transforming Lumina-mGPT into a foundation model that seamlessly achieves omnipotent task unification.","The resulting model demonstrates versatile multimodal capabilities, including visual generation tasks like flexible text-to-image generation and controllable generation, visual recognition tasks like segmentation and depth estimation, and vision-language tasks like multiturn visual question answering.","Additionally, we analyze the differences and similarities between diffusion-based and autoregressive methods in a direct comparison."],"url":"http://arxiv.org/abs/2408.02657v1"}
{"created":"2024-08-05 17:33:09","title":"On Using Quasirandom Sequences in Machine Learning for Model Weight Initialization","abstract":"The effectiveness of training neural networks directly impacts computational costs, resource allocation, and model development timelines in machine learning applications. An optimizer's ability to train the model adequately (in terms of trained model performance) depends on the model's initial weights. Model weight initialization schemes use pseudorandom number generators (PRNGs) as a source of randomness.   We investigate whether substituting PRNGs for low-discrepancy quasirandom number generators (QRNGs) -- namely Sobol' sequences -- as a source of randomness for initializers can improve model performance. We examine Multi-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), and Transformer architectures trained on MNIST, CIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses ten initialization schemes: Glorot, He, Lecun (both Uniform and Normal); Orthogonal, Random Normal, Truncated Normal, and Random Uniform. Models with weights set using PRNG- and QRNG-based initializers are compared pairwise for each combination of dataset, architecture, optimizer, and initialization scheme.   Our findings indicate that QRNG-based neural network initializers either reach a higher accuracy or achieve the same accuracy more quickly than PRNG-based initializers in 60% of the 120 experiments conducted. Thus, using QRNG-based initializers instead of PRNG-based initializers can speed up and improve model training.","sentences":["The effectiveness of training neural networks directly impacts computational costs, resource allocation, and model development timelines in machine learning applications.","An optimizer's ability to train the model adequately (in terms of trained model performance) depends on the model's initial weights.","Model weight initialization schemes use pseudorandom number generators (PRNGs) as a source of randomness.   ","We investigate whether substituting PRNGs for low-discrepancy quasirandom number generators (QRNGs) -- namely Sobol' sequences -- as a source of randomness for initializers can improve model performance.","We examine Multi-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long Short-Term Memory (LSTM), and Transformer architectures trained on MNIST, CIFAR-10, and IMDB datasets using SGD and Adam optimizers.","Our analysis uses ten initialization schemes: Glorot, He, Lecun (both Uniform and Normal); Orthogonal, Random Normal, Truncated Normal, and Random Uniform.","Models with weights set using PRNG- and QRNG-based initializers are compared pairwise for each combination of dataset, architecture, optimizer, and initialization scheme.   ","Our findings indicate that QRNG-based neural network initializers either reach a higher accuracy or achieve the same accuracy more quickly than PRNG-based initializers in 60% of the 120 experiments conducted.","Thus, using QRNG-based initializers instead of PRNG-based initializers can speed up and improve model training."],"url":"http://arxiv.org/abs/2408.02654v1"}
{"created":"2024-08-05 17:27:29","title":"Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large Language Models?","abstract":"Large Language Models (LLMs) have demonstrated impressive capabilities in natural language tasks, but their safety and morality remain contentious due to their training on internet text corpora. To address these concerns, alignment techniques have been developed to improve the public usability and safety of LLMs. Yet, the potential for generating harmful content through these models seems to persist. This paper explores the concept of jailbreaking LLMs-reversing their alignment through adversarial triggers. Previous methods, such as soft embedding prompts, manually crafted prompts, and gradient-based automatic prompts, have had limited success on black-box models due to their requirements for model access and for producing a low variety of manually crafted prompts, making them susceptible to being blocked. This paper introduces a novel approach using reinforcement learning to optimize adversarial triggers, requiring only inference API access to the target model and a small surrogate model. Our method, which leverages a BERTScore-based reward function, enhances the transferability and effectiveness of adversarial triggers on new black-box models. We demonstrate that this approach improves the performance of adversarial triggers on a previously untested language model.","sentences":["Large Language Models (LLMs) have demonstrated impressive capabilities in natural language tasks, but their safety and morality remain contentious due to their training on internet text corpora.","To address these concerns, alignment techniques have been developed to improve the public usability and safety of LLMs.","Yet, the potential for generating harmful content through these models seems to persist.","This paper explores the concept of jailbreaking LLMs-reversing their alignment through adversarial triggers.","Previous methods, such as soft embedding prompts, manually crafted prompts, and gradient-based automatic prompts, have had limited success on black-box models due to their requirements for model access and for producing a low variety of manually crafted prompts, making them susceptible to being blocked.","This paper introduces a novel approach using reinforcement learning to optimize adversarial triggers, requiring only inference API access to the target model and a small surrogate model.","Our method, which leverages a BERTScore-based reward function, enhances the transferability and effectiveness of adversarial triggers on new black-box models.","We demonstrate that this approach improves the performance of adversarial triggers on a previously untested language model."],"url":"http://arxiv.org/abs/2408.02651v1"}
{"created":"2024-08-05 17:14:35","title":"Detection of Compromised Functions in a Serverless Cloud Environment","abstract":"Serverless computing is an emerging cloud paradigm with serverless functions at its core. While serverless environments enable software developers to focus on developing applications without the need to actively manage the underlying runtime infrastructure, they open the door to a wide variety of security threats that can be challenging to mitigate with existing methods. Existing security solutions do not apply to all serverless architectures, since they require significant modifications to the serverless infrastructure or rely on third-party services for the collection of more detailed data. In this paper, we present an extendable serverless security threat detection model that leverages cloud providers' native monitoring tools to detect anomalous behavior in serverless applications. Our model aims to detect compromised serverless functions by identifying post-exploitation abnormal behavior related to different types of attacks on serverless functions, and therefore, it is a last line of defense. Our approach is not tied to any specific serverless application, is agnostic to the type of threats, and is adaptable through model adjustments. To evaluate our model's performance, we developed a serverless cybersecurity testbed in an AWS cloud environment, which includes two different serverless applications and simulates a variety of attack scenarios that cover the main security threats faced by serverless functions. Our evaluation demonstrates our model's ability to detect all implemented attacks while maintaining a negligible false alarm rate.","sentences":["Serverless computing is an emerging cloud paradigm with serverless functions at its core.","While serverless environments enable software developers to focus on developing applications without the need to actively manage the underlying runtime infrastructure, they open the door to a wide variety of security threats that can be challenging to mitigate with existing methods.","Existing security solutions do not apply to all serverless architectures, since they require significant modifications to the serverless infrastructure or rely on third-party services for the collection of more detailed data.","In this paper, we present an extendable serverless security threat detection model that leverages cloud providers' native monitoring tools to detect anomalous behavior in serverless applications.","Our model aims to detect compromised serverless functions by identifying post-exploitation abnormal behavior related to different types of attacks on serverless functions, and therefore, it is a last line of defense.","Our approach is not tied to any specific serverless application, is agnostic to the type of threats, and is adaptable through model adjustments.","To evaluate our model's performance, we developed a serverless cybersecurity testbed in an AWS cloud environment, which includes two different serverless applications and simulates a variety of attack scenarios that cover the main security threats faced by serverless functions.","Our evaluation demonstrates our model's ability to detect all implemented attacks while maintaining a negligible false alarm rate."],"url":"http://arxiv.org/abs/2408.02641v1"}
{"created":"2024-08-05 17:01:33","title":"Command-line Obfuscation Detection using Small Language Models","abstract":"To avoid detection, adversaries often use command-line obfuscation. There are numerous techniques of the command-line obfuscation, all designed to alter the command-line syntax without affecting its original functionality. This variability forces most security solutions to create an exhaustive enumeration of signatures for even a single pattern. In contrast to using signatures, we have implemented a scalable NLP-based detection method that leverages a custom-trained, small transformer language model that can be applied to any source of execution logs. The evaluation on top of real-world telemetry demonstrates that our approach yields high-precision detections even on high-volume telemetry from a diverse set of environments spanning from universities and businesses to healthcare or finance. The practical value is demonstrated in a case study of real-world samples detected by our model. We show the model's superiority to signatures on established malware known to employ obfuscation and showcase previously unseen obfuscated samples detected by our model.","sentences":["To avoid detection, adversaries often use command-line obfuscation.","There are numerous techniques of the command-line obfuscation, all designed to alter the command-line syntax without affecting its original functionality.","This variability forces most security solutions to create an exhaustive enumeration of signatures for even a single pattern.","In contrast to using signatures, we have implemented a scalable NLP-based detection method that leverages a custom-trained, small transformer language model that can be applied to any source of execution logs.","The evaluation on top of real-world telemetry demonstrates that our approach yields high-precision detections even on high-volume telemetry from a diverse set of environments spanning from universities and businesses to healthcare or finance.","The practical value is demonstrated in a case study of real-world samples detected by our model.","We show the model's superiority to signatures on established malware known to employ obfuscation and showcase previously unseen obfuscated samples detected by our model."],"url":"http://arxiv.org/abs/2408.02637v1"}
{"created":"2024-08-05 16:58:56","title":"Interactive 3D Medical Image Segmentation with SAM 2","abstract":"Interactive medical image segmentation (IMIS) has shown significant potential in enhancing segmentation accuracy by integrating iterative feedback from medical professionals. However, the limited availability of enough 3D medical data restricts the generalization and robustness of most IMIS methods. The Segment Anything Model (SAM), though effective for 2D images, requires expensive semi-auto slice-by-slice annotations for 3D medical images. In this paper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta SAM model trained on videos, for 3D medical image segmentation. By treating sequential 2D slices of 3D images as video frames, SAM 2 can fully automatically propagate annotations from a single frame to the entire 3D volume. We propose a practical pipeline for using SAM 2 in 3D medical image segmentation and present key findings highlighting its efficiency and potential for further optimization. Concretely, numerical experiments on the BraTS2020 and the medical segmentation decathlon datasets demonstrate that SAM 2 still has a gap with supervised methods but can narrow the gap in specific settings and organ types, significantly reducing the annotation burden on medical professionals. Our code will be open-sourced and available at https://github.com/Chuyun-Shen/SAM_2_Medical_3D.","sentences":["Interactive medical image segmentation (IMIS) has shown significant potential in enhancing segmentation accuracy by integrating iterative feedback from medical professionals.","However, the limited availability of enough 3D medical data restricts the generalization and robustness of most IMIS methods.","The Segment Anything Model (SAM), though effective for 2D images, requires expensive semi-auto slice-by-slice annotations for 3D medical images.","In this paper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta SAM model trained on videos, for 3D medical image segmentation.","By treating sequential 2D slices of 3D images as video frames, SAM 2 can fully automatically propagate annotations from a single frame to the entire 3D volume.","We propose a practical pipeline for using SAM 2 in 3D medical image segmentation and present key findings highlighting its efficiency and potential for further optimization.","Concretely, numerical experiments on the BraTS2020 and the medical segmentation decathlon datasets demonstrate that SAM 2 still has a gap with supervised methods but can narrow the gap in specific settings and organ types, significantly reducing the annotation burden on medical professionals.","Our code will be open-sourced and available at https://github.com/Chuyun-Shen/SAM_2_Medical_3D."],"url":"http://arxiv.org/abs/2408.02635v1"}
{"created":"2024-08-05 16:58:48","title":"CLVR Ordering of Transactions on AMMs","abstract":"Trading on decentralized exchanges via an Automated Market Maker (AMM) mechanism has been massively adopted, with a daily trading volume reaching $1B. This trading method has also received close attention from researchers, central banks, and financial firms, who have the potential to adopt it to traditional financial markets such as foreign exchanges and stock markets. A critical challenge of AMM-powered trading is that transaction order has high financial value, so a policy or method to order transactions in a \"good\" (optimal) manner is vital. We offer economic measures of both price stability (low volatility) and inequality that inform how a \"social planner\" should pick an optimal ordering. We show that there is a trade-off between achieving price stability and reducing inequality, and that policymakers must choose which to prioritize. In addition, picking the optimal order can often be costly, especially when performing an exhaustive search over trade orderings (permutations). As an alternative we provide a simple algorithm, Clever Look-ahead Volatility Reduction (CLVR). This algorithm constructs an ordering which approximately minimizes price volatility with a small computation cost. We also provide insight into the strategy changes that may occur if traders are subject to this sequencing algorithm.","sentences":["Trading on decentralized exchanges via an Automated Market Maker (AMM) mechanism has been massively adopted, with a daily trading volume reaching $1B. This trading method has also received close attention from researchers, central banks, and financial firms, who have the potential to adopt it to traditional financial markets such as foreign exchanges and stock markets.","A critical challenge of AMM-powered trading is that transaction order has high financial value, so a policy or method to order transactions in a \"good\" (optimal) manner is vital.","We offer economic measures of both price stability (low volatility) and inequality that inform how a \"social planner\" should pick an optimal ordering.","We show that there is a trade-off between achieving price stability and reducing inequality, and that policymakers must choose which to prioritize.","In addition, picking the optimal order can often be costly, especially when performing an exhaustive search over trade orderings (permutations).","As an alternative we provide a simple algorithm, Clever Look-ahead Volatility Reduction (CLVR).","This algorithm constructs an ordering which approximately minimizes price volatility with a small computation cost.","We also provide insight into the strategy changes that may occur if traders are subject to this sequencing algorithm."],"url":"http://arxiv.org/abs/2408.02634v1"}
{"created":"2024-08-05 16:55:06","title":"SEAS: Self-Evolving Adversarial Safety Optimization for Large Language Models","abstract":"As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial. A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming. However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models. To tackle these challenges, we introduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving }\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$ optimization framework, which enhances security by leveraging data generated by the model itself. SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety. This framework reduces reliance on manual testing and significantly enhances the security capabilities of LLMs. Our contributions include a novel adversarial framework, a comprehensive safety dataset, and after three iterations, the Target model achieves a security level comparable to GPT-4, while the Red Team model shows a marked increase in attack success rate (ASR) against advanced models.","sentences":["As large language models (LLMs) continue to advance in capability and influence, ensuring their security and preventing harmful outputs has become crucial.","A promising approach to address these concerns involves training models to automatically generate adversarial prompts for red teaming.","However, the evolving subtlety of vulnerabilities in LLMs challenges the effectiveness of current adversarial methods, which struggle to specifically target and explore the weaknesses of these models.","To tackle these challenges, we introduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving }\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$ optimization framework, which enhances security by leveraging data generated by the model itself.","SEAS operates through three iterative stages: Initialization, Attack, and Adversarial Optimization, refining both the Red Team and Target models to improve robustness and safety.","This framework reduces reliance on manual testing and significantly enhances the security capabilities of LLMs.","Our contributions include a novel adversarial framework, a comprehensive safety dataset, and after three iterations, the Target model achieves a security level comparable to GPT-4, while the Red Team model shows a marked increase in attack success rate (ASR) against advanced models."],"url":"http://arxiv.org/abs/2408.02632v1"}
{"created":"2024-08-05 16:53:23","title":"VidGen-1M: A Large-Scale Dataset for Text-to-video Generation","abstract":"The quality of video-text pairs fundamentally determines the upper bound of text-to-video models. Currently, the datasets used for training these models suffer from significant shortcomings, including low temporal consistency, poor-quality captions, substandard video quality, and imbalanced data distribution. The prevailing video curation process, which depends on image models for tagging and manual rule-based curation, leads to a high computational load and leaves behind unclean data. As a result, there is a lack of appropriate training datasets for text-to-video models. To address this problem, we present VidGen-1M, a superior training dataset for text-to-video models. Produced through a coarse-to-fine curation strategy, this dataset guarantees high-quality videos and detailed captions with excellent temporal consistency. When used to train the video generation model, this dataset has led to experimental results that surpass those obtained with other models.","sentences":["The quality of video-text pairs fundamentally determines the upper bound of text-to-video models.","Currently, the datasets used for training these models suffer from significant shortcomings, including low temporal consistency, poor-quality captions, substandard video quality, and imbalanced data distribution.","The prevailing video curation process, which depends on image models for tagging and manual rule-based curation, leads to a high computational load and leaves behind unclean data.","As a result, there is a lack of appropriate training datasets for text-to-video models.","To address this problem, we present VidGen-1M, a superior training dataset for text-to-video models.","Produced through a coarse-to-fine curation strategy, this dataset guarantees high-quality videos and detailed captions with excellent temporal consistency.","When used to train the video generation model, this dataset has led to experimental results that surpass those obtained with other models."],"url":"http://arxiv.org/abs/2408.02629v1"}
{"created":"2024-08-05 16:48:03","title":"YOWOv3: An Efficient and Generalized Framework for Human Action Detection and Recognition","abstract":"In this paper, we propose a new framework called YOWOv3, which is an improved version of YOWOv2, designed specifically for the task of Human Action Detection and Recognition. This framework is designed to facilitate extensive experimentation with different configurations and supports easy customization of various components within the model, reducing efforts required for understanding and modifying the code. YOWOv3 demonstrates its superior performance compared to YOWOv2 on two widely used datasets for Human Action Detection and Recognition: UCF101-24 and AVAv2.2. Specifically, the predecessor model YOWOv2 achieves an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2, respectively, with 109.7M parameters and 53.6 GFLOPs. In contrast, our model - YOWOv3, with only 59.8M parameters and 39.8 GFLOPs, achieves an mAP of 88.33% and 20.31% on UCF101-24 and AVAv2.2, respectively. The results demonstrate that YOWOv3 significantly reduces the number of parameters and GFLOPs while still achieving comparable performance.","sentences":["In this paper, we propose a new framework called YOWOv3, which is an improved version of YOWOv2, designed specifically for the task of Human Action Detection and Recognition.","This framework is designed to facilitate extensive experimentation with different configurations and supports easy customization of various components within the model, reducing efforts required for understanding and modifying the code.","YOWOv3 demonstrates its superior performance compared to YOWOv2 on two widely used datasets for Human Action Detection and Recognition: UCF101-24 and AVAv2.2.","Specifically, the predecessor model YOWOv2 achieves an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2, respectively, with 109.7M parameters and 53.6 GFLOPs.","In contrast, our model - YOWOv3, with only 59.8M parameters and 39.8 GFLOPs, achieves an mAP of 88.33% and 20.31% on UCF101-24 and AVAv2.2, respectively.","The results demonstrate that YOWOv3 significantly reduces the number of parameters and GFLOPs while still achieving comparable performance."],"url":"http://arxiv.org/abs/2408.02623v1"}
{"created":"2024-08-05 16:47:22","title":"Language Model Can Listen While Speaking","abstract":"Dialogue serves as the most natural manner of human-computer interaction (HCI). Recent advancements in speech language models (SLM) have significantly enhanced speech-based conversational AI. However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory. To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption. We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels. Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio input. LSLM fuses both channels for autoregressive generation and detects turn-taking in real time. Three fusion strategies -- early fusion, middle fusion, and late fusion -- are explored, with middle fusion achieving an optimal balance between speech generation and real-time interaction. Two experimental settings, command-based FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity to diverse instructions. Our results highlight LSLM's capability to achieve duplex communication with minimal impact on existing systems. This study aims to advance the development of interactive speech dialogue systems, enhancing their applicability in real-world contexts.","sentences":["Dialogue serves as the most natural manner of human-computer interaction (HCI).","Recent advancements in speech language models (SLM) have significantly enhanced speech-based conversational AI.","However, these models are limited to turn-based conversation, lacking the ability to interact with humans in real-time spoken scenarios, for example, being interrupted when the generated content is not satisfactory.","To address these limitations, we explore full duplex modeling (FDM) in interactive speech language models (iSLM), focusing on enhancing real-time interaction and, more explicitly, exploring the quintessential ability of interruption.","We introduce a novel model design, namely listening-while-speaking language model (LSLM), an end-to-end system equipped with both listening and speaking channels.","Our LSLM employs a token-based decoder-only TTS for speech generation and a streaming self-supervised learning (SSL) encoder for real-time audio input.","LSLM fuses both channels for autoregressive generation and detects turn-taking in real time.","Three fusion strategies -- early fusion, middle fusion, and late fusion -- are explored, with middle fusion achieving an optimal balance between speech generation and real-time interaction.","Two experimental settings, command-based FDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity to diverse instructions.","Our results highlight LSLM's capability to achieve duplex communication with minimal impact on existing systems.","This study aims to advance the development of interactive speech dialogue systems, enhancing their applicability in real-world contexts."],"url":"http://arxiv.org/abs/2408.02622v1"}
{"created":"2024-08-05 16:43:11","title":"Mastering Agile Jumping Skills from Simple Practices with Iterative Learning Control","abstract":"Achieving precise target jumping with legged robots poses a significant challenge due to the long flight phase and the uncertainties inherent in contact dynamics and hardware. Forcefully attempting these agile motions on hardware could result in severe failures and potential damage. Motivated by these challenging problems, we propose an Iterative Learning Control (ILC) approach that aims to learn and refine jumping skills from easy to difficult, instead of directly learning these challenging tasks. We verify that learning from simplicity can enhance safety and target jumping accuracy over trials. Compared to other ILC approaches for legged locomotion, our method can tackle the problem of a long flight phase where control input is not available. In addition, our approach allows the robot to apply what it learns from a simple jumping task to accomplish more challenging tasks within a few trials directly in hardware, instead of learning from scratch. We validate the method via extensive experiments in the A1 model and hardware for various jumping tasks. Starting from a small jump (e.g., a forward leap of 40cm), our learning approach empowers the robot to accomplish a variety of challenging targets, including jumping onto a 20cm high box, jumping to a greater distance of up to 60cm, as well as performing jumps while carrying an unknown payload of 2kg. Our framework can allow the robot to reach the desired position and orientation targets with approximate errors of 1cm and 1 degree within a few trials.","sentences":["Achieving precise target jumping with legged robots poses a significant challenge due to the long flight phase and the uncertainties inherent in contact dynamics and hardware.","Forcefully attempting these agile motions on hardware could result in severe failures and potential damage.","Motivated by these challenging problems, we propose an Iterative Learning Control (ILC) approach that aims to learn and refine jumping skills from easy to difficult, instead of directly learning these challenging tasks.","We verify that learning from simplicity can enhance safety and target jumping accuracy over trials.","Compared to other ILC approaches for legged locomotion, our method can tackle the problem of a long flight phase where control input is not available.","In addition, our approach allows the robot to apply what it learns from a simple jumping task to accomplish more challenging tasks within a few trials directly in hardware, instead of learning from scratch.","We validate the method via extensive experiments in the A1 model and hardware for various jumping tasks.","Starting from a small jump (e.g., a forward leap of 40cm), our learning approach empowers the robot to accomplish a variety of challenging targets, including jumping onto a 20cm high box, jumping to a greater distance of up to 60cm, as well as performing jumps while carrying an unknown payload of 2kg.","Our framework can allow the robot to reach the desired position and orientation targets with approximate errors of 1cm and 1 degree within a few trials."],"url":"http://arxiv.org/abs/2408.02619v1"}
{"created":"2024-08-05 16:39:39","title":"LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local Attention and Mamba","abstract":"Recent Transformer-based diffusion models have shown remarkable performance, largely attributed to the ability of the self-attention mechanism to accurately capture both global and local contexts by computing all-pair interactions among input tokens. However, their quadratic complexity poses significant computational challenges for long-sequence inputs. Conversely, a recent state space model called Mamba offers linear complexity by compressing a filtered global context into a hidden state. Despite its efficiency, compression inevitably leads to information loss of fine-grained local dependencies among tokens, which are crucial for effective visual generative modeling. Motivated by these observations, we introduce Local Attentional Mamba (LaMamba) blocks that combine the strengths of self-attention and Mamba, capturing both global contexts and local details with linear complexity. Leveraging the efficient U-Net architecture, our model exhibits exceptional scalability and surpasses the performance of DiT across various model scales on ImageNet at 256x256 resolution, all while utilizing substantially fewer GFLOPs and a comparable number of parameters. Compared to state-of-the-art diffusion models on ImageNet 256x256 and 512x512, our largest model presents notable advantages, such as a reduction of up to 62\\% GFLOPs compared to DiT-XL/2, while achieving superior performance with comparable or fewer parameters.","sentences":["Recent Transformer-based diffusion models have shown remarkable performance, largely attributed to the ability of the self-attention mechanism to accurately capture both global and local contexts by computing all-pair interactions among input tokens.","However, their quadratic complexity poses significant computational challenges for long-sequence inputs.","Conversely, a recent state space model called Mamba offers linear complexity by compressing a filtered global context into a hidden state.","Despite its efficiency, compression inevitably leads to information loss of fine-grained local dependencies among tokens, which are crucial for effective visual generative modeling.","Motivated by these observations, we introduce Local Attentional Mamba (LaMamba) blocks that combine the strengths of self-attention and Mamba, capturing both global contexts and local details with linear complexity.","Leveraging the efficient U-Net architecture, our model exhibits exceptional scalability and surpasses the performance of DiT across various model scales on ImageNet at 256x256 resolution, all while utilizing substantially fewer GFLOPs and a comparable number of parameters.","Compared to state-of-the-art diffusion models on ImageNet 256x256 and 512x512, our largest model presents notable advantages, such as a reduction of up to 62\\% GFLOPs compared to DiT-XL/2, while achieving superior performance with comparable or fewer parameters."],"url":"http://arxiv.org/abs/2408.02615v1"}
{"created":"2024-08-05 16:39:04","title":"Comparison of Code Quality and Best Practices in IoT and non-IoT Software","abstract":"Context: IoT systems, networks of connected devices powered by software, require studying software quality for maintenance. Despite extensive studies on non-IoT software quality, research on IoT software quality is lacking. It is uncertain if IoT and non-IoT systems software are comparable, hindering the confident application of results and best practices gained on non-IoT systems.   Objective: Therefore, we compare the code quality of two equivalent sets of IoT and non-IoT systems to determine whether there are similarities and differences. We also collect and revisit software-engineering best practices in non-IoT contexts to apply them to IoT.   Method: We design and apply a systematic method to select two sets of 94 non-IoT and IoT systems software from GitHub with comparable characteristics. We compute quality metrics on the systems in these two sets and then analyse and compare the metric values. We analyse in depth and provide specific examples of IoT system's complexity and how it manifests in the codebases. After the comparison, We systematically select and present a list of best practices to address the observed difference between IoT and non-IoT code.   Results: Through a comparison of metrics, we conclude that software for IoT systems is more complex, coupled, larger, less maintainable, and cohesive than non-IoT systems. Several factors, such as integrating multiple hardware and software components and managing data communication between them, contribute to these differences. Considering these differences, we present a revisited best practices list with approaches, tools, or techniques for developing IoT systems. As example, applying modularity, and refactoring are best practices for lowering the complexity.   Conclusion: Based on our work, researchers can now make an informed decision using existing studies on the quality of non-IoT systems for IoT systems.","sentences":["Context: IoT systems, networks of connected devices powered by software, require studying software quality for maintenance.","Despite extensive studies on non-IoT software quality, research on IoT software quality is lacking.","It is uncertain if IoT and non-IoT systems software are comparable, hindering the confident application of results and best practices gained on non-IoT systems.   ","Objective:","Therefore, we compare the code quality of two equivalent sets of IoT and non-IoT systems to determine whether there are similarities and differences.","We also collect and revisit software-engineering best practices in non-IoT contexts to apply them to IoT.   Method: We design and apply a systematic method to select two sets of 94 non-IoT and IoT systems software from GitHub with comparable characteristics.","We compute quality metrics on the systems in these two sets and then analyse and compare the metric values.","We analyse in depth and provide specific examples of IoT system's complexity and how it manifests in the codebases.","After the comparison, We systematically select and present a list of best practices to address the observed difference between IoT and non-IoT code.   ","Results: Through a comparison of metrics, we conclude that software for IoT systems is more complex, coupled, larger, less maintainable, and cohesive than non-IoT systems.","Several factors, such as integrating multiple hardware and software components and managing data communication between them, contribute to these differences.","Considering these differences, we present a revisited best practices list with approaches, tools, or techniques for developing IoT systems.","As example, applying modularity, and refactoring are best practices for lowering the complexity.   ","Conclusion: Based on our work, researchers can now make an informed decision using existing studies on the quality of non-IoT systems for IoT systems."],"url":"http://arxiv.org/abs/2408.02614v1"}
{"created":"2024-08-05 16:31:38","title":"Backward explanations via redefinition of predicates","abstract":"History eXplanation based on Predicates (HXP), studies the behavior of a Reinforcement Learning (RL) agent in a sequence of agent's interactions with the environment (a history), through the prism of an arbitrary predicate. To this end, an action importance score is computed for each action in the history. The explanation consists in displaying the most important actions to the user. As the calculation of an action's importance is #W[1]-hard, it is necessary for long histories to approximate the scores, at the expense of their quality. We therefore propose a new HXP method, called Backward-HXP, to provide explanations for these histories without having to approximate scores. Experiments show the ability of B-HXP to summarise long histories.","sentences":["History eXplanation based on Predicates (HXP), studies the behavior of a Reinforcement Learning (RL) agent in a sequence of agent's interactions with the environment (a history), through the prism of an arbitrary predicate.","To this end, an action importance score is computed for each action in the history.","The explanation consists in displaying the most important actions to the user.","As the calculation of an action's importance is #W[1]-hard, it is necessary for long histories to approximate the scores, at the expense of their quality.","We therefore propose a new HXP method, called Backward-HXP, to provide explanations for these histories without having to approximate scores.","Experiments show the ability of B-HXP to summarise long histories."],"url":"http://arxiv.org/abs/2408.02606v1"}
{"created":"2024-08-05 16:31:28","title":"Trade-offs of Dynamic Control Structure in Human-swarm Systems","abstract":"Swarm robotics is a study of simple robots that exhibit complex behaviour only by interacting locally with other robots and their environment. The control in swarm robotics is mainly distributed whereas centralised control is widely used in other fields of robotics. Centralised and decentralised control strategies both pose a unique set of benefits and drawbacks for the control of multi-robot systems. While decentralised systems are more scalable and resilient, they are less efficient compared to the centralised systems and they lead to excessive data transmissions to the human operators causing cognitive overload. We examine the trade-offs of each of these approaches in a human-swarm system to perform an environmental monitoring task and propose a flexible hybrid approach, which combines elements of hierarchical and decentralised systems. We find that a flexible hybrid system can outperform a centralised system (in our environmental monitoring task by 19.2%) while reducing the number of messages sent to a human operator (here by 23.1%). We conclude that establishing centralisation for a system is not always optimal for performance and that utilising aspects of centralised and decentralised systems can keep the swarm from hindering its performance.","sentences":["Swarm robotics is a study of simple robots that exhibit complex behaviour only by interacting locally with other robots and their environment.","The control in swarm robotics is mainly distributed whereas centralised control is widely used in other fields of robotics.","Centralised and decentralised control strategies both pose a unique set of benefits and drawbacks for the control of multi-robot systems.","While decentralised systems are more scalable and resilient, they are less efficient compared to the centralised systems and they lead to excessive data transmissions to the human operators causing cognitive overload.","We examine the trade-offs of each of these approaches in a human-swarm system to perform an environmental monitoring task and propose a flexible hybrid approach, which combines elements of hierarchical and decentralised systems.","We find that a flexible hybrid system can outperform a centralised system (in our environmental monitoring task by 19.2%) while reducing the number of messages sent to a human operator (here by 23.1%).","We conclude that establishing centralisation for a system is not always optimal for performance and that utilising aspects of centralised and decentralised systems can keep the swarm from hindering its performance."],"url":"http://arxiv.org/abs/2408.02605v1"}
{"created":"2024-08-05 16:21:36","title":"BioMamba: A Pre-trained Biomedical Language Representation Model Leveraging Mamba","abstract":"The advancement of natural language processing (NLP) in biology hinges on models' ability to interpret intricate biomedical literature. Traditional models often struggle with the complex and domain-specific language in this field. In this paper, we present BioMamba, a pre-trained model specifically designed for biomedical text mining. BioMamba builds upon the Mamba architecture and is pre-trained on an extensive corpus of biomedical literature. Our empirical studies demonstrate that BioMamba significantly outperforms models like BioBERT and general-domain Mamba across various biomedical tasks. For instance, BioMamba achieves a 100 times reduction in perplexity and a 4 times reduction in cross-entropy loss on the BioASQ test set. We provide an overview of the model architecture, pre-training process, and fine-tuning techniques. Additionally, we release the code and trained model to facilitate further research.","sentences":["The advancement of natural language processing (NLP) in biology hinges on models' ability to interpret intricate biomedical literature.","Traditional models often struggle with the complex and domain-specific language in this field.","In this paper, we present BioMamba, a pre-trained model specifically designed for biomedical text mining.","BioMamba builds upon the Mamba architecture and is pre-trained on an extensive corpus of biomedical literature.","Our empirical studies demonstrate that BioMamba significantly outperforms models like BioBERT and general-domain Mamba across various biomedical tasks.","For instance, BioMamba achieves a 100 times reduction in perplexity and a 4 times reduction in cross-entropy loss on the BioASQ test set.","We provide an overview of the model architecture, pre-training process, and fine-tuning techniques.","Additionally, we release the code and trained model to facilitate further research."],"url":"http://arxiv.org/abs/2408.02600v1"}
{"created":"2024-08-05 16:21:17","title":"Progressively Selective Label Enhancement for Language Model Alignment","abstract":"Large Language Models have demonstrated impressive capabilities in various language tasks but may produce content that misaligns with human expectations, raising ethical and legal concerns. Therefore, it is important to explore the limitations and implement restrictions on the models to ensure safety and compliance, with Reinforcement Learning from Human Feedback (RLHF) being the primary method. Due to challenges in stability and scalability with the RLHF stages, researchers are exploring alternative methods to achieve effects comparable to those of RLHF. However, these methods often depend on large high-quality datasets and inefficiently utilize generated data. To deal with this problem, we propose PSLE, i.e., Progressively Selective Label Enhancement for Language Model Alignment, a framework that fully utilizes all generated data by guiding the model with principles to align outputs with human expectations. Using a dynamically updated threshold, our approach ensures efficient data utilization by incorporating all generated responses and weighting them based on their corresponding reward scores. Experimental results on multiple datasets demonstrate the effectiveness of PSLE compared to existing language model alignment methods.","sentences":["Large Language Models have demonstrated impressive capabilities in various language tasks but may produce content that misaligns with human expectations, raising ethical and legal concerns.","Therefore, it is important to explore the limitations and implement restrictions on the models to ensure safety and compliance, with Reinforcement Learning from Human Feedback (RLHF) being the primary method.","Due to challenges in stability and scalability with the RLHF stages, researchers are exploring alternative methods to achieve effects comparable to those of RLHF.","However, these methods often depend on large high-quality datasets and inefficiently utilize generated data.","To deal with this problem, we propose PSLE, i.e., Progressively Selective Label Enhancement for Language Model Alignment, a framework that fully utilizes all generated data by guiding the model with principles to align outputs with human expectations.","Using a dynamically updated threshold, our approach ensures efficient data utilization by incorporating all generated responses and weighting them based on their corresponding reward scores.","Experimental results on multiple datasets demonstrate the effectiveness of PSLE compared to existing language model alignment methods."],"url":"http://arxiv.org/abs/2408.02599v1"}
{"created":"2024-08-05 16:15:31","title":"AI-Driven Strategies for Reducing Student Withdrawal -- A Study of EMU Student Stopout","abstract":"Not everyone who enrolls in college will leave with a certificate or degree, but the number of people who drop out or take a break is much higher than experts previously believed. In December 2013, there were 29 million people with some college education but no degree. That number jumped to 36 million by December of 2018, according to a new report from the National Student Clearinghouse Research Center[1]. It is imperative to understand the underlying factors contributing to student withdrawal and to assist decision-makers to identify effective strategies to prevent it. By analyzing the characteristics and educational pathways of the stopout student population, our aim is to provide actionable insights that can benefit institutions facing similar challenges. Eastern Michigan University (EMU) faces significant challenges in student retention, with approximately 55% of its undergraduate students not completing their degrees within six years. As an institution committed to student success, EMU conducted a comprehensive study of student withdrawals to understand the influencing factors. And the paper revealed a high correlation between certain factors and withdrawals, even in the early stages of university attendance. Based on these findings, we developed a predictive model that employs artificial intelligence techniques to assess the potential risk that students abandon their studies. These models enable universities to implement early intervention strategies, support at-risk students, and improve overall higher education success.","sentences":["Not everyone who enrolls in college will leave with a certificate or degree, but the number of people who drop out or take a break is much higher than experts previously believed.","In December 2013, there were 29 million people with some college education but no degree.","That number jumped to 36 million by December of 2018, according to a new report from the National Student Clearinghouse Research Center[1].","It is imperative to understand the underlying factors contributing to student withdrawal and to assist decision-makers to identify effective strategies to prevent it.","By analyzing the characteristics and educational pathways of the stopout student population, our aim is to provide actionable insights that can benefit institutions facing similar challenges.","Eastern Michigan University (EMU) faces significant challenges in student retention, with approximately 55% of its undergraduate students not completing their degrees within six years.","As an institution committed to student success, EMU conducted a comprehensive study of student withdrawals to understand the influencing factors.","And the paper revealed a high correlation between certain factors and withdrawals, even in the early stages of university attendance.","Based on these findings, we developed a predictive model that employs artificial intelligence techniques to assess the potential risk that students abandon their studies.","These models enable universities to implement early intervention strategies, support at-risk students, and improve overall higher education success."],"url":"http://arxiv.org/abs/2408.02598v1"}
{"created":"2024-08-05 16:07:31","title":"Modelling Visual Semantics via Image Captioning to extract Enhanced Multi-Level Cross-Modal Semantic Incongruity Representation with Attention for Multimodal Sarcasm Detection","abstract":"Sarcasm is a type of irony, characterized by an inherent mismatch between the literal interpretation and the intended connotation. Though sarcasm detection in text has been extensively studied, there are situations in which textual input alone might be insufficient to perceive sarcasm. The inclusion of additional contextual cues, such as images, is essential to recognize sarcasm in social media data effectively. This study presents a novel framework for multimodal sarcasm detection that can process input triplets. Two components of these triplets comprise the input text and its associated image, as provided in the datasets. Additionally, a supplementary modality is introduced in the form of descriptive image captions. The motivation behind incorporating this visual semantic representation is to more accurately capture the discrepancies between the textual and visual content, which are fundamental to the sarcasm detection task. The primary contributions of this study are: (1) a robust textual feature extraction branch that utilizes a cross-lingual language model; (2) a visual feature extraction branch that incorporates a self-regulated residual ConvNet integrated with a lightweight spatially aware attention module; (3) an additional modality in the form of image captions generated using an encoder-decoder architecture capable of reading text embedded in images; (4) distinct attention modules to effectively identify the incongruities between the text and two levels of image representations; (5) multi-level cross-domain semantic incongruity representation achieved through feature fusion. Compared with cutting-edge baselines, the proposed model achieves the best accuracy of 92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and MultiBully datasets.","sentences":["Sarcasm is a type of irony, characterized by an inherent mismatch between the literal interpretation and the intended connotation.","Though sarcasm detection in text has been extensively studied, there are situations in which textual input alone might be insufficient to perceive sarcasm.","The inclusion of additional contextual cues, such as images, is essential to recognize sarcasm in social media data effectively.","This study presents a novel framework for multimodal sarcasm detection that can process input triplets.","Two components of these triplets comprise the input text and its associated image, as provided in the datasets.","Additionally, a supplementary modality is introduced in the form of descriptive image captions.","The motivation behind incorporating this visual semantic representation is to more accurately capture the discrepancies between the textual and visual content, which are fundamental to the sarcasm detection task.","The primary contributions of this study are: (1) a robust textual feature extraction branch that utilizes a cross-lingual language model; (2) a visual feature extraction branch that incorporates a self-regulated residual ConvNet integrated with a lightweight spatially aware attention module; (3) an additional modality in the form of image captions generated using an encoder-decoder architecture capable of reading text embedded in images; (4) distinct attention modules to effectively identify the incongruities between the text and two levels of image representations; (5) multi-level cross-domain semantic incongruity representation achieved through feature fusion.","Compared with cutting-edge baselines, the proposed model achieves the best accuracy of 92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and MultiBully datasets."],"url":"http://arxiv.org/abs/2408.02595v1"}
{"created":"2024-08-05 16:00:36","title":"Massive MIMO-OTFS-Based Random Access for Cooperative LEO Satellite Constellations","abstract":"This paper investigates joint device identification, channel estimation, and symbol detection for cooperative multi-satellite-enhanced random access, where orthogonal time-frequency space modulation with the large antenna array is utilized to combat the dynamics of the terrestrial-satellite links (TSLs). We introduce the generalized complex exponential basis expansion model to parameterize TSLs, thereby reducing the pilot overhead. By exploiting the block sparsity of the TSLs in the angular domain, a message passing algorithm is designed for initial channel estimation. Subsequently, we examine two cooperative modes to leverage the spatial diversity within satellite constellations: the centralized mode, where computations are performed at a high-power central server, and the distributed mode, where computations are offloaded to edge satellites with minimal signaling overhead. Specifically, in the centralized mode, device identification is achieved by aggregating backhaul information from edge satellites, and channel estimation and symbol detection are jointly enhanced through a structured approximate expectation propagation (AEP) algorithm. In the distributed mode, edge satellites share channel information and exchange soft information about data symbols, leading to a distributed version of AEP. The introduced basis expansion model for TSLs enables the efficient implementation of both centralized and distributed algorithms via fast Fourier transform. Simulation results demonstrate that proposed schemes significantly outperform conventional algorithms in terms of the activity error rate, the normalized mean squared error, and the symbol error rate. Notably, the distributed mode achieves performance comparable to the centralized mode with only two exchanges of soft information about data symbols within the constellation.","sentences":["This paper investigates joint device identification, channel estimation, and symbol detection for cooperative multi-satellite-enhanced random access, where orthogonal time-frequency space modulation with the large antenna array is utilized to combat the dynamics of the terrestrial-satellite links (TSLs).","We introduce the generalized complex exponential basis expansion model to parameterize TSLs, thereby reducing the pilot overhead.","By exploiting the block sparsity of the TSLs in the angular domain, a message passing algorithm is designed for initial channel estimation.","Subsequently, we examine two cooperative modes to leverage the spatial diversity within satellite constellations: the centralized mode, where computations are performed at a high-power central server, and the distributed mode, where computations are offloaded to edge satellites with minimal signaling overhead.","Specifically, in the centralized mode, device identification is achieved by aggregating backhaul information from edge satellites, and channel estimation and symbol detection are jointly enhanced through a structured approximate expectation propagation (AEP) algorithm.","In the distributed mode, edge satellites share channel information and exchange soft information about data symbols, leading to a distributed version of AEP.","The introduced basis expansion model for TSLs enables the efficient implementation of both centralized and distributed algorithms via fast Fourier transform.","Simulation results demonstrate that proposed schemes significantly outperform conventional algorithms in terms of the activity error rate, the normalized mean squared error, and the symbol error rate.","Notably, the distributed mode achieves performance comparable to the centralized mode with only two exchanges of soft information about data symbols within the constellation."],"url":"http://arxiv.org/abs/2408.02586v1"}
{"created":"2024-08-05 16:00:21","title":"Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality Aspect-Based Summarization","abstract":"The ever-increasing volume of digital information necessitates efficient methods for users to extract key insights from lengthy documents. Aspect-based summarization offers a targeted approach, generating summaries focused on specific aspects within a document. Despite advancements in aspect-based summarization research, there is a continuous quest for improved model performance. Given that large language models (LLMs) have demonstrated the potential to revolutionize diverse tasks within natural language processing, particularly in the problem of summarization, this paper explores the potential of fine-tuning LLMs for the aspect-based summarization task. We evaluate the impact of fine-tuning open-source foundation LLMs, including Llama2, Mistral, Gemma and Aya, on a publicly available domain-specific aspect based summary dataset. We hypothesize that this approach will enable these models to effectively identify and extract aspect-related information, leading to superior quality aspect-based summaries compared to the state-of-the-art. We establish a comprehensive evaluation framework to compare the performance of fine-tuned LLMs against competing aspect-based summarization methods and vanilla counterparts of the fine-tuned LLMs. Our work contributes to the field of aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs for generating high-quality aspect-based summaries. Furthermore, it opens doors for further exploration of using LLMs for targeted information extraction tasks across various NLP domains.","sentences":["The ever-increasing volume of digital information necessitates efficient methods for users to extract key insights from lengthy documents.","Aspect-based summarization offers a targeted approach, generating summaries focused on specific aspects within a document.","Despite advancements in aspect-based summarization research, there is a continuous quest for improved model performance.","Given that large language models (LLMs) have demonstrated the potential to revolutionize diverse tasks within natural language processing, particularly in the problem of summarization, this paper explores the potential of fine-tuning LLMs for the aspect-based summarization task.","We evaluate the impact of fine-tuning open-source foundation LLMs, including Llama2, Mistral, Gemma and Aya, on a publicly available domain-specific aspect based summary dataset.","We hypothesize that this approach will enable these models to effectively identify and extract aspect-related information, leading to superior quality aspect-based summaries compared to the state-of-the-art.","We establish a comprehensive evaluation framework to compare the performance of fine-tuned LLMs against competing aspect-based summarization methods and vanilla counterparts of the fine-tuned LLMs.","Our work contributes to the field of aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs for generating high-quality aspect-based summaries.","Furthermore, it opens doors for further exploration of using LLMs for targeted information extraction tasks across various NLP domains."],"url":"http://arxiv.org/abs/2408.02584v1"}
{"created":"2024-08-05 16:00:07","title":"Clustering and Mining Accented Speech for Inclusive and Fair Speech Recognition","abstract":"Modern automatic speech recognition (ASR) systems are typically trained on more than tens of thousands hours of speech data, which is one of the main factors for their great success. However, the distribution of such data is typically biased towards common accents or typical speech patterns. As a result, those systems often poorly perform on atypical accented speech. In this paper, we present accent clustering and mining schemes for fair speech recognition systems which can perform equally well on under-represented accented speech. For accent recognition, we applied three schemes to overcome limited size of supervised accent data: supervised or unsupervised pre-training, distributionally robust optimization (DRO) and unsupervised clustering. Three schemes can significantly improve the accent recognition model especially for unbalanced and small accented speech. Fine-tuning ASR on the mined Indian accent speech using the proposed supervised or unsupervised clustering schemes showed 10.0% and 5.3% relative improvements compared to fine-tuning on the randomly sampled speech, respectively.","sentences":["Modern automatic speech recognition (ASR) systems are typically trained on more than tens of thousands hours of speech data, which is one of the main factors for their great success.","However, the distribution of such data is typically biased towards common accents or typical speech patterns.","As a result, those systems often poorly perform on atypical accented speech.","In this paper, we present accent clustering and mining schemes for fair speech recognition systems which can perform equally well on under-represented accented speech.","For accent recognition, we applied three schemes to overcome limited size of supervised accent data: supervised or unsupervised pre-training, distributionally robust optimization (DRO) and unsupervised clustering.","Three schemes can significantly improve the accent recognition model especially for unbalanced and small accented speech.","Fine-tuning ASR on the mined Indian accent speech using the proposed supervised or unsupervised clustering schemes showed 10.0% and 5.3% relative improvements compared to fine-tuning on the randomly sampled speech, respectively."],"url":"http://arxiv.org/abs/2408.02582v1"}
{"created":"2024-08-05 15:59:36","title":"Operational range bounding of spectroscopy models with anomaly detection","abstract":"Safe operation of machine learning models requires architectures that explicitly delimit their operational ranges. We evaluate the ability of anomaly detection algorithms to provide indicators correlated with degraded model performance. By placing acceptance thresholds over such indicators, hard boundaries are formed that define the model's coverage. As a use case, we consider the extraction of exoplanetary spectra from transit light curves, specifically within the context of ESA's upcoming Ariel mission. Isolation Forests are shown to effectively identify contexts where prediction models are likely to fail. Coverage/error trade-offs are evaluated under conditions of data and concept drift. The best performance is seen when Isolation Forests model projections of the prediction model's explainability SHAP values.","sentences":["Safe operation of machine learning models requires architectures that explicitly delimit their operational ranges.","We evaluate the ability of anomaly detection algorithms to provide indicators correlated with degraded model performance.","By placing acceptance thresholds over such indicators, hard boundaries are formed that define the model's coverage.","As a use case, we consider the extraction of exoplanetary spectra from transit light curves, specifically within the context of ESA's upcoming Ariel mission.","Isolation Forests are shown to effectively identify contexts where prediction models are likely to fail.","Coverage/error trade-offs are evaluated under conditions of data and concept drift.","The best performance is seen when Isolation Forests model projections of the prediction model's explainability SHAP values."],"url":"http://arxiv.org/abs/2408.02581v1"}
{"created":"2024-08-05 15:48:51","title":"Artificial Intelligence for Public Health Surveillance in Africa: Applications and Opportunities","abstract":"Artificial Intelligence (AI) is revolutionizing various fields, including public health surveillance. In Africa, where health systems frequently encounter challenges such as limited resources, inadequate infrastructure, failed health information systems and a shortage of skilled health professionals, AI offers a transformative opportunity. This paper investigates the applications of AI in public health surveillance across the continent, presenting successful case studies and examining the benefits, opportunities, and challenges of implementing AI technologies in African healthcare settings. Our paper highlights AI's potential to enhance disease monitoring and health outcomes, and support effective public health interventions. The findings presented in the paper demonstrate that AI can significantly improve the accuracy and timeliness of disease detection and prediction, optimize resource allocation, and facilitate targeted public health strategies. Additionally, our paper identified key barriers to the widespread adoption of AI in African public health systems and proposed actionable recommendations to overcome these challenges.","sentences":["Artificial Intelligence (AI) is revolutionizing various fields, including public health surveillance.","In Africa, where health systems frequently encounter challenges such as limited resources, inadequate infrastructure, failed health information systems and a shortage of skilled health professionals, AI offers a transformative opportunity.","This paper investigates the applications of AI in public health surveillance across the continent, presenting successful case studies and examining the benefits, opportunities, and challenges of implementing AI technologies in African healthcare settings.","Our paper highlights AI's potential to enhance disease monitoring and health outcomes, and support effective public health interventions.","The findings presented in the paper demonstrate that AI can significantly improve the accuracy and timeliness of disease detection and prediction, optimize resource allocation, and facilitate targeted public health strategies.","Additionally, our paper identified key barriers to the widespread adoption of AI in African public health systems and proposed actionable recommendations to overcome these challenges."],"url":"http://arxiv.org/abs/2408.02575v1"}
{"created":"2024-08-05 15:48:19","title":"DanModCap: Designing a Danmaku Moderation Tool for Video-Sharing Platforms that Leverages Impact Captions","abstract":"Online video platforms have gained increased popularity due to their ability to support information consumption and sharing and the diverse social interactions they afford. Danmaku, a real-time commentary feature that overlays user comments on a video, has been found to improve user engagement, however, the use of Danmaku can lead to toxic behaviors and inappropriate comments. To address these issues, we propose a proactive moderation approach inspired by Impact Captions, a visual technique used in East Asian variety shows. Impact Captions combine textual content and visual elements to construct emotional and cognitive resonance. Within the context of this work, Impact Captions were used to guide viewers towards positive Danmaku-related activities and elicit more pro-social behaviors. Leveraging Impact Captions, we developed DanModCap, an moderation tool that collected and analyzed Danmaku and used it as input to large generative language models to produce Impact Captions. Our evaluation of DanModCap demonstrated that Impact Captions reduced negative antagonistic emotions, increased users' desire to share positive content, and elicited self-control in Danmaku social action to fostering proactive community maintenance behaviors. Our approach highlights the benefits of using LLM-supported content moderation methods for proactive moderation in a large-scale live content contexts.","sentences":["Online video platforms have gained increased popularity due to their ability to support information consumption and sharing and the diverse social interactions they afford.","Danmaku, a real-time commentary feature that overlays user comments on a video, has been found to improve user engagement, however, the use of Danmaku can lead to toxic behaviors and inappropriate comments.","To address these issues, we propose a proactive moderation approach inspired by Impact Captions, a visual technique used in East Asian variety shows.","Impact Captions combine textual content and visual elements to construct emotional and cognitive resonance.","Within the context of this work, Impact Captions were used to guide viewers towards positive Danmaku-related activities and elicit more pro-social behaviors.","Leveraging Impact Captions, we developed DanModCap, an moderation tool that collected and analyzed Danmaku and used it as input to large generative language models to produce Impact Captions.","Our evaluation of DanModCap demonstrated that Impact Captions reduced negative antagonistic emotions, increased users' desire to share positive content, and elicited self-control in Danmaku social action to fostering proactive community maintenance behaviors.","Our approach highlights the benefits of using LLM-supported content moderation methods for proactive moderation in a large-scale live content contexts."],"url":"http://arxiv.org/abs/2408.02574v1"}
{"created":"2024-08-05 15:45:59","title":"Contrastive Learning-based Multi Modal Architecture for Emoticon Prediction by Employing Image-Text Pairs","abstract":"The emoticons are symbolic representations that generally accompany the textual content to visually enhance or summarize the true intention of a written message. Although widely utilized in the realm of social media, the core semantics of these emoticons have not been extensively explored based on multiple modalities. Incorporating textual and visual information within a single message develops an advanced way of conveying information. Hence, this research aims to analyze the relationship among sentences, visuals, and emoticons. For an orderly exposition, this paper initially provides a detailed examination of the various techniques for extracting multimodal features, emphasizing the pros and cons of each method. Through conducting a comprehensive examination of several multimodal algorithms, with specific emphasis on the fusion approaches, we have proposed a novel contrastive learning based multimodal architecture. The proposed model employs the joint training of dual-branch encoder along with the contrastive learning to accurately map text and images into a common latent space. Our key finding is that by integrating the principle of contrastive learning with that of the other two branches yields superior results. The experimental results demonstrate that our suggested methodology surpasses existing multimodal approaches in terms of accuracy and robustness. The proposed model attained an accuracy of 91% and an MCC-score of 90% while assessing emoticons using the Multimodal-Twitter Emoticon dataset acquired from Twitter. We provide evidence that deep features acquired by contrastive learning are more efficient, suggesting that the proposed fusion technique also possesses strong generalisation capabilities for recognising emoticons across several modes.","sentences":["The emoticons are symbolic representations that generally accompany the textual content to visually enhance or summarize the true intention of a written message.","Although widely utilized in the realm of social media, the core semantics of these emoticons have not been extensively explored based on multiple modalities.","Incorporating textual and visual information within a single message develops an advanced way of conveying information.","Hence, this research aims to analyze the relationship among sentences, visuals, and emoticons.","For an orderly exposition, this paper initially provides a detailed examination of the various techniques for extracting multimodal features, emphasizing the pros and cons of each method.","Through conducting a comprehensive examination of several multimodal algorithms, with specific emphasis on the fusion approaches, we have proposed a novel contrastive learning based multimodal architecture.","The proposed model employs the joint training of dual-branch encoder along with the contrastive learning to accurately map text and images into a common latent space.","Our key finding is that by integrating the principle of contrastive learning with that of the other two branches yields superior results.","The experimental results demonstrate that our suggested methodology surpasses existing multimodal approaches in terms of accuracy and robustness.","The proposed model attained an accuracy of 91% and an MCC-score of 90% while assessing emoticons using the Multimodal-Twitter Emoticon dataset acquired from Twitter.","We provide evidence that deep features acquired by contrastive learning are more efficient, suggesting that the proposed fusion technique also possesses strong generalisation capabilities for recognising emoticons across several modes."],"url":"http://arxiv.org/abs/2408.02571v1"}
{"created":"2024-08-05 15:43:56","title":"Cross-Modality Clustering-based Self-Labeling for Multimodal Data Classification","abstract":"Technological advances facilitate the ability to acquire multimodal data, posing a challenge for recognition systems while also providing an opportunity to use the heterogeneous nature of the information to increase the generalization capability of models. An often overlooked issue is the cost of the labeling process, which is typically high due to the need for a significant investment in time and money associated with human experts. Existing semi-supervised learning methods often focus on operating in the feature space created by the fusion of available modalities, neglecting the potential for cross-utilizing complementary information available in each modality. To address this problem, we propose Cross-Modality Clustering-based Self-Labeling (CMCSL). Based on a small set of pre-labeled data, CMCSL groups instances belonging to each modality in the deep feature space and then propagates known labels within the resulting clusters. Next, information about the instances' class membership in each modality is exchanged based on the Euclidean distance to ensure more accurate labeling. Experimental evaluation conducted on 20 datasets derived from the MM-IMDb dataset indicates that cross-propagation of labels between modalities -- especially when the number of pre-labeled instances is small -- can allow for more reliable labeling and thus increase the classification performance in each modality.","sentences":["Technological advances facilitate the ability to acquire multimodal data, posing a challenge for recognition systems while also providing an opportunity to use the heterogeneous nature of the information to increase the generalization capability of models.","An often overlooked issue is the cost of the labeling process, which is typically high due to the need for a significant investment in time and money associated with human experts.","Existing semi-supervised learning methods often focus on operating in the feature space created by the fusion of available modalities, neglecting the potential for cross-utilizing complementary information available in each modality.","To address this problem, we propose Cross-Modality Clustering-based Self-Labeling (CMCSL).","Based on a small set of pre-labeled data, CMCSL groups instances belonging to each modality in the deep feature space and then propagates known labels within the resulting clusters.","Next, information about the instances' class membership in each modality is exchanged based on the Euclidean distance to ensure more accurate labeling.","Experimental evaluation conducted on 20 datasets derived from the MM-IMDb dataset indicates that cross-propagation of labels between modalities -- especially when the number of pre-labeled instances is small -- can allow for more reliable labeling and thus increase the classification performance in each modality."],"url":"http://arxiv.org/abs/2408.02568v1"}
{"created":"2024-08-05 15:42:51","title":"Reasons to Doubt the Impact of AI Risk Evaluations","abstract":"AI safety practitioners invest considerable resources in AI system evaluations, but these investments may be wasted if evaluations fail to realize their impact. This paper questions the core value proposition of evaluations: that they significantly improve our understanding of AI risks and, consequently, our ability to mitigate those risks. Evaluations may fail to improve understanding in six ways, such as risks manifesting beyond the AI system or insignificant returns from evaluations compared to real-world observations. Improved understanding may also not lead to better risk mitigation in four ways, including challenges in upholding and enforcing commitments. Evaluations could even be harmful, for example, by triggering the weaponization of dual-use capabilities or invoking high opportunity costs for AI safety. This paper concludes with considerations for improving evaluation practices and 12 recommendations for AI labs, external evaluators, regulators, and academic researchers to encourage a more strategic and impactful approach to AI risk assessment and mitigation.","sentences":["AI safety practitioners invest considerable resources in AI system evaluations, but these investments may be wasted if evaluations fail to realize their impact.","This paper questions the core value proposition of evaluations: that they significantly improve our understanding of AI risks and, consequently, our ability to mitigate those risks.","Evaluations may fail to improve understanding in six ways, such as risks manifesting beyond the AI system or insignificant returns from evaluations compared to real-world observations.","Improved understanding may also not lead to better risk mitigation in four ways, including challenges in upholding and enforcing commitments.","Evaluations could even be harmful, for example, by triggering the weaponization of dual-use capabilities or invoking high opportunity costs for AI safety.","This paper concludes with considerations for improving evaluation practices and 12 recommendations for AI labs, external evaluators, regulators, and academic researchers to encourage a more strategic and impactful approach to AI risk assessment and mitigation."],"url":"http://arxiv.org/abs/2408.02565v1"}
{"created":"2024-08-05 15:38:41","title":"Asynchronous Latency and Fast Atomic Snapshot","abstract":"The original goal of this paper was a novel, fast atomic-snapshot protocol for asynchronous message-passing systems. In the process of defining what fast means exactly, we faced a number of interesting issues that arise when conventional time metrics are applied to asynchronous implementations. We discovered some gaps in latency claims made in earlier work on snapshot algorithms, which hampers their comparative time-complexity analysis. We then came up with a new unifying time-complexity analysis that captures the latency of an operation in an asynchronous, long-lived implementation, which allowed us to formally grasp latency improvements of our solution with respect to the state-of-the-art protocols: optimal latency in fault-free runs without contention, short constant latency in fault-free runs with contention, the worst-case latency proportional to the number of failures, and constant, close to optimal amortized latency.","sentences":["The original goal of this paper was a novel, fast atomic-snapshot protocol for asynchronous message-passing systems.","In the process of defining what fast means exactly, we faced a number of interesting issues that arise when conventional time metrics are applied to asynchronous implementations.","We discovered some gaps in latency claims made in earlier work on snapshot algorithms, which hampers their comparative time-complexity analysis.","We then came up with a new unifying time-complexity analysis that captures the latency of an operation in an asynchronous, long-lived implementation, which allowed us to formally grasp latency improvements of our solution with respect to the state-of-the-art protocols: optimal latency in fault-free runs without contention, short constant latency in fault-free runs with contention, the worst-case latency proportional to the number of failures, and constant, close to optimal amortized latency."],"url":"http://arxiv.org/abs/2408.02562v1"}
{"created":"2024-08-05 15:37:18","title":"HQOD: Harmonious Quantization for Object Detection","abstract":"Task inharmony problem commonly occurs in modern object detectors, leading to inconsistent qualities between classification and regression tasks. The predicted boxes with high classification scores but poor localization positions or low classification scores but accurate localization positions will worsen the performance of detectors after Non-Maximum Suppression. Furthermore, when object detectors collaborate with Quantization-Aware Training (QAT), we observe that the task inharmony problem will be further exacerbated, which is considered one of the main causes of the performance degradation of quantized detectors. To tackle this issue, we propose the Harmonious Quantization for Object Detection (HQOD) framework, which consists of two components. Firstly, we propose a task-correlated loss to encourage detectors to focus on improving samples with lower task harmony quality during QAT. Secondly, a harmonious Intersection over Union (IoU) loss is incorporated to balance the optimization of the regression branch across different IoU levels. The proposed HQOD can be easily integrated into different QAT algorithms and detectors. Remarkably, on the MS COCO dataset, our 4-bit ATSS with ResNet-50 backbone achieves a state-of-the-art mAP of 39.6%, even surpassing the full-precision one.","sentences":["Task inharmony problem commonly occurs in modern object detectors, leading to inconsistent qualities between classification and regression tasks.","The predicted boxes with high classification scores but poor localization positions or low classification scores but accurate localization positions will worsen the performance of detectors after Non-Maximum Suppression.","Furthermore, when object detectors collaborate with Quantization-Aware Training (QAT), we observe that the task inharmony problem will be further exacerbated, which is considered one of the main causes of the performance degradation of quantized detectors.","To tackle this issue, we propose the Harmonious Quantization for Object Detection (HQOD) framework, which consists of two components.","Firstly, we propose a task-correlated loss to encourage detectors to focus on improving samples with lower task harmony quality during QAT.","Secondly, a harmonious Intersection over Union (IoU) loss is incorporated to balance the optimization of the regression branch across different IoU levels.","The proposed HQOD can be easily integrated into different QAT algorithms and detectors.","Remarkably, on the MS COCO dataset, our 4-bit ATSS with ResNet-50 backbone achieves a state-of-the-art mAP of 39.6%, even surpassing the full-precision one."],"url":"http://arxiv.org/abs/2408.02561v1"}
{"created":"2024-08-05 15:36:46","title":"Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan: A Multi-Player Cooperative Game under Imperfect Information","abstract":"Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored. This study investigates the applicability of knowledge acquired by open-source and API-based LLMs to sophisticated text-based games requiring agent collaboration under imperfect information, comparing their performance to established baselines using other types of agents. We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input. An external tool was incorporated to mitigate the challenge of dynamic and extensive action spaces in this card game. Our results show that although a performance gap exists between current LLMs and state-of-the-art reinforcement learning (RL) models, LLMs demonstrate ToM capabilities in this game setting. It consistently improves their performance against opposing agents, suggesting their ability to understand the actions of allies and adversaries and establish collaboration with allies. To encourage further research and understanding, we have made our codebase openly accessible.","sentences":["Large language models (LLMs) have shown success in handling simple games with imperfect information and enabling multi-agent coordination, but their ability to facilitate practical collaboration against other agents in complex, imperfect information environments, especially in a non-English environment, still needs to be explored.","This study investigates the applicability of knowledge acquired by open-source and API-based LLMs to sophisticated text-based games requiring agent collaboration under imperfect information, comparing their performance to established baselines using other types of agents.","We propose a Theory of Mind (ToM) planning technique that allows LLM agents to adapt their strategy against various adversaries using only game rules, current state, and historical context as input.","An external tool was incorporated to mitigate the challenge of dynamic and extensive action spaces in this card game.","Our results show that although a performance gap exists between current LLMs and state-of-the-art reinforcement learning (RL) models, LLMs demonstrate ToM capabilities in this game setting.","It consistently improves their performance against opposing agents, suggesting their ability to understand the actions of allies and adversaries and establish collaboration with allies.","To encourage further research and understanding, we have made our codebase openly accessible."],"url":"http://arxiv.org/abs/2408.02559v1"}
{"created":"2024-08-05 15:34:26","title":"AutoFL: A Tool for Automatic Multi-granular Labelling of Software Repositories","abstract":"Software comprehension, especially of new code bases, is time consuming for developers, especially in large projects with multiple functionalities spanning various domains. One strategy to reduce this effort involves annotating files with meaningful labels that describe the functionalities contained. However, prior research has so far focused on classifying the whole project using README files as a proxy, resulting in little information gained for the developers.   Our objective is to streamline the labelling of files with the correct application domains using source code as input. To achieve this, in prior work, we evaluated the ability to annotate files automatically using a weak labelling approach.   This paper presents AutoFL, a tool for automatically labelling software repositories from source code. AutoFL allows multi-granular annotations including: \\textit{file}, \\textit{package}, and \\textit{project} -level.   We provide an overview of the tool's internals, present an example analysis for which AutoFL can be used, and discuss limitations and future work.","sentences":["Software comprehension, especially of new code bases, is time consuming for developers, especially in large projects with multiple functionalities spanning various domains.","One strategy to reduce this effort involves annotating files with meaningful labels that describe the functionalities contained.","However, prior research has so far focused on classifying the whole project using README files as a proxy, resulting in little information gained for the developers.   ","Our objective is to streamline the labelling of files with the correct application domains using source code as input.","To achieve this, in prior work, we evaluated the ability to annotate files automatically using a weak labelling approach.   ","This paper presents AutoFL, a tool for automatically labelling software repositories from source code.","AutoFL allows multi-granular annotations including: \\textit{file}, \\textit{package}, and \\textit{project} -level.   ","We provide an overview of the tool's internals, present an example analysis for which AutoFL can be used, and discuss limitations and future work."],"url":"http://arxiv.org/abs/2408.02557v1"}
{"created":"2024-08-05 15:33:45","title":"MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh Tokenization","abstract":"We introduce MeshAnything V2, an autoregressive transformer that generates Artist-Created Meshes (AM) aligned to given shapes. It can be integrated with various 3D asset production pipelines to achieve high-quality, highly controllable AM generation. MeshAnything V2 surpasses previous methods in both efficiency and performance using models of the same size. These improvements are due to our newly proposed mesh tokenization method: Adjacent Mesh Tokenization (AMT). Different from previous methods that represent each face with three vertices, AMT uses a single vertex whenever possible. Compared to previous methods, AMT requires about half the token sequence length to represent the same mesh in average. Furthermore, the token sequences from AMT are more compact and well-structured, fundamentally benefiting AM generation. Our extensive experiments show that AMT significantly improves the efficiency and performance of AM generation. Project Page: https://buaacyw.github.io/meshanything-v2/","sentences":["We introduce MeshAnything V2, an autoregressive transformer that generates Artist-Created Meshes (AM) aligned to given shapes.","It can be integrated with various 3D asset production pipelines to achieve high-quality, highly controllable AM generation.","MeshAnything V2 surpasses previous methods in both efficiency and performance using models of the same size.","These improvements are due to our newly proposed mesh tokenization method: Adjacent Mesh Tokenization (AMT).","Different from previous methods that represent each face with three vertices, AMT uses a single vertex whenever possible.","Compared to previous methods, AMT requires about half the token sequence length to represent the same mesh in average.","Furthermore, the token sequences from AMT are more compact and well-structured, fundamentally benefiting AM generation.","Our extensive experiments show that AMT significantly improves the efficiency and performance of AM generation.","Project Page: https://buaacyw.github.io/meshanything-v2/"],"url":"http://arxiv.org/abs/2408.02555v1"}
{"created":"2024-08-05 15:26:39","title":"Process-constrained batch Bayesian approaches for yield optimization in multi-reactor systems","abstract":"The optimization of yields in multi-reactor systems, which are advanced tools in heterogeneous catalysis research, presents a significant challenge due to hierarchical technical constraints. To this respect, this work introduces a novel approach called process-constrained batch Bayesian optimization via Thompson sampling (pc-BO-TS) and its generalized hierarchical extension (hpc-BO-TS). This method, tailored for the efficiency demands in multi-reactor systems, integrates experimental constraints and balances between exploration and exploitation in a sequential batch optimization strategy. It offers an improvement over other Bayesian optimization methods. The performance of pc-BO-TS and hpc-BO-TS is validated in synthetic cases as well as in a realistic scenario based on data obtained from high-throughput experiments done on a multi-reactor system available in the REALCAT platform. The proposed methods often outperform other sequential Bayesian optimizations and existing process-constrained batch Bayesian optimization methods. This work proposes a novel approach to optimize the yield of a reaction in a multi-reactor system, marking a significant step forward in digital catalysis and generally in optimization methods for chemical engineering.","sentences":["The optimization of yields in multi-reactor systems, which are advanced tools in heterogeneous catalysis research, presents a significant challenge due to hierarchical technical constraints.","To this respect, this work introduces a novel approach called process-constrained batch Bayesian optimization via Thompson sampling (pc-BO-TS) and its generalized hierarchical extension (hpc-BO-TS).","This method, tailored for the efficiency demands in multi-reactor systems, integrates experimental constraints and balances between exploration and exploitation in a sequential batch optimization strategy.","It offers an improvement over other Bayesian optimization methods.","The performance of pc-BO-TS and hpc-BO-TS is validated in synthetic cases as well as in a realistic scenario based on data obtained from high-throughput experiments done on a multi-reactor system available in the REALCAT platform.","The proposed methods often outperform other sequential Bayesian optimizations and existing process-constrained batch Bayesian optimization methods.","This work proposes a novel approach to optimize the yield of a reaction in a multi-reactor system, marking a significant step forward in digital catalysis and generally in optimization methods for chemical engineering."],"url":"http://arxiv.org/abs/2408.02551v1"}
{"created":"2024-08-05 15:20:29","title":"On the Complexity of the Conditional Independence Implication Problem With Bounded Cardinalities","abstract":"We show that the conditional independence (CI) implication problem with bounded cardinalities, which asks whether a given CI implication holds for all discrete random variables with given cardinalities, is co-NEXPTIME-hard. The problem remains co-NEXPTIME-hard if all variables are binary. The reduction goes from a variant of the tiling problem and is based on a prior construction used by Cheuk Ting Li to show the undecidability of a related problem where the cardinality of some variables remains unbounded. The CI implication problem with bounded cardinalities is known to be in EXPSPACE, as its negation can be stated as an existential first-order logic formula over the reals of size exponential with regard to the size of the input.","sentences":["We show that the conditional independence (CI) implication problem with bounded cardinalities, which asks whether a given CI implication holds for all discrete random variables with given cardinalities, is co-NEXPTIME-hard.","The problem remains co-NEXPTIME-hard if all variables are binary.","The reduction goes from a variant of the tiling problem and is based on a prior construction used by Cheuk Ting Li to show the undecidability of a related problem where the cardinality of some variables remains unbounded.","The CI implication problem with bounded cardinalities is known to be in EXPSPACE, as its negation can be stated as an existential first-order logic formula over the reals of size exponential with regard to the size of the input."],"url":"http://arxiv.org/abs/2408.02550v1"}
{"created":"2024-08-05 15:17:34","title":"The Role of Functional Muscle Networks in Improving Hand Gesture Perception for Human-Machine Interfaces","abstract":"Developing accurate hand gesture perception models is critical for various robotic applications, enabling effective communication between humans and machines and directly impacting neurorobotics and interactive robots. Recently, surface electromyography (sEMG) has been explored for its rich informational context and accessibility when combined with advanced machine learning approaches and wearable systems. The literature presents numerous approaches to boost performance while ensuring robustness for neurorobots using sEMG, often resulting in models requiring high processing power, large datasets, and less scalable solutions. This paper addresses this challenge by proposing the decoding of muscle synchronization rather than individual muscle activation. We study coherence-based functional muscle networks as the core of our perception model, proposing that functional synchronization between muscles and the graph-based network of muscle connectivity encode contextual information about intended hand gestures. This can be decoded using shallow machine learning approaches without the need for deep temporal networks. Our technique could impact myoelectric control of neurorobots by reducing computational burdens and enhancing efficiency. The approach is benchmarked on the Ninapro database, which contains 12 EMG signals from 40 subjects performing 17 hand gestures. It achieves an accuracy of 85.1%, demonstrating improved performance compared to existing methods while requiring much less computational power. The results support the hypothesis that a coherence-based functional muscle network encodes critical information related to gesture execution, significantly enhancing hand gesture perception with potential applications for neurorobotic systems and interactive machines.","sentences":["Developing accurate hand gesture perception models is critical for various robotic applications, enabling effective communication between humans and machines and directly impacting neurorobotics and interactive robots.","Recently, surface electromyography (sEMG) has been explored for its rich informational context and accessibility when combined with advanced machine learning approaches and wearable systems.","The literature presents numerous approaches to boost performance while ensuring robustness for neurorobots using sEMG, often resulting in models requiring high processing power, large datasets, and less scalable solutions.","This paper addresses this challenge by proposing the decoding of muscle synchronization rather than individual muscle activation.","We study coherence-based functional muscle networks as the core of our perception model, proposing that functional synchronization between muscles and the graph-based network of muscle connectivity encode contextual information about intended hand gestures.","This can be decoded using shallow machine learning approaches without the need for deep temporal networks.","Our technique could impact myoelectric control of neurorobots by reducing computational burdens and enhancing efficiency.","The approach is benchmarked on the Ninapro database, which contains 12 EMG signals from 40 subjects performing 17 hand gestures.","It achieves an accuracy of 85.1%, demonstrating improved performance compared to existing methods while requiring much less computational power.","The results support the hypothesis that a coherence-based functional muscle network encodes critical information related to gesture execution, significantly enhancing hand gesture perception with potential applications for neurorobotic systems and interactive machines."],"url":"http://arxiv.org/abs/2408.02547v1"}
{"created":"2024-08-05 15:16:24","title":"RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation","abstract":"Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions. Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach. We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases. RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings. This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources. We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets. Code is released as open-source in https://github.com/IntelLabs/RAGFoundry.","sentences":["Implementing Retrieval-Augmented Generation (RAG) systems is inherently complex, requiring deep understanding of data, use cases, and intricate design decisions.","Additionally, evaluating these systems presents significant challenges, necessitating assessment of both retrieval accuracy and generative quality through a multi-faceted approach.","We introduce RAG Foundry, an open-source framework for augmenting large language models for RAG use cases.","RAG Foundry integrates data creation, training, inference and evaluation into a single workflow, facilitating the creation of data-augmented datasets for training and evaluating large language models in RAG settings.","This integration enables rapid prototyping and experimentation with various RAG techniques, allowing users to easily generate datasets and train RAG models using internal or specialized knowledge sources.","We demonstrate the framework effectiveness by augmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG configurations, showcasing consistent improvements across three knowledge-intensive datasets.","Code is released as open-source in https://github.com/IntelLabs/RAGFoundry."],"url":"http://arxiv.org/abs/2408.02545v1"}
{"created":"2024-08-05 15:16:22","title":"Caution for the Environment: Multimodal Agents are Susceptible to Environmental Distractions","abstract":"This paper investigates the faithfulness of multimodal large language model (MLLM) agents in the graphical user interface (GUI) environment, aiming to address the research question of whether multimodal GUI agents can be distracted by environmental context. A general setting is proposed where both the user and the agent are benign, and the environment, while not malicious, contains unrelated content. A wide range of MLLMs are evaluated as GUI agents using our simulated dataset, following three working patterns with different levels of perception. Experimental results reveal that even the most powerful models, whether generalist agents or specialist GUI agents, are susceptible to distractions. While recent studies predominantly focus on the helpfulness (i.e., action accuracy) of multimodal agents, our findings indicate that these agents are prone to environmental distractions, resulting in unfaithful behaviors. Furthermore, we switch to the adversarial perspective and implement environment injection, demonstrating that such unfaithfulness can be exploited, leading to unexpected risks.","sentences":["This paper investigates the faithfulness of multimodal large language model (MLLM) agents in the graphical user interface (GUI) environment, aiming to address the research question of whether multimodal GUI agents can be distracted by environmental context.","A general setting is proposed where both the user and the agent are benign, and the environment, while not malicious, contains unrelated content.","A wide range of MLLMs are evaluated as GUI agents using our simulated dataset, following three working patterns with different levels of perception.","Experimental results reveal that even the most powerful models, whether generalist agents or specialist GUI agents, are susceptible to distractions.","While recent studies predominantly focus on the helpfulness (i.e., action accuracy) of multimodal agents, our findings indicate that these agents are prone to environmental distractions, resulting in unfaithful behaviors.","Furthermore, we switch to the adversarial perspective and implement environment injection, demonstrating that such unfaithfulness can be exploited, leading to unexpected risks."],"url":"http://arxiv.org/abs/2408.02544v1"}
{"created":"2024-08-05 15:08:26","title":"Towards Coarse-grained Visual Language Navigation Task Planning Enhanced by Event Knowledge Graph","abstract":"Visual language navigation (VLN) is one of the important research in embodied AI. It aims to enable an agent to understand the surrounding environment and complete navigation tasks. VLN instructions could be categorized into coarse-grained and fine-grained commands. Fine-grained command describes a whole task with subtasks step-by-step. In contrast, coarse-grained command gives an abstract task description, which more suites human habits. Most existing work focuses on the former kind of instruction in VLN tasks, ignoring the latter abstract instructions belonging to daily life scenarios. To overcome the above challenge in abstract instruction, we attempt to consider coarse-grained instruction in VLN by event knowledge enhancement. Specifically, we first propose a prompt-based framework to extract an event knowledge graph (named VLN-EventKG) for VLN integrally over multiple mainstream benchmark datasets. Through small and large language model collaboration, we realize knowledge-enhanced navigation planning (named EventNav) for VLN tasks with coarse-grained instruction input. Additionally, we design a novel dynamic history backtracking module to correct potential error action planning in real time. Experimental results in various public benchmarks show our knowledge-enhanced method has superiority in coarse-grained-instruction VLN using our proposed VLN-EventKG with over $5\\%$ improvement in success rate. Our project is available at https://sites.google.com/view/vln-eventkg","sentences":["Visual language navigation (VLN) is one of the important research in embodied AI.","It aims to enable an agent to understand the surrounding environment and complete navigation tasks.","VLN instructions could be categorized into coarse-grained and fine-grained commands.","Fine-grained command describes a whole task with subtasks step-by-step.","In contrast, coarse-grained command gives an abstract task description, which more suites human habits.","Most existing work focuses on the former kind of instruction in VLN tasks, ignoring the latter abstract instructions belonging to daily life scenarios.","To overcome the above challenge in abstract instruction, we attempt to consider coarse-grained instruction in VLN by event knowledge enhancement.","Specifically, we first propose a prompt-based framework to extract an event knowledge graph (named VLN-EventKG) for VLN integrally over multiple mainstream benchmark datasets.","Through small and large language model collaboration, we realize knowledge-enhanced navigation planning (named EventNav) for VLN tasks with coarse-grained instruction input.","Additionally, we design a novel dynamic history backtracking module to correct potential error action planning in real time.","Experimental results in various public benchmarks show our knowledge-enhanced method has superiority in coarse-grained-instruction VLN using our proposed VLN-EventKG with over $5\\%$ improvement in success rate.","Our project is available at https://sites.google.com/view/vln-eventkg"],"url":"http://arxiv.org/abs/2408.02535v1"}
{"created":"2024-08-05 15:03:19","title":"LMEMs for post-hoc analysis of HPO Benchmarking","abstract":"The importance of tuning hyperparameters in Machine Learning (ML) and Deep Learning (DL) is established through empirical research and applications, evident from the increase in new hyperparameter optimization (HPO) algorithms and benchmarks steadily added by the community. However, current benchmarking practices using averaged performance across many datasets may obscure key differences between HPO methods, especially for pairwise comparisons. In this work, we apply Linear Mixed-Effect Models-based (LMEMs) significance testing for post-hoc analysis of HPO benchmarking runs. LMEMs allow flexible and expressive modeling on the entire experiment data, including information such as benchmark meta-features, offering deeper insights than current analysis practices. We demonstrate this through a case study on the PriorBand paper's experiment data to find insights not reported in the original work.","sentences":["The importance of tuning hyperparameters in Machine Learning (ML) and Deep Learning (DL) is established through empirical research and applications, evident from the increase in new hyperparameter optimization (HPO) algorithms and benchmarks steadily added by the community.","However, current benchmarking practices using averaged performance across many datasets may obscure key differences between HPO methods, especially for pairwise comparisons.","In this work, we apply Linear Mixed-Effect Models-based (LMEMs) significance testing for post-hoc analysis of HPO benchmarking runs.","LMEMs allow flexible and expressive modeling on the entire experiment data, including information such as benchmark meta-features, offering deeper insights than current analysis practices.","We demonstrate this through a case study on the PriorBand paper's experiment data to find insights not reported in the original work."],"url":"http://arxiv.org/abs/2408.02533v1"}
{"created":"2024-08-05 14:49:12","title":"Counterfactual Shapley Values for Explaining Reinforcement Learning","abstract":"This paper introduces a novel approach Counterfactual Shapley Values (CSV), which enhances explainability in reinforcement learning (RL) by integrating counterfactual analysis with Shapley Values. The approach aims to quantify and compare the contributions of different state dimensions to various action choices. To more accurately analyze these impacts, we introduce new characteristic value functions, the ``Counterfactual Difference Characteristic Value\" and the ``Average Counterfactual Difference Characteristic Value.\" These functions help calculate the Shapley values to evaluate the differences in contributions between optimal and non-optimal actions. Experiments across several RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate the effectiveness of the CSV method. The results show that this method not only improves transparency in complex RL systems but also quantifies the differences across various decisions.","sentences":["This paper introduces a novel approach Counterfactual Shapley Values (CSV), which enhances explainability in reinforcement learning (RL) by integrating counterfactual analysis with Shapley Values.","The approach aims to quantify and compare the contributions of different state dimensions to various action choices.","To more accurately analyze these impacts, we introduce new characteristic value functions, the ``Counterfactual Difference Characteristic Value\" and the ``Average Counterfactual Difference Characteristic Value.\"","These functions help calculate the Shapley values to evaluate the differences in contributions between optimal and non-optimal actions.","Experiments across several RL domains, such as GridWorld, FrozenLake, and Taxi, demonstrate the effectiveness of the CSV method.","The results show that this method not only improves transparency in complex RL systems but also quantifies the differences across various decisions."],"url":"http://arxiv.org/abs/2408.02529v1"}
{"created":"2024-08-05 14:46:35","title":"Online Deterministic Minimum Cost Bipartite Matching with Delays on a Line","abstract":"We study the online minimum cost bipartite perfect matching with delays problem. In this problem, $m$ servers and $m$ requests arrive over time, and an online algorithm can delay the matching between servers and requests by paying the delay cost. The objective is to minimize the total distance and delay cost. When servers and requests lie in a known metric space, there is a randomized $O(\\log n)$-competitive algorithm, where $n$ is the size of the metric space. When the metric space is unknown a priori, Azar and Jacob-Fanani proposed a deterministic $O\\left(\\frac{1}{\\epsilon}m^{\\log\\left(\\frac{3+\\epsilon}{2}\\right)}\\right)$-competitive algorithm for any fixed $\\epsilon > 0$. This competitive ratio is tight when $n = 1$ and becomes $O(m^{0.59})$ for sufficiently small $\\epsilon$.   In this paper, we improve upon the result of Azar and Jacob-Fanani for the case where servers and requests are on the real line, providing a deterministic $\\tilde{O}(m^{0.5})$-competitive algorithm. Our algorithm is based on the Robust Matching (RM) algorithm proposed by Raghvendra for the minimum cost bipartite perfect matching problem. In this problem, delay is not allowed, and all servers arrive in the beginning. When a request arrives, the RM algorithm immediately matches the request to a free server based on the request's minimum $t$-net-cost augmenting path, where $t > 1$ is a constant. In our algorithm, we delay the matching of a request until its waiting time exceeds its minimum $t$-net-cost divided by $t$.","sentences":["We study the online minimum cost bipartite perfect matching with delays problem.","In this problem, $m$ servers and $m$ requests arrive over time, and an online algorithm can delay the matching between servers and requests by paying the delay cost.","The objective is to minimize the total distance and delay cost.","When servers and requests lie in a known metric space, there is a randomized $O(\\log n)$-competitive algorithm, where $n$ is the size of the metric space.","When the metric space is unknown a priori, Azar and Jacob-Fanani proposed a deterministic $O\\left(\\frac{1}{\\epsilon}m^{\\log\\left(\\frac{3+\\epsilon}{2}\\right)}\\right)$-competitive algorithm for any fixed $\\epsilon > 0$.","This competitive ratio is tight when $n = 1$ and becomes $O(m^{0.59})$ for sufficiently small $\\epsilon$.   In this paper, we improve upon the result of Azar and Jacob-Fanani for the case where servers and requests are on the real line, providing a deterministic $\\tilde{O}(m^{0.5})$-competitive algorithm.","Our algorithm is based on the Robust Matching (RM) algorithm proposed by Raghvendra for the minimum cost bipartite perfect matching problem.","In this problem, delay is not allowed, and all servers arrive in the beginning.","When a request arrives, the RM algorithm immediately matches the request to a free server based on the request's minimum $t$-net-cost augmenting path, where $t > 1$ is a constant.","In our algorithm, we delay the matching of a request until its waiting time exceeds its minimum $t$-net-cost divided by $t$."],"url":"http://arxiv.org/abs/2408.02526v1"}
{"created":"2024-08-05 14:46:04","title":"Single-tap Latency Reduction with Single- or Double- tap Prediction","abstract":"Touch surfaces are widely utilized for smartphones, tablet PCs, and laptops (touchpad), and single and double taps are the most basic and common operations on them. The detection of single or double taps causes the single-tap latency problem, which creates a bottleneck in terms of the sensitivity of touch inputs. To reduce the single-tap latency, we propose a novel machine-learning-based tap prediction method called PredicTaps. Our method predicts whether a detected tap is a single tap or the first contact of a double tap without having to wait for the hundreds of milliseconds conventionally required. We present three evaluations and one user evaluation that demonstrate its broad applicability and usability for various tap situations on two form factors (touchpad and smartphone). The results showed PredicTaps reduces the single-tap latency from 150-500 ms to 12 ms on laptops and to 17.6 ms on smartphones without reducing usability.","sentences":["Touch surfaces are widely utilized for smartphones, tablet PCs, and laptops (touchpad), and single and double taps are the most basic and common operations on them.","The detection of single or double taps causes the single-tap latency problem, which creates a bottleneck in terms of the sensitivity of touch inputs.","To reduce the single-tap latency, we propose a novel machine-learning-based tap prediction method called PredicTaps.","Our method predicts whether a detected tap is a single tap or the first contact of a double tap without having to wait for the hundreds of milliseconds conventionally required.","We present three evaluations and one user evaluation that demonstrate its broad applicability and usability for various tap situations on two form factors (touchpad and smartphone).","The results showed PredicTaps reduces the single-tap latency from 150-500 ms to 12 ms on laptops and to 17.6 ms on smartphones without reducing usability."],"url":"http://arxiv.org/abs/2408.02525v1"}
{"created":"2024-08-05 14:40:41","title":"Introducing a Comprehensive, Continuous, and Collaborative Survey of Intrusion Detection Datasets","abstract":"Researchers in the highly active field of intrusion detection largely rely on public datasets for their experimental evaluations. However, the large number of existing datasets, the discovery of previously unknown flaws therein, and the frequent publication of new datasets make it hard to select suitable options and sufficiently understand their respective limitations. Hence, there is a great risk of drawing invalid conclusions from experimental results with respect to detection performance of novel methods in the real world. While there exist various surveys on intrusion detection datasets, they have deficiencies in providing researchers with a profound decision basis since they lack comprehensiveness, actionable details, and up-to-dateness. In this paper, we present COMIDDS, an ongoing effort to comprehensively survey intrusion detection datasets with an unprecedented level of detail, implemented as a website backed by a public GitHub repository. COMIDDS allows researchers to quickly identify suitable datasets depending on their requirements and provides structured and critical information on each dataset, including actual data samples and links to relevant publications. COMIDDS is freely accessible, regularly updated, and open to contributions.","sentences":["Researchers in the highly active field of intrusion detection largely rely on public datasets for their experimental evaluations.","However, the large number of existing datasets, the discovery of previously unknown flaws therein, and the frequent publication of new datasets make it hard to select suitable options and sufficiently understand their respective limitations.","Hence, there is a great risk of drawing invalid conclusions from experimental results with respect to detection performance of novel methods in the real world.","While there exist various surveys on intrusion detection datasets, they have deficiencies in providing researchers with a profound decision basis since they lack comprehensiveness, actionable details, and up-to-dateness.","In this paper, we present COMIDDS, an ongoing effort to comprehensively survey intrusion detection datasets with an unprecedented level of detail, implemented as a website backed by a public GitHub repository.","COMIDDS allows researchers to quickly identify suitable datasets depending on their requirements and provides structured and critical information on each dataset, including actual data samples and links to relevant publications.","COMIDDS is freely accessible, regularly updated, and open to contributions."],"url":"http://arxiv.org/abs/2408.02521v1"}
{"created":"2024-08-05 14:40:40","title":"OneLove beyond the field -- A few-shot pipeline for topic and sentiment analysis during the FIFA World Cup in Qatar","abstract":"The FIFA World Cup in Qatar was discussed extensively in the news and on social media. Due to news reports with allegations of human rights violations, there were calls to boycott it. Wearing a OneLove armband was part of a planned protest activity. Controversy around the armband arose when FIFA threatened to sanction captains who wear it. To understand what topics Twitter users Tweeted about and what the opinion of German Twitter users was towards the OneLove armband, we performed an analysis of German Tweets published during the World Cup using in-context learning with LLMs. We validated the labels on human annotations. We found that Twitter users initially discussed the armband's impact, LGBT rights, and politics; after the ban, the conversation shifted towards politics in sports in general, accompanied by a subtle shift in sentiment towards neutrality. Our evaluation serves as a framework for future research to explore the impact of sports activism and evolving public sentiment. This is especially useful in settings where labeling datasets for specific opinions is unfeasible, such as when events are unfolding.","sentences":["The FIFA World Cup in Qatar was discussed extensively in the news and on social media.","Due to news reports with allegations of human rights violations, there were calls to boycott it.","Wearing a OneLove armband was part of a planned protest activity.","Controversy around the armband arose when FIFA threatened to sanction captains who wear it.","To understand what topics Twitter users Tweeted about and what the opinion of German Twitter users was towards the OneLove armband, we performed an analysis of German Tweets published during the World Cup using in-context learning with LLMs.","We validated the labels on human annotations.","We found that Twitter users initially discussed the armband's impact, LGBT rights, and politics; after the ban, the conversation shifted towards politics in sports in general, accompanied by a subtle shift in sentiment towards neutrality.","Our evaluation serves as a framework for future research to explore the impact of sports activism and evolving public sentiment.","This is especially useful in settings where labeling datasets for specific opinions is unfeasible, such as when events are unfolding."],"url":"http://arxiv.org/abs/2408.02520v1"}
{"created":"2024-08-05 14:34:40","title":"Stem-JEPA: A Joint-Embedding Predictive Architecture for Musical Stem Compatibility Estimation","abstract":"This paper explores the automated process of determining stem compatibility by identifying audio recordings of single instruments that blend well with a given musical context. To tackle this challenge, we present Stem-JEPA, a novel Joint-Embedding Predictive Architecture (JEPA) trained on a multi-track dataset using a self-supervised learning approach.   Our model comprises two networks: an encoder and a predictor, which are jointly trained to predict the embeddings of compatible stems from the embeddings of a given context, typically a mix of several instruments. Training a model in this manner allows its use in estimating stem compatibility - retrieving, aligning, or generating a stem to match a given mix - or for downstream tasks such as genre or key estimation, as the training paradigm requires the model to learn information related to timbre, harmony, and rhythm.   We evaluate our model's performance on a retrieval task on the MUSDB18 dataset, testing its ability to find the missing stem from a mix and through a subjective user study. We also show that the learned embeddings capture temporal alignment information and, finally, evaluate the representations learned by our model on several downstream tasks, highlighting that they effectively capture meaningful musical features.","sentences":["This paper explores the automated process of determining stem compatibility by identifying audio recordings of single instruments that blend well with a given musical context.","To tackle this challenge, we present Stem-JEPA, a novel Joint-Embedding Predictive Architecture (JEPA) trained on a multi-track dataset using a self-supervised learning approach.   ","Our model comprises two networks: an encoder and a predictor, which are jointly trained to predict the embeddings of compatible stems from the embeddings of a given context, typically a mix of several instruments.","Training a model in this manner allows its use in estimating stem compatibility - retrieving, aligning, or generating a stem to match a given mix - or for downstream tasks such as genre or key estimation, as the training paradigm requires the model to learn information related to timbre, harmony, and rhythm.   ","We evaluate our model's performance on a retrieval task on the MUSDB18 dataset, testing its ability to find the missing stem from a mix and through a subjective user study.","We also show that the learned embeddings capture temporal alignment information and, finally, evaluate the representations learned by our model on several downstream tasks, highlighting that they effectively capture meaningful musical features."],"url":"http://arxiv.org/abs/2408.02514v1"}
{"created":"2024-08-05 14:31:26","title":"Practical Attacks against Black-box Code Completion Engines","abstract":"Modern code completion engines, powered by large language models, have demonstrated impressive capabilities to generate functionally correct code based on surrounding context. As these tools are extensively used by millions of developers, it is crucial to investigate their security implications. In this work, we present INSEC, a novel attack that directs code completion engines towards generating vulnerable code. In line with most commercial completion engines, such as GitHub Copilot, INSEC assumes only black-box query access to the targeted engine, without requiring any knowledge of the engine's internals. Our attack works by inserting a malicious attack string as a short comment in the completion input. To derive the attack string, we design a series of specialized initialization schemes and an optimization procedure for further refinement. We demonstrate the strength of INSEC not only on state-of-the-art open-source models but also on black-box commercial services such as the OpenAI API and GitHub Copilot. On a comprehensive set of security-critical test cases covering 16 CWEs across 5 programming languages, INSEC significantly increases the likelihood of the considered completion engines in generating unsafe code by >50% in absolute, while maintaining the ability in producing functionally correct code. At the same time, our attack has low resource requirements, and can be developed for a cost of well under ten USD on commodity hardware.","sentences":["Modern code completion engines, powered by large language models, have demonstrated impressive capabilities to generate functionally correct code based on surrounding context.","As these tools are extensively used by millions of developers, it is crucial to investigate their security implications.","In this work, we present INSEC, a novel attack that directs code completion engines towards generating vulnerable code.","In line with most commercial completion engines, such as GitHub Copilot, INSEC assumes only black-box query access to the targeted engine, without requiring any knowledge of the engine's internals.","Our attack works by inserting a malicious attack string as a short comment in the completion input.","To derive the attack string, we design a series of specialized initialization schemes and an optimization procedure for further refinement.","We demonstrate the strength of INSEC not only on state-of-the-art open-source models but also on black-box commercial services such as the OpenAI API and GitHub Copilot.","On a comprehensive set of security-critical test cases covering 16 CWEs across 5 programming languages, INSEC significantly increases the likelihood of the considered completion engines in generating unsafe code by >50% in absolute, while maintaining the ability in producing functionally correct code.","At the same time, our attack has low resource requirements, and can be developed for a cost of well under ten USD on commodity hardware."],"url":"http://arxiv.org/abs/2408.02509v1"}
{"created":"2024-08-05 14:31:20","title":"PUREsuggest: Citation-based Literature Search and Visual Exploration with Keyword-controlled Rankings","abstract":"Citations allow quickly identifying related research. If multiple publications are selected as seeds, specific suggestions for related literature can be made based on the number of incoming and outgoing citation links to this selection. Interactively adding recommended publications to the selection refines the next suggestion and incrementally builds a relevant collection of publications. Following this approach, the paper presents a search and foraging approach, PUREsuggest, which combines citation-based suggestions with augmented visualizations of the citation network. The focus and novelty of the approach is, first, the transparency of how the rankings are explained visually and, second, that the process can be steered through user-defined keywords, which reflect topics of interests. The system can be used to build new literature collections, to update and assess existing ones, as well as to use the collected literature for identifying relevant experts in the field. We evaluated the recommendation approach through simulated sessions and performed a user study investigating search strategies and usage patterns supported by the interface.","sentences":["Citations allow quickly identifying related research.","If multiple publications are selected as seeds, specific suggestions for related literature can be made based on the number of incoming and outgoing citation links to this selection.","Interactively adding recommended publications to the selection refines the next suggestion and incrementally builds a relevant collection of publications.","Following this approach, the paper presents a search and foraging approach, PUREsuggest, which combines citation-based suggestions with augmented visualizations of the citation network.","The focus and novelty of the approach is, first, the transparency of how the rankings are explained visually and, second, that the process can be steered through user-defined keywords, which reflect topics of interests.","The system can be used to build new literature collections, to update and assess existing ones, as well as to use the collected literature for identifying relevant experts in the field.","We evaluated the recommendation approach through simulated sessions and performed a user study investigating search strategies and usage patterns supported by the interface."],"url":"http://arxiv.org/abs/2408.02508v1"}
{"created":"2024-08-05 14:31:09","title":"Estimating Pore Location of PBF-LB/M Processes with Segmentation Models","abstract":"Reliably manufacturing defect free products is still an open challenge for Laser Powder Bed Fusion processes. Particularly, pores that occur frequently have a negative impact on mechanical properties like fatigue performance. Therefore, an accurate localisation of pores is mandatory for quality assurance, but requires time-consuming post-processing steps like computer tomography scans. Although existing solutions using in-situ monitoring data can detect pore occurrence within a layer, they are limited in their localisation precision. Therefore, we propose a pore localisation approach that estimates their position within a single layer using a Gaussian kernel density estimation. This allows segmentation models to learn the correlation between in-situ monitoring data and the derived probability distribution of pore occurrence. Within our experiments, we compare the prediction performance of different segmentation models depending on machine parameter configuration and geometry features. From our results, we conclude that our approach allows a precise localisation of pores that requires minimal data preprocessing. Our research extends the literature by providing a foundation for more precise pore detection systems.","sentences":["Reliably manufacturing defect free products is still an open challenge for Laser Powder Bed Fusion processes.","Particularly, pores that occur frequently have a negative impact on mechanical properties like fatigue performance.","Therefore, an accurate localisation of pores is mandatory for quality assurance, but requires time-consuming post-processing steps like computer tomography scans.","Although existing solutions using in-situ monitoring data can detect pore occurrence within a layer, they are limited in their localisation precision.","Therefore, we propose a pore localisation approach that estimates their position within a single layer using a Gaussian kernel density estimation.","This allows segmentation models to learn the correlation between in-situ monitoring data and the derived probability distribution of pore occurrence.","Within our experiments, we compare the prediction performance of different segmentation models depending on machine parameter configuration and geometry features.","From our results, we conclude that our approach allows a precise localisation of pores that requires minimal data preprocessing.","Our research extends the literature by providing a foundation for more precise pore detection systems."],"url":"http://arxiv.org/abs/2408.02507v1"}
{"created":"2024-08-05 14:27:39","title":"UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model","abstract":"Significant advancements has recently been achieved in the field of multi-modal large language models (MLLMs), demonstrating their remarkable capabilities in understanding and reasoning across diverse tasks. However, these models are often trained for specific tasks and rely on task-specific input-output formats, limiting their applicability to a broader range of tasks. This raises a fundamental question: Can we develop a unified approach to represent and handle different multi-modal tasks to maximize the generalizability of MLLMs? In this paper, we propose UnifiedMLLM, a comprehensive model designed to represent various tasks using a unified representation. Our model exhibits strong capabilities in comprehending the implicit intent of user instructions and preforming reasoning. In addition to generating textual responses, our model also outputs task tokens and grounding tokens, serving as indicators of task types and task granularity. These outputs are subsequently routed through the task router and directed to specific expert models for task completion. To train our model, we construct a task-specific dataset and an 100k multi-task dataset encompassing complex scenarios. Employing a three-stage training strategy, we equip our model with robust reasoning and task processing capabilities while preserving its generalization capacity and knowledge reservoir. Extensive experiments showcase the impressive performance of our unified representation approach across various tasks, surpassing existing methodologies. Furthermore, our approach exhibits exceptional scalability and generality. Our code, model, and dataset will be available at \\url{https://github.com/lzw-lzw/UnifiedMLLM}.","sentences":["Significant advancements has recently been achieved in the field of multi-modal large language models (MLLMs), demonstrating their remarkable capabilities in understanding and reasoning across diverse tasks.","However, these models are often trained for specific tasks and rely on task-specific input-output formats, limiting their applicability to a broader range of tasks.","This raises a fundamental question: Can we develop a unified approach to represent and handle different multi-modal tasks to maximize the generalizability of MLLMs?","In this paper, we propose UnifiedMLLM, a comprehensive model designed to represent various tasks using a unified representation.","Our model exhibits strong capabilities in comprehending the implicit intent of user instructions and preforming reasoning.","In addition to generating textual responses, our model also outputs task tokens and grounding tokens, serving as indicators of task types and task granularity.","These outputs are subsequently routed through the task router and directed to specific expert models for task completion.","To train our model, we construct a task-specific dataset and an 100k multi-task dataset encompassing complex scenarios.","Employing a three-stage training strategy, we equip our model with robust reasoning and task processing capabilities while preserving its generalization capacity and knowledge reservoir.","Extensive experiments showcase the impressive performance of our unified representation approach across various tasks, surpassing existing methodologies.","Furthermore, our approach exhibits exceptional scalability and generality.","Our code, model, and dataset will be available at \\url{https://github.com/lzw-lzw/UnifiedMLLM}."],"url":"http://arxiv.org/abs/2408.02503v1"}
{"created":"2024-08-05 14:26:41","title":"Context Conquers Parameters: Outperforming Proprietary LLM in Commit Message Generation","abstract":"Commit messages provide descriptions of the modifications made in a commit using natural language, making them crucial for software maintenance and evolution. Recent developments in Large Language Models (LLMs) have led to their use in generating high-quality commit messages, such as the Omniscient Message Generator (OMG). This method employs GPT-4 to produce state-of-the-art commit messages. However, the use of proprietary LLMs like GPT-4 in coding tasks raises privacy and sustainability concerns, which may hinder their industrial adoption. Considering that open-source LLMs have achieved competitive performance in developer tasks such as compiler validation, this study investigates whether they can be used to generate commit messages that are comparable with OMG. Our experiments show that an open-source LLM can generate commit messages that are comparable to those produced by OMG. In addition, through a series of contextual refinements, we propose lOcal MessagE GenerAtor (OMEGA) , a CMG approach that uses a 4-bit quantized 8B open-source LLM. OMEGA produces state-of-the-art commit messages, surpassing the performance of GPT-4 in practitioners' preference.","sentences":["Commit messages provide descriptions of the modifications made in a commit using natural language, making them crucial for software maintenance and evolution.","Recent developments in Large Language Models (LLMs) have led to their use in generating high-quality commit messages, such as the Omniscient Message Generator (OMG).","This method employs GPT-4 to produce state-of-the-art commit messages.","However, the use of proprietary LLMs like GPT-4 in coding tasks raises privacy and sustainability concerns, which may hinder their industrial adoption.","Considering that open-source LLMs have achieved competitive performance in developer tasks such as compiler validation, this study investigates whether they can be used to generate commit messages that are comparable with OMG.","Our experiments show that an open-source LLM can generate commit messages that are comparable to those produced by OMG.","In addition, through a series of contextual refinements, we propose lOcal MessagE GenerAtor (OMEGA) , a CMG approach that uses a 4-bit quantized 8B open-source LLM.","OMEGA produces state-of-the-art commit messages, surpassing the performance of GPT-4 in practitioners' preference."],"url":"http://arxiv.org/abs/2408.02502v1"}
{"created":"2024-08-05 14:22:52","title":"Fair Resource Allocation For Hierarchical Federated Edge Learning in Space-Air-Ground Integrated Networks via Deep Reinforcement Learning with Hybrid Control","abstract":"The space-air-ground integrated network (SAGIN) has become a crucial research direction in future wireless communications due to its ubiquitous coverage, rapid and flexible deployment, and multi-layer cooperation capabilities. However, integrating hierarchical federated learning (HFL) with edge computing and SAGINs remains a complex open issue to be resolved. This paper proposes a novel framework for applying HFL in SAGINs, utilizing aerial platforms and low Earth orbit (LEO) satellites as edge servers and cloud servers, respectively, to provide multi-layer aggregation capabilities for HFL. The proposed system also considers the presence of inter-satellite links (ISLs), enabling satellites to exchange federated learning models with each other. Furthermore, we consider multiple different computational tasks that need to be completed within a limited satellite service time. To maximize the convergence performance of all tasks while ensuring fairness, we propose the use of the distributional soft-actor-critic (DSAC) algorithm to optimize resource allocation in the SAGIN and aggregation weights in HFL. Moreover, we address the efficiency issue of hybrid action spaces in deep reinforcement learning (DRL) through a decoupling and recoupling approach, and design a new dynamic adjusting reward function to ensure fairness among multiple tasks in federated learning. Simulation results demonstrate the superiority of our proposed algorithm, consistently outperforming baseline approaches and offering a promising solution for addressing highly complex optimization problems in SAGINs.","sentences":["The space-air-ground integrated network (SAGIN) has become a crucial research direction in future wireless communications due to its ubiquitous coverage, rapid and flexible deployment, and multi-layer cooperation capabilities.","However, integrating hierarchical federated learning (HFL) with edge computing and SAGINs remains a complex open issue to be resolved.","This paper proposes a novel framework for applying HFL in SAGINs, utilizing aerial platforms and low Earth orbit (LEO) satellites as edge servers and cloud servers, respectively, to provide multi-layer aggregation capabilities for HFL.","The proposed system also considers the presence of inter-satellite links (ISLs), enabling satellites to exchange federated learning models with each other.","Furthermore, we consider multiple different computational tasks that need to be completed within a limited satellite service time.","To maximize the convergence performance of all tasks while ensuring fairness, we propose the use of the distributional soft-actor-critic (DSAC) algorithm to optimize resource allocation in the SAGIN and aggregation weights in HFL.","Moreover, we address the efficiency issue of hybrid action spaces in deep reinforcement learning (DRL) through a decoupling and recoupling approach, and design a new dynamic adjusting reward function to ensure fairness among multiple tasks in federated learning.","Simulation results demonstrate the superiority of our proposed algorithm, consistently outperforming baseline approaches and offering a promising solution for addressing highly complex optimization problems in SAGINs."],"url":"http://arxiv.org/abs/2408.02501v1"}
{"created":"2024-08-05 14:21:00","title":"Flow with FlorDB: Incremental Context Maintenance for the Machine Learning Lifecycle","abstract":"The metadata involved in integrating code, data, configuration, and feedback into predictive models is varied and complex. This complexity is further compounded by the agile development practices favored by data scientists and machine learning engineers. These practices emphasize high experimentation velocity and frequent deployments, which can make it challenging to keep track of all the relevant metadata. The iterative nature of agile methods means that models, datasets, and configurations are constantly evolving. Each experiment might involve tweaks to the data preprocessing steps, changes in model hyperparameters, or updates to the deployment environment. The need for rapid iteration can lead to shortcuts or oversights in documentation and metadata management. Effective metadata management requires robust yet flexible tools and practices that can integrate and organize this information without slowing down the development process. Traditional context management often emphasizes a ``metadata first'' approach, which can introduce significant friction for developers. FlorDB reduces this friction through multiversion hindsight logging and incremental context maintenance, allowing developers to add and refine metadata after the fact. This ``metadata later'' approach enables a more flexible and incremental development process, allowing data scientists to focus on model creation and refinement without the burden of documentation upfront. As shown in a demo, FlorDB can be used to build AI/ML applications with integrated train-infer pipelines and managed feedback loops. Ultimately, the goal of FlorDB is to ensure that critical metadata is maintained accurately and efficiently, even in fast-paced agile workflows.","sentences":["The metadata involved in integrating code, data, configuration, and feedback into predictive models is varied and complex.","This complexity is further compounded by the agile development practices favored by data scientists and machine learning engineers.","These practices emphasize high experimentation velocity and frequent deployments, which can make it challenging to keep track of all the relevant metadata.","The iterative nature of agile methods means that models, datasets, and configurations are constantly evolving.","Each experiment might involve tweaks to the data preprocessing steps, changes in model hyperparameters, or updates to the deployment environment.","The need for rapid iteration can lead to shortcuts or oversights in documentation and metadata management.","Effective metadata management requires robust yet flexible tools and practices that can integrate and organize this information without slowing down the development process.","Traditional context management often emphasizes a ``metadata first'' approach, which can introduce significant friction for developers.","FlorDB reduces this friction through multiversion hindsight logging and incremental context maintenance, allowing developers to add and refine metadata after the fact.","This ``metadata later'' approach enables a more flexible and incremental development process, allowing data scientists to focus on model creation and refinement without the burden of documentation upfront.","As shown in a demo, FlorDB can be used to build AI/ML applications with integrated train-infer pipelines and managed feedback loops.","Ultimately, the goal of FlorDB is to ensure that critical metadata is maintained accurately and efficiently, even in fast-paced agile workflows."],"url":"http://arxiv.org/abs/2408.02498v1"}
{"created":"2024-08-05 14:18:29","title":"HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions","abstract":"Traditional deep learning models rely on methods such as softmax cross-entropy and ArcFace loss for tasks like classification and face recognition. These methods mainly explore angular features in a hyperspherical space, often resulting in entangled inter-class features due to dense angular data across many classes. In this paper, a new field of feature exploration is proposed known as HyperSpaceX which enhances class discrimination by exploring both angular and radial dimensions in multi-hyperspherical spaces, facilitated by a novel DistArc loss. The proposed DistArc loss encompasses three feature arrangement components: two angular and one radial, enforcing intra-class binding and inter-class separation in multi-radial arrangement, improving feature discriminability. Evaluation of HyperSpaceX framework for the novel representation utilizes a proposed predictive measure that accounts for both angular and radial elements, providing a more comprehensive assessment of model accuracy beyond standard metrics. Experiments across seven object classification and six face recognition datasets demonstrate state-of-the-art (SoTA) results obtained from HyperSpaceX, achieving up to a 20% performance improvement on large-scale object datasets in lower dimensions and up to 6% gain in higher dimensions.","sentences":["Traditional deep learning models rely on methods such as softmax cross-entropy and ArcFace loss for tasks like classification and face recognition.","These methods mainly explore angular features in a hyperspherical space, often resulting in entangled inter-class features due to dense angular data across many classes.","In this paper, a new field of feature exploration is proposed known as HyperSpaceX which enhances class discrimination by exploring both angular and radial dimensions in multi-hyperspherical spaces, facilitated by a novel DistArc loss.","The proposed DistArc loss encompasses three feature arrangement components: two angular and one radial, enforcing intra-class binding and inter-class separation in multi-radial arrangement, improving feature discriminability.","Evaluation of HyperSpaceX framework for the novel representation utilizes a proposed predictive measure that accounts for both angular and radial elements, providing a more comprehensive assessment of model accuracy beyond standard metrics.","Experiments across seven object classification and six face recognition datasets demonstrate state-of-the-art (SoTA) results obtained from HyperSpaceX, achieving up to a 20% performance improvement on large-scale object datasets in lower dimensions and up to 6% gain in higher dimensions."],"url":"http://arxiv.org/abs/2408.02494v1"}
{"created":"2024-08-05 14:09:30","title":"A First Look at License Compliance Capability of LLMs in Code Generation","abstract":"Recent advances in Large Language Models (LLMs) have revolutionized code generation, leading to widespread adoption of AI coding tools by developers. However, LLMs can generate license-protected code without providing the necessary license information, leading to potential intellectual property violations during software production. This paper addresses the critical, yet underexplored, issue of license compliance in LLM-generated code by establishing a benchmark to evaluate the ability of LLMs to provide accurate license information for their generated code. To establish this benchmark, we conduct an empirical study to identify a reasonable standard for \"striking similarity\" that excludes the possibility of independent creation, indicating a copy relationship between the LLM output and certain open-source code. Based on this standard, we propose an evaluation benchmark LiCoEval, to evaluate the license compliance capabilities of LLMs. Using LiCoEval, we evaluate 14 popular LLMs, finding that even top-performing LLMs produce a non-negligible proportion (0.88% to 2.01%) of code strikingly similar to existing open-source implementations. Notably, most LLMs fail to provide accurate license information, particularly for code under copyleft licenses. These findings underscore the urgent need to enhance LLM compliance capabilities in code generation tasks. Our study provides a foundation for future research and development to improve license compliance in AI-assisted software development, contributing to both the protection of open-source software copyrights and the mitigation of legal risks for LLM users.","sentences":["Recent advances in Large Language Models (LLMs) have revolutionized code generation, leading to widespread adoption of AI coding tools by developers.","However, LLMs can generate license-protected code without providing the necessary license information, leading to potential intellectual property violations during software production.","This paper addresses the critical, yet underexplored, issue of license compliance in LLM-generated code by establishing a benchmark to evaluate the ability of LLMs to provide accurate license information for their generated code.","To establish this benchmark, we conduct an empirical study to identify a reasonable standard for \"striking similarity\" that excludes the possibility of independent creation, indicating a copy relationship between the LLM output and certain open-source code.","Based on this standard, we propose an evaluation benchmark LiCoEval, to evaluate the license compliance capabilities of LLMs.","Using LiCoEval, we evaluate 14 popular LLMs, finding that even top-performing LLMs produce a non-negligible proportion (0.88% to 2.01%) of code strikingly similar to existing open-source implementations.","Notably, most LLMs fail to provide accurate license information, particularly for code under copyleft licenses.","These findings underscore the urgent need to enhance LLM compliance capabilities in code generation tasks.","Our study provides a foundation for future research and development to improve license compliance in AI-assisted software development, contributing to both the protection of open-source software copyrights and the mitigation of legal risks for LLM users."],"url":"http://arxiv.org/abs/2408.02487v1"}
{"created":"2024-08-05 14:05:25","title":"Exploring Conditional Multi-Modal Prompts for Zero-shot HOI Detection","abstract":"Zero-shot Human-Object Interaction (HOI) detection has emerged as a frontier topic due to its capability to detect HOIs beyond a predefined set of categories. This task entails not only identifying the interactiveness of human-object pairs and localizing them but also recognizing both seen and unseen interaction categories. In this paper, we introduce a novel framework for zero-shot HOI detection using Conditional Multi-Modal Prompts, namely CMMP. This approach enhances the generalization of large foundation models, such as CLIP, when fine-tuned for HOI detection. Unlike traditional prompt-learning methods, we propose learning decoupled vision and language prompts for interactiveness-aware visual feature extraction and generalizable interaction classification, respectively. Specifically, we integrate prior knowledge of different granularity into conditional vision prompts, including an input-conditioned instance prior and a global spatial pattern prior. The former encourages the image encoder to treat instances belonging to seen or potentially unseen HOI concepts equally while the latter provides representative plausible spatial configuration of the human and object under interaction. Besides, we employ language-aware prompt learning with a consistency constraint to preserve the knowledge of the large foundation model to enable better generalization in the text branch. Extensive experiments demonstrate the efficacy of our detector with conditional multi-modal prompts, outperforming previous state-of-the-art on unseen classes of various zero-shot settings. The code and models are available at \\url{https://github.com/ltttpku/CMMP}.","sentences":["Zero-shot Human-Object Interaction (HOI) detection has emerged as a frontier topic due to its capability to detect HOIs beyond a predefined set of categories.","This task entails not only identifying the interactiveness of human-object pairs and localizing them but also recognizing both seen and unseen interaction categories.","In this paper, we introduce a novel framework for zero-shot HOI detection using Conditional Multi-Modal Prompts, namely CMMP.","This approach enhances the generalization of large foundation models, such as CLIP, when fine-tuned for HOI detection.","Unlike traditional prompt-learning methods, we propose learning decoupled vision and language prompts for interactiveness-aware visual feature extraction and generalizable interaction classification, respectively.","Specifically, we integrate prior knowledge of different granularity into conditional vision prompts, including an input-conditioned instance prior and a global spatial pattern prior.","The former encourages the image encoder to treat instances belonging to seen or potentially unseen HOI concepts equally while the latter provides representative plausible spatial configuration of the human and object under interaction.","Besides, we employ language-aware prompt learning with a consistency constraint to preserve the knowledge of the large foundation model to enable better generalization in the text branch.","Extensive experiments demonstrate the efficacy of our detector with conditional multi-modal prompts, outperforming previous state-of-the-art on unseen classes of various zero-shot settings.","The code and models are available at \\url{https://github.com/ltttpku/CMMP}."],"url":"http://arxiv.org/abs/2408.02484v1"}
{"created":"2024-08-05 14:01:15","title":"From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future","abstract":"With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.","sentences":["With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering.","LLMs have achieved remarkable success in areas including code generation and vulnerability detection.","However, they also exhibit numerous limitations and shortcomings.","LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement.","Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents.","It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain.","In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering.","In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance.","We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics.","Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering.","We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research."],"url":"http://arxiv.org/abs/2408.02479v1"}
{"created":"2024-08-05 13:57:32","title":"Toward Attention-based TinyML: A Heterogeneous Accelerated Architecture and Automated Deployment Flow","abstract":"One of the challenges for Tiny Machine Learning (tinyML) is keeping up with the evolution of Machine Learning models from Convolutional Neural Networks to Transformers. We address this by leveraging a heterogeneous architectural template coupling RISC-V processors with hardwired accelerators supported by an automated deployment flow. We demonstrate an Attention-based model in a tinyML power envelope with an octa-core cluster coupled with an accelerator for quantized Attention. Our deployment flow enables an end-to-end 8-bit MobileBERT, achieving leading-edge energy efficiency and throughput of 2960 GOp/J and 154 GOp/s at 32.5 Inf/s consuming 52.0 mW (0.65 V, 22 nm FD-SOI technology).","sentences":["One of the challenges for Tiny Machine Learning (tinyML) is keeping up with the evolution of Machine Learning models from Convolutional Neural Networks to Transformers.","We address this by leveraging a heterogeneous architectural template coupling RISC-V processors with hardwired accelerators supported by an automated deployment flow.","We demonstrate an Attention-based model in a tinyML power envelope with an octa-core cluster coupled with an accelerator for quantized Attention.","Our deployment flow enables an end-to-end 8-bit MobileBERT, achieving leading-edge energy efficiency and throughput of 2960 GOp/J and 154 GOp/s at 32.5 Inf/s consuming 52.0 mW (0.65 V, 22 nm FD-SOI technology)."],"url":"http://arxiv.org/abs/2408.02473v1"}
