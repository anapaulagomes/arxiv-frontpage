{"created":"2024-08-26 17:59:03","title":"Advancing Humanoid Locomotion: Mastering Challenging Terrains with Denoising World Model Learning","abstract":"Humanoid robots, with their human-like skeletal structure, are especially suited for tasks in human-centric environments. However, this structure is accompanied by additional challenges in locomotion controller design, especially in complex real-world environments. As a result, existing humanoid robots are limited to relatively simple terrains, either with model-based control or model-free reinforcement learning. In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world's first humanoid robot to master real-world challenging terrains such as snowy and inclined land in the wild, up and down stairs, and extremely uneven terrains. All scenarios run the same learned neural network with zero-shot sim-to-real transfer, indicating the superior robustness and generalization capability of the proposed method.","sentences":["Humanoid robots, with their human-like skeletal structure, are especially suited for tasks in human-centric environments.","However, this structure is accompanied by additional challenges in locomotion controller design, especially in complex real-world environments.","As a result, existing humanoid robots are limited to relatively simple terrains, either with model-based control or model-free reinforcement learning.","In this work, we introduce Denoising World Model Learning (DWL), an end-to-end reinforcement learning framework for humanoid locomotion control, which demonstrates the world's first humanoid robot to master real-world challenging terrains such as snowy and inclined land in the wild, up and down stairs, and extremely uneven terrains.","All scenarios run the same learned neural network with zero-shot sim-to-real transfer, indicating the superior robustness and generalization capability of the proposed method."],"url":"http://arxiv.org/abs/2408.14472v1"}
{"created":"2024-08-26 17:59:01","title":"A Practitioner's Guide to Continual Multimodal Pretraining","abstract":"Multimodal foundation models serve numerous applications at the intersection of vision and language. Still, despite being pretrained on extensive data, they become outdated over time. To keep models updated, research into continual pretraining mainly explores scenarios with either (1) infrequent, indiscriminate updates on large-scale new data, or (2) frequent, sample-level updates. However, practical model deployment often operates in the gap between these two limit cases, as real-world applications often demand adaptation to specific subdomains, tasks or concepts -- spread over the entire, varying life cycle of a model. In this work, we complement current perspectives on continual pretraining through a research test bed as well as provide comprehensive guidance for effective continual model updates in such scenarios. We first introduce FoMo-in-Flux, a continual multimodal pretraining benchmark with realistic compute constraints and practical deployment requirements, constructed over 63 datasets with diverse visual and semantic coverage. Using FoMo-in-Flux, we explore the complex landscape of practical continual pretraining through multiple perspectives: (1) A data-centric investigation of data mixtures and stream orderings that emulate real-world deployment situations, (2) a method-centric investigation ranging from simple fine-tuning and traditional continual learning strategies to parameter-efficient updates and model merging, (3) meta learning rate schedules and mechanistic design choices, and (4) the influence of model and compute scaling. Together, our insights provide a practitioner's guide to continual multimodal pretraining for real-world deployment. Our benchmark and code is here: https://github.com/ExplainableML/fomo_in_flux.","sentences":["Multimodal foundation models serve numerous applications at the intersection of vision and language.","Still, despite being pretrained on extensive data, they become outdated over time.","To keep models updated, research into continual pretraining mainly explores scenarios with either (1) infrequent, indiscriminate updates on large-scale new data, or (2) frequent, sample-level updates.","However, practical model deployment often operates in the gap between these two limit cases, as real-world applications often demand adaptation to specific subdomains, tasks or concepts -- spread over the entire, varying life cycle of a model.","In this work, we complement current perspectives on continual pretraining through a research test bed as well as provide comprehensive guidance for effective continual model updates in such scenarios.","We first introduce FoMo-in-Flux, a continual multimodal pretraining benchmark with realistic compute constraints and practical deployment requirements, constructed over 63 datasets with diverse visual and semantic coverage.","Using FoMo-in-Flux, we explore the complex landscape of practical continual pretraining through multiple perspectives: (1) A data-centric investigation of data mixtures and stream orderings that emulate real-world deployment situations, (2) a method-centric investigation ranging from simple fine-tuning and traditional continual learning strategies to parameter-efficient updates and model merging, (3) meta learning rate schedules and mechanistic design choices, and (4) the influence of model and compute scaling.","Together, our insights provide a practitioner's guide to continual multimodal pretraining for real-world deployment.","Our benchmark and code is here: https://github.com/ExplainableML/fomo_in_flux."],"url":"http://arxiv.org/abs/2408.14471v1"}
{"created":"2024-08-26 17:58:53","title":"Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models","abstract":"Fine-tuning large language models (LLMs) on downstream tasks requires substantial computational resources. A class of parameter-efficient fine-tuning (PEFT) aims to mitigate these computational challenges by selectively fine-tuning only a small fraction of the model parameters. Although computationally efficient, these techniques often fail to match the performance of fully fine-tuned models, primarily due to inherent biases introduced during parameter selection. Traditional selective PEFT techniques use a fixed set of parameters based on a predefined budget (a process also known as unmasking), failing to capture parameter importance dynamically and often ending up exceeding the budget. We introduce $\\text{ID}^3$, a novel selective PEFT method that calculates parameter importance continually and dynamically unmasks parameters by balancing exploration and exploitation in parameter selection. Our empirical study on 15 tasks spanning natural language understanding and generative tasks demonstrates the effectiveness of our method compared to fixed-masking-based PEFT techniques. We analytically show that $\\text{ID}^3$ reduces the number of gradient updates by a factor of two, enhancing computational efficiency. $\\text{ID}^3$ is robust to random initialization of neurons and, therefore, can be seamlessly integrated into existing additive and reparametrization-based PEFT modules such as adapters and LoRA for dynamic sparsification.","sentences":["Fine-tuning large language models (LLMs) on downstream tasks requires substantial computational resources.","A class of parameter-efficient fine-tuning (PEFT) aims to mitigate these computational challenges by selectively fine-tuning only a small fraction of the model parameters.","Although computationally efficient, these techniques often fail to match the performance of fully fine-tuned models, primarily due to inherent biases introduced during parameter selection.","Traditional selective PEFT techniques use a fixed set of parameters based on a predefined budget (a process also known as unmasking), failing to capture parameter importance dynamically and often ending up exceeding the budget.","We introduce $\\text{ID}^3$, a novel selective PEFT method that calculates parameter importance continually and dynamically unmasks parameters by balancing exploration and exploitation in parameter selection.","Our empirical study on 15 tasks spanning natural language understanding and generative tasks demonstrates the effectiveness of our method compared to fixed-masking-based PEFT techniques.","We analytically show that $\\text{ID}^3$ reduces the number of gradient updates by a factor of two, enhancing computational efficiency.","$\\text{ID}^3$ is robust to random initialization of neurons and, therefore, can be seamlessly integrated into existing additive and reparametrization-based PEFT modules such as adapters and LoRA for dynamic sparsification."],"url":"http://arxiv.org/abs/2408.14470v1"}
{"created":"2024-08-26 17:58:47","title":"Grounded Multi-Hop VideoQA in Long-Form Egocentric Videos","abstract":"This paper considers the problem of Multi-Hop Video Question Answering (MH-VidQA) in long-form egocentric videos. This task not only requires to answer visual questions, but also to localize multiple relevant time intervals within the video as visual evidences. We develop an automated pipeline to create multi-hop question-answering pairs with associated temporal evidence, enabling to construct a large-scale dataset for instruction-tuning. To monitor the progress of this new task, we further curate a high-quality benchmark, MultiHop-EgoQA, with careful manual verification and refinement. Experimental results reveal that existing multi-modal systems exhibit inadequate multi-hop grounding and reasoning abilities, resulting in unsatisfactory performance. We then propose a novel architecture, termed as Grounding Scattered Evidence with Large Language Model (GeLM), that enhances multi-modal large language models (MLLMs) by incorporating a grounding module to retrieve temporal evidence from videos using flexible grounding tokens. Trained on our visual instruction data, GeLM demonstrates improved multi-hop grounding and reasoning capabilities, setting a new baseline for this challenging task. Furthermore, when trained on third-person view videos, the same architecture also achieves state-of-the-art performance on the single-hop VidQA benchmark, ActivityNet-RTL, demonstrating its effectiveness.","sentences":["This paper considers the problem of Multi-Hop Video Question Answering (MH-VidQA) in long-form egocentric videos.","This task not only requires to answer visual questions, but also to localize multiple relevant time intervals within the video as visual evidences.","We develop an automated pipeline to create multi-hop question-answering pairs with associated temporal evidence, enabling to construct a large-scale dataset for instruction-tuning.","To monitor the progress of this new task, we further curate a high-quality benchmark, MultiHop-EgoQA, with careful manual verification and refinement.","Experimental results reveal that existing multi-modal systems exhibit inadequate multi-hop grounding and reasoning abilities, resulting in unsatisfactory performance.","We then propose a novel architecture, termed as Grounding Scattered Evidence with Large Language Model (GeLM), that enhances multi-modal large language models (MLLMs) by incorporating a grounding module to retrieve temporal evidence from videos using flexible grounding tokens.","Trained on our visual instruction data, GeLM demonstrates improved multi-hop grounding and reasoning capabilities, setting a new baseline for this challenging task.","Furthermore, when trained on third-person view videos, the same architecture also achieves state-of-the-art performance on the single-hop VidQA benchmark, ActivityNet-RTL, demonstrating its effectiveness."],"url":"http://arxiv.org/abs/2408.14469v1"}
{"created":"2024-08-26 17:58:20","title":"K-Sort Arena: Efficient and Reliable Benchmarking for Generative Models via K-wise Human Preferences","abstract":"The rapid advancement of visual generative models necessitates efficient and reliable evaluation methods. Arena platform, which gathers user votes on model comparisons, can rank models with human preferences. However, traditional Arena methods, while established, require an excessive number of comparisons for ranking to converge and are vulnerable to preference noise in voting, suggesting the need for better approaches tailored to contemporary evaluation challenges. In this paper, we introduce K-Sort Arena, an efficient and reliable platform based on a key insight: images and videos possess higher perceptual intuitiveness than texts, enabling rapid evaluation of multiple samples simultaneously. Consequently, K-Sort Arena employs K-wise comparisons, allowing K models to engage in free-for-all competitions, which yield much richer information than pairwise comparisons. To enhance the robustness of the system, we leverage probabilistic modeling and Bayesian updating techniques. We propose an exploration-exploitation-based matchmaking strategy to facilitate more informative comparisons. In our experiments, K-Sort Arena exhibits 16.3x faster convergence compared to the widely used ELO algorithm. To further validate the superiority and obtain a comprehensive leaderboard, we collect human feedback via crowdsourced evaluations of numerous cutting-edge text-to-image and text-to-video models. Thanks to its high efficiency, K-Sort Arena can continuously incorporate emerging models and update the leaderboard with minimal votes. Our project has undergone several months of internal testing and is now available at https://huggingface.co/spaces/ksort/K-Sort-Arena","sentences":["The rapid advancement of visual generative models necessitates efficient and reliable evaluation methods.","Arena platform, which gathers user votes on model comparisons, can rank models with human preferences.","However, traditional Arena methods, while established, require an excessive number of comparisons for ranking to converge and are vulnerable to preference noise in voting, suggesting the need for better approaches tailored to contemporary evaluation challenges.","In this paper, we introduce K-Sort Arena, an efficient and reliable platform based on a key insight: images and videos possess higher perceptual intuitiveness than texts, enabling rapid evaluation of multiple samples simultaneously.","Consequently, K-Sort Arena employs K-wise comparisons, allowing K models to engage in free-for-all competitions, which yield much richer information than pairwise comparisons.","To enhance the robustness of the system, we leverage probabilistic modeling and Bayesian updating techniques.","We propose an exploration-exploitation-based matchmaking strategy to facilitate more informative comparisons.","In our experiments, K-Sort Arena exhibits 16.3x faster convergence compared to the widely used ELO algorithm.","To further validate the superiority and obtain a comprehensive leaderboard, we collect human feedback via crowdsourced evaluations of numerous cutting-edge text-to-image and text-to-video models.","Thanks to its high efficiency, K-Sort Arena can continuously incorporate emerging models and update the leaderboard with minimal votes.","Our project has undergone several months of internal testing and is now available at https://huggingface.co/spaces/ksort/K-Sort-Arena"],"url":"http://arxiv.org/abs/2408.14468v1"}
{"created":"2024-08-26 17:58:17","title":"Explicit Inductive Inference using Large Language Models","abstract":"Large Language Models (LLMs) are reported to hold undesirable attestation bias on inference tasks: when asked to predict if a premise P entails a hypothesis H, instead of considering H's conditional truthfulness entailed by P, LLMs tend to use the out-of-context truth label of H as a fragile proxy. In this paper, we propose a pipeline that exploits this bias to do explicit inductive inference. Our pipeline uses an LLM to transform a premise into a set of attested alternatives, and then aggregate answers of the derived new entailment inquiries to support the original inference prediction. On a directional predicate entailment benchmark, we demonstrate that by applying this simple pipeline, we can improve the overall performance of LLMs on inference and substantially alleviate the impact of their attestation bias.","sentences":["Large Language Models (LLMs) are reported to hold undesirable attestation bias on inference tasks: when asked to predict if a premise P entails a hypothesis H, instead of considering H's conditional truthfulness entailed by P, LLMs tend to use the out-of-context truth label of H as a fragile proxy.","In this paper, we propose a pipeline that exploits this bias to do explicit inductive inference.","Our pipeline uses an LLM to transform a premise into a set of attested alternatives, and then aggregate answers of the derived new entailment inquiries to support the original inference prediction.","On a directional predicate entailment benchmark, we demonstrate that by applying this simple pipeline, we can improve the overall performance of LLMs on inference and substantially alleviate the impact of their attestation bias."],"url":"http://arxiv.org/abs/2408.14467v1"}
{"created":"2024-08-26 17:50:47","title":"A domain decomposition-based autoregressive deep learning model for unsteady and nonlinear partial differential equations","abstract":"In this paper, we propose a domain-decomposition-based deep learning (DL) framework, named transient-CoMLSim, for accurately modeling unsteady and nonlinear partial differential equations (PDEs). The framework consists of two key components: (a) a convolutional neural network (CNN)-based autoencoder architecture and (b) an autoregressive model composed of fully connected layers. Unlike existing state-of-the-art methods that operate on the entire computational domain, our CNN-based autoencoder computes a lower-dimensional basis for solution and condition fields represented on subdomains. Timestepping is performed entirely in the latent space, generating embeddings of the solution variables from the time history of embeddings of solution and condition variables. This approach not only reduces computational complexity but also enhances scalability, making it well-suited for large-scale simulations. Furthermore, to improve the stability of our rollouts, we employ a curriculum learning (CL) approach during the training of the autoregressive model. The domain-decomposition strategy enables scaling to out-of-distribution domain sizes while maintaining the accuracy of predictions -- a feature not easily integrated into popular DL-based approaches for physics simulations. We benchmark our model against two widely-used DL architectures, Fourier Neural Operator (FNO) and U-Net, and demonstrate that our framework outperforms them in terms of accuracy, extrapolation to unseen timesteps, and stability for a wide range of use cases.","sentences":["In this paper, we propose a domain-decomposition-based deep learning (DL) framework, named transient-CoMLSim, for accurately modeling unsteady and nonlinear partial differential equations (PDEs).","The framework consists of two key components: (a) a convolutional neural network (CNN)-based autoencoder architecture and (b) an autoregressive model composed of fully connected layers.","Unlike existing state-of-the-art methods that operate on the entire computational domain, our CNN-based autoencoder computes a lower-dimensional basis for solution and condition fields represented on subdomains.","Timestepping is performed entirely in the latent space, generating embeddings of the solution variables from the time history of embeddings of solution and condition variables.","This approach not only reduces computational complexity but also enhances scalability, making it well-suited for large-scale simulations.","Furthermore, to improve the stability of our rollouts, we employ a curriculum learning (CL) approach during the training of the autoregressive model.","The domain-decomposition strategy enables scaling to out-of-distribution domain sizes while maintaining the accuracy of predictions -- a feature not easily integrated into popular DL-based approaches for physics simulations.","We benchmark our model against two widely-used DL architectures, Fourier Neural Operator (FNO) and U-Net, and demonstrate that our framework outperforms them in terms of accuracy, extrapolation to unseen timesteps, and stability for a wide range of use cases."],"url":"http://arxiv.org/abs/2408.14461v1"}
{"created":"2024-08-26 17:49:27","title":"Dense Center-Direction Regression for Object Counting and Localization with Point Supervision","abstract":"Object counting and localization problems are commonly addressed with point supervised learning, which allows the use of less labor-intensive point annotations. However, learning based on point annotations poses challenges due to the high imbalance between the sets of annotated and unannotated pixels, which is often treated with Gaussian smoothing of point annotations and focal loss. However, these approaches still focus on the pixels in the immediate vicinity of the point annotations and exploit the rest of the data only indirectly. In this work, we propose a novel approach termed CeDiRNet for point-supervised learning that uses a dense regression of directions pointing towards the nearest object centers, i.e. center-directions. This provides greater support for each center point arising from many surrounding pixels pointing towards the object center. We propose a formulation of center-directions that allows the problem to be split into the domain-specific dense regression of center-directions and the final localization task based on a small, lightweight, and domain-agnostic localization network that can be trained with synthetic data completely independent of the target domain. We demonstrate the performance of the proposed method on six different datasets for object counting and localization, and show that it outperforms the existing state-of-the-art methods. The code is accessible on GitHub at https://github.com/vicoslab/CeDiRNet.git.","sentences":["Object counting and localization problems are commonly addressed with point supervised learning, which allows the use of less labor-intensive point annotations.","However, learning based on point annotations poses challenges due to the high imbalance between the sets of annotated and unannotated pixels, which is often treated with Gaussian smoothing of point annotations and focal loss.","However, these approaches still focus on the pixels in the immediate vicinity of the point annotations and exploit the rest of the data only indirectly.","In this work, we propose a novel approach termed CeDiRNet for point-supervised learning that uses a dense regression of directions pointing towards the nearest object centers, i.e. center-directions.","This provides greater support for each center point arising from many surrounding pixels pointing towards the object center.","We propose a formulation of center-directions that allows the problem to be split into the domain-specific dense regression of center-directions and the final localization task based on a small, lightweight, and domain-agnostic localization network that can be trained with synthetic data completely independent of the target domain.","We demonstrate the performance of the proposed method on six different datasets for object counting and localization, and show that it outperforms the existing state-of-the-art methods.","The code is accessible on GitHub at https://github.com/vicoslab/CeDiRNet.git."],"url":"http://arxiv.org/abs/2408.14457v1"}
{"created":"2024-08-26 17:49:05","title":"Center Direction Network for Grasping Point Localization on Cloths","abstract":"Object grasping is a fundamental challenge in robotics and computer vision, critical for advancing robotic manipulation capabilities. Deformable objects, like fabrics and cloths, pose additional challenges due to their non-rigid nature. In this work, we introduce CeDiRNet-3DoF, a deep-learning model for grasp point detection, with a particular focus on cloth objects. CeDiRNet-3DoF employs center direction regression alongside a localization network, attaining first place in the perception task of ICRA 2023's Cloth Manipulation Challenge. Recognizing the lack of standardized benchmarks in the literature that hinder effective method comparison, we present the ViCoS Towel Dataset. This extensive benchmark dataset comprises 8,000 real and 12,000 synthetic images, serving as a robust resource for training and evaluating contemporary data-driven deep-learning approaches. Extensive evaluation revealed CeDiRNet-3DoF's robustness in real-world performance, outperforming state-of-the-art methods, including the latest transformer-based models. Our work bridges a crucial gap, offering a robust solution and benchmark for cloth grasping in computer vision and robotics. Code and dataset are available at: https://github.com/vicoslab/CeDiRNet-3DoF","sentences":["Object grasping is a fundamental challenge in robotics and computer vision, critical for advancing robotic manipulation capabilities.","Deformable objects, like fabrics and cloths, pose additional challenges due to their non-rigid nature.","In this work, we introduce CeDiRNet-3DoF, a deep-learning model for grasp point detection, with a particular focus on cloth objects.","CeDiRNet-3DoF employs center direction regression alongside a localization network, attaining first place in the perception task of ICRA 2023's Cloth Manipulation Challenge.","Recognizing the lack of standardized benchmarks in the literature that hinder effective method comparison, we present the ViCoS Towel Dataset.","This extensive benchmark dataset comprises 8,000 real and 12,000 synthetic images, serving as a robust resource for training and evaluating contemporary data-driven deep-learning approaches.","Extensive evaluation revealed CeDiRNet-3DoF's robustness in real-world performance, outperforming state-of-the-art methods, including the latest transformer-based models.","Our work bridges a crucial gap, offering a robust solution and benchmark for cloth grasping in computer vision and robotics.","Code and dataset are available at: https://github.com/vicoslab/CeDiRNet-3DoF"],"url":"http://arxiv.org/abs/2408.14456v1"}
{"created":"2024-08-26 17:48:42","title":"Reconstructing physiological signals from fMRI across the adult lifespan","abstract":"Interactions between the brain and body are of fundamental importance for human behavior and health. Functional magnetic resonance imaging (fMRI) captures whole-brain activity noninvasively, and modeling how fMRI signals interact with physiological dynamics of the body can provide new insight into brain function and offer potential biomarkers of disease. However, physiological recordings are not always possible to acquire since they require extra equipment and setup, and even when they are, the recorded physiological signals may contain substantial artifacts. To overcome this limitation, machine learning models have been proposed to directly extract features of respiratory and cardiac activity from resting-state fMRI signals. To date, such work has been carried out only in healthy young adults and in a pediatric population, leaving open questions about the efficacy of these approaches on older adults. Here, we propose a novel framework that leverages Transformer-based architectures for reconstructing two key physiological signals - low-frequency respiratory volume (RV) and heart rate (HR) fluctuations - from fMRI data, and test these models on a dataset of individuals aged 36-89 years old. Our framework outperforms previously proposed approaches (attaining median correlations between predicted and measured signals of r ~ .698 for RV and r ~ .618 for HR), indicating the potential of leveraging attention mechanisms to model fMRI-physiological signal relationships. We also evaluate several model training and fine-tuning strategies, and find that incorporating young-adult data during training improves the performance when predicting physiological signals in the aging cohort. Overall, our approach successfully infers key physiological variables directly from fMRI data from individuals across a wide range of the adult lifespan.","sentences":["Interactions between the brain and body are of fundamental importance for human behavior and health.","Functional magnetic resonance imaging (fMRI) captures whole-brain activity noninvasively, and modeling how fMRI signals interact with physiological dynamics of the body can provide new insight into brain function and offer potential biomarkers of disease.","However, physiological recordings are not always possible to acquire since they require extra equipment and setup, and even when they are, the recorded physiological signals may contain substantial artifacts.","To overcome this limitation, machine learning models have been proposed to directly extract features of respiratory and cardiac activity from resting-state fMRI signals.","To date, such work has been carried out only in healthy young adults and in a pediatric population, leaving open questions about the efficacy of these approaches on older adults.","Here, we propose a novel framework that leverages Transformer-based architectures for reconstructing two key physiological signals - low-frequency respiratory volume (RV) and heart rate (HR) fluctuations - from fMRI data, and test these models on a dataset of individuals aged 36-89 years old.","Our framework outperforms previously proposed approaches (attaining median correlations between predicted and measured signals of r ~ .698 for RV and r ~ .618 for HR), indicating the potential of leveraging attention mechanisms to model fMRI-physiological signal relationships.","We also evaluate several model training and fine-tuning strategies, and find that incorporating young-adult data during training improves the performance when predicting physiological signals in the aging cohort.","Overall, our approach successfully infers key physiological variables directly from fMRI data from individuals across a wide range of the adult lifespan."],"url":"http://arxiv.org/abs/2408.14453v1"}
{"created":"2024-08-26 17:44:30","title":"An optimization-based coupling of reduced order models with efficient reduced adjoint basis generation approach","abstract":"Optimization-based coupling (OBC) is an attractive alternative to traditional Lagrange multiplier approaches in multiple modeling and simulation contexts. However, application of OBC to time-dependent problem has been hindered by the computational costs of finding the stationary points of the associated Lagrangian, which requires primal and adjoint solves. This issue can be mitigated by using OBC in conjunction with computationally efficient reduced order models (ROM). To demonstrate the potential of this combination, in this paper we develop an optimization-based ROM-ROM coupling for a transient advection-diffusion transmission problem. The main challenge in this formulation is the generation of adjoint snapshots and reduced bases for the adjoint systems required by the optimizer. One of the main contributions of the paper is a new technique for efficient adjoint snapshot collection for gradient-based optimizers in the context of optimization-based ROM-ROM couplings. We present numerical studies demonstrating the accuracy of the approach along with comparison between various approaches for selecting a reduced order basis for the adjoint systems, including decay of snapshot energy, iteration counts, and timings.","sentences":["Optimization-based coupling (OBC) is an attractive alternative to traditional Lagrange multiplier approaches in multiple modeling and simulation contexts.","However, application of OBC to time-dependent problem has been hindered by the computational costs of finding the stationary points of the associated Lagrangian, which requires primal and adjoint solves.","This issue can be mitigated by using OBC in conjunction with computationally efficient reduced order models (ROM).","To demonstrate the potential of this combination, in this paper we develop an optimization-based ROM-ROM coupling for a transient advection-diffusion transmission problem.","The main challenge in this formulation is the generation of adjoint snapshots and reduced bases for the adjoint systems required by the optimizer.","One of the main contributions of the paper is a new technique for efficient adjoint snapshot collection for gradient-based optimizers in the context of optimization-based ROM-ROM couplings.","We present numerical studies demonstrating the accuracy of the approach along with comparison between various approaches for selecting a reduced order basis for the adjoint systems, including decay of snapshot energy, iteration counts, and timings."],"url":"http://arxiv.org/abs/2408.14450v1"}
{"created":"2024-08-26 17:36:51","title":"Symmetry & Critical Points","abstract":"Critical points of an invariant function may or may not be symmetric. We prove, however, that if a symmetric critical point exists, those adjacent to it are generically symmetry breaking. This mathematical mechanism is shown to carry important implications for our ability to efficiently minimize invariant nonconvex functions, in particular those associated with neural networks.","sentences":["Critical points of an invariant function may or may not be symmetric.","We prove, however, that if a symmetric critical point exists, those adjacent to it are generically symmetry breaking.","This mathematical mechanism is shown to carry important implications for our ability to efficiently minimize invariant nonconvex functions, in particular those associated with neural networks."],"url":"http://arxiv.org/abs/2408.14445v1"}
{"created":"2024-08-26 17:36:25","title":"Temporal Ensemble Logic","abstract":"We introduce Temporal Ensemble Logic (TEL), a monadic, first-order modal logic for linear-time temporal reasoning. TEL includes primitive temporal constructs such as ``always up to $t$ time later'' ($\\Box_t$), ``sometimes before $t$ time in the future'' ($\\Diamond_t$), and ``$t$-time later'' $\\varphi_t$. TEL has been motivated from the requirement for rigor and reproducibility for cohort specification and discovery in clinical and population health research, to fill a gap in formalizing temporal reasoning in biomedicine. In this paper, we first introduce TEL in a general set up, with discrete and dense time as special cases. We then focus on the theoretical development of discrete TEL on the temporal domain of positive integers $\\mathbb{N}^+$, denoted as ${\\rm TEL}_{\\mathbb{N}^+}$. ${\\rm TEL}_{\\mathbb{N}^+}$ is strictly more expressive than the standard monadic second order logic, characterized by B\\\"{u}chi automata. We present its formal semantics, a proof system, and provide a proof for the undecidability of the satisfiability of ${\\rm TEL}_{\\mathbb{N}^+}$. We also discuss expressiveness and decidability fragments for ${\\rm TEL}_{\\mathbb{N}^+}$, followed by illustrative applications.","sentences":["We introduce Temporal Ensemble Logic (TEL), a monadic, first-order modal logic for linear-time temporal reasoning.","TEL includes primitive temporal constructs such as ``always up to $t$ time later'' ($\\Box_t$), ``sometimes before $t$ time in the future'' ($\\Diamond_t$), and ``$t$-time later'' $\\varphi_t$. TEL has been motivated from the requirement for rigor and reproducibility for cohort specification and discovery in clinical and population health research, to fill a gap in formalizing temporal reasoning in biomedicine.","In this paper, we first introduce TEL in a general set up, with discrete and dense time as special cases.","We then focus on the theoretical development of discrete TEL on the temporal domain of positive integers $\\mathbb{N}^+$, denoted as ${\\rm TEL}_{\\mathbb{N}^+}$. ${\\rm TEL}_{\\mathbb{N}^+}$ is strictly more expressive than the standard monadic second order logic, characterized by B\\\"{u}chi automata.","We present its formal semantics, a proof system, and provide a proof for the undecidability of the satisfiability of ${\\rm","TEL}_{\\mathbb{N}^+}$.","We also discuss expressiveness and decidability fragments for ${\\rm TEL}_{\\mathbb{N}^+}$, followed by illustrative applications."],"url":"http://arxiv.org/abs/2408.14443v1"}
{"created":"2024-08-26 17:35:01","title":"Model Parallel Training and Transfer Learning for Convolutional Neural Networks by Domain Decomposition","abstract":"Deep convolutional neural networks (CNNs) have been shown to be very successful in a wide range of image processing applications. However, due to their increasing number of model parameters and an increasing availability of large amounts of training data, parallelization strategies to efficiently train complex CNNs are necessary. In previous work by the authors, a novel model parallel CNN architecture was proposed which is loosely inspired by domain decomposition. In particular, the novel network architecture is based on a decomposition of the input data into smaller subimages. For each of these subimages, local CNNs with a proportionally smaller number of parameters are trained in parallel and the resulting local classifications are then aggregated in a second step by a dense feedforward neural network (DNN). In the present work, we compare the resulting CNN-DNN architecture to less costly alternatives to combine the local classifications into a final, global decision. Additionally, we investigate the performance of the CNN-DNN trained as one coherent model as well as using a transfer learning strategy, where the parameters of the pre-trained local CNNs are used as initial values for a subsequently trained global coherent CNN-DNN model.","sentences":["Deep convolutional neural networks (CNNs) have been shown to be very successful in a wide range of image processing applications.","However, due to their increasing number of model parameters and an increasing availability of large amounts of training data, parallelization strategies to efficiently train complex CNNs are necessary.","In previous work by the authors, a novel model parallel CNN architecture was proposed which is loosely inspired by domain decomposition.","In particular, the novel network architecture is based on a decomposition of the input data into smaller subimages.","For each of these subimages, local CNNs with a proportionally smaller number of parameters are trained in parallel and the resulting local classifications are then aggregated in a second step by a dense feedforward neural network (DNN).","In the present work, we compare the resulting CNN-DNN architecture to less costly alternatives to combine the local classifications into a final, global decision.","Additionally, we investigate the performance of the CNN-DNN trained as one coherent model as well as using a transfer learning strategy, where the parameters of the pre-trained local CNNs are used as initial values for a subsequently trained global coherent CNN-DNN model."],"url":"http://arxiv.org/abs/2408.14442v1"}
{"created":"2024-08-26 17:33:47","title":"Attend-Fusion: Efficient Audio-Visual Fusion for Video Classification","abstract":"Exploiting both audio and visual modalities for video classification is a challenging task, as the existing methods require large model architectures, leading to high computational complexity and resource requirements. Smaller architectures, on the other hand, struggle to achieve optimal performance. In this paper, we propose Attend-Fusion, an audio-visual (AV) fusion approach that introduces a compact model architecture specifically designed to capture intricate audio-visual relationships in video data. Through extensive experiments on the challenging YouTube-8M dataset, we demonstrate that Attend-Fusion achieves an F1 score of 75.64\\% with only 72M parameters, which is comparable to the performance of larger baseline models such as Fully-Connected Late Fusion (75.96\\% F1 score, 341M parameters). Attend-Fusion achieves similar performance to the larger baseline model while reducing the model size by nearly 80\\%, highlighting its efficiency in terms of model complexity. Our work demonstrates that the Attend-Fusion model effectively combines audio and visual information for video classification, achieving competitive performance with significantly reduced model size. This approach opens new possibilities for deploying high-performance video understanding systems in resource-constrained environments across various applications.","sentences":["Exploiting both audio and visual modalities for video classification is a challenging task, as the existing methods require large model architectures, leading to high computational complexity and resource requirements.","Smaller architectures, on the other hand, struggle to achieve optimal performance.","In this paper, we propose Attend-Fusion, an audio-visual (AV) fusion approach that introduces a compact model architecture specifically designed to capture intricate audio-visual relationships in video data.","Through extensive experiments on the challenging YouTube-8M dataset, we demonstrate that Attend-Fusion achieves an F1 score of 75.64\\% with only 72M parameters, which is comparable to the performance of larger baseline models such as Fully-Connected Late Fusion (75.96\\% F1 score, 341M parameters).","Attend-Fusion achieves similar performance to the larger baseline model while reducing the model size by nearly 80\\%, highlighting its efficiency in terms of model complexity.","Our work demonstrates that the Attend-Fusion model effectively combines audio and visual information for video classification, achieving competitive performance with significantly reduced model size.","This approach opens new possibilities for deploying high-performance video understanding systems in resource-constrained environments across various applications."],"url":"http://arxiv.org/abs/2408.14441v1"}
{"created":"2024-08-26 17:25:16","title":"Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study","abstract":"The advent of large language models such as ChatGPT, Gemini, and others has underscored the importance of evaluating their diverse capabilities, ranging from natural language understanding to code generation. However, their performance on spatial tasks has not been comprehensively assessed. This study addresses this gap by introducing a novel multi-task spatial evaluation dataset, designed to systematically explore and compare the performance of several advanced models on spatial tasks. The dataset encompasses twelve distinct task types, including spatial understanding and path planning, each with verified, accurate answers. We evaluated multiple models, including OpenAI's gpt-3.5-turbo, gpt-4o, and ZhipuAI's glm-4, through a two-phase testing approach. Initially, we conducted zero-shot testing, followed by categorizing the dataset by difficulty and performing prompt tuning tests. Results indicate that gpt-4o achieved the highest overall accuracy in the first phase, with an average of 71.3%. Although moonshot-v1-8k slightly underperformed overall, it surpassed gpt-4o in place name recognition tasks. The study also highlights the impact of prompt strategies on model performance in specific tasks. For example, the Chain-of-Thought (COT) strategy increased gpt-4o's accuracy in path planning from 12.4% to 87.5%, while a one-shot strategy enhanced moonshot-v1-8k's accuracy in mapping tasks from 10.1% to 76.3%.","sentences":["The advent of large language models such as ChatGPT, Gemini, and others has underscored the importance of evaluating their diverse capabilities, ranging from natural language understanding to code generation.","However, their performance on spatial tasks has not been comprehensively assessed.","This study addresses this gap by introducing a novel multi-task spatial evaluation dataset, designed to systematically explore and compare the performance of several advanced models on spatial tasks.","The dataset encompasses twelve distinct task types, including spatial understanding and path planning, each with verified, accurate answers.","We evaluated multiple models, including OpenAI's gpt-3.5-turbo, gpt-4o, and ZhipuAI's glm-4, through a two-phase testing approach.","Initially, we conducted zero-shot testing, followed by categorizing the dataset by difficulty and performing prompt tuning tests.","Results indicate that gpt-4o achieved the highest overall accuracy in the first phase, with an average of 71.3%.","Although moonshot-v1-8k slightly underperformed overall, it surpassed gpt-4o in place name recognition tasks.","The study also highlights the impact of prompt strategies on model performance in specific tasks.","For example, the Chain-of-Thought (COT) strategy increased gpt-4o's accuracy in path planning from 12.4% to 87.5%, while a one-shot strategy enhanced moonshot-v1-8k's accuracy in mapping tasks from 10.1% to 76.3%."],"url":"http://arxiv.org/abs/2408.14438v1"}
{"created":"2024-08-26 17:22:11","title":"Sparsity-Aware Hardware-Software Co-Design of Spiking Neural Networks: An Overview","abstract":"Spiking Neural Networks (SNNs) are inspired by the sparse and event-driven nature of biological neural processing, and offer the potential for ultra-low-power artificial intelligence. However, realizing their efficiency benefits requires specialized hardware and a co-design approach that effectively leverages sparsity. We explore the hardware-software co-design of sparse SNNs, examining how sparsity representation, hardware architectures, and training techniques influence hardware efficiency. We analyze the impact of static and dynamic sparsity, discuss the implications of different neuron models and encoding schemes, and investigate the need for adaptability in hardware designs. Our work aims to illuminate the path towards embedded neuromorphic systems that fully exploit the computational advantages of sparse SNNs.","sentences":["Spiking Neural Networks (SNNs) are inspired by the sparse and event-driven nature of biological neural processing, and offer the potential for ultra-low-power artificial intelligence.","However, realizing their efficiency benefits requires specialized hardware and a co-design approach that effectively leverages sparsity.","We explore the hardware-software co-design of sparse SNNs, examining how sparsity representation, hardware architectures, and training techniques influence hardware efficiency.","We analyze the impact of static and dynamic sparsity, discuss the implications of different neuron models and encoding schemes, and investigate the need for adaptability in hardware designs.","Our work aims to illuminate the path towards embedded neuromorphic systems that fully exploit the computational advantages of sparse SNNs."],"url":"http://arxiv.org/abs/2408.14437v1"}
{"created":"2024-08-26 17:21:54","title":"Social perception of faces in a vision-language model","abstract":"We explore social perception of human faces in CLIP, a widely used open-source vision-language model. To this end, we compare the similarity in CLIP embeddings between different textual prompts and a set of face images. Our textual prompts are constructed from well-validated social psychology terms denoting social perception. The face images are synthetic and are systematically and independently varied along six dimensions: the legally protected attributes of age, gender, and race, as well as facial expression, lighting, and pose. Independently and systematically manipulating face attributes allows us to study the effect of each on social perception and avoids confounds that can occur in wild-collected data due to uncontrolled systematic correlations between attributes. Thus, our findings are experimental rather than observational. Our main findings are three. First, while CLIP is trained on the widest variety of images and texts, it is able to make fine-grained human-like social judgments on face images. Second, age, gender, and race do systematically impact CLIP's social perception of faces, suggesting an undesirable bias in CLIP vis-a-vis legally protected attributes. Most strikingly, we find a strong pattern of bias concerning the faces of Black women, where CLIP produces extreme values of social perception across different ages and facial expressions. Third, facial expression impacts social perception more than age and lighting as much as age. The last finding predicts that studies that do not control for unprotected visual attributes may reach the wrong conclusions on bias. Our novel method of investigation, which is founded on the social psychology literature and on the experiments involving the manipulation of individual attributes, yields sharper and more reliable observations than previous observational methods and may be applied to study biases in any vision-language model.","sentences":["We explore social perception of human faces in CLIP, a widely used open-source vision-language model.","To this end, we compare the similarity in CLIP embeddings between different textual prompts and a set of face images.","Our textual prompts are constructed from well-validated social psychology terms denoting social perception.","The face images are synthetic and are systematically and independently varied along six dimensions: the legally protected attributes of age, gender, and race, as well as facial expression, lighting, and pose.","Independently and systematically manipulating face attributes allows us to study the effect of each on social perception and avoids confounds that can occur in wild-collected data due to uncontrolled systematic correlations between attributes.","Thus, our findings are experimental rather than observational.","Our main findings are three.","First, while CLIP is trained on the widest variety of images and texts, it is able to make fine-grained human-like social judgments on face images.","Second, age, gender, and race do systematically impact CLIP's social perception of faces, suggesting an undesirable bias in CLIP vis-a-vis legally protected attributes.","Most strikingly, we find a strong pattern of bias concerning the faces of Black women, where CLIP produces extreme values of social perception across different ages and facial expressions.","Third, facial expression impacts social perception more than age and lighting as much as age.","The last finding predicts that studies that do not control for unprotected visual attributes may reach the wrong conclusions on bias.","Our novel method of investigation, which is founded on the social psychology literature and on the experiments involving the manipulation of individual attributes, yields sharper and more reliable observations than previous observational methods and may be applied to study biases in any vision-language model."],"url":"http://arxiv.org/abs/2408.14435v1"}
{"created":"2024-08-26 17:21:54","title":"STAR-RIS-Aided Cell-Free Massive MIMO with Imperfect Hardware","abstract":"This paper considers a simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-aided cell-free massive multiple-input multiple-output (CF-mMIMO) system, accounting for imperfect hardware in spatially correlated fading channels. Specifically, we consider the hardware impairments and phase noise at transceivers, as well as the phase shift errors generated within the STAR-RIS. We commence by introducing the STAR-RIS signal model, channel model, and imperfect hardware components. Then, the linear minimum mean-square error (MMSE) channel estimate is derived with pilot contamination, which provides sufficient information for sequential data processing. Moreover, a channel capacity lower bound is derived in the case of a finite number of RIS elements and access points (APs), while a closed-form expression for the downlink ergodic spectral efficiency (SE) for maximum ratio (MR) precoding is also deduced, where only the channel statistics are used. Our numerical results demonstrate that the STAR-RIS-aided CF-mMIMO system achieves higher SE compared to the conventional CF-mMIMO system, even with imperfect hardware.","sentences":["This paper considers a simultaneously transmitting and reflecting reconfigurable intelligent surface (STAR-RIS)-aided cell-free massive multiple-input multiple-output (CF-mMIMO) system, accounting for imperfect hardware in spatially correlated fading channels.","Specifically, we consider the hardware impairments and phase noise at transceivers, as well as the phase shift errors generated within the STAR-RIS.","We commence by introducing the STAR-RIS signal model, channel model, and imperfect hardware components.","Then, the linear minimum mean-square error (MMSE) channel estimate is derived with pilot contamination, which provides sufficient information for sequential data processing.","Moreover, a channel capacity lower bound is derived in the case of a finite number of RIS elements and access points (APs), while a closed-form expression for the downlink ergodic spectral efficiency (SE) for maximum ratio (MR) precoding is also deduced, where only the channel statistics are used.","Our numerical results demonstrate that the STAR-RIS-aided CF-mMIMO system achieves higher SE compared to the conventional CF-mMIMO system, even with imperfect hardware."],"url":"http://arxiv.org/abs/2408.14436v1"}
{"created":"2024-08-26 17:21:19","title":"Employing Artificial Intelligence to Steer Exascale Workflows with Colmena","abstract":"Computational workflows are a common class of application on supercomputers, yet the loosely coupled and heterogeneous nature of workflows often fails to take full advantage of their capabilities. We created Colmena to leverage the massive parallelism of a supercomputer by using Artificial Intelligence (AI) to learn from and adapt a workflow as it executes. Colmena allows scientists to define how their application should respond to events (e.g., task completion) as a series of cooperative agents. In this paper, we describe the design of Colmena, the challenges we overcame while deploying applications on exascale systems, and the science workflows we have enhanced through interweaving AI. The scaling challenges we discuss include developing steering strategies that maximize node utilization, introducing data fabrics that reduce communication overhead of data-intensive tasks, and implementing workflow tasks that cache costly operations between invocations. These innovations coupled with a variety of application patterns accessible through our agent-based steering model have enabled science advances in chemistry, biophysics, and materials science using different types of AI. Our vision is that Colmena will spur creative solutions that harness AI across many domains of scientific computing.","sentences":["Computational workflows are a common class of application on supercomputers, yet the loosely coupled and heterogeneous nature of workflows often fails to take full advantage of their capabilities.","We created Colmena to leverage the massive parallelism of a supercomputer by using Artificial Intelligence (AI) to learn from and adapt a workflow as it executes.","Colmena allows scientists to define how their application should respond to events (e.g., task completion) as a series of cooperative agents.","In this paper, we describe the design of Colmena, the challenges we overcame while deploying applications on exascale systems, and the science workflows we have enhanced through interweaving AI.","The scaling challenges we discuss include developing steering strategies that maximize node utilization, introducing data fabrics that reduce communication overhead of data-intensive tasks, and implementing workflow tasks that cache costly operations between invocations.","These innovations coupled with a variety of application patterns accessible through our agent-based steering model have enabled science advances in chemistry, biophysics, and materials science using different types of AI.","Our vision is that Colmena will spur creative solutions that harness AI across many domains of scientific computing."],"url":"http://arxiv.org/abs/2408.14434v1"}
{"created":"2024-08-26 17:20:34","title":"Contextual Bandit with Herding Effects: Algorithms and Recommendation Applications","abstract":"Contextual bandits serve as a fundamental algorithmic framework for optimizing recommendation decisions online. Though extensive attention has been paid to tailoring contextual bandits for recommendation applications, the \"herding effects\" in user feedback have been ignored. These herding effects bias user feedback toward historical ratings, breaking down the assumption of unbiased feedback inherent in contextual bandits. This paper develops a novel variant of the contextual bandit that is tailored to address the feedback bias caused by the herding effects. A user feedback model is formulated to capture this feedback bias. We design the TS-Conf (Thompson Sampling under Conformity) algorithm, which employs posterior sampling to balance the exploration and exploitation tradeoff. We prove an upper bound for the regret of the algorithm, revealing the impact of herding effects on learning speed. Extensive experiments on datasets demonstrate that TS-Conf outperforms four benchmark algorithms. Analysis reveals that TS-Conf effectively mitigates the negative impact of herding effects, resulting in faster learning and improved recommendation accuracy.","sentences":["Contextual bandits serve as a fundamental algorithmic framework for optimizing recommendation decisions online.","Though extensive attention has been paid to tailoring contextual bandits for recommendation applications, the \"herding effects\" in user feedback have been ignored.","These herding effects bias user feedback toward historical ratings, breaking down the assumption of unbiased feedback inherent in contextual bandits.","This paper develops a novel variant of the contextual bandit that is tailored to address the feedback bias caused by the herding effects.","A user feedback model is formulated to capture this feedback bias.","We design the TS-Conf (Thompson Sampling under Conformity) algorithm, which employs posterior sampling to balance the exploration and exploitation tradeoff.","We prove an upper bound for the regret of the algorithm, revealing the impact of herding effects on learning speed.","Extensive experiments on datasets demonstrate that TS-Conf outperforms four benchmark algorithms.","Analysis reveals that TS-Conf effectively mitigates the negative impact of herding effects, resulting in faster learning and improved recommendation accuracy."],"url":"http://arxiv.org/abs/2408.14432v1"}
{"created":"2024-08-26 17:18:38","title":"Towards Better Comprehension of Breaking Changes in the NPM Ecosystem","abstract":"Breaking changes cause a lot of effort to both downstream and upstream developers: downstream developers need to adapt to breaking changes and upstream developers are responsible for identifying and documenting them. In the NPM ecosystem, characterized by frequent code changes and a high tolerance for making breaking changes, the effort is larger.   For better comprehension of breaking changes in the NPM ecosystem and to enhance breaking change detection tools, we conduct a large-scale empirical study to investigate breaking changes in the NPM ecosystem. We construct a dataset of explicitly documented breaking changes from 381 popular NPM projects. We find that 93.6% of the detected breaking changes can be covered by developers' documentation, and about 19% of the breaking changes cannot be detected by regression testing. Then in the process of investigating source code of our collected breaking changes, we yield a taxonomy of JavaScript and TypeScript-specific syntactic breaking changes and a taxonomy of major types of behavioral breaking changes. Additionally, we investigate the reasons why developers make breaking changes in NPM and find three major reasons, i.e., to reduce code redundancy, to improve identifier name, and to improve API design, and each category contains several sub-items.   We provide actionable implications for future research, e.g., automatic naming and renaming techniques should be applied in JavaScript projects to improve identifier names, future research can try to detect more types of behavioral breaking changes. By presenting the implications, we also discuss the weakness of automatic renaming and BC detection approaches.","sentences":["Breaking changes cause a lot of effort to both downstream and upstream developers: downstream developers need to adapt to breaking changes and upstream developers are responsible for identifying and documenting them.","In the NPM ecosystem, characterized by frequent code changes and a high tolerance for making breaking changes, the effort is larger.   ","For better comprehension of breaking changes in the NPM ecosystem and to enhance breaking change detection tools, we conduct a large-scale empirical study to investigate breaking changes in the NPM ecosystem.","We construct a dataset of explicitly documented breaking changes from 381 popular NPM projects.","We find that 93.6% of the detected breaking changes can be covered by developers' documentation, and about 19% of the breaking changes cannot be detected by regression testing.","Then in the process of investigating source code of our collected breaking changes, we yield a taxonomy of JavaScript and TypeScript-specific syntactic breaking changes and a taxonomy of major types of behavioral breaking changes.","Additionally, we investigate the reasons why developers make breaking changes in NPM and find three major reasons, i.e., to reduce code redundancy, to improve identifier name, and to improve API design, and each category contains several sub-items.   ","We provide actionable implications for future research, e.g., automatic naming and renaming techniques should be applied in JavaScript projects to improve identifier names, future research can try to detect more types of behavioral breaking changes.","By presenting the implications, we also discuss the weakness of automatic renaming and BC detection approaches."],"url":"http://arxiv.org/abs/2408.14431v1"}
{"created":"2024-08-26 17:15:37","title":"Few-Shot 3D Volumetric Segmentation with Multi-Surrogate Fusion","abstract":"Conventional 3D medical image segmentation methods typically require learning heavy 3D networks (e.g., 3D-UNet), as well as large amounts of in-domain data with accurate pixel/voxel-level labels to avoid overfitting. These solutions are thus extremely time- and labor-expensive, but also may easily fail to generalize to unseen objects during training. To alleviate this issue, we present MSFSeg, a novel few-shot 3D segmentation framework with a lightweight multi-surrogate fusion (MSF). MSFSeg is able to automatically segment unseen 3D objects/organs (during training) provided with one or a few annotated 2D slices or 3D sequence segments, via learning dense query-support organ/lesion anatomy correlations across patient populations. Our proposed MSF module mines comprehensive and diversified morphology correlations between unlabeled and the few labeled slices/sequences through multiple designated surrogates, making it able to generate accurate cross-domain 3D segmentation masks given annotated slices or sequences. We demonstrate the effectiveness of our proposed framework by showing superior performance on conventional few-shot segmentation benchmarks compared to prior art, and remarkable cross-domain cross-volume segmentation performance on proprietary 3D segmentation datasets for challenging entities, i.e., tubular structures, with only limited 2D or 3D labels.","sentences":["Conventional 3D medical image segmentation methods typically require learning heavy 3D networks (e.g., 3D-UNet), as well as large amounts of in-domain data with accurate pixel/voxel-level labels to avoid overfitting.","These solutions are thus extremely time- and labor-expensive, but also may easily fail to generalize to unseen objects during training.","To alleviate this issue, we present MSFSeg, a novel few-shot 3D segmentation framework with a lightweight multi-surrogate fusion (MSF).","MSFSeg is able to automatically segment unseen 3D objects/organs (during training) provided with one or a few annotated 2D slices or 3D sequence segments, via learning dense query-support organ/lesion anatomy correlations across patient populations.","Our proposed MSF module mines comprehensive and diversified morphology correlations between unlabeled and the few labeled slices/sequences through multiple designated surrogates, making it able to generate accurate cross-domain 3D segmentation masks given annotated slices or sequences.","We demonstrate the effectiveness of our proposed framework by showing superior performance on conventional few-shot segmentation benchmarks compared to prior art, and remarkable cross-domain cross-volume segmentation performance on proprietary 3D segmentation datasets for challenging entities, i.e., tubular structures, with only limited 2D or 3D labels."],"url":"http://arxiv.org/abs/2408.14427v1"}
{"created":"2024-08-26 17:04:52","title":"Evaluating saliency scores in point clouds of natural environments by learning surface anomalies","abstract":"In recent years, three-dimensional point clouds are used increasingly to document natural environments. Each dataset contains a diverse set of objects, at varying shapes and sizes, distributed throughout the data and intricately intertwined with the topography. Therefore, regions of interest are difficult to find and consequent analyses become a challenge. Inspired from visual perception principles, we propose to differentiate objects of interest from the cluttered environment by evaluating how much they stand out from their surroundings, i.e., their geometric salience. Previous saliency detection approaches suggested mostly handcrafted attributes for the task. However, such methods fail when the data are too noisy or have high levels of texture. Here we propose a learning-based mechanism that accommodates noise and textured surfaces. We assume that within the natural environment any change from the prevalent surface would suggest a salient object. Thus, we first learn the underlying surface and then search for anomalies within it. Initially, a deep neural network is trained to reconstruct the surface. Regions where the reconstructed part deviates significantly from the original point cloud yield a substantial reconstruction error, signifying an anomaly, i.e., saliency. We demonstrate the effectiveness of the proposed approach by searching for salient features in various natural scenarios, which were acquired by different acquisition platforms. We show the strong correlation between the reconstruction error and salient objects.","sentences":["In recent years, three-dimensional point clouds are used increasingly to document natural environments.","Each dataset contains a diverse set of objects, at varying shapes and sizes, distributed throughout the data and intricately intertwined with the topography.","Therefore, regions of interest are difficult to find and consequent analyses become a challenge.","Inspired from visual perception principles, we propose to differentiate objects of interest from the cluttered environment by evaluating how much they stand out from their surroundings, i.e., their geometric salience.","Previous saliency detection approaches suggested mostly handcrafted attributes for the task.","However, such methods fail when the data are too noisy or have high levels of texture.","Here we propose a learning-based mechanism that accommodates noise and textured surfaces.","We assume that within the natural environment any change from the prevalent surface would suggest a salient object.","Thus, we first learn the underlying surface and then search for anomalies within it.","Initially, a deep neural network is trained to reconstruct the surface.","Regions where the reconstructed part deviates significantly from the original point cloud yield a substantial reconstruction error, signifying an anomaly, i.e., saliency.","We demonstrate the effectiveness of the proposed approach by searching for salient features in various natural scenarios, which were acquired by different acquisition platforms.","We show the strong correlation between the reconstruction error and salient objects."],"url":"http://arxiv.org/abs/2408.14421v1"}
{"created":"2024-08-26 17:04:23","title":"CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models","abstract":"We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large language models. CHARTOM consists of specially designed data visualizing charts. Given a chart, a language model needs to not only correctly comprehend the chart (the FACT question) but also judge if the chart will be misleading to a human reader (the MIND question). Both questions have significant societal benefits. We detail the construction of the CHARTOM benchmark including its calibration on human performance.","sentences":["We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large language models.","CHARTOM consists of specially designed data visualizing charts.","Given a chart, a language model needs to not only correctly comprehend the chart (the FACT question) but also judge if the chart will be misleading to a human reader (the MIND question).","Both questions have significant societal benefits.","We detail the construction of the CHARTOM benchmark including its calibration on human performance."],"url":"http://arxiv.org/abs/2408.14419v1"}
{"created":"2024-08-26 17:04:00","title":"MEDSAGE: Enhancing Robustness of Medical Dialogue Summarization to ASR Errors with LLM-generated Synthetic Dialogues","abstract":"Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech into text, yet the errors they introduce can significantly degrade the performance of downstream tasks like summarization. This issue is particularly pronounced in clinical dialogue summarization, a low-resource domain where supervised data for fine-tuning is scarce, necessitating the use of ASR models as black-box solutions. Employing conventional data augmentation for enhancing the noise robustness of summarization models is not feasible either due to the unavailability of sufficient medical dialogue audio recordings and corresponding ASR transcripts. To address this challenge, we propose MEDSAGE, an approach for generating synthetic samples for data augmentation using Large Language Models (LLMs). Specifically, we leverage the in-context learning capabilities of LLMs and instruct them to generate ASR-like errors based on a few available medical dialogue examples with audio recordings. Experimental results show that LLMs can effectively model ASR noise, and incorporating this noisy data into the training process significantly improves the robustness and accuracy of medical dialogue summarization systems. This approach addresses the challenges of noisy ASR outputs in critical applications, offering a robust solution to enhance the reliability of clinical dialogue summarization.","sentences":["Automatic Speech Recognition (ASR) systems are pivotal in transcribing speech into text, yet the errors they introduce can significantly degrade the performance of downstream tasks like summarization.","This issue is particularly pronounced in clinical dialogue summarization, a low-resource domain where supervised data for fine-tuning is scarce, necessitating the use of ASR models as black-box solutions.","Employing conventional data augmentation for enhancing the noise robustness of summarization models is not feasible either due to the unavailability of sufficient medical dialogue audio recordings and corresponding ASR transcripts.","To address this challenge, we propose MEDSAGE, an approach for generating synthetic samples for data augmentation using Large Language Models (LLMs).","Specifically, we leverage the in-context learning capabilities of LLMs and instruct them to generate ASR-like errors based on a few available medical dialogue examples with audio recordings.","Experimental results show that LLMs can effectively model ASR noise, and incorporating this noisy data into the training process significantly improves the robustness and accuracy of medical dialogue summarization systems.","This approach addresses the challenges of noisy ASR outputs in critical applications, offering a robust solution to enhance the reliability of clinical dialogue summarization."],"url":"http://arxiv.org/abs/2408.14418v1"}
{"created":"2024-08-26 17:03:14","title":"Hyperdimensional Computing Empowered Federated Foundation Model over Wireless Networks for Metaverse","abstract":"The Metaverse, a burgeoning collective virtual space merging augmented reality and persistent virtual worlds, necessitates advanced artificial intelligence (AI) and communication technologies to support immersive and interactive experiences. Federated learning (FL) has emerged as a promising technique for collaboratively training AI models while preserving data privacy. However, FL faces challenges such as high communication overhead and substantial computational demands, particularly for neural network (NN) models. To address these issues, we propose an integrated federated split learning and hyperdimensional computing (FSL-HDC) framework for emerging foundation models. This novel approach reduces communication costs, computation load, and privacy risks, making it particularly suitable for resource-constrained edge devices in the Metaverse, ensuring real-time responsive interactions. Additionally, we introduce an optimization algorithm that concurrently optimizes transmission power and bandwidth to minimize the maximum transmission time among all users to the server. The simulation results based on the MNIST dataset indicate that FSL-HDC achieves an accuracy rate of approximately 87.5%, which is slightly lower than that of FL-HDC. However, FSL-HDC exhibits a significantly faster convergence speed, approximately 3.733x that of FSL-NN, and demonstrates robustness to non-IID data distributions. Moreover, our proposed optimization algorithm can reduce the maximum transmission time by up to 64% compared with the baseline.","sentences":["The Metaverse, a burgeoning collective virtual space merging augmented reality and persistent virtual worlds, necessitates advanced artificial intelligence (AI) and communication technologies to support immersive and interactive experiences.","Federated learning (FL) has emerged as a promising technique for collaboratively training AI models while preserving data privacy.","However, FL faces challenges such as high communication overhead and substantial computational demands, particularly for neural network (NN) models.","To address these issues, we propose an integrated federated split learning and hyperdimensional computing (FSL-HDC) framework for emerging foundation models.","This novel approach reduces communication costs, computation load, and privacy risks, making it particularly suitable for resource-constrained edge devices in the Metaverse, ensuring real-time responsive interactions.","Additionally, we introduce an optimization algorithm that concurrently optimizes transmission power and bandwidth to minimize the maximum transmission time among all users to the server.","The simulation results based on the MNIST dataset indicate that FSL-HDC achieves an accuracy rate of approximately 87.5%, which is slightly lower than that of FL-HDC.","However, FSL-HDC exhibits a significantly faster convergence speed, approximately 3.733x that of FSL-NN, and demonstrates robustness to non-IID data distributions.","Moreover, our proposed optimization algorithm can reduce the maximum transmission time by up to 64% compared with the baseline."],"url":"http://arxiv.org/abs/2408.14416v1"}
{"created":"2024-08-26 17:02:25","title":"LoG-VMamba: Local-Global Vision Mamba for Medical Image Segmentation","abstract":"Mamba, a State Space Model (SSM), has recently shown competitive performance to Convolutional Neural Networks (CNNs) and Transformers in Natural Language Processing and general sequence modeling. Various attempts have been made to adapt Mamba to Computer Vision tasks, including medical image segmentation (MIS). Vision Mamba (VM)-based networks are particularly attractive due to their ability to achieve global receptive fields, similar to Vision Transformers, while also maintaining linear complexity in the number of tokens. However, the existing VM models still struggle to maintain both spatially local and global dependencies of tokens in high dimensional arrays due to their sequential nature. Employing multiple and/or complicated scanning strategies is computationally costly, which hinders applications of SSMs to high-dimensional 2D and 3D images that are common in MIS problems. In this work, we propose Local-Global Vision Mamba, LoG-VMamba, that explicitly enforces spatially adjacent tokens to remain nearby on the channel axis, and retains the global context in a compressed form. Our method allows the SSMs to access the local and global contexts even before reaching the last token while requiring only a simple scanning strategy. Our segmentation models are computationally efficient and substantially outperform both CNN and Transformers-based baselines on a diverse set of 2D and 3D MIS tasks. The implementation of LoG-VMamba is available at \\url{https://github.com/Oulu-IMEDS/LoG-VMamba}.","sentences":["Mamba, a State Space Model (SSM), has recently shown competitive performance to Convolutional Neural Networks (CNNs) and Transformers in Natural Language Processing and general sequence modeling.","Various attempts have been made to adapt Mamba to Computer Vision tasks, including medical image segmentation (MIS).","Vision Mamba (VM)-based networks are particularly attractive due to their ability to achieve global receptive fields, similar to Vision Transformers, while also maintaining linear complexity in the number of tokens.","However, the existing VM models still struggle to maintain both spatially local and global dependencies of tokens in high dimensional arrays due to their sequential nature.","Employing multiple and/or complicated scanning strategies is computationally costly, which hinders applications of SSMs to high-dimensional 2D and 3D images that are common in MIS problems.","In this work, we propose Local-Global Vision Mamba, LoG-VMamba, that explicitly enforces spatially adjacent tokens to remain nearby on the channel axis, and retains the global context in a compressed form.","Our method allows the SSMs to access the local and global contexts even before reaching the last token while requiring only a simple scanning strategy.","Our segmentation models are computationally efficient and substantially outperform both CNN and Transformers-based baselines on a diverse set of 2D and 3D MIS tasks.","The implementation of LoG-VMamba is available at \\url{https://github.com/Oulu-IMEDS/LoG-VMamba}."],"url":"http://arxiv.org/abs/2408.14415v1"}
{"created":"2024-08-26 16:48:50","title":"Fully Dynamic Shortest Paths in Sparse Digraphs","abstract":"We study the exact fully dynamic shortest paths problem. For real-weighted directed graphs, we show a deterministic fully dynamic data structure with $\\tilde{O}(mn^{4/5})$ worst-case update time processing arbitrary $s,t$-distance queries in $\\tilde{O}(n^{4/5})$ time. This constitutes the first non-trivial update/query tradeoff for this problem in the regime of sparse weighted directed graphs.","sentences":["We study the exact fully dynamic shortest paths problem.","For real-weighted directed graphs, we show a deterministic fully dynamic data structure with $\\tilde{O}(mn^{4/5})$ worst-case update time processing arbitrary $s,t$-distance queries in $\\tilde{O}(n^{4/5})$ time.","This constitutes the first non-trivial update/query tradeoff for this problem in the regime of sparse weighted directed graphs."],"url":"http://arxiv.org/abs/2408.14406v1"}
{"created":"2024-08-26 16:34:13","title":"Satellite Sunroof: High-res Digital Surface Models and Roof Segmentation for Global Solar Mapping","abstract":"The transition to renewable energy, particularly solar, is key to mitigating climate change. Google's Solar API aids this transition by estimating solar potential from aerial imagery, but its impact is constrained by geographical coverage. This paper proposes expanding the API's reach using satellite imagery, enabling global solar potential assessment. We tackle challenges involved in building a Digital Surface Model (DSM) and roof instance segmentation from lower resolution and single oblique views using deep learning models. Our models, trained on aligned satellite and aerial datasets, produce 25cm DSMs and roof segments. With ~1m DSM MAE on buildings, ~5deg roof pitch error and ~56% IOU on roof segmentation, they significantly enhance the Solar API's potential to promote solar adoption.","sentences":["The transition to renewable energy, particularly solar, is key to mitigating climate change.","Google's Solar API aids this transition by estimating solar potential from aerial imagery, but its impact is constrained by geographical coverage.","This paper proposes expanding the API's reach using satellite imagery, enabling global solar potential assessment.","We tackle challenges involved in building a Digital Surface Model (DSM) and roof instance segmentation from lower resolution and single oblique views using deep learning models.","Our models, trained on aligned satellite and aerial datasets, produce 25cm DSMs and roof segments.","With ~1m DSM MAE on buildings, ~5deg roof pitch error and ~56% IOU on roof segmentation, they significantly enhance the Solar API's potential to promote solar adoption."],"url":"http://arxiv.org/abs/2408.14400v1"}
{"created":"2024-08-26 16:29:13","title":"Language-specific Calibration for Pruning Multilingual Language Models","abstract":"Recent advances in large language model (LLM) pruning have shown state-of-the-art compression results in post-training and retraining-free settings while maintaining high predictive performance. However, such research mainly considers calibrating pruning using English text, despite the multilingual nature of modern LLMs and their frequent uses in non-English languages. In this paper, we set out to explore effective strategies for calibrating the pruning of multilingual language models. We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse tasks, models, and state-of-the-art pruning techniques. Our results present practical suggestions, for example, calibrating in the target language can efficiently yield lower perplexity, but does not necessarily benefit downstream tasks. Our further analysis experiments unveil that calibration in the target language mainly contributes to preserving language-specific features related to fluency and coherence, but might not contribute to capturing language-agnostic features such as language understanding and reasoning. Last, we provide practical recommendations for future practitioners.","sentences":["Recent advances in large language model (LLM) pruning have shown state-of-the-art compression results in post-training and retraining-free settings while maintaining high predictive performance.","However, such research mainly considers calibrating pruning using English text, despite the multilingual nature of modern LLMs and their frequent uses in non-English languages.","In this paper, we set out to explore effective strategies for calibrating the pruning of multilingual language models.","We present the first comprehensive empirical study, comparing different calibration languages for pruning multilingual models across diverse tasks, models, and state-of-the-art pruning techniques.","Our results present practical suggestions, for example, calibrating in the target language can efficiently yield lower perplexity, but does not necessarily benefit downstream tasks.","Our further analysis experiments unveil that calibration in the target language mainly contributes to preserving language-specific features related to fluency and coherence, but might not contribute to capturing language-agnostic features such as language understanding and reasoning.","Last, we provide practical recommendations for future practitioners."],"url":"http://arxiv.org/abs/2408.14398v1"}
{"created":"2024-08-26 16:28:56","title":"Uncovering Knowledge Gaps in Radiology Report Generation Models through Knowledge Graphs","abstract":"Recent advancements in artificial intelligence have significantly improved the automatic generation of radiology reports. However, existing evaluation methods fail to reveal the models' understanding of radiological images and their capacity to achieve human-level granularity in descriptions. To bridge this gap, we introduce a system, named ReXKG, which extracts structured information from processed reports to construct a comprehensive radiology knowledge graph. We then propose three metrics to evaluate the similarity of nodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs (ReXKG-SCS) across various knowledge graphs. We conduct an in-depth comparative analysis of AI-generated and human-written radiology reports, assessing the performance of both specialist and generalist models. Our study provides a deeper understanding of the capabilities and limitations of current AI models in radiology report generation, offering valuable insights for improving model performance and clinical applicability.","sentences":["Recent advancements in artificial intelligence have significantly improved the automatic generation of radiology reports.","However, existing evaluation methods fail to reveal the models' understanding of radiological images and their capacity to achieve human-level granularity in descriptions.","To bridge this gap, we introduce a system, named ReXKG, which extracts structured information from processed reports to construct a comprehensive radiology knowledge graph.","We then propose three metrics to evaluate the similarity of nodes (ReXKG-NSC), distribution of edges (ReXKG-AMS), and coverage of subgraphs (ReXKG-SCS) across various knowledge graphs.","We conduct an in-depth comparative analysis of AI-generated and human-written radiology reports, assessing the performance of both specialist and generalist models.","Our study provides a deeper understanding of the capabilities and limitations of current AI models in radiology report generation, offering valuable insights for improving model performance and clinical applicability."],"url":"http://arxiv.org/abs/2408.14397v1"}
{"created":"2024-08-26 16:21:50","title":"CURE4Rec: A Benchmark for Recommendation Unlearning with Deeper Influence","abstract":"With increasing privacy concerns in artificial intelligence, regulations have mandated the right to be forgotten, granting individuals the right to withdraw their data from models. Machine unlearning has emerged as a potential solution to enable selective forgetting in models, particularly in recommender systems where historical data contains sensitive user information. Despite recent advances in recommendation unlearning, evaluating unlearning methods comprehensively remains challenging due to the absence of a unified evaluation framework and overlooked aspects of deeper influence, e.g., fairness. To address these gaps, we propose CURE4Rec, the first comprehensive benchmark for recommendation unlearning evaluation. CURE4Rec covers four aspects, i.e., unlearning Completeness, recommendation Utility, unleaRning efficiency, and recommendation fairnEss, under three data selection strategies, i.e., core data, edge data, and random data. Specifically, we consider the deeper influence of unlearning on recommendation fairness and robustness towards data with varying impact levels. We construct multiple datasets with CURE4Rec evaluation and conduct extensive experiments on existing recommendation unlearning methods. Our code is released at https://github.com/xiye7lai/CURE4Rec.","sentences":["With increasing privacy concerns in artificial intelligence, regulations have mandated the right to be forgotten, granting individuals the right to withdraw their data from models.","Machine unlearning has emerged as a potential solution to enable selective forgetting in models, particularly in recommender systems where historical data contains sensitive user information.","Despite recent advances in recommendation unlearning, evaluating unlearning methods comprehensively remains challenging due to the absence of a unified evaluation framework and overlooked aspects of deeper influence, e.g., fairness.","To address these gaps, we propose CURE4Rec, the first comprehensive benchmark for recommendation unlearning evaluation.","CURE4Rec covers four aspects, i.e., unlearning Completeness, recommendation Utility, unleaRning efficiency, and recommendation fairnEss, under three data selection strategies, i.e., core data, edge data, and random data.","Specifically, we consider the deeper influence of unlearning on recommendation fairness and robustness towards data with varying impact levels.","We construct multiple datasets with CURE4Rec evaluation and conduct extensive experiments on existing recommendation unlearning methods.","Our code is released at https://github.com/xiye7lai/CURE4Rec."],"url":"http://arxiv.org/abs/2408.14393v1"}
{"created":"2024-08-26 16:11:53","title":"Reprogramming Foundational Large Language Models(LLMs) for Enterprise Adoption for Spatio-Temporal Forecasting Applications: Unveiling a New Era in Copilot-Guided Cross-Modal Time Series Representation Learning","abstract":"Spatio-temporal forecasting plays a crucial role in various sectors such as transportation systems, logistics, and supply chain management. However, existing methods are limited by their ability to handle large, complex datasets. To overcome this limitation, we introduce a hybrid approach that combines the strengths of open-source large and small-scale language models (LLMs and LMs) with traditional forecasting methods. We augment traditional methods with dynamic prompting and a grouped-query, multi-head attention mechanism to more effectively capture both intra-series and inter-series dependencies in evolving nonlinear time series data. In addition, we facilitate on-premises customization by fine-tuning smaller open-source LMs for time series trend analysis utilizing descriptions generated by open-source large LMs on consumer-grade hardware using Low-Rank Adaptation with Activation Memory Reduction (LoRA-AMR) technique to reduce computational overhead and activation storage memory demands while preserving inference latency. We combine language model processing for time series trend analysis with traditional time series representation learning method for cross-modal integration, achieving robust and accurate forecasts. The framework effectiveness is demonstrated through extensive experiments on various real-world datasets, outperforming existing methods by significant margins in terms of forecast accuracy.","sentences":["Spatio-temporal forecasting plays a crucial role in various sectors such as transportation systems, logistics, and supply chain management.","However, existing methods are limited by their ability to handle large, complex datasets.","To overcome this limitation, we introduce a hybrid approach that combines the strengths of open-source large and small-scale language models (LLMs and LMs) with traditional forecasting methods.","We augment traditional methods with dynamic prompting and a grouped-query, multi-head attention mechanism to more effectively capture both intra-series and inter-series dependencies in evolving nonlinear time series data.","In addition, we facilitate on-premises customization by fine-tuning smaller open-source LMs for time series trend analysis utilizing descriptions generated by open-source large LMs on consumer-grade hardware using Low-Rank Adaptation with Activation Memory Reduction (LoRA-AMR) technique to reduce computational overhead and activation storage memory demands while preserving inference latency.","We combine language model processing for time series trend analysis with traditional time series representation learning method for cross-modal integration, achieving robust and accurate forecasts.","The framework effectiveness is demonstrated through extensive experiments on various real-world datasets, outperforming existing methods by significant margins in terms of forecast accuracy."],"url":"http://arxiv.org/abs/2408.14387v1"}
{"created":"2024-08-26 16:04:13","title":"Learning Tree-Structured Composition of Data Augmentation","abstract":"Data augmentation is widely used for training a neural network given little labeled data. A common practice of augmentation training is applying a composition of multiple transformations sequentially to the data. Existing augmentation methods such as RandAugment randomly sample from a list of pre-selected transformations, while methods such as AutoAugment apply advanced search to optimize over an augmentation set of size $k^d$, which is the number of transformation sequences of length $d$, given a list of $k$ transformations.   In this paper, we design efficient algorithms whose running time complexity is much faster than the worst-case complexity of $O(k^d)$, provably. We propose a new algorithm to search for a binary tree-structured composition of $k$ transformations, where each tree node corresponds to one transformation. The binary tree generalizes sequential augmentations, such as the SimCLR augmentation scheme for contrastive learning. Using a top-down, recursive search procedure, our algorithm achieves a runtime complexity of $O(2^d k)$, which is much faster than $O(k^d)$ as $k$ increases above $2$. We apply our algorithm to tackle data distributions with heterogeneous subpopulations by searching for one tree in each subpopulation and then learning a weighted combination, resulting in a forest of trees.   We validate our proposed algorithms on numerous graph and image datasets, including a multi-label graph classification dataset we collected. The dataset exhibits significant variations in the sizes of graphs and their average degrees, making it ideal for studying data augmentation. We show that our approach can reduce the computation cost by 43% over existing search methods while improving performance by 4.3%. The tree structures can be used to interpret the relative importance of each transformation, such as identifying the important transformations on small vs. large graphs.","sentences":["Data augmentation is widely used for training a neural network given little labeled data.","A common practice of augmentation training is applying a composition of multiple transformations sequentially to the data.","Existing augmentation methods such as RandAugment randomly sample from a list of pre-selected transformations, while methods such as AutoAugment apply advanced search to optimize over an augmentation set of size $k^d$, which is the number of transformation sequences of length $d$, given a list of $k$ transformations.   ","In this paper, we design efficient algorithms whose running time complexity is much faster than the worst-case complexity of $O(k^d)$, provably.","We propose a new algorithm to search for a binary tree-structured composition of $k$ transformations, where each tree node corresponds to one transformation.","The binary tree generalizes sequential augmentations, such as the SimCLR augmentation scheme for contrastive learning.","Using a top-down, recursive search procedure, our algorithm achieves a runtime complexity of $O(2^d k)$, which is much faster than $O(k^d)$ as $k$ increases above $2$. We apply our algorithm to tackle data distributions with heterogeneous subpopulations by searching for one tree in each subpopulation and then learning a weighted combination, resulting in a forest of trees.   ","We validate our proposed algorithms on numerous graph and image datasets, including a multi-label graph classification dataset we collected.","The dataset exhibits significant variations in the sizes of graphs and their average degrees, making it ideal for studying data augmentation.","We show that our approach can reduce the computation cost by 43% over existing search methods while improving performance by 4.3%.","The tree structures can be used to interpret the relative importance of each transformation, such as identifying the important transformations on small vs. large graphs."],"url":"http://arxiv.org/abs/2408.14381v1"}
{"created":"2024-08-26 16:00:41","title":"Probing Causality Manipulation of Large Language Models","abstract":"Large language models (LLMs) have shown various ability on natural language processing, including problems about causality. It is not intuitive for LLMs to command causality, since pretrained models usually work on statistical associations, and do not focus on causes and effects in sentences. So that probing internal manipulation of causality is necessary for LLMs. This paper proposes a novel approach to probe causality manipulation hierarchically, by providing different shortcuts to models and observe behaviors. We exploit retrieval augmented generation (RAG) and in-context learning (ICL) for models on a designed causality classification task. We conduct experiments on mainstream LLMs, including GPT-4 and some smaller and domain-specific models. Our results suggest that LLMs can detect entities related to causality and recognize direct causal relationships. However, LLMs lack specialized cognition for causality, merely treating them as part of the global semantic of the sentence.","sentences":["Large language models (LLMs) have shown various ability on natural language processing, including problems about causality.","It is not intuitive for LLMs to command causality, since pretrained models usually work on statistical associations, and do not focus on causes and effects in sentences.","So that probing internal manipulation of causality is necessary for LLMs.","This paper proposes a novel approach to probe causality manipulation hierarchically, by providing different shortcuts to models and observe behaviors.","We exploit retrieval augmented generation (RAG) and in-context learning (ICL) for models on a designed causality classification task.","We conduct experiments on mainstream LLMs, including GPT-4 and some smaller and domain-specific models.","Our results suggest that LLMs can detect entities related to causality and recognize direct causal relationships.","However, LLMs lack specialized cognition for causality, merely treating them as part of the global semantic of the sentence."],"url":"http://arxiv.org/abs/2408.14380v1"}
{"created":"2024-08-26 16:00:40","title":"Synergistic and Efficient Edge-Host Communication for Energy Harvesting Wireless Sensor Networks","abstract":"There is an increasing demand for intelligent processing on ultra-low-power internet of things (IoT) device. Recent works have shown substantial efficiency boosts by executing inferences directly on the IoT device (node) rather than transmitting data. However, the computation and power demands of Deep Neural Network (DNN)-based inference pose significant challenges in an energy-harvesting wireless sensor network (EH-WSN). Moreover, these tasks often require responses from multiple physically distributed EH sensor nodes, which impose crucial system optimization challenges in addition to per-node constraints. To address these challenges, we propose Seeker, a hardware-software co-design approach for increasing on-sensor computation, reducing communication volume, and maximizing inference completion, without violating the quality of service, in EH-WSNs coordinated by a mobile device. Seeker uses a store-and-execute approach to complete a subset of inferences on the EH sensor node, reducing communication with the mobile host. Further, for those inferences unfinished because of the harvested energy constraints, it leverages task-aware coreset construction to efficiently communicate compact features to the host device. We evaluate Seeker for human activity recognition, as well as predictive maintenance and show ~8.9x reduction in communication data volume with 86.8% accuracy, surpassing the 81.2% accuracy of the state-of-the-art.","sentences":["There is an increasing demand for intelligent processing on ultra-low-power internet of things (IoT) device.","Recent works have shown substantial efficiency boosts by executing inferences directly on the IoT device (node) rather than transmitting data.","However, the computation and power demands of Deep Neural Network (DNN)-based inference pose significant challenges in an energy-harvesting wireless sensor network (EH-WSN).","Moreover, these tasks often require responses from multiple physically distributed EH sensor nodes, which impose crucial system optimization challenges in addition to per-node constraints.","To address these challenges, we propose Seeker, a hardware-software co-design approach for increasing on-sensor computation, reducing communication volume, and maximizing inference completion, without violating the quality of service, in EH-WSNs coordinated by a mobile device.","Seeker uses a store-and-execute approach to complete a subset of inferences on the EH sensor node, reducing communication with the mobile host.","Further, for those inferences unfinished because of the harvested energy constraints, it leverages task-aware coreset construction to efficiently communicate compact features to the host device.","We evaluate Seeker for human activity recognition, as well as predictive maintenance and show ~8.9x reduction in communication data volume with 86.8% accuracy, surpassing the 81.2% accuracy of the state-of-the-art."],"url":"http://arxiv.org/abs/2408.14379v1"}
{"created":"2024-08-26 16:00:05","title":"User-Access Point Association for High Density MIMO Wireless LANs","abstract":"Wireless local area network (WLAN) access points (APs) are being deployed in high density to improve coverage and throughput. The emerging multiple-input multiple-output (MIMO) implementation for uplink (UL) transmissions promises high per-user throughput and improved aggregate network throughput. However, the high throughput potential of dense UL-MIMO WLAN is impaired by multiple access channel interference and high contention among densely distributed user stations (STAs). We investigate the problem of actualizing the throughput potential of UL-MIMO in high density WLANs via user-AP association. Since user-AP association influences interference and STA contention, a method to optimally distribute STAs among APs is proposed to maximize aggregate users' throughput utility. This problem is transformed into a graph matching problem with the throughput utility function as the graph edge weights. The graph matching problem is solved as a combinatorial problem using a modified classical Kuhn-Munkres algorithm. A dynamic implementation of the proposed algorithm is used to periodically update user-AP associations when there are changes in the network due to new entrants and/or user mobility. Simulated dense UL-MIMO WLAN scenarios reveal that the proposed scheme achieves an average of $36.9 \\%$, $33.5 \\%$, $20.4 \\%$ and $11.3 \\%$ gains over the default strongest signal first (SSF) association scheme used in conventional WLAN, Greedy [14], SmartAssoc [13] and best performance first (BPF) [5] algorithms, respectively.","sentences":["Wireless local area network (WLAN) access points (APs) are being deployed in high density to improve coverage and throughput.","The emerging multiple-input multiple-output (MIMO) implementation for uplink (UL) transmissions promises high per-user throughput and improved aggregate network throughput.","However, the high throughput potential of dense UL-MIMO WLAN is impaired by multiple access channel interference and high contention among densely distributed user stations (STAs).","We investigate the problem of actualizing the throughput potential of UL-MIMO in high density WLANs via user-AP association.","Since user-AP association influences interference and STA contention, a method to optimally distribute STAs among APs is proposed to maximize aggregate users' throughput utility.","This problem is transformed into a graph matching problem with the throughput utility function as the graph edge weights.","The graph matching problem is solved as a combinatorial problem using a modified classical Kuhn-Munkres algorithm.","A dynamic implementation of the proposed algorithm is used to periodically update user-AP associations when there are changes in the network due to new entrants and/or user mobility.","Simulated dense UL-MIMO WLAN scenarios reveal that the proposed scheme achieves an average of $36.9 \\%$, $33.5 \\%$, $20.4 \\%$ and $11.3 \\%$ gains over the default strongest signal first (SSF) association scheme used in conventional WLAN, Greedy","[14],","SmartAssoc [13] and best performance first (BPF)","[5] algorithms, respectively."],"url":"http://arxiv.org/abs/2408.14378v1"}
{"created":"2024-08-26 15:53:50","title":"SelEx: Self-Expertise in Fine-Grained Generalized Category Discovery","abstract":"In this paper, we address Generalized Category Discovery, aiming to simultaneously uncover novel categories and accurately classify known ones. Traditional methods, which lean heavily on self-supervision and contrastive learning, often fall short when distinguishing between fine-grained categories. To address this, we introduce a novel concept called `self-expertise', which enhances the model's ability to recognize subtle differences and uncover unknown categories. Our approach combines unsupervised and supervised self-expertise strategies to refine the model's discernment and generalization. Initially, hierarchical pseudo-labeling is used to provide `soft supervision', improving the effectiveness of self-expertise. Our supervised technique differs from traditional methods by utilizing more abstract positive and negative samples, aiding in the formation of clusters that can generalize to novel categories. Meanwhile, our unsupervised strategy encourages the model to sharpen its category distinctions by considering within-category examples as `hard' negatives. Supported by theoretical insights, our empirical results showcase that our method outperforms existing state-of-the-art techniques in Generalized Category Discovery across several fine-grained datasets. Our code is available at: https://github.com/SarahRastegar/SelEx.","sentences":["In this paper, we address Generalized Category Discovery, aiming to simultaneously uncover novel categories and accurately classify known ones.","Traditional methods, which lean heavily on self-supervision and contrastive learning, often fall short when distinguishing between fine-grained categories.","To address this, we introduce a novel concept called `self-expertise', which enhances the model's ability to recognize subtle differences and uncover unknown categories.","Our approach combines unsupervised and supervised self-expertise strategies to refine the model's discernment and generalization.","Initially, hierarchical pseudo-labeling is used to provide `soft supervision', improving the effectiveness of self-expertise.","Our supervised technique differs from traditional methods by utilizing more abstract positive and negative samples, aiding in the formation of clusters that can generalize to novel categories.","Meanwhile, our unsupervised strategy encourages the model to sharpen its category distinctions by considering within-category examples as `hard' negatives.","Supported by theoretical insights, our empirical results showcase that our method outperforms existing state-of-the-art techniques in Generalized Category Discovery across several fine-grained datasets.","Our code is available at: https://github.com/SarahRastegar/SelEx."],"url":"http://arxiv.org/abs/2408.14371v1"}
{"created":"2024-08-26 15:49:31","title":"Exploiting Conjugate Label Information for Multi-Instance Partial-Label Learning","abstract":"Multi-instance partial-label learning (MIPL) addresses scenarios where each training sample is represented as a multi-instance bag associated with a candidate label set containing one true label and several false positives. Existing MIPL algorithms have primarily focused on mapping multi-instance bags to candidate label sets for disambiguation, disregarding the intrinsic properties of the label space and the supervised information provided by non-candidate label sets. In this paper, we propose an algorithm named ELIMIPL, i.e., Exploiting conjugate Label Information for Multi-Instance Partial-Label learning, which exploits the conjugate label information to improve the disambiguation performance. To achieve this, we extract the label information embedded in both candidate and non-candidate label sets, incorporating the intrinsic properties of the label space. Experimental results obtained from benchmark and real-world datasets demonstrate the superiority of the proposed ELIMIPL over existing MIPL algorithms and other well-established partial-label learning algorithms.","sentences":["Multi-instance partial-label learning (MIPL) addresses scenarios where each training sample is represented as a multi-instance bag associated with a candidate label set containing one true label and several false positives.","Existing MIPL algorithms have primarily focused on mapping multi-instance bags to candidate label sets for disambiguation, disregarding the intrinsic properties of the label space and the supervised information provided by non-candidate label sets.","In this paper, we propose an algorithm named ELIMIPL, i.e., Exploiting conjugate Label Information for Multi-Instance Partial-Label learning, which exploits the conjugate label information to improve the disambiguation performance.","To achieve this, we extract the label information embedded in both candidate and non-candidate label sets, incorporating the intrinsic properties of the label space.","Experimental results obtained from benchmark and real-world datasets demonstrate the superiority of the proposed ELIMIPL over existing MIPL algorithms and other well-established partial-label learning algorithms."],"url":"http://arxiv.org/abs/2408.14369v1"}
{"created":"2024-08-26 15:46:41","title":"GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal Conditioned Policy","abstract":"The robotics community has consistently aimed to achieve generalizable robot manipulation with flexible natural language instructions. One of the primary challenges is that obtaining robot data fully annotated with both actions and texts is time-consuming and labor-intensive. However, partially annotated data, such as human activity videos without action labels and robot play data without language labels, is much easier to collect. Can we leverage these data to enhance the generalization capability of robots? In this paper, we propose GR-MG, a novel method which supports conditioning on both a language instruction and a goal image. During training, GR-MG samples goal images from trajectories and conditions on both the text and the goal image or solely on the image when text is unavailable. During inference, where only the text is provided, GR-MG generates the goal image via a diffusion-based image-editing model and condition on both the text and the generated image. This approach enables GR-MG to leverage large amounts of partially annotated data while still using language to flexibly specify tasks. To generate accurate goal images, we propose a novel progress-guided goal image generation model which injects task progress information into the generation process, significantly improving the fidelity and the performance. In simulation experiments, GR-MG improves the average number of tasks completed in a row of 5 from 3.35 to 4.04. In real-robot experiments, GR-MG is able to perform 47 different tasks and improves the success rate from 62.5% to 75.0% and 42.4% to 57.6% in simple and generalization settings, respectively. Code and checkpoints will be available at the project page: https://gr-mg.github.io/.","sentences":["The robotics community has consistently aimed to achieve generalizable robot manipulation with flexible natural language instructions.","One of the primary challenges is that obtaining robot data fully annotated with both actions and texts is time-consuming and labor-intensive.","However, partially annotated data, such as human activity videos without action labels and robot play data without language labels, is much easier to collect.","Can we leverage these data to enhance the generalization capability of robots?","In this paper, we propose GR-MG, a novel method which supports conditioning on both a language instruction and a goal image.","During training, GR-MG samples goal images from trajectories and conditions on both the text and the goal image or solely on the image when text is unavailable.","During inference, where only the text is provided, GR-MG generates the goal image via a diffusion-based image-editing model and condition on both the text and the generated image.","This approach enables GR-MG to leverage large amounts of partially annotated data while still using language to flexibly specify tasks.","To generate accurate goal images, we propose a novel progress-guided goal image generation model which injects task progress information into the generation process, significantly improving the fidelity and the performance.","In simulation experiments, GR-MG improves the average number of tasks completed in a row of 5 from 3.35 to 4.04.","In real-robot experiments, GR-MG is able to perform 47 different tasks and improves the success rate from 62.5% to 75.0% and 42.4% to 57.6% in simple and generalization settings, respectively.","Code and checkpoints will be available at the project page: https://gr-mg.github.io/."],"url":"http://arxiv.org/abs/2408.14368v1"}
{"created":"2024-08-26 15:37:14","title":"IQ-aware precoding for atomic MIMO receivers","abstract":"Leveraging the strong atom-light interaction, Rydberg atomic receivers significantly enhance the sensitivity of electromagnetic signal measurements, outperforming traditional antennas. Existing research primarily focuses on improving the architecture and signal detection algorithms of atomic receivers, while established signal processing schemes at the transmitter end have remained constant. However, these schemes fail to maximize the throughput of atomic receivers due to the nonlinearity of transmission model. To address this issue, we propose to design transmitter precoding in multiple-input multiple-output systems to achieve the capacity of atomic receivers. Initially, we harness a strong reference approximation to convert the nonlinear magnitude-detection model of atomic receivers into a linear real-part detector. Based on this approximation, we prove that the degree of freedom is min{Nr/2,Nt} for a MIMO system comprising an Nr-antenna atomic receiver and an Nt-antenna classic transmitter. To achieve the system capacity, we propose an IQ-aware fully digital precoding method. Unlike traditional complex-valued digital precoders that jointly manipulate the inphase and quadrature (IQ) symbols, our method employs four real matrices to independently precode the IQ baseband symbols, which is shown to be optimal for atomic receivers. Then, to eliminate the reliance on fully digital precoding architecture, we further explore IQ-aware hybrid precoding techniques. Our design incorporates a low-dimensional IQ-aware digital precoder and a high-dimensional complex analog precoder. Alternating minimization algorithms are proposed to produce IQ-aware hybrid precoders, with the objective of approaching the optimal IQ-aware fully digital precoder. Simulation results validate the superiority of proposed IQ-aware precoding methods over existing techniques in atomic MIMO communications.","sentences":["Leveraging the strong atom-light interaction, Rydberg atomic receivers significantly enhance the sensitivity of electromagnetic signal measurements, outperforming traditional antennas.","Existing research primarily focuses on improving the architecture and signal detection algorithms of atomic receivers, while established signal processing schemes at the transmitter end have remained constant.","However, these schemes fail to maximize the throughput of atomic receivers due to the nonlinearity of transmission model.","To address this issue, we propose to design transmitter precoding in multiple-input multiple-output systems to achieve the capacity of atomic receivers.","Initially, we harness a strong reference approximation to convert the nonlinear magnitude-detection model of atomic receivers into a linear real-part detector.","Based on this approximation, we prove that the degree of freedom is min{Nr/2,Nt} for a MIMO system comprising an Nr-antenna atomic receiver and an Nt-antenna classic transmitter.","To achieve the system capacity, we propose an IQ-aware fully digital precoding method.","Unlike traditional complex-valued digital precoders that jointly manipulate the inphase and quadrature (IQ) symbols, our method employs four real matrices to independently precode the IQ baseband symbols, which is shown to be optimal for atomic receivers.","Then, to eliminate the reliance on fully digital precoding architecture, we further explore IQ-aware hybrid precoding techniques.","Our design incorporates a low-dimensional IQ-aware digital precoder and a high-dimensional complex analog precoder.","Alternating minimization algorithms are proposed to produce IQ-aware hybrid precoders, with the objective of approaching the optimal IQ-aware fully digital precoder.","Simulation results validate the superiority of proposed IQ-aware precoding methods over existing techniques in atomic MIMO communications."],"url":"http://arxiv.org/abs/2408.14366v1"}
{"created":"2024-08-26 15:36:03","title":"Model Predictive Parkour Control of a Monoped Hopper in Dynamically Changing Environments","abstract":"A great advantage of legged robots is their ability to operate on particularly difficult and obstructed terrain, which demands dynamic, robust, and precise movements. The study of obstacle courses provides invaluable insights into the challenges legged robots face, offering a controlled environment to assess and enhance their capabilities. Traversing it with a one-legged hopper introduces intricate challenges, such as planning over contacts and dealing with flight phases, which necessitates a sophisticated controller. A novel model predictive parkour controller is introduced, that finds an optimal path through a real-time changing obstacle course with mixed integer motion planning. The execution of this optimized path is then achieved through a state machine employing a PD control scheme with feedforward torques, ensuring robust and accurate performance.","sentences":["A great advantage of legged robots is their ability to operate on particularly difficult and obstructed terrain, which demands dynamic, robust, and precise movements.","The study of obstacle courses provides invaluable insights into the challenges legged robots face, offering a controlled environment to assess and enhance their capabilities.","Traversing it with a one-legged hopper introduces intricate challenges, such as planning over contacts and dealing with flight phases, which necessitates a sophisticated controller.","A novel model predictive parkour controller is introduced, that finds an optimal path through a real-time changing obstacle course with mixed integer motion planning.","The execution of this optimized path is then achieved through a state machine employing a PD control scheme with feedforward torques, ensuring robust and accurate performance."],"url":"http://arxiv.org/abs/2408.14362v1"}
{"created":"2024-08-26 15:35:44","title":"Functional kinematic and kinetic requirements of the upper limb during activities of daily living: a recommendation on necessary joint capabilities for prosthetic arms","abstract":"Prosthetic limb abandonment remains an unsolved challenge as amputees consistently reject their devices. Current prosthetic designs often fail to balance human-like perfomance with acceptable device weight, highlighting the need for optimised designs tailored to modern tasks. This study aims to provide a comprehensive dataset of joint kinematics and kinetics essential for performing activities of daily living (ADL), thereby informing the design of more functional and user-friendly prosthetic devices. Functionally required Ranges of Motion (ROM), velocities, and torques for the Glenohumeral (rotation), elbow, Radioulnar, and wrist joints were computed using motion capture data from 12 subjects performing 24 ADLs. Our approach included the computation of joint torques for varying mass and inertia properties of the upper limb, while torques induced by the manipulation of experimental objects were considered by their interaction wrench with the subjects hand. Joint torques pertaining to individual ADL scaled linearly with limb and object mass and mass distribution, permitting their generalisation to not explicitly simulated limb and object dynamics with linear regressors (LRM), exhibiting coefficients of determination R = 0.99 pm 0.01. Exemplifying an application of data-driven prosthesis design, we optimise wrist axes orientations for two serial and two differential joint configurations. Optimised axes reduced peak power requirements, between 22 to 38 percent compared to anatomical configurations, by exploiting high torque correlations between Ulnar deviation and wrist flexion/extension joints. This study offers critical insights into the functional requirements of upper limb prostheses, providing a valuable foundation for data-driven prosthetic design that addresses key user concerns and enhances device adoption.","sentences":["Prosthetic limb abandonment remains an unsolved challenge as amputees consistently reject their devices.","Current prosthetic designs often fail to balance human-like perfomance with acceptable device weight, highlighting the need for optimised designs tailored to modern tasks.","This study aims to provide a comprehensive dataset of joint kinematics and kinetics essential for performing activities of daily living (ADL), thereby informing the design of more functional and user-friendly prosthetic devices.","Functionally required Ranges of Motion (ROM), velocities, and torques for the Glenohumeral (rotation), elbow, Radioulnar, and wrist joints were computed using motion capture data from 12 subjects performing 24 ADLs.","Our approach included the computation of joint torques for varying mass and inertia properties of the upper limb, while torques induced by the manipulation of experimental objects were considered by their interaction wrench with the subjects hand.","Joint torques pertaining to individual ADL scaled linearly with limb and object mass and mass distribution, permitting their generalisation to not explicitly simulated limb and object dynamics with linear regressors (LRM), exhibiting coefficients of determination R = 0.99 pm 0.01.","Exemplifying an application of data-driven prosthesis design, we optimise wrist axes orientations for two serial and two differential joint configurations.","Optimised axes reduced peak power requirements, between 22 to 38 percent compared to anatomical configurations, by exploiting high torque correlations between Ulnar deviation and wrist flexion/extension joints.","This study offers critical insights into the functional requirements of upper limb prostheses, providing a valuable foundation for data-driven prosthetic design that addresses key user concerns and enhances device adoption."],"url":"http://arxiv.org/abs/2408.14361v1"}
{"created":"2024-08-26 15:32:31","title":"An Embedding is Worth a Thousand Noisy Labels","abstract":"The performance of deep neural networks scales with dataset size and label quality, rendering the efficient mitigation of low-quality data annotations crucial for building robust and cost-effective systems. Existing strategies to address label noise exhibit severe limitations due to computational complexity and application dependency. In this work, we propose WANN, a Weighted Adaptive Nearest Neighbor approach that builds on self-supervised feature representations obtained from foundation models. To guide the weighted voting scheme, we introduce a reliability score, which measures the likelihood of a data label being correct. WANN outperforms reference methods, including a linear layer trained with robust loss functions, on diverse datasets of varying size and under various noise types and severities. WANN also exhibits superior generalization on imbalanced data compared to both Adaptive-NNs (ANN) and fixed k-NNs. Furthermore, the proposed weighting scheme enhances supervised dimensionality reduction under noisy labels. This yields a significant boost in classification performance with 10x and 100x smaller image embeddings, minimizing latency and storage requirements. Our approach, emphasizing efficiency and explainability, emerges as a simple, robust solution to overcome the inherent limitations of deep neural network training. The code is available at https://github.com/francescodisalvo05/wann-noisy-labels .","sentences":["The performance of deep neural networks scales with dataset size and label quality, rendering the efficient mitigation of low-quality data annotations crucial for building robust and cost-effective systems.","Existing strategies to address label noise exhibit severe limitations due to computational complexity and application dependency.","In this work, we propose WANN, a Weighted Adaptive Nearest Neighbor approach that builds on self-supervised feature representations obtained from foundation models.","To guide the weighted voting scheme, we introduce a reliability score, which measures the likelihood of a data label being correct.","WANN outperforms reference methods, including a linear layer trained with robust loss functions, on diverse datasets of varying size and under various noise types and severities.","WANN also exhibits superior generalization on imbalanced data compared to both Adaptive-NNs (ANN) and fixed k-NNs.","Furthermore, the proposed weighting scheme enhances supervised dimensionality reduction under noisy labels.","This yields a significant boost in classification performance with 10x and 100x smaller image embeddings, minimizing latency and storage requirements.","Our approach, emphasizing efficiency and explainability, emerges as a simple, robust solution to overcome the inherent limitations of deep neural network training.","The code is available at https://github.com/francescodisalvo05/wann-noisy-labels ."],"url":"http://arxiv.org/abs/2408.14358v1"}
{"created":"2024-08-26 15:31:58","title":"Exploring ChatGPT App Ecosystem: Distribution, Deployment and Security","abstract":"ChatGPT has enabled third-party developers to create plugins to expand ChatGPT's capabilities.These plugins are distributed through OpenAI's plugin store, making them easily accessible to users. With ChatGPT as the backbone, this app ecosystem has illustrated great business potential by offering users personalized services in a conversational manner. Nonetheless, many crucial aspects regarding app development, deployment, and security of this ecosystem have yet to be thoroughly studied in the research community, potentially hindering a broader adoption by both developers and users. In this work, we conduct the first comprehensive study of the ChatGPT app ecosystem, aiming to illuminate its landscape for our research community. Our study examines the distribution and deployment models in the integration of LLMs and third-party apps, and assesses their security and privacy implications. We uncover an uneven distribution of functionality among ChatGPT plugins, highlighting prevalent and emerging topics. We also identify severe flaws in the authentication and user data protection for third-party app APIs integrated within LLMs, revealing a concerning status quo of security and privacy in this app ecosystem. Our work provides insights for the secure and sustainable development of this rapidly evolving ecosystem.","sentences":["ChatGPT has enabled third-party developers to create plugins to expand ChatGPT's capabilities.","These plugins are distributed through OpenAI's plugin store, making them easily accessible to users.","With ChatGPT as the backbone, this app ecosystem has illustrated great business potential by offering users personalized services in a conversational manner.","Nonetheless, many crucial aspects regarding app development, deployment, and security of this ecosystem have yet to be thoroughly studied in the research community, potentially hindering a broader adoption by both developers and users.","In this work, we conduct the first comprehensive study of the ChatGPT app ecosystem, aiming to illuminate its landscape for our research community.","Our study examines the distribution and deployment models in the integration of LLMs and third-party apps, and assesses their security and privacy implications.","We uncover an uneven distribution of functionality among ChatGPT plugins, highlighting prevalent and emerging topics.","We also identify severe flaws in the authentication and user data protection for third-party app APIs integrated within LLMs, revealing a concerning status quo of security and privacy in this app ecosystem.","Our work provides insights for the secure and sustainable development of this rapidly evolving ecosystem."],"url":"http://arxiv.org/abs/2408.14357v1"}
{"created":"2024-08-26 15:30:05","title":"SWE-bench-java: A GitHub Issue Resolving Benchmark for Java","abstract":"GitHub issue resolving is a critical task in software engineering, recently gaining significant attention in both industry and academia. Within this task, SWE-bench has been released to evaluate issue resolving capabilities of large language models (LLMs), but has so far only focused on Python version. However, supporting more programming languages is also important, as there is a strong demand in industry. As a first step toward multilingual support, we have developed a Java version of SWE-bench, called SWE-bench-java. We have publicly released the dataset, along with the corresponding Docker-based evaluation environment and leaderboard, which will be continuously maintained and updated in the coming months. To verify the reliability of SWE-bench-java, we implement a classic method SWE-agent and test several powerful LLMs on it. As is well known, developing a high-quality multi-lingual benchmark is time-consuming and labor-intensive, so we welcome contributions through pull requests or collaboration to accelerate its iteration and refinement, paving the way for fully automated programming.","sentences":["GitHub issue resolving is a critical task in software engineering, recently gaining significant attention in both industry and academia.","Within this task, SWE-bench has been released to evaluate issue resolving capabilities of large language models (LLMs), but has so far only focused on Python version.","However, supporting more programming languages is also important, as there is a strong demand in industry.","As a first step toward multilingual support, we have developed a Java version of SWE-bench, called SWE-bench-java.","We have publicly released the dataset, along with the corresponding Docker-based evaluation environment and leaderboard, which will be continuously maintained and updated in the coming months.","To verify the reliability of SWE-bench-java, we implement a classic method SWE-agent and test several powerful LLMs on it.","As is well known, developing a high-quality multi-lingual benchmark is time-consuming and labor-intensive, so we welcome contributions through pull requests or collaboration to accelerate its iteration and refinement, paving the way for fully automated programming."],"url":"http://arxiv.org/abs/2408.14354v1"}
{"created":"2024-08-26 15:29:34","title":"Assessing Contamination in Large Language Models: Introducing the LogProber method","abstract":"In machine learning, contamination refers to situations where testing data leak into the training set. The issue is particularly relevant for the evaluation of the performance of Large Language Models (LLMs), which are generally trained on gargantuan, and generally opaque, corpora of text scraped from the world wide web. Developing tools to detect contamination is therefore crucial to be able to fairly and properly track the evolution of the performance of LLMs. Most recent works in the field are not tailored to quantify contamination on short sequences of text like we find in psychology questionnaires. In the present paper we introduce LogProber, a novel, efficient, algorithm that we show able to detect contamination using token probability in given sentences. In the second part we investigate the limitations of the method and discuss how different training methods can contaminate models without leaving traces in the token probabilities.","sentences":["In machine learning, contamination refers to situations where testing data leak into the training set.","The issue is particularly relevant for the evaluation of the performance of Large Language Models (LLMs), which are generally trained on gargantuan, and generally opaque, corpora of text scraped from the world wide web.","Developing tools to detect contamination is therefore crucial to be able to fairly and properly track the evolution of the performance of LLMs.","Most recent works in the field are not tailored to quantify contamination on short sequences of text like we find in psychology questionnaires.","In the present paper we introduce LogProber, a novel, efficient, algorithm that we show able to detect contamination using token probability in given sentences.","In the second part we investigate the limitations of the method and discuss how different training methods can contaminate models without leaving traces in the token probabilities."],"url":"http://arxiv.org/abs/2408.14352v1"}
{"created":"2024-08-26 15:26:27","title":"Deep learning-based ecological analysis of camera trap images is impacted by training data quality and size","abstract":"Large wildlife image collections from camera traps are crucial for biodiversity monitoring, offering insights into species richness, occupancy, and activity patterns. However, manual processing of these data is time-consuming, hindering analytical processes. To address this, deep neural networks have been widely adopted to automate image analysis. Despite their growing use, the impact of model training decisions on downstream ecological metrics remains unclear. Here, we analyse camera trap data from an African savannah and an Asian sub-tropical dry forest to compare key ecological metrics derived from expert-generated species identifications with those generated from deep neural networks. We assess the impact of model architecture, training data noise, and dataset size on ecological metrics, including species richness, occupancy, and activity patterns. Our results show that while model architecture has minimal impact, large amounts of noise and reduced dataset size significantly affect these metrics. Nonetheless, estimated ecological metrics are resilient to considerable noise, tolerating up to 10% error in species labels and a 50% reduction in training set size without changing significantly. We also highlight that conventional metrics like classification error may not always be representative of a model's ability to accurately measure ecological metrics. We conclude that ecological metrics derived from deep neural network predictions closely match those calculated from expert labels and remain robust to variations in the factors explored. However, training decisions for deep neural networks can impact downstream ecological analysis. Therefore, practitioners should prioritize creating large, clean training sets and evaluate deep neural network solutions based on their ability to measure the ecological metrics of interest.","sentences":["Large wildlife image collections from camera traps are crucial for biodiversity monitoring, offering insights into species richness, occupancy, and activity patterns.","However, manual processing of these data is time-consuming, hindering analytical processes.","To address this, deep neural networks have been widely adopted to automate image analysis.","Despite their growing use, the impact of model training decisions on downstream ecological metrics remains unclear.","Here, we analyse camera trap data from an African savannah and an Asian sub-tropical dry forest to compare key ecological metrics derived from expert-generated species identifications with those generated from deep neural networks.","We assess the impact of model architecture, training data noise, and dataset size on ecological metrics, including species richness, occupancy, and activity patterns.","Our results show that while model architecture has minimal impact, large amounts of noise and reduced dataset size significantly affect these metrics.","Nonetheless, estimated ecological metrics are resilient to considerable noise, tolerating up to 10% error in species labels and a 50% reduction in training set size without changing significantly.","We also highlight that conventional metrics like classification error may not always be representative of a model's ability to accurately measure ecological metrics.","We conclude that ecological metrics derived from deep neural network predictions closely match those calculated from expert labels and remain robust to variations in the factors explored.","However, training decisions for deep neural networks can impact downstream ecological analysis.","Therefore, practitioners should prioritize creating large, clean training sets and evaluate deep neural network solutions based on their ability to measure the ecological metrics of interest."],"url":"http://arxiv.org/abs/2408.14348v1"}
{"created":"2024-08-26 15:20:09","title":"Guard Analysis and Safe Erasure Gradual Typing: a Type System for Elixir","abstract":"We define several techniques to extend gradual typing with semantic subtyping, specifically targeting dynamic languages. Focusing on the Elixir programming language, we provide the theoretical foundations for its type system. Our approach demonstrates how to achieve type soundness for gradual typing in existing dynamic languages without modifying their compilation, while still maintaining high precision. This is accomplished through the static detection of \"strong functions\", which leverage runtime checks inserted by the programmer or performed by the virtual machine, and through a fine-grained type analysis of pattern-matching expressions with guards.","sentences":["We define several techniques to extend gradual typing with semantic subtyping, specifically targeting dynamic languages.","Focusing on the Elixir programming language, we provide the theoretical foundations for its type system.","Our approach demonstrates how to achieve type soundness for gradual typing in existing dynamic languages without modifying their compilation, while still maintaining high precision.","This is accomplished through the static detection of \"strong functions\", which leverage runtime checks inserted by the programmer or performed by the virtual machine, and through a fine-grained type analysis of pattern-matching expressions with guards."],"url":"http://arxiv.org/abs/2408.14345v1"}
{"created":"2024-08-26 15:16:28","title":"A Brief Analysis of the Iterative Next Boundary Detection Network for Tree Rings Delineation in Images of Pinus taeda","abstract":"This work presents the INBD network proposed by Gillert et al. in CVPR-2023 and studies its application for delineating tree rings in RGB images of Pinus taeda cross sections captured by a smartphone (UruDendro dataset), which are images with different characteristics from the ones used to train the method. The INBD network operates in two stages: first, it segments the background, pith, and ring boundaries. In the second stage, the image is transformed into polar coordinates, and ring boundaries are iteratively segmented from the pith to the bark. Both stages are based on the U-Net architecture. The method achieves an F-Score of 77.5, a mAR of 0.540, and an ARAND of 0.205 on the evaluation set. The code for the experiments is available at https://github.com/hmarichal93/mlbrief_inbd.","sentences":["This work presents the INBD network proposed by Gillert et al. in CVPR-2023 and studies its application for delineating tree rings in RGB images of Pinus taeda cross sections captured by a smartphone (UruDendro dataset), which are images with different characteristics from the ones used to train the method.","The INBD network operates in two stages: first, it segments the background, pith, and ring boundaries.","In the second stage, the image is transformed into polar coordinates, and ring boundaries are iteratively segmented from the pith to the bark.","Both stages are based on the U-Net architecture.","The method achieves an F-Score of 77.5, a mAR of 0.540, and an ARAND of 0.205 on the evaluation set.","The code for the experiments is available at https://github.com/hmarichal93/mlbrief_inbd."],"url":"http://arxiv.org/abs/2408.14343v1"}
{"created":"2024-08-26 15:13:14","title":"Foundation Models for Music: A Survey","abstract":"In recent years, foundation models (FMs) such as large language models (LLMs) and latent diffusion models (LDMs) have profoundly impacted diverse sectors, including music. This comprehensive review examines state-of-the-art (SOTA) pre-trained models and foundation models in music, spanning from representation learning, generative learning and multimodal learning. We first contextualise the significance of music in various industries and trace the evolution of AI in music. By delineating the modalities targeted by foundation models, we discover many of the music representations are underexplored in FM development. Then, emphasis is placed on the lack of versatility of previous methods on diverse music applications, along with the potential of FMs in music understanding, generation and medical application. By comprehensively exploring the details of the model pre-training paradigm, architectural choices, tokenisation, finetuning methodologies and controllability, we emphasise the important topics that should have been well explored, like instruction tuning and in-context learning, scaling law and emergent ability, as well as long-sequence modelling etc. A dedicated section presents insights into music agents, accompanied by a thorough analysis of datasets and evaluations essential for pre-training and downstream tasks. Finally, by underscoring the vital importance of ethical considerations, we advocate that following research on FM for music should focus more on such issues as interpretability, transparency, human responsibility, and copyright issues. The paper offers insights into future challenges and trends on FMs for music, aiming to shape the trajectory of human-AI collaboration in the music realm.","sentences":["In recent years, foundation models (FMs) such as large language models (LLMs) and latent diffusion models (LDMs) have profoundly impacted diverse sectors, including music.","This comprehensive review examines state-of-the-art (SOTA) pre-trained models and foundation models in music, spanning from representation learning, generative learning and multimodal learning.","We first contextualise the significance of music in various industries and trace the evolution of AI in music.","By delineating the modalities targeted by foundation models, we discover many of the music representations are underexplored in FM development.","Then, emphasis is placed on the lack of versatility of previous methods on diverse music applications, along with the potential of FMs in music understanding, generation and medical application.","By comprehensively exploring the details of the model pre-training paradigm, architectural choices, tokenisation, finetuning methodologies and controllability, we emphasise the important topics that should have been well explored, like instruction tuning and in-context learning, scaling law and emergent ability, as well as long-sequence modelling etc.","A dedicated section presents insights into music agents, accompanied by a thorough analysis of datasets and evaluations essential for pre-training and downstream tasks.","Finally, by underscoring the vital importance of ethical considerations, we advocate that following research on FM for music should focus more on such issues as interpretability, transparency, human responsibility, and copyright issues.","The paper offers insights into future challenges and trends on FMs for music, aiming to shape the trajectory of human-AI collaboration in the music realm."],"url":"http://arxiv.org/abs/2408.14340v1"}
{"created":"2024-08-26 15:08:12","title":"ConceptMix: A Compositional Image Generation Benchmark with Controllable Difficulty","abstract":"Compositionality is a critical capability in Text-to-Image (T2I) models, as it reflects their ability to understand and combine multiple concepts from text descriptions. Existing evaluations of compositional capability rely heavily on human-designed text prompts or fixed templates, limiting their diversity and complexity, and yielding low discriminative power. We propose ConceptMix, a scalable, controllable, and customizable benchmark which automatically evaluates compositional generation ability of T2I models. This is done in two stages. First, ConceptMix generates the text prompts: concretely, using categories of visual concepts (e.g., objects, colors, shapes, spatial relationships), it randomly samples an object and k-tuples of visual concepts, then uses GPT4-o to generate text prompts for image generation based on these sampled concepts. Second, ConceptMix evaluates the images generated in response to these prompts: concretely, it checks how many of the k concepts actually appeared in the image by generating one question per visual concept and using a strong VLM to answer them. Through administering ConceptMix to a diverse set of T2I models (proprietary as well as open ones) using increasing values of k, we show that our ConceptMix has higher discrimination power than earlier benchmarks. Specifically, ConceptMix reveals that the performance of several models, especially open models, drops dramatically with increased k. Importantly, it also provides insight into the lack of prompt diversity in widely-used training datasets. Additionally, we conduct extensive human studies to validate the design of ConceptMix and compare our automatic grading with human judgement. We hope it will guide future T2I model development.","sentences":["Compositionality is a critical capability in Text-to-Image (T2I) models, as it reflects their ability to understand and combine multiple concepts from text descriptions.","Existing evaluations of compositional capability rely heavily on human-designed text prompts or fixed templates, limiting their diversity and complexity, and yielding low discriminative power.","We propose ConceptMix, a scalable, controllable, and customizable benchmark which automatically evaluates compositional generation ability of T2I models.","This is done in two stages.","First, ConceptMix generates the text prompts: concretely, using categories of visual concepts (e.g., objects, colors, shapes, spatial relationships), it randomly samples an object and k-tuples of visual concepts, then uses GPT4-o to generate text prompts for image generation based on these sampled concepts.","Second, ConceptMix evaluates the images generated in response to these prompts: concretely, it checks how many of the k concepts actually appeared in the image by generating one question per visual concept and using a strong VLM to answer them.","Through administering ConceptMix to a diverse set of T2I models (proprietary as well as open ones) using increasing values of k, we show that our ConceptMix has higher discrimination power than earlier benchmarks.","Specifically, ConceptMix reveals that the performance of several models, especially open models, drops dramatically with increased k.","Importantly, it also provides insight into the lack of prompt diversity in widely-used training datasets.","Additionally, we conduct extensive human studies to validate the design of ConceptMix and compare our automatic grading with human judgement.","We hope it will guide future T2I model development."],"url":"http://arxiv.org/abs/2408.14339v1"}
{"created":"2024-08-26 15:07:35","title":"Machine Learning for Quantifier Selection in cvc5","abstract":"In this work we considerably improve the state-of-the-art SMT solving on first-order quantified problems by efficient machine learning guidance of quantifier selection. Quantifiers represent a significant challenge for SMT and are technically a source of undecidability. In our approach, we train an efficient machine learning model that informs the solver which quantifiers should be instantiated and which not. Each quantifier may be instantiated multiple times and the set of the active quantifiers changes as the solving progresses. Therefore, we invoke the ML predictor many times, during the whole run of the solver. To make this efficient, we use fast ML models based on gradient boosting decision trees. We integrate our approach into the state-of-the-art cvc5 SMT solver and show a considerable increase of the system's holdout-set performance after training it on a large set of first-order problems collected from the Mizar Mathematical Library.","sentences":["In this work we considerably improve the state-of-the-art SMT solving on first-order quantified problems by efficient machine learning guidance of quantifier selection.","Quantifiers represent a significant challenge for SMT and are technically a source of undecidability.","In our approach, we train an efficient machine learning model that informs the solver which quantifiers should be instantiated and which not.","Each quantifier may be instantiated multiple times and the set of the active quantifiers changes as the solving progresses.","Therefore, we invoke the ML predictor many times, during the whole run of the solver.","To make this efficient, we use fast ML models based on gradient boosting decision trees.","We integrate our approach into the state-of-the-art cvc5 SMT solver and show a considerable increase of the system's holdout-set performance after training it on a large set of first-order problems collected from the Mizar Mathematical Library."],"url":"http://arxiv.org/abs/2408.14338v1"}
{"created":"2024-08-26 15:07:01","title":"Equivariant Reinforcement Learning under Partial Observability","abstract":"Incorporating inductive biases is a promising approach for tackling challenging robot learning domains with sample-efficient solutions. This paper identifies partially observable domains where symmetries can be a useful inductive bias for efficient learning. Specifically, by encoding the equivariance regarding specific group symmetries into the neural networks, our actor-critic reinforcement learning agents can reuse solutions in the past for related scenarios. Consequently, our equivariant agents outperform non-equivariant approaches significantly in terms of sample efficiency and final performance, demonstrated through experiments on a range of robotic tasks in simulation and real hardware.","sentences":["Incorporating inductive biases is a promising approach for tackling challenging robot learning domains with sample-efficient solutions.","This paper identifies partially observable domains where symmetries can be a useful inductive bias for efficient learning.","Specifically, by encoding the equivariance regarding specific group symmetries into the neural networks, our actor-critic reinforcement learning agents can reuse solutions in the past for related scenarios.","Consequently, our equivariant agents outperform non-equivariant approaches significantly in terms of sample efficiency and final performance, demonstrated through experiments on a range of robotic tasks in simulation and real hardware."],"url":"http://arxiv.org/abs/2408.14336v1"}
{"created":"2024-08-26 15:01:04","title":"One-layer transformers fail to solve the induction heads task","abstract":"A simple communication complexity argument proves that no one-layer transformer can solve the induction heads task unless its size is exponentially larger than the size sufficient for a two-layer transformer.","sentences":["A simple communication complexity argument proves that no one-layer transformer can solve the induction heads task unless its size is exponentially larger than the size sufficient for a two-layer transformer."],"url":"http://arxiv.org/abs/2408.14332v1"}
{"created":"2024-08-26 14:55:40","title":"Automated Machine Learning in Insurance","abstract":"Machine Learning (ML) has gained popularity in actuarial research and insurance industrial applications. However, the performance of most ML tasks heavily depends on data preprocessing, model selection, and hyperparameter optimization, which are considered to be intensive in terms of domain knowledge, experience, and manual labor. Automated Machine Learning (AutoML) aims to automatically complete the full life-cycle of ML tasks and provides state-of-the-art ML models without human intervention or supervision. This paper introduces an AutoML workflow that allows users without domain knowledge or prior experience to achieve robust and effortless ML deployment by writing only a few lines of code. This proposed AutoML is specifically tailored for the insurance application, with features like the balancing step in data preprocessing, ensemble pipelines, and customized loss functions. These features are designed to address the unique challenges of the insurance domain, including the imbalanced nature of common insurance datasets. The full code and documentation are available on the GitHub repository. (https://github.com/PanyiDong/InsurAutoML)","sentences":["Machine Learning (ML) has gained popularity in actuarial research and insurance industrial applications.","However, the performance of most ML tasks heavily depends on data preprocessing, model selection, and hyperparameter optimization, which are considered to be intensive in terms of domain knowledge, experience, and manual labor.","Automated Machine Learning (AutoML) aims to automatically complete the full life-cycle of ML tasks and provides state-of-the-art ML models without human intervention or supervision.","This paper introduces an AutoML workflow that allows users without domain knowledge or prior experience to achieve robust and effortless ML deployment by writing only a few lines of code.","This proposed AutoML is specifically tailored for the insurance application, with features like the balancing step in data preprocessing, ensemble pipelines, and customized loss functions.","These features are designed to address the unique challenges of the insurance domain, including the imbalanced nature of common insurance datasets.","The full code and documentation are available on the GitHub repository.","(https://github.com/PanyiDong/InsurAutoML)"],"url":"http://arxiv.org/abs/2408.14331v1"}
{"created":"2024-08-26 14:55:23","title":"PHEVA: A Privacy-preserving Human-centric Video Anomaly Detection Dataset","abstract":"PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection dataset. By removing pixel information and providing only de-identified human annotations, PHEVA safeguards personally identifiable information. The dataset includes seven indoor/outdoor scenes, featuring one novel, context-specific camera, and offers over 5x the pose-annotated frames compared to the largest previous dataset. This study benchmarks state-of-the-art methods on PHEVA using a comprehensive set of metrics, including the 10% Error Rate (10ER), a metric used for anomaly detection for the first time providing insights relevant to real-world deployment. As the first of its kind, PHEVA bridges the gap between conventional training and real-world deployment by introducing continual learning benchmarks, with models outperforming traditional methods in 82.14% of cases. The dataset is publicly available at https://github.com/TeCSAR-UNCC/PHEVA.git.","sentences":["PHEVA, a Privacy-preserving Human-centric Ethical Video Anomaly detection dataset.","By removing pixel information and providing only de-identified human annotations, PHEVA safeguards personally identifiable information.","The dataset includes seven indoor/outdoor scenes, featuring one novel, context-specific camera, and offers over 5x the pose-annotated frames compared to the largest previous dataset.","This study benchmarks state-of-the-art methods on PHEVA using a comprehensive set of metrics, including the 10% Error Rate (10ER), a metric used for anomaly detection for the first time providing insights relevant to real-world deployment.","As the first of its kind, PHEVA bridges the gap between conventional training and real-world deployment by introducing continual learning benchmarks, with models outperforming traditional methods in 82.14% of cases.","The dataset is publicly available at https://github.com/TeCSAR-UNCC/PHEVA.git."],"url":"http://arxiv.org/abs/2408.14329v1"}
{"created":"2024-08-26 14:54:14","title":"Streamline tractography of the fetal brain in utero with machine learning","abstract":"Diffusion-weighted magnetic resonance imaging (dMRI) is the only non-invasive tool for studying white matter tracts and structural connectivity of the brain. These assessments rely heavily on tractography techniques, which reconstruct virtual streamlines representing white matter fibers. Much effort has been devoted to improving tractography methodology for adult brains, while tractography of the fetal brain has been largely neglected. Fetal tractography faces unique difficulties due to low dMRI signal quality, immature and rapidly developing brain structures, and paucity of reference data. This work presents the first machine learning model for fetal tractography. The model input consists of five sources of information: (1) Fiber orientation, inferred from a diffusion tensor fit to the dMRI signal; (2) Directions of recent propagation steps; (3) Global spatial information, encoded as distances to keypoints in the brain cortex; (4) Tissue segmentation information; and (5) Prior information about the expected local fiber orientations supplied with an atlas. In order to mitigate the local tensor estimation error, a large spatial context around the current point in the diffusion tensor image is encoded using convolutional and attention neural network modules. Moreover, the diffusion tensor information at a hypothetical next point is included in the model input. Filtering rules based on anatomically constrained tractography are applied to prune implausible streamlines. We trained the model on manually-refined whole-brain fetal tractograms and validated the trained model on an independent set of 11 test scans with gestational ages between 23 and 36 weeks. Results show that our proposed method achieves superior performance across all evaluated tracts. The new method can significantly advance the capabilities of dMRI for studying normal and abnormal brain development in utero.","sentences":["Diffusion-weighted magnetic resonance imaging (dMRI) is the only non-invasive tool for studying white matter tracts and structural connectivity of the brain.","These assessments rely heavily on tractography techniques, which reconstruct virtual streamlines representing white matter fibers.","Much effort has been devoted to improving tractography methodology for adult brains, while tractography of the fetal brain has been largely neglected.","Fetal tractography faces unique difficulties due to low dMRI signal quality, immature and rapidly developing brain structures, and paucity of reference data.","This work presents the first machine learning model for fetal tractography.","The model input consists of five sources of information: (1) Fiber orientation, inferred from a diffusion tensor fit to the dMRI signal; (2) Directions of recent propagation steps; (3) Global spatial information, encoded as distances to keypoints in the brain cortex; (4) Tissue segmentation information; and (5) Prior information about the expected local fiber orientations supplied with an atlas.","In order to mitigate the local tensor estimation error, a large spatial context around the current point in the diffusion tensor image is encoded using convolutional and attention neural network modules.","Moreover, the diffusion tensor information at a hypothetical next point is included in the model input.","Filtering rules based on anatomically constrained tractography are applied to prune implausible streamlines.","We trained the model on manually-refined whole-brain fetal tractograms and validated the trained model on an independent set of 11 test scans with gestational ages between 23 and 36 weeks.","Results show that our proposed method achieves superior performance across all evaluated tracts.","The new method can significantly advance the capabilities of dMRI for studying normal and abnormal brain development in utero."],"url":"http://arxiv.org/abs/2408.14326v1"}
{"created":"2024-08-26 14:54:13","title":"Function-Space MCMC for Bayesian Wide Neural Networks","abstract":"Bayesian Neural Networks represent a fascinating confluence of deep learning and probabilistic reasoning, offering a compelling framework for understanding uncertainty in complex predictive models. In this paper, we investigate the use of the preconditioned Crank-Nicolson algorithm and its Langevin version to sample from the reparametrised posterior distribution of the weights as the widths of Bayesian Neural Networks grow larger. In addition to being robust in the infinite-dimensional setting, we prove that the acceptance probabilities of the proposed methods approach 1 as the width of the network increases, independently of any stepsize tuning. Moreover, we examine and compare how the mixing speeds of the underdamped Langevin Monte Carlo, the preconditioned Crank-Nicolson and the preconditioned Crank-Nicolson Langevin samplers are influenced by changes in the network width in some real-world cases. Our findings suggest that, in wide Bayesian Neural Networks configurations, the preconditioned Crank-Nicolson method allows for more efficient sampling of the reparametrised posterior distribution, as evidenced by a higher effective sample size and improved diagnostic results compared with the other analysed algorithms.","sentences":["Bayesian Neural Networks represent a fascinating confluence of deep learning and probabilistic reasoning, offering a compelling framework for understanding uncertainty in complex predictive models.","In this paper, we investigate the use of the preconditioned Crank-Nicolson algorithm and its Langevin version to sample from the reparametrised posterior distribution of the weights as the widths of Bayesian Neural Networks grow larger.","In addition to being robust in the infinite-dimensional setting, we prove that the acceptance probabilities of the proposed methods approach 1 as the width of the network increases, independently of any stepsize tuning.","Moreover, we examine and compare how the mixing speeds of the underdamped Langevin Monte Carlo, the preconditioned Crank-Nicolson and the preconditioned Crank-Nicolson Langevin samplers are influenced by changes in the network width in some real-world cases.","Our findings suggest that, in wide Bayesian Neural Networks configurations, the preconditioned Crank-Nicolson method allows for more efficient sampling of the reparametrised posterior distribution, as evidenced by a higher effective sample size and improved diagnostic results compared with the other analysed algorithms."],"url":"http://arxiv.org/abs/2408.14325v1"}
{"created":"2024-08-26 14:51:49","title":"Investigating Persuasive Socially Assistive Robot Behavior Strategies for Sustained Engagement in Long-Term Care","abstract":"Socially assistive robots are increasingly being used to support the social, cognitive, and physical well-being of those who provide care (healthcare professionals) and those in need of care (older adults). However, the effectiveness of persuasive socially assistive robot behaviors and their impact on the sustained motivation of older adults is still not well understood. This extended abstract describes our prior human-robot interaction study on investigating the effectiveness of persuasive social robot behaviors with care providers, followed by our current research assessing the impact of these persuasive robot behaviors on the well-being of older adults in long-term care. The findings provide insights into engagement and sustained motivation of older adults when providing assistance.","sentences":["Socially assistive robots are increasingly being used to support the social, cognitive, and physical well-being of those who provide care (healthcare professionals) and those in need of care (older adults).","However, the effectiveness of persuasive socially assistive robot behaviors and their impact on the sustained motivation of older adults is still not well understood.","This extended abstract describes our prior human-robot interaction study on investigating the effectiveness of persuasive social robot behaviors with care providers, followed by our current research assessing the impact of these persuasive robot behaviors on the well-being of older adults in long-term care.","The findings provide insights into engagement and sustained motivation of older adults when providing assistance."],"url":"http://arxiv.org/abs/2408.14322v1"}
{"created":"2024-08-26 14:51:26","title":"Rethinking Knowledge Transfer in Learning Using Privileged Information","abstract":"In supervised machine learning, privileged information (PI) is information that is unavailable at inference, but is accessible during training time. Research on learning using privileged information (LUPI) aims to transfer the knowledge captured in PI onto a model that can perform inference without PI. It seems that this extra bit of information ought to make the resulting model better. However, finding conclusive theoretical or empirical evidence that supports the ability to transfer knowledge using PI has been challenging. In this paper, we critically examine the assumptions underlying existing theoretical analyses and argue that there is little theoretical justification for when LUPI should work. We analyze LUPI methods and reveal that apparent improvements in empirical risk of existing research may not directly result from PI. Instead, these improvements often stem from dataset anomalies or modifications in model design misguidedly attributed to PI. Our experiments for a wide variety of application domains further demonstrate that state-of-the-art LUPI approaches fail to effectively transfer knowledge from PI. Thus, we advocate for practitioners to exercise caution when working with PI to avoid unintended inductive biases.","sentences":["In supervised machine learning, privileged information (PI) is information that is unavailable at inference, but is accessible during training time.","Research on learning using privileged information (LUPI) aims to transfer the knowledge captured in PI onto a model that can perform inference without PI.","It seems that this extra bit of information ought to make the resulting model better.","However, finding conclusive theoretical or empirical evidence that supports the ability to transfer knowledge using PI has been challenging.","In this paper, we critically examine the assumptions underlying existing theoretical analyses and argue that there is little theoretical justification for when LUPI should work.","We analyze LUPI methods and reveal that apparent improvements in empirical risk of existing research may not directly result from PI.","Instead, these improvements often stem from dataset anomalies or modifications in model design misguidedly attributed to PI.","Our experiments for a wide variety of application domains further demonstrate that state-of-the-art LUPI approaches fail to effectively transfer knowledge from PI.","Thus, we advocate for practitioners to exercise caution when working with PI to avoid unintended inductive biases."],"url":"http://arxiv.org/abs/2408.14319v1"}
{"created":"2024-08-26 14:45:03","title":"Claim Verification in the Age of Large Language Models: A Survey","abstract":"The large and ever-increasing amount of data available on the Internet coupled with the laborious task of manual claim and fact verification has sparked the interest in the development of automated claim verification systems. Several deep learning and transformer-based models have been proposed for this task over the years. With the introduction of Large Language Models (LLMs) and their superior performance in several NLP tasks, we have seen a surge of LLM-based approaches to claim verification along with the use of novel methods such as Retrieval Augmented Generation (RAG). In this survey, we present a comprehensive account of recent claim verification frameworks using LLMs. We describe the different components of the claim verification pipeline used in these frameworks in detail including common approaches to retrieval, prompting, and fine-tuning. Finally, we describe publicly available English datasets created for this task.","sentences":["The large and ever-increasing amount of data available on the Internet coupled with the laborious task of manual claim and fact verification has sparked the interest in the development of automated claim verification systems.","Several deep learning and transformer-based models have been proposed for this task over the years.","With the introduction of Large Language Models (LLMs) and their superior performance in several NLP tasks, we have seen a surge of LLM-based approaches to claim verification along with the use of novel methods such as Retrieval Augmented Generation (RAG).","In this survey, we present a comprehensive account of recent claim verification frameworks using LLMs.","We describe the different components of the claim verification pipeline used in these frameworks in detail including common approaches to retrieval, prompting, and fine-tuning.","Finally, we describe publicly available English datasets created for this task."],"url":"http://arxiv.org/abs/2408.14317v1"}
{"created":"2024-08-26 14:43:56","title":"Overcoming the Barriers of Using Linked Open Data in Smart City Applications","abstract":"We study the benefits and challenges of using Linked Open Data in smart city applications and propose a set of open source, highly scalable tools within the case of a public-rental bicycle system, which can act as a reference guide for other smart city applications.","sentences":["We study the benefits and challenges of using Linked Open Data in smart city applications and propose a set of open source, highly scalable tools within the case of a public-rental bicycle system, which can act as a reference guide for other smart city applications."],"url":"http://arxiv.org/abs/2408.14315v1"}
{"created":"2024-08-26 14:43:43","title":"Logic interpretations of ANN partition cells","abstract":"Consider a binary classification problem solved using a feed-forward artificial neural network (ANN). Let the ANN be composed of a ReLU layer and several linear layers (convolution, sum-pooling, or fully connected). We assume the network was trained with high accuracy. Despite numerous suggested approaches, interpreting an artificial neural network remains challenging for humans. For a new method of interpretation, we construct a bridge between a simple ANN and logic. As a result, we can analyze and manipulate the semantics of an ANN using the powerful tool set of logic. To achieve this, we decompose the input space of the ANN into several network partition cells. Each network partition cell represents a linear combination that maps input values to a classifying output value. For interpreting the linear map of a partition cell using logic expressions, we suggest minterm values as the input of a simple ANN. We derive logic expressions representing interaction patterns for separating objects classified as 1 from those classified as 0. To facilitate an interpretation of logic expressions, we present them as binary logic trees.","sentences":["Consider a binary classification problem solved using a feed-forward artificial neural network (ANN).","Let the ANN be composed of a ReLU layer and several linear layers (convolution, sum-pooling, or fully connected).","We assume the network was trained with high accuracy.","Despite numerous suggested approaches, interpreting an artificial neural network remains challenging for humans.","For a new method of interpretation, we construct a bridge between a simple ANN and logic.","As a result, we can analyze and manipulate the semantics of an ANN using the powerful tool set of logic.","To achieve this, we decompose the input space of the ANN into several network partition cells.","Each network partition cell represents a linear combination that maps input values to a classifying output value.","For interpreting the linear map of a partition cell using logic expressions, we suggest minterm values as the input of a simple ANN.","We derive logic expressions representing interaction patterns for separating objects classified as 1 from those classified as 0.","To facilitate an interpretation of logic expressions, we present them as binary logic trees."],"url":"http://arxiv.org/abs/2408.14314v1"}
{"created":"2024-08-26 14:42:00","title":"The Power of Proportional Fairness for Non-Clairvoyant Scheduling under Polyhedral Constraints","abstract":"The Polytope Scheduling Problem (PSP) was introduced by Im, Kulkarni, and Munagala (JACM 2018) as a very general abstraction of resource allocation over time and captures many well-studied problems including classical unrelated machine scheduling, multidimensional scheduling, and broadcast scheduling. In PSP, jobs with different arrival times receive processing rates that are subject to arbitrary packing constraints. An elegant and well-known algorithm for instantaneous rate allocation with good fairness and efficiency properties is the Proportional Fairness algorithm (PF), which was analyzed for PSP by Im et al.   We drastically improve the analysis of the PF algorithm for both the general PSP and several of its important special cases subject to the objective of minimizing the sum of weighted completion times. We reduce the upper bound on the competitive ratio from 128 to 27 for general PSP and to 4 for the prominent class of monotone PSP. For certain heterogeneous machine environments we even close the substantial gap to the lower bound of 2 for non-clairvoyant scheduling. Our analysis also gives the first polynomial-time improvements over the nearly 30-year-old bounds on the competitive ratio of the doubling framework by Hall, Shmoys, and Wein (SODA 1996) for clairvoyant online preemptive scheduling on unrelated machines. Somewhat surprisingly, we achieve this improvement by a non-clairvoyant algorithm, thereby demonstrating that non-clairvoyance is not a (significant) hurdle.   Our improvements are based on exploiting monotonicity properties of PSP, providing tight dual fitting arguments on structured instances, and showing new additivity properties on the optimal objective value for scheduling on unrelated machines. Finally, we establish new connections of PF to matching markets, and thereby provide new insights on equilibria and their computational complexity.","sentences":["The Polytope Scheduling Problem (PSP) was introduced by Im, Kulkarni, and Munagala (JACM 2018) as a very general abstraction of resource allocation over time and captures many well-studied problems including classical unrelated machine scheduling, multidimensional scheduling, and broadcast scheduling.","In PSP, jobs with different arrival times receive processing rates that are subject to arbitrary packing constraints.","An elegant and well-known algorithm for instantaneous rate allocation with good fairness and efficiency properties is the Proportional Fairness algorithm (PF), which was analyzed for PSP by Im et al.   ","We drastically improve the analysis of the PF algorithm for both the general PSP and several of its important special cases subject to the objective of minimizing the sum of weighted completion times.","We reduce the upper bound on the competitive ratio from 128 to 27 for general PSP and to 4 for the prominent class of monotone PSP.","For certain heterogeneous machine environments we even close the substantial gap to the lower bound of 2 for non-clairvoyant scheduling.","Our analysis also gives the first polynomial-time improvements over the nearly 30-year-old bounds on the competitive ratio of the doubling framework by Hall, Shmoys, and Wein (SODA 1996) for clairvoyant online preemptive scheduling on unrelated machines.","Somewhat surprisingly, we achieve this improvement by a non-clairvoyant algorithm, thereby demonstrating that non-clairvoyance is not a (significant) hurdle.   ","Our improvements are based on exploiting monotonicity properties of PSP, providing tight dual fitting arguments on structured instances, and showing new additivity properties on the optimal objective value for scheduling on unrelated machines.","Finally, we establish new connections of PF to matching markets, and thereby provide new insights on equilibria and their computational complexity."],"url":"http://arxiv.org/abs/2408.14310v1"}
{"created":"2024-08-26 14:38:19","title":"LLM-3D Print: Large Language Models To Monitor and Control 3D Printing","abstract":"Industry 4.0 has revolutionized manufacturing by driving digitalization and shifting the paradigm toward additive manufacturing (AM). Fused Deposition Modeling (FDM), a key AM technology, enables the creation of highly customized, cost-effective products with minimal material waste through layer-by-layer extrusion, posing a significant challenge to traditional subtractive methods. However, the susceptibility of material extrusion techniques to errors often requires expert intervention to detect and mitigate defects that can severely compromise product quality. While automated error detection and machine learning models exist, their generalizability across diverse 3D printer setups, firmware, and sensors is limited, and deep learning methods require extensive labeled datasets, hindering scalability and adaptability. To address these challenges, we present a process monitoring and control framework that leverages pre-trained Large Language Models (LLMs) alongside 3D printers to detect and address printing defects. The LLM evaluates print quality by analyzing images captured after each layer or print segment, identifying failure modes and querying the printer for relevant parameters. It then generates and executes a corrective action plan. We validated the effectiveness of the proposed framework in identifying defects by comparing it against a control group of engineers with diverse AM expertise. Our evaluation demonstrated that LLM-based agents not only accurately identify common 3D printing errors, such as inconsistent extrusion, stringing, warping, and layer adhesion, but also effectively determine the parameters causing these failures and autonomously correct them without any need for human intervention.","sentences":["Industry 4.0 has revolutionized manufacturing by driving digitalization and shifting the paradigm toward additive manufacturing (AM).","Fused Deposition Modeling (FDM), a key AM technology, enables the creation of highly customized, cost-effective products with minimal material waste through layer-by-layer extrusion, posing a significant challenge to traditional subtractive methods.","However, the susceptibility of material extrusion techniques to errors often requires expert intervention to detect and mitigate defects that can severely compromise product quality.","While automated error detection and machine learning models exist, their generalizability across diverse 3D printer setups, firmware, and sensors is limited, and deep learning methods require extensive labeled datasets, hindering scalability and adaptability.","To address these challenges, we present a process monitoring and control framework that leverages pre-trained Large Language Models (LLMs) alongside 3D printers to detect and address printing defects.","The LLM evaluates print quality by analyzing images captured after each layer or print segment, identifying failure modes and querying the printer for relevant parameters.","It then generates and executes a corrective action plan.","We validated the effectiveness of the proposed framework in identifying defects by comparing it against a control group of engineers with diverse AM expertise.","Our evaluation demonstrated that LLM-based agents not only accurately identify common 3D printing errors, such as inconsistent extrusion, stringing, warping, and layer adhesion, but also effectively determine the parameters causing these failures and autonomously correct them without any need for human intervention."],"url":"http://arxiv.org/abs/2408.14307v1"}
{"created":"2024-08-26 14:37:16","title":"Collaborative XRTactics: A Formative Study on Tactical Communication in Outdoor Team Sports","abstract":"In team sports, effective tactical communication is crucial for success, particularly in the fast-paced and complex environment of outdoor athletics. This paper investigates the challenges faced in transmitting strategic plans to players and explores potential solutions using eXtended Reality (XR) technologies. We conducted a formative study involving interviews with 4 Division I professional soccer coaches, 4 professional players, 2 college club coaches, and 2 college club players, as well as a survey among 17 Division I players. The study identified key requirements for tactical communication tools, including the need for rapid communication, minimal disruption to game flow, reduced cognitive load, clear visualization for all players, and enhanced auditory clarity. Based on these insights, we propose a potential solution - a Mobile Augmented Reality (AR) system designed to address these challenges by providing real-time, intuitive tactical visualization and communication. The system aims to improve strategic planning and execution, thereby enhancing team performance and cohesion. This work represents a significant step towards integrating XR technologies into sports coaching, offering a modern and practical solution for real-time tactical communication.","sentences":["In team sports, effective tactical communication is crucial for success, particularly in the fast-paced and complex environment of outdoor athletics.","This paper investigates the challenges faced in transmitting strategic plans to players and explores potential solutions using eXtended Reality (XR) technologies.","We conducted a formative study involving interviews with 4 Division I professional soccer coaches, 4 professional players, 2 college club coaches, and 2 college club players, as well as a survey among 17 Division I players.","The study identified key requirements for tactical communication tools, including the need for rapid communication, minimal disruption to game flow, reduced cognitive load, clear visualization for all players, and enhanced auditory clarity.","Based on these insights, we propose a potential solution - a Mobile Augmented Reality (AR) system designed to address these challenges by providing real-time, intuitive tactical visualization and communication.","The system aims to improve strategic planning and execution, thereby enhancing team performance and cohesion.","This work represents a significant step towards integrating XR technologies into sports coaching, offering a modern and practical solution for real-time tactical communication."],"url":"http://arxiv.org/abs/2408.14305v1"}
{"created":"2024-08-26 14:30:07","title":"Monotone Arc Diagrams with few Biarcs","abstract":"We show that every planar graph has a monotone topological 2-page book embedding where at most (4n-10)/5 (of potentially 3n-6) edges cross the spine, and every edge crosses the spine at most once; such an edge is called a biarc. We can also guarantee that all edges that cross the spine cross it in the same direction (e.g., from bottom to top). For planar 3-trees we can further improve the bound to (3n-9)/4, and for so-called Kleetopes we obtain a bound of at most (n-8)/3 edges that cross the spine. The bound for Kleetopes is tight, even if the drawing is not required to be monotone. A Kleetope is a plane triangulation that is derived from another plane triangulation T by inserting a new vertex v_f into each face f of T and then connecting v_f to the three vertices of f.","sentences":["We show that every planar graph has a monotone topological 2-page book embedding where at most (4n-10)/5 (of potentially 3n-6) edges cross the spine, and every edge crosses the spine at most once; such an edge is called a biarc.","We can also guarantee that all edges that cross the spine cross it in the same direction (e.g., from bottom to top).","For planar 3-trees we can further improve the bound to (3n-9)/4, and for so-called Kleetopes we obtain a bound of at most (n-8)/3 edges that cross the spine.","The bound for Kleetopes is tight, even if the drawing is not required to be monotone.","A Kleetope is a plane triangulation that is derived from another plane triangulation T by inserting a new vertex v_f","into each face f of T and then connecting v_f to the three vertices of f."],"url":"http://arxiv.org/abs/2408.14299v1"}
{"created":"2024-08-26 14:28:51","title":"Resource Efficient Asynchronous Federated Learning for Digital Twin Empowered IoT Network","abstract":"As an emerging technology, digital twin (DT) can provide real-time status and dynamic topology mapping for Internet of Things (IoT) devices. However, DT and its implementation within industrial IoT networks necessitates substantial, distributed data support, which often leads to ``data silos'' and raises privacy concerns. To address these issues, we develop a dynamic resource scheduling algorithm tailored for the asynchronous federated learning (FL)-based lightweight DT empowered IoT network. Specifically, our approach aims to minimize a multi-objective function that encompasses both energy consumption and latency by optimizing IoT device selection and transmit power control, subject to FL model performance constraints. We utilize the Lyapunov method to decouple the formulated problem into a series of one-slot optimization problems and develop a two-stage optimization algorithm to achieve the optimal transmission power control and IoT device scheduling strategies. In the first stage, we derive closed-form solutions for optimal transmit power on the IoT device side. In the second stage, since partial state information is unknown, e.g., the transmitting power and computational frequency of IoT device, the edge server employs a multi-armed bandit (MAB) framework to model the IoT device selection problem and utilizes an efficient online algorithm, namely the client utility-based upper confidence bound (CU-UCB), to address it. Numerical results validate our algorithm's superiority over benchmark schemes, and simulations demonstrate that our algorithm achieves faster training speeds on the Fashion-MNIST and CIFAR-10 datasets within the same training duration.","sentences":["As an emerging technology, digital twin (DT) can provide real-time status and dynamic topology mapping for Internet of Things (IoT) devices.","However, DT and its implementation within industrial IoT networks necessitates substantial, distributed data support, which often leads to ``data silos'' and raises privacy concerns.","To address these issues, we develop a dynamic resource scheduling algorithm tailored for the asynchronous federated learning (FL)-based lightweight DT empowered IoT network.","Specifically, our approach aims to minimize a multi-objective function that encompasses both energy consumption and latency by optimizing IoT device selection and transmit power control, subject to FL model performance constraints.","We utilize the Lyapunov method to decouple the formulated problem into a series of one-slot optimization problems and develop a two-stage optimization algorithm to achieve the optimal transmission power control and IoT device scheduling strategies.","In the first stage, we derive closed-form solutions for optimal transmit power on the IoT device side.","In the second stage, since partial state information is unknown, e.g., the transmitting power and computational frequency of IoT device, the edge server employs a multi-armed bandit (MAB) framework to model the IoT device selection problem and utilizes an efficient online algorithm, namely the client utility-based upper confidence bound (CU-UCB), to address it.","Numerical results validate our algorithm's superiority over benchmark schemes, and simulations demonstrate that our algorithm achieves faster training speeds on the Fashion-MNIST and CIFAR-10 datasets within the same training duration."],"url":"http://arxiv.org/abs/2408.14298v1"}
{"created":"2024-08-26 14:25:30","title":"Investigating the Effectiveness of Bayesian Spam Filters in Detecting LLM-modified Spam Mails","abstract":"Spam and phishing remain critical threats in cybersecurity, responsible for nearly 90% of security incidents. As these attacks grow in sophistication, the need for robust defensive mechanisms intensifies. Bayesian spam filters, like the widely adopted open-source SpamAssassin, are essential tools in this fight. However, the emergence of large language models (LLMs) such as ChatGPT presents new challenges. These models are not only powerful and accessible, but also inexpensive to use, raising concerns about their misuse in crafting sophisticated spam emails that evade traditional spam filters. This work aims to evaluate the robustness and effectiveness of SpamAssassin against LLM-modified email content. We developed a pipeline to test this vulnerability. Our pipeline modifies spam emails using GPT-3.5 Turbo and assesses SpamAssassin's ability to classify these modified emails correctly. The results show that SpamAssassin misclassified up to 73.7% of LLM-modified spam emails as legitimate. In contrast, a simpler dictionary-replacement attack showed a maximum success rate of only 0.4%. These findings highlight the significant threat posed by LLM-modified spam, especially given the cost-efficiency of such attacks (0.17 cents per email). This paper provides crucial insights into the vulnerabilities of current spam filters and the need for continuous improvement in cybersecurity measures.","sentences":["Spam and phishing remain critical threats in cybersecurity, responsible for nearly 90% of security incidents.","As these attacks grow in sophistication, the need for robust defensive mechanisms intensifies.","Bayesian spam filters, like the widely adopted open-source SpamAssassin, are essential tools in this fight.","However, the emergence of large language models (LLMs) such as ChatGPT presents new challenges.","These models are not only powerful and accessible, but also inexpensive to use, raising concerns about their misuse in crafting sophisticated spam emails that evade traditional spam filters.","This work aims to evaluate the robustness and effectiveness of SpamAssassin against LLM-modified email content.","We developed a pipeline to test this vulnerability.","Our pipeline modifies spam emails using GPT-3.5 Turbo and assesses SpamAssassin's ability to classify these modified emails correctly.","The results show that SpamAssassin misclassified up to 73.7% of LLM-modified spam emails as legitimate.","In contrast, a simpler dictionary-replacement attack showed a maximum success rate of only 0.4%.","These findings highlight the significant threat posed by LLM-modified spam, especially given the cost-efficiency of such attacks (0.17 cents per email).","This paper provides crucial insights into the vulnerabilities of current spam filters and the need for continuous improvement in cybersecurity measures."],"url":"http://arxiv.org/abs/2408.14293v1"}
{"created":"2024-08-26 14:09:40","title":"May the Forgetting Be with You: Alternate Replay for Learning with Noisy Labels","abstract":"Forgetting presents a significant challenge during incremental training, making it particularly demanding for contemporary AI systems to assimilate new knowledge in streaming data environments. To address this issue, most approaches in Continual Learning (CL) rely on the replay of a restricted buffer of past data. However, the presence of noise in real-world scenarios, where human annotation is constrained by time limitations or where data is automatically gathered from the web, frequently renders these strategies vulnerable. In this study, we address the problem of CL under Noisy Labels (CLN) by introducing Alternate Experience Replay (AER), which takes advantage of forgetting to maintain a clear distinction between clean, complex, and noisy samples in the memory buffer. The idea is that complex or mislabeled examples, which hardly fit the previously learned data distribution, are most likely to be forgotten. To grasp the benefits of such a separation, we equip AER with Asymmetric Balanced Sampling (ABS): a new sample selection strategy that prioritizes purity on the current task while retaining relevant samples from the past. Through extensive computational comparisons, we demonstrate the effectiveness of our approach in terms of both accuracy and purity of the obtained buffer, resulting in a remarkable average gain of 4.71% points in accuracy with respect to existing loss-based purification strategies. Code is available at https://github.com/aimagelab/mammoth.","sentences":["Forgetting presents a significant challenge during incremental training, making it particularly demanding for contemporary AI systems to assimilate new knowledge in streaming data environments.","To address this issue, most approaches in Continual Learning (CL) rely on the replay of a restricted buffer of past data.","However, the presence of noise in real-world scenarios, where human annotation is constrained by time limitations or where data is automatically gathered from the web, frequently renders these strategies vulnerable.","In this study, we address the problem of CL under Noisy Labels (CLN) by introducing Alternate Experience Replay (AER), which takes advantage of forgetting to maintain a clear distinction between clean, complex, and noisy samples in the memory buffer.","The idea is that complex or mislabeled examples, which hardly fit the previously learned data distribution, are most likely to be forgotten.","To grasp the benefits of such a separation, we equip AER with Asymmetric Balanced Sampling (ABS): a new sample selection strategy that prioritizes purity on the current task while retaining relevant samples from the past.","Through extensive computational comparisons, we demonstrate the effectiveness of our approach in terms of both accuracy and purity of the obtained buffer, resulting in a remarkable average gain of 4.71% points in accuracy with respect to existing loss-based purification strategies.","Code is available at https://github.com/aimagelab/mammoth."],"url":"http://arxiv.org/abs/2408.14284v1"}
{"created":"2024-08-26 14:09:28","title":"Predictability and Causality in Spanish and English Natural Language Generation","abstract":"In recent years, the field of Natural Language Generation (NLG) has been boosted by the recent advances in deep learning technologies. Nonetheless, these new data-intensive methods introduce language-dependent disparities in NLG as the main training data sets are in English. Also, most neural NLG systems use decoder-only (causal) transformer language models, which work well for English, but were not designed with other languages in mind. In this work we depart from the hypothesis that they may introduce generation bias in target languages with less rigid word ordering, subject omission, or different attachment preferences for relative clauses, so that for these target languages other language generation strategies may be more desirable. This paper first compares causal and non-causal language modeling for English and Spanish, two languages with different grammatical structures and over 1.5 billion and 0.5 billion speakers, respectively. For this purpose, we define a novel metric of average causal and non-causal context-conditioned entropy of the grammatical category distribution for both languages as an information-theoretic a priori approach. The evaluation of natural text sources (such as training data) in both languages reveals lower average non-causal conditional entropy in Spanish and lower causal conditional entropy in English. According to this experiment, Spanish is more predictable than English given a non-causal context. Then, by applying a conditional relative entropy metric to text generation experiments, we obtain as insights that the best performance is respectively achieved with causal NLG in English, and with non-causal NLG in Spanish. These insights support further research in NLG in Spanish using bidirectional transformer language models.","sentences":["In recent years, the field of Natural Language Generation (NLG) has been boosted by the recent advances in deep learning technologies.","Nonetheless, these new data-intensive methods introduce language-dependent disparities in NLG as the main training data sets are in English.","Also, most neural NLG systems use decoder-only (causal) transformer language models, which work well for English, but were not designed with other languages in mind.","In this work we depart from the hypothesis that they may introduce generation bias in target languages with less rigid word ordering, subject omission, or different attachment preferences for relative clauses, so that for these target languages other language generation strategies may be more desirable.","This paper first compares causal and non-causal language modeling for English and Spanish, two languages with different grammatical structures and over 1.5 billion and 0.5 billion speakers, respectively.","For this purpose, we define a novel metric of average causal and non-causal context-conditioned entropy of the grammatical category distribution for both languages as an information-theoretic a priori approach.","The evaluation of natural text sources (such as training data) in both languages reveals lower average non-causal conditional entropy in Spanish and lower causal conditional entropy in English.","According to this experiment, Spanish is more predictable than English given a non-causal context.","Then, by applying a conditional relative entropy metric to text generation experiments, we obtain as insights that the best performance is respectively achieved with causal NLG in English, and with non-causal NLG in Spanish.","These insights support further research in NLG in Spanish using bidirectional transformer language models."],"url":"http://arxiv.org/abs/2408.14283v1"}
{"created":"2024-08-26 14:02:30","title":"Uncertainties of Latent Representations in Computer Vision","abstract":"Uncertainty quantification is a key pillar of trustworthy machine learning. It enables safe reactions under unsafe inputs, like predicting only when the machine learning model detects sufficient evidence, discarding anomalous data, or emitting warnings when an error is likely to be inbound. This is particularly crucial in safety-critical areas like medical image classification or self-driving cars. Despite the plethora of proposed uncertainty quantification methods achieving increasingly higher scores on performance benchmarks, uncertainty estimates are often shied away from in practice. Many machine learning projects start from pretrained latent representations that come without uncertainty estimates. Uncertainties would need to be trained by practitioners on their own, which is notoriously difficult and resource-intense.   This thesis makes uncertainty estimates easily accessible by adding them to the latent representation vectors of pretrained computer vision models. Besides proposing approaches rooted in probability and decision theory, such as Monte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both theoretical and empirical questions. We show that these unobservable uncertainties about unobservable latent representations are indeed provably correct. We also provide an uncertainty-aware representation learning (URL) benchmark to compare these unobservables against observable ground-truths. Finally, we compile our findings to pretrain lightweight representation uncertainties on large-scale computer vision models that transfer to unseen datasets in a zero-shot manner.   Our findings do not only advance the current theoretical understanding of uncertainties over latent variables, but also facilitate the access to uncertainty quantification for future researchers inside and outside the field, enabling straightforward but trustworthy machine learning.","sentences":["Uncertainty quantification is a key pillar of trustworthy machine learning.","It enables safe reactions under unsafe inputs, like predicting only when the machine learning model detects sufficient evidence, discarding anomalous data, or emitting warnings when an error is likely to be inbound.","This is particularly crucial in safety-critical areas like medical image classification or self-driving cars.","Despite the plethora of proposed uncertainty quantification methods achieving increasingly higher scores on performance benchmarks, uncertainty estimates are often shied away from in practice.","Many machine learning projects start from pretrained latent representations that come without uncertainty estimates.","Uncertainties would need to be trained by practitioners on their own, which is notoriously difficult and resource-intense.   ","This thesis makes uncertainty estimates easily accessible by adding them to the latent representation vectors of pretrained computer vision models.","Besides proposing approaches rooted in probability and decision theory, such as Monte-Carlo InfoNCE (MCInfoNCE) and loss prediction, we delve into both theoretical and empirical questions.","We show that these unobservable uncertainties about unobservable latent representations are indeed provably correct.","We also provide an uncertainty-aware representation learning (URL) benchmark to compare these unobservables against observable ground-truths.","Finally, we compile our findings to pretrain lightweight representation uncertainties on large-scale computer vision models that transfer to unseen datasets in a zero-shot manner.   ","Our findings do not only advance the current theoretical understanding of uncertainties over latent variables, but also facilitate the access to uncertainty quantification for future researchers inside and outside the field, enabling straightforward but trustworthy machine learning."],"url":"http://arxiv.org/abs/2408.14281v1"}
{"created":"2024-08-26 13:55:42","title":"Learning Local Pattern Modularization for Point Cloud Reconstruction from Unseen Classes","abstract":"It is challenging to reconstruct 3D point clouds in unseen classes from single 2D images. Instead of object-centered coordinate system, current methods generalized global priors learned in seen classes to reconstruct 3D shapes from unseen classes in viewer-centered coordinate system. However, the reconstruction accuracy and interpretability are still eager to get improved. To resolve this issue, we introduce to learn local pattern modularization for reconstructing 3D shapes in unseen classes, which achieves both good generalization ability and high reconstruction accuracy. Our insight is to learn a local prior which is class-agnostic and easy to generalize in object-centered coordinate system. Specifically, the local prior is learned via a process of learning and customizing local pattern modularization in seen classes. During this process, we first learn a set of patterns in local regions, which is the basis in the object-centered coordinate system to represent an arbitrary region on shapes across different classes. Then, we modularize each region on an initially reconstructed shape using the learned local patterns. Based on that, we customize the local pattern modularization using the input image by refining the reconstruction with more details. Our method enables to reconstruct high fidelity point clouds from unseen classes in object-centered coordinate system without requiring a large number of patterns or any additional information, such as segmentation supervision or camera poses. Our experimental results under widely used benchmarks show that our method achieves the state-of-the-art reconstruction accuracy for shapes from unseen classes. The code is available at https://github.com/chenchao15/Unseen.","sentences":["It is challenging to reconstruct 3D point clouds in unseen classes from single 2D images.","Instead of object-centered coordinate system, current methods generalized global priors learned in seen classes to reconstruct 3D shapes from unseen classes in viewer-centered coordinate system.","However, the reconstruction accuracy and interpretability are still eager to get improved.","To resolve this issue, we introduce to learn local pattern modularization for reconstructing 3D shapes in unseen classes, which achieves both good generalization ability and high reconstruction accuracy.","Our insight is to learn a local prior which is class-agnostic and easy to generalize in object-centered coordinate system.","Specifically, the local prior is learned via a process of learning and customizing local pattern modularization in seen classes.","During this process, we first learn a set of patterns in local regions, which is the basis in the object-centered coordinate system to represent an arbitrary region on shapes across different classes.","Then, we modularize each region on an initially reconstructed shape using the learned local patterns.","Based on that, we customize the local pattern modularization using the input image by refining the reconstruction with more details.","Our method enables to reconstruct high fidelity point clouds from unseen classes in object-centered coordinate system without requiring a large number of patterns or any additional information, such as segmentation supervision or camera poses.","Our experimental results under widely used benchmarks show that our method achieves the state-of-the-art reconstruction accuracy for shapes from unseen classes.","The code is available at https://github.com/chenchao15/Unseen."],"url":"http://arxiv.org/abs/2408.14279v1"}
{"created":"2024-08-26 13:53:04","title":"Epidemic Information Extraction for Event-Based Surveillance using Large Language Models","abstract":"This paper presents a novel approach to epidemic surveillance, leveraging the power of Artificial Intelligence and Large Language Models (LLMs) for effective interpretation of unstructured big data sources, like the popular ProMED and WHO Disease Outbreak News. We explore several LLMs, evaluating their capabilities in extracting valuable epidemic information. We further enhance the capabilities of the LLMs using in-context learning, and test the performance of an ensemble model incorporating multiple open-source LLMs. The findings indicate that LLMs can significantly enhance the accuracy and timeliness of epidemic modelling and forecasting, offering a promising tool for managing future pandemic events.","sentences":["This paper presents a novel approach to epidemic surveillance, leveraging the power of Artificial Intelligence and Large Language Models (LLMs) for effective interpretation of unstructured big data sources, like the popular ProMED and WHO Disease Outbreak News.","We explore several LLMs, evaluating their capabilities in extracting valuable epidemic information.","We further enhance the capabilities of the LLMs using in-context learning, and test the performance of an ensemble model incorporating multiple open-source LLMs.","The findings indicate that LLMs can significantly enhance the accuracy and timeliness of epidemic modelling and forecasting, offering a promising tool for managing future pandemic events."],"url":"http://arxiv.org/abs/2408.14277v1"}
{"created":"2024-08-26 13:46:48","title":"Trust, but Verify: Evaluating Developer Behavior in Mitigating Security Vulnerabilities in Open-Source Software Projects","abstract":"This study investigates vulnerabilities in dependencies of sampled open-source software (OSS) projects, the relationship between these and overall project security, and how developers' behaviors and practices influence their mitigation. Through analysis of OSS projects, we have identified common issues in outdated or unmaintained dependencies, including pointer dereferences and array bounds violations, that pose significant security risks. We have also examined developer responses to formal verifier reports, noting a tendency to dismiss potential issues as false positives, which can lead to overlooked vulnerabilities. Our results suggest that reducing the number of direct dependencies and prioritizing well-established libraries with strong security records are effective strategies for enhancing the software security landscape. Notably, four vulnerabilities were fixed as a result of this study, demonstrating the effectiveness of our mitigation strategies.","sentences":["This study investigates vulnerabilities in dependencies of sampled open-source software (OSS) projects, the relationship between these and overall project security, and how developers' behaviors and practices influence their mitigation.","Through analysis of OSS projects, we have identified common issues in outdated or unmaintained dependencies, including pointer dereferences and array bounds violations, that pose significant security risks.","We have also examined developer responses to formal verifier reports, noting a tendency to dismiss potential issues as false positives, which can lead to overlooked vulnerabilities.","Our results suggest that reducing the number of direct dependencies and prioritizing well-established libraries with strong security records are effective strategies for enhancing the software security landscape.","Notably, four vulnerabilities were fixed as a result of this study, demonstrating the effectiveness of our mitigation strategies."],"url":"http://arxiv.org/abs/2408.14273v1"}
