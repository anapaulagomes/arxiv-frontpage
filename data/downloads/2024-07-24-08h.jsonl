{"created":"2024-07-23 17:59:59","title":"Diffusion Models for Monocular Depth Estimation: Overcoming Challenging Conditions","abstract":"We present a novel approach designed to address the complexities posed by challenging, out-of-distribution data in the single-image depth estimation task. Starting with images that facilitate depth prediction due to the absence of unfavorable factors, we systematically generate new, user-defined scenes with a comprehensive set of challenges and associated depth information. This is achieved by leveraging cutting-edge text-to-image diffusion models with depth-aware control, known for synthesizing high-quality image content from textual prompts while preserving the coherence of 3D structure between generated and source imagery. Subsequent fine-tuning of any monocular depth network is carried out through a self-distillation protocol that takes into account images generated using our strategy and its own depth predictions on simple, unchallenging scenes. Experiments on benchmarks tailored for our purposes demonstrate the effectiveness and versatility of our proposal.","sentences":["We present a novel approach designed to address the complexities posed by challenging, out-of-distribution data in the single-image depth estimation task.","Starting with images that facilitate depth prediction due to the absence of unfavorable factors, we systematically generate new, user-defined scenes with a comprehensive set of challenges and associated depth information.","This is achieved by leveraging cutting-edge text-to-image diffusion models with depth-aware control, known for synthesizing high-quality image content from textual prompts while preserving the coherence of 3D structure between generated and source imagery.","Subsequent fine-tuning of any monocular depth network is carried out through a self-distillation protocol that takes into account images generated using our strategy and its own depth predictions on simple, unchallenging scenes.","Experiments on benchmarks tailored for our purposes demonstrate the effectiveness and versatility of our proposal."],"url":"http://arxiv.org/abs/2407.16698v1"}
{"created":"2024-07-23 17:59:44","title":"AbdomenAtlas: A Large-Scale, Detailed-Annotated, & Multi-Center Dataset for Efficient Transfer Learning and Open Algorithmic Benchmarking","abstract":"We introduce the largest abdominal CT dataset (termed AbdomenAtlas) of 20,460 three-dimensional CT volumes sourced from 112 hospitals across diverse populations, geographies, and facilities. AbdomenAtlas provides 673K high-quality masks of anatomical structures in the abdominal region annotated by a team of 10 radiologists with the help of AI algorithms. We start by having expert radiologists manually annotate 22 anatomical structures in 5,246 CT volumes. Following this, a semi-automatic annotation procedure is performed for the remaining CT volumes, where radiologists revise the annotations predicted by AI, and in turn, AI improves its predictions by learning from revised annotations. Such a large-scale, detailed-annotated, and multi-center dataset is needed for two reasons. Firstly, AbdomenAtlas provides important resources for AI development at scale, branded as large pre-trained models, which can alleviate the annotation workload of expert radiologists to transfer to broader clinical applications. Secondly, AbdomenAtlas establishes a large-scale benchmark for evaluating AI algorithms -- the more data we use to test the algorithms, the better we can guarantee reliable performance in complex clinical scenarios. An ISBI & MICCAI challenge named BodyMaps: Towards 3D Atlas of Human Body was launched using a subset of our AbdomenAtlas, aiming to stimulate AI innovation and to benchmark segmentation accuracy, inference efficiency, and domain generalizability. We hope our AbdomenAtlas can set the stage for larger-scale clinical trials and offer exceptional opportunities to practitioners in the medical imaging community. Codes, models, and datasets are available at https://www.zongweiz.com/dataset","sentences":["We introduce the largest abdominal CT dataset (termed AbdomenAtlas) of 20,460 three-dimensional CT volumes sourced from 112 hospitals across diverse populations, geographies, and facilities.","AbdomenAtlas provides 673K high-quality masks of anatomical structures in the abdominal region annotated by a team of 10 radiologists with the help of AI algorithms.","We start by having expert radiologists manually annotate 22 anatomical structures in 5,246 CT volumes.","Following this, a semi-automatic annotation procedure is performed for the remaining CT volumes, where radiologists revise the annotations predicted by AI, and in turn, AI improves its predictions by learning from revised annotations.","Such a large-scale, detailed-annotated, and multi-center dataset is needed for two reasons.","Firstly, AbdomenAtlas provides important resources for AI development at scale, branded as large pre-trained models, which can alleviate the annotation workload of expert radiologists to transfer to broader clinical applications.","Secondly, AbdomenAtlas establishes a large-scale benchmark for evaluating AI algorithms -- the more data we use to test the algorithms, the better we can guarantee reliable performance in complex clinical scenarios.","An ISBI & MICCAI challenge named BodyMaps: Towards 3D Atlas of Human Body was launched using a subset of our AbdomenAtlas, aiming to stimulate AI innovation and to benchmark segmentation accuracy, inference efficiency, and domain generalizability.","We hope our AbdomenAtlas can set the stage for larger-scale clinical trials and offer exceptional opportunities to practitioners in the medical imaging community.","Codes, models, and datasets are available at https://www.zongweiz.com/dataset"],"url":"http://arxiv.org/abs/2407.16697v1"}
{"created":"2024-07-23 17:58:26","title":"PartGLEE: A Foundation Model for Recognizing and Parsing Any Objects","abstract":"We present PartGLEE, a part-level foundation model for locating and identifying both objects and parts in images. Through a unified framework, PartGLEE accomplishes detection, segmentation, and grounding of instances at any granularity in the open world scenario. Specifically, we propose a Q-Former to construct the hierarchical relationship between objects and parts, parsing every object into corresponding semantic parts. By incorporating a large amount of object-level data, the hierarchical relationships can be extended, enabling PartGLEE to recognize a rich variety of parts. We conduct comprehensive studies to validate the effectiveness of our method, PartGLEE achieves the state-of-the-art performance across various part-level tasks and obtain competitive results on object-level tasks. The proposed PartGLEE significantly enhances hierarchical modeling capabilities and part-level perception over our previous GLEE model. Further analysis indicates that the hierarchical cognitive ability of PartGLEE is able to facilitate a detailed comprehension in images for mLLMs. The model and code will be released at https://provencestar.github.io/PartGLEE-Vision/ .","sentences":["We present PartGLEE, a part-level foundation model for locating and identifying both objects and parts in images.","Through a unified framework, PartGLEE accomplishes detection, segmentation, and grounding of instances at any granularity in the open world scenario.","Specifically, we propose a Q-Former to construct the hierarchical relationship between objects and parts, parsing every object into corresponding semantic parts.","By incorporating a large amount of object-level data, the hierarchical relationships can be extended, enabling PartGLEE to recognize a rich variety of parts.","We conduct comprehensive studies to validate the effectiveness of our method, PartGLEE achieves the state-of-the-art performance across various part-level tasks and obtain competitive results on object-level tasks.","The proposed PartGLEE significantly enhances hierarchical modeling capabilities and part-level perception over our previous GLEE model.","Further analysis indicates that the hierarchical cognitive ability of PartGLEE is able to facilitate a detailed comprehension in images for mLLMs.","The model and code will be released at https://provencestar.github.io/PartGLEE-Vision/ ."],"url":"http://arxiv.org/abs/2407.16696v1"}
{"created":"2024-07-23 17:57:41","title":"Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack","abstract":"We introduce Lifelong ICL, a problem setting that challenges long-context language models (LMs) to learn from a sequence of language tasks through in-context learning (ICL). We further introduce Task Haystack, an evaluation suite dedicated to assessing and diagnosing how long-context LMs utilizes contexts in Lifelong ICL. When given a task instruction and test inputs, long-context LMs are expected to leverage the relevant demonstrations in the Lifelong ICL prompt, avoid distraction and interference from other tasks, and achieve test accuracies that are not significantly worse than the Single-task ICL baseline.   Task Haystack draws inspiration from the widely-adopted \"needle-in-a-haystack\" (NIAH) evaluation, but presents new and unique challenges. It demands that models (1) utilize the contexts with deeper understanding, rather than resorting to simple copying and pasting; (2) navigate through long streams of evolving topics and tasks, which closely approximates the complexities of real-world usage of long-context LMs. Additionally, Task Haystack inherits the controllability aspect of NIAH, providing model developers with tools and visualizations to identify model vulnerabilities effectively.   We benchmark 12 long-context LMs using Task Haystack. We find that state-of-the-art closed models such as GPT-4o still struggle in this setting, failing 15% of the cases on average, while all open-weight models we evaluate further lack behind by a large margin, failing up to 61% of the cases. In our controlled analysis, we identify factors such as distraction and recency bias as contributors to these failure cases. Further, we observe declines in performance when task instructions are paraphrased at test time or when ICL demonstrations are repeated excessively, raising concerns about the robustness, instruction understanding, and true context utilization of current long-context LMs.","sentences":["We introduce Lifelong ICL, a problem setting that challenges long-context language models (LMs) to learn from a sequence of language tasks through in-context learning (ICL).","We further introduce Task Haystack, an evaluation suite dedicated to assessing and diagnosing how long-context LMs utilizes contexts in Lifelong ICL.","When given a task instruction and test inputs, long-context LMs are expected to leverage the relevant demonstrations in the Lifelong ICL prompt, avoid distraction and interference from other tasks, and achieve test accuracies that are not significantly worse than the Single-task ICL baseline.   ","Task Haystack draws inspiration from the widely-adopted \"needle-in-a-haystack\" (NIAH) evaluation, but presents new and unique challenges.","It demands that models (1) utilize the contexts with deeper understanding, rather than resorting to simple copying and pasting; (2) navigate through long streams of evolving topics and tasks, which closely approximates the complexities of real-world usage of long-context LMs.","Additionally, Task Haystack inherits the controllability aspect of NIAH, providing model developers with tools and visualizations to identify model vulnerabilities effectively.   ","We benchmark 12 long-context LMs using Task Haystack.","We find that state-of-the-art closed models such as GPT-4o still struggle in this setting, failing 15% of the cases on average, while all open-weight models we evaluate further lack behind by a large margin, failing up to 61% of the cases.","In our controlled analysis, we identify factors such as distraction and recency bias as contributors to these failure cases.","Further, we observe declines in performance when task instructions are paraphrased at test time or when ICL demonstrations are repeated excessively, raising concerns about the robustness, instruction understanding, and true context utilization of current long-context LMs."],"url":"http://arxiv.org/abs/2407.16695v1"}
{"created":"2024-07-23 17:57:36","title":"Aster: Fixing the Android TEE Ecosystem with Arm CCA","abstract":"The Android ecosystem relies on either TrustZone (e.g., OP-TEE, QTEE, Trusty) or trusted hypervisors (pKVM, Gunyah) to isolate security-sensitive services from malicious apps and Android bugs. TrustZone allows any secure world code to access the normal world that runs Android. Similarly, a trusted hypervisor has full access to Android running in one VM and security services in other VMs. In this paper, we motivate the need for mutual isolation, wherein Android, hypervisors, and the secure world are isolated from each other. Then, we propose a sandboxed service abstraction, such that a sandboxed execution cannot access any other sandbox, Android, hypervisor, or secure world memory. We present Aster which achieves these goals while ensuring that sandboxed execution can still communicate with Android to get inputs and provide outputs securely. Our main insight is to leverage the hardware isolation offered by Arm Confidential Computing Architecture (CCA). However, since CCA does not satisfy our sandboxing and mutual isolation requirements, Aster repurposes its hardware enforcement to meet its goals while addressing challenges such as secure interfaces, virtio, and protection against interrupts. We implement Aster to demonstrate its feasibility and assess its compatibility. We take three case studies, including one currently deployed on Android phones and insufficiently secured using a trusted hypervisor, to demonstrate that they can be protected by Aster.","sentences":["The Android ecosystem relies on either TrustZone (e.g., OP-TEE, QTEE, Trusty) or trusted hypervisors (pKVM, Gunyah) to isolate security-sensitive services from malicious apps and Android bugs.","TrustZone allows any secure world code to access the normal world that runs Android.","Similarly, a trusted hypervisor has full access to Android running in one VM and security services in other VMs.","In this paper, we motivate the need for mutual isolation, wherein Android, hypervisors, and the secure world are isolated from each other.","Then, we propose a sandboxed service abstraction, such that a sandboxed execution cannot access any other sandbox, Android, hypervisor, or secure world memory.","We present Aster which achieves these goals while ensuring that sandboxed execution can still communicate with Android to get inputs and provide outputs securely.","Our main insight is to leverage the hardware isolation offered by Arm Confidential Computing Architecture (CCA).","However, since CCA does not satisfy our sandboxing and mutual isolation requirements, Aster repurposes its hardware enforcement to meet its goals while addressing challenges such as secure interfaces, virtio, and protection against interrupts.","We implement Aster to demonstrate its feasibility and assess its compatibility.","We take three case studies, including one currently deployed on Android phones and insufficiently secured using a trusted hypervisor, to demonstrate that they can be protected by Aster."],"url":"http://arxiv.org/abs/2407.16694v1"}
{"created":"2024-07-23 17:56:32","title":"Explanation Regularisation through the Lens of Attributions","abstract":"Explanation regularisation (ER) has been introduced as a way to guide models to make their predictions in a manner more akin to humans, i.e., making their attributions \"plausible\". This is achieved by introducing an auxiliary explanation loss, that measures how well the output of an input attribution technique for the model agrees with relevant human-annotated rationales. One positive outcome of using ER appears to be improved performance in out-of-domain (OOD) settings, presumably due to an increased reliance on \"plausible\" tokens. However, previous work has under-explored the impact of the ER objective on model attributions, in particular when obtained with techniques other than the one used to train ER. In this work, we contribute a study of ER's effectiveness at informing classification decisions on plausible tokens, and the relationship between increased plausibility and robustness to OOD conditions. Through a series of analyses, we find that the connection between ER and the ability of a classifier to rely on plausible features has been overstated and that a stronger reliance on plausible tokens does not seem to be the cause for any perceived OOD improvements.","sentences":["Explanation regularisation (ER) has been introduced as a way to guide models to make their predictions in a manner more akin to humans, i.e., making their attributions \"plausible\".","This is achieved by introducing an auxiliary explanation loss, that measures how well the output of an input attribution technique for the model agrees with relevant human-annotated rationales.","One positive outcome of using ER appears to be improved performance in out-of-domain (OOD) settings, presumably due to an increased reliance on \"plausible\" tokens.","However, previous work has under-explored the impact of the ER objective on model attributions, in particular when obtained with techniques other than the one used to train ER.","In this work, we contribute a study of ER's effectiveness at informing classification decisions on plausible tokens, and the relationship between increased plausibility and robustness to OOD conditions.","Through a series of analyses, we find that the connection between ER and the ability of a classifier to rely on plausible features has been overstated and that a stronger reliance on plausible tokens does not seem to be the cause for any perceived OOD improvements."],"url":"http://arxiv.org/abs/2407.16693v1"}
{"created":"2024-07-23 17:50:45","title":"Can Large Language Models Automatically Jailbreak GPT-4V?","abstract":"GPT-4V has attracted considerable attention due to its extraordinary capacity for integrating and processing multimodal information. At the same time, its ability of face recognition raises new safety concerns of privacy leakage. Despite researchers' efforts in safety alignment through RLHF or preprocessing filters, vulnerabilities might still be exploited. In our study, we introduce AutoJailbreak, an innovative automatic jailbreak technique inspired by prompt optimization. We leverage Large Language Models (LLMs) for red-teaming to refine the jailbreak prompt and employ weak-to-strong in-context learning prompts to boost efficiency. Furthermore, we present an effective search method that incorporates early stopping to minimize optimization time and token expenditure. Our experiments demonstrate that AutoJailbreak significantly surpasses conventional methods, achieving an Attack Success Rate (ASR) exceeding 95.3\\%. This research sheds light on strengthening GPT-4V security, underscoring the potential for LLMs to be exploited in compromising GPT-4V integrity.","sentences":["GPT-4V has attracted considerable attention due to its extraordinary capacity for integrating and processing multimodal information.","At the same time, its ability of face recognition raises new safety concerns of privacy leakage.","Despite researchers' efforts in safety alignment through RLHF or preprocessing filters, vulnerabilities might still be exploited.","In our study, we introduce AutoJailbreak, an innovative automatic jailbreak technique inspired by prompt optimization.","We leverage Large Language Models (LLMs) for red-teaming to refine the jailbreak prompt and employ weak-to-strong in-context learning prompts to boost efficiency.","Furthermore, we present an effective search method that incorporates early stopping to minimize optimization time and token expenditure.","Our experiments demonstrate that AutoJailbreak significantly surpasses conventional methods, achieving an Attack Success Rate (ASR) exceeding 95.3\\%.","This research sheds light on strengthening GPT-4V security, underscoring the potential for LLMs to be exploited in compromising GPT-4V integrity."],"url":"http://arxiv.org/abs/2407.16686v1"}
{"created":"2024-07-23 17:48:17","title":"G\u00f6del logics: Prenex fragments","abstract":"In this paper, we provide a complete classification for the first-order G\\\"odel logics concerning the property that the formulas admit logically equivalent prenex normal forms. We show that the only first-order G\\\"odel logics that admit such prenex forms are those with finite truth value sets since they allow all quantifier-shift rules and the logic \\(G_\\uparrow\\) with only one accumulation point at $1$ in the infinite truth values set. In all the other cases, there are generally no logically equivalent prenex normal forms. We will also see that \\(G_\\uparrow\\) is the intersection of all finite first-order G\\\"odel logics.\\\\ The second part of this paper investigates the existence of effective equivalence between the validity of a formula and the validity of some prenex normal form. The existence of such a normal form is obvious for finite valued G\\\"odel logic and \\(G_\\uparrow\\). G\\\"odel logics with an uncountable truth value set admit the prenex normal forms if and only if every surrounding of \\(0\\) is uncountable or \\(0\\) is an isolated point. Otherwise, uncountable G\\\"odel logics are not recursively enumerable, however, the prenex fragment is always recursively enumerable. Therefore, there is no effective translation between the valid formula and the valid prenex normal form. However, the existence of effectively constructible validity equivalent prenex forms for the countable case is still up for debate.","sentences":["In this paper, we provide a complete classification for the first-order G\\\"odel logics concerning the property that the formulas admit logically equivalent prenex normal forms.","We show that the only first-order G\\\"odel logics that admit such prenex forms are those with finite truth value sets since they allow all quantifier-shift rules and the logic \\(G_\\uparrow\\) with only one accumulation point at $1$ in the infinite truth values set.","In all the other cases, there are generally no logically equivalent prenex normal forms.","We will also see that \\(G_\\uparrow\\) is the intersection of all finite first-order G\\\"odel logics.\\\\ The second part of this paper investigates the existence of effective equivalence between the validity of a formula and the validity of some prenex normal form.","The existence of such a normal form is obvious for finite valued G\\\"odel logic and \\(G_\\uparrow\\).","G\\\"odel logics with an uncountable truth value set admit the prenex normal forms if and only if every surrounding of \\(0\\) is uncountable or \\(0\\) is an isolated point.","Otherwise, uncountable G\\\"odel logics are not recursively enumerable, however, the prenex fragment is always recursively enumerable.","Therefore, there is no effective translation between the valid formula and the valid prenex normal form.","However, the existence of effectively constructible validity equivalent prenex forms for the countable case is still up for debate."],"url":"http://arxiv.org/abs/2407.16683v1"}
{"created":"2024-07-23 17:47:25","title":"SAM-CP: Marrying SAM with Composable Prompts for Versatile Segmentation","abstract":"The Segment Anything model (SAM) has shown a generalized ability to group image pixels into patches, but applying it to semantic-aware segmentation still faces major challenges. This paper presents SAM-CP, a simple approach that establishes two types of composable prompts beyond SAM and composes them for versatile segmentation. Specifically, given a set of classes (in texts) and a set of SAM patches, the Type-I prompt judges whether a SAM patch aligns with a text label, and the Type-II prompt judges whether two SAM patches with the same text label also belong to the same instance. To decrease the complexity in dealing with a large number of semantic classes and patches, we establish a unified framework that calculates the affinity between (semantic and instance) queries and SAM patches and merges patches with high affinity to the query. Experiments show that SAM-CP achieves semantic, instance, and panoptic segmentation in both open and closed domains. In particular, it achieves state-of-the-art performance in open-vocabulary segmentation. Our research offers a novel and generalized methodology for equipping vision foundation models like SAM with multi-grained semantic perception abilities.","sentences":["The Segment Anything model (SAM) has shown a generalized ability to group image pixels into patches, but applying it to semantic-aware segmentation still faces major challenges.","This paper presents SAM-CP, a simple approach that establishes two types of composable prompts beyond SAM and composes them for versatile segmentation.","Specifically, given a set of classes (in texts) and a set of SAM patches, the Type-I prompt judges whether a SAM patch aligns with a text label, and the Type-II prompt judges whether two SAM patches with the same text label also belong to the same instance.","To decrease the complexity in dealing with a large number of semantic classes and patches, we establish a unified framework that calculates the affinity between (semantic and instance) queries and SAM patches and merges patches with high affinity to the query.","Experiments show that SAM-CP achieves semantic, instance, and panoptic segmentation in both open and closed domains.","In particular, it achieves state-of-the-art performance in open-vocabulary segmentation.","Our research offers a novel and generalized methodology for equipping vision foundation models like SAM with multi-grained semantic perception abilities."],"url":"http://arxiv.org/abs/2407.16682v1"}
{"created":"2024-07-23 17:45:16","title":"A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data","abstract":"Despite the availability of international prize-money competitions, scaled vehicles, and simulation environments, research on autonomous racing and the control of sports cars operating close to the limit of handling has been limited by the high costs of vehicle acquisition and management, as well as the limited physics accuracy of open-source simulators. In this paper, we propose a racing simulation platform based on the simulator Assetto Corsa to test, validate, and benchmark autonomous driving algorithms, including reinforcement learning (RL) and classical Model Predictive Control (MPC), in realistic and challenging scenarios. Our contributions include the development of this simulation platform, several state-of-the-art algorithms tailored to the racing environment, and a comprehensive dataset collected from human drivers. Additionally, we evaluate algorithms in the offline RL setting. All the necessary code (including environment and benchmarks), working examples, datasets, and videos are publicly released and can be found at: \\url{https://assetto-corsa-gym.github.io}.","sentences":["Despite the availability of international prize-money competitions, scaled vehicles, and simulation environments, research on autonomous racing and the control of sports cars operating close to the limit of handling has been limited by the high costs of vehicle acquisition and management, as well as the limited physics accuracy of open-source simulators.","In this paper, we propose a racing simulation platform based on the simulator Assetto Corsa to test, validate, and benchmark autonomous driving algorithms, including reinforcement learning (RL) and classical Model Predictive Control (MPC), in realistic and challenging scenarios.","Our contributions include the development of this simulation platform, several state-of-the-art algorithms tailored to the racing environment, and a comprehensive dataset collected from human drivers.","Additionally, we evaluate algorithms in the offline RL setting.","All the necessary code (including environment and benchmarks), working examples, datasets, and videos are publicly released and can be found at: \\url{https://assetto-corsa-gym.github.io}."],"url":"http://arxiv.org/abs/2407.16680v1"}
{"created":"2024-07-23 17:44:54","title":"From Imitation to Refinement -- Residual RL for Precise Visual Assembly","abstract":"Behavior cloning (BC) currently stands as a dominant paradigm for learning real-world visual manipulation. However, in tasks that require locally corrective behaviors like multi-part assembly, learning robust policies purely from human demonstrations remains challenging. Reinforcement learning (RL) can mitigate these limitations by allowing policies to acquire locally corrective behaviors through task reward supervision and exploration. This paper explores the use of RL fine-tuning to improve upon BC-trained policies in precise manipulation tasks. We analyze and overcome technical challenges associated with using RL to directly train policy networks that incorporate modern architectural components like diffusion models and action chunking. We propose training residual policies on top of frozen BC-trained diffusion models using standard policy gradient methods and sparse rewards, an approach we call ResiP (Residual for Precise manipulation). Our experimental results demonstrate that this residual learning framework can significantly improve success rates beyond the base BC-trained models in high-precision assembly tasks by learning corrective actions. We also show that by combining ResiP with teacher-student distillation and visual domain randomization, our method can enable learning real-world policies for robotic assembly directly from RGB images. Find videos and code at \\url{https://residual-assembly.github.io}.","sentences":["Behavior cloning (BC) currently stands as a dominant paradigm for learning real-world visual manipulation.","However, in tasks that require locally corrective behaviors like multi-part assembly, learning robust policies purely from human demonstrations remains challenging.","Reinforcement learning (RL) can mitigate these limitations by allowing policies to acquire locally corrective behaviors through task reward supervision and exploration.","This paper explores the use of RL fine-tuning to improve upon BC-trained policies in precise manipulation tasks.","We analyze and overcome technical challenges associated with using RL to directly train policy networks that incorporate modern architectural components like diffusion models and action chunking.","We propose training residual policies on top of frozen BC-trained diffusion models using standard policy gradient methods and sparse rewards, an approach we call ResiP (Residual for Precise manipulation).","Our experimental results demonstrate that this residual learning framework can significantly improve success rates beyond the base BC-trained models in high-precision assembly tasks by learning corrective actions.","We also show that by combining ResiP with teacher-student distillation and visual domain randomization, our method can enable learning real-world policies for robotic assembly directly from RGB images.","Find videos and code at \\url{https://residual-assembly.github.io}."],"url":"http://arxiv.org/abs/2407.16677v1"}
{"created":"2024-07-23 17:43:35","title":"KAN or MLP: A Fairer Comparison","abstract":"This paper does not introduce a novel method. Instead, it offers a fairer and more comprehensive comparison of KAN and MLP models across various tasks, including machine learning, computer vision, audio processing, natural language processing, and symbolic formula representation. Specifically, we control the number of parameters and FLOPs to compare the performance of KAN and MLP. Our main observation is that, except for symbolic formula representation tasks, MLP generally outperforms KAN. We also conduct ablation studies on KAN and find that its advantage in symbolic formula representation mainly stems from its B-spline activation function. When B-spline is applied to MLP, performance in symbolic formula representation significantly improves, surpassing or matching that of KAN. However, in other tasks where MLP already excels over KAN, B-spline does not substantially enhance MLP's performance. Furthermore, we find that KAN's forgetting issue is more severe than that of MLP in a standard class-incremental continual learning setting, which differs from the findings reported in the KAN paper. We hope these results provide insights for future research on KAN and other MLP alternatives. Project link: https://github.com/yu-rp/KANbeFair","sentences":["This paper does not introduce a novel method.","Instead, it offers a fairer and more comprehensive comparison of KAN and MLP models across various tasks, including machine learning, computer vision, audio processing, natural language processing, and symbolic formula representation.","Specifically, we control the number of parameters and FLOPs to compare the performance of KAN and MLP.","Our main observation is that, except for symbolic formula representation tasks, MLP generally outperforms KAN.","We also conduct ablation studies on KAN and find that its advantage in symbolic formula representation mainly stems from its B-spline activation function.","When B-spline is applied to MLP, performance in symbolic formula representation significantly improves, surpassing or matching that of KAN.","However, in other tasks where MLP already excels over KAN, B-spline does not substantially enhance MLP's performance.","Furthermore, we find that KAN's forgetting issue is more severe than that of MLP in a standard class-incremental continual learning setting, which differs from the findings reported in the KAN paper.","We hope these results provide insights for future research on KAN and other MLP alternatives.","Project link: https://github.com/yu-rp/KANbeFair"],"url":"http://arxiv.org/abs/2407.16674v1"}
{"created":"2024-07-23 17:42:57","title":"6G at $\\frac{1}{6}g$: The Future of Cislunar Communications","abstract":"What will the future of cislunar communications be? The ever-expanding horizons of the space exploration missions, and the need for establishing sustainable space communication and navigation infrastructure necessitate to think this question thoroughly. In this article, we examine how some of the concepts of 6G technologies developed for terrestrial networks can be relevant in the context of cislunar networks. We discuss how 6G concepts, such as reconfigurable intelligent surfaces, quantum-resistant physical layer security, private information read/write/cache networks, semantic and goal-oriented communications, information freshness based quality of communication metrics, multi-relay and cooperative networks, hold the potential to shape the future of cislunar communications.","sentences":["What will the future of cislunar communications be?","The ever-expanding horizons of the space exploration missions, and the need for establishing sustainable space communication and navigation infrastructure necessitate to think this question thoroughly.","In this article, we examine how some of the concepts of 6G technologies developed for terrestrial networks can be relevant in the context of cislunar networks.","We discuss how 6G concepts, such as reconfigurable intelligent surfaces, quantum-resistant physical layer security, private information read/write/cache networks, semantic and goal-oriented communications, information freshness based quality of communication metrics, multi-relay and cooperative networks, hold the potential to shape the future of cislunar communications."],"url":"http://arxiv.org/abs/2407.16672v1"}
{"created":"2024-07-23 17:39:49","title":"FakingRecipe: Detecting Fake News on Short Video Platforms from the Perspective of Creative Process","abstract":"As short-form video-sharing platforms become a significant channel for news consumption, fake news in short videos has emerged as a serious threat in the online information ecosystem, making developing detection methods for this new scenario an urgent need. Compared with that in text and image formats, fake news on short video platforms contains rich but heterogeneous information in various modalities, posing a challenge to effective feature utilization. Unlike existing works mostly focusing on analyzing what is presented, we introduce a novel perspective that considers how it might be created. Through the lens of the creative process behind news video production, our empirical analysis uncovers the unique characteristics of fake news videos in material selection and editing. Based on the obtained insights, we design FakingRecipe, a creative process-aware model for detecting fake news short videos. It captures the fake news preferences in material selection from sentimental and semantic aspects and considers the traits of material editing from spatial and temporal aspects. To improve evaluation comprehensiveness, we first construct FakeTT, an English dataset for this task, and conduct experiments on both FakeTT and the existing Chinese FakeSV dataset. The results show FakingRecipe's superiority in detecting fake news on short video platforms.","sentences":["As short-form video-sharing platforms become a significant channel for news consumption, fake news in short videos has emerged as a serious threat in the online information ecosystem, making developing detection methods for this new scenario an urgent need.","Compared with that in text and image formats, fake news on short video platforms contains rich but heterogeneous information in various modalities, posing a challenge to effective feature utilization.","Unlike existing works mostly focusing on analyzing what is presented, we introduce a novel perspective that considers how it might be created.","Through the lens of the creative process behind news video production, our empirical analysis uncovers the unique characteristics of fake news videos in material selection and editing.","Based on the obtained insights, we design FakingRecipe, a creative process-aware model for detecting fake news short videos.","It captures the fake news preferences in material selection from sentimental and semantic aspects and considers the traits of material editing from spatial and temporal aspects.","To improve evaluation comprehensiveness, we first construct FakeTT, an English dataset for this task, and conduct experiments on both FakeTT and the existing Chinese FakeSV dataset.","The results show FakingRecipe's superiority in detecting fake news on short video platforms."],"url":"http://arxiv.org/abs/2407.16670v1"}
{"created":"2024-07-23 17:34:36","title":"RedAgent: Red Teaming Large Language Models with Context-aware Autonomous Language Agent","abstract":"Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot. These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of threats. Among them, jailbreak attacks that induce toxic responses through jailbreak prompts have raised critical safety concerns. To identify these threats, a growing number of red teaming approaches simulate potential adversarial scenarios by crafting jailbreak prompts to test the target LLM. However, existing red teaming methods do not consider the unique vulnerabilities of LLM in different scenarios, making it difficult to adjust the jailbreak prompts to find context-specific vulnerabilities. Meanwhile, these methods are limited to refining jailbreak templates using a few mutation operations, lacking the automation and scalability to adapt to different scenarios. To enable context-aware and efficient red teaming, we abstract and model existing attacks into a coherent concept called \"jailbreak strategy\" and propose a multi-agent LLM system named RedAgent that leverages these strategies to generate context-aware jailbreak prompts. By self-reflecting on contextual feedback in an additional memory buffer, RedAgent continuously learns how to leverage these strategies to achieve effective jailbreaks in specific contexts. Extensive experiments demonstrate that our system can jailbreak most black-box LLMs in just five queries, improving the efficiency of existing red teaming methods by two times. Additionally, RedAgent can jailbreak customized LLM applications more efficiently. By generating context-aware jailbreak prompts towards applications on GPTs, we discover 60 severe vulnerabilities of these real-world applications with only two queries per vulnerability. We have reported all found issues and communicated with OpenAI and Meta for bug fixes.","sentences":["Recently, advanced Large Language Models (LLMs) such as GPT-4 have been integrated into many real-world applications like Code Copilot.","These applications have significantly expanded the attack surface of LLMs, exposing them to a variety of threats.","Among them, jailbreak attacks that induce toxic responses through jailbreak prompts have raised critical safety concerns.","To identify these threats, a growing number of red teaming approaches simulate potential adversarial scenarios by crafting jailbreak prompts to test the target LLM.","However, existing red teaming methods do not consider the unique vulnerabilities of LLM in different scenarios, making it difficult to adjust the jailbreak prompts to find context-specific vulnerabilities.","Meanwhile, these methods are limited to refining jailbreak templates using a few mutation operations, lacking the automation and scalability to adapt to different scenarios.","To enable context-aware and efficient red teaming, we abstract and model existing attacks into a coherent concept called \"jailbreak strategy\" and propose a multi-agent LLM system named RedAgent that leverages these strategies to generate context-aware jailbreak prompts.","By self-reflecting on contextual feedback in an additional memory buffer, RedAgent continuously learns how to leverage these strategies to achieve effective jailbreaks in specific contexts.","Extensive experiments demonstrate that our system can jailbreak most black-box LLMs in just five queries, improving the efficiency of existing red teaming methods by two times.","Additionally, RedAgent can jailbreak customized LLM applications more efficiently.","By generating context-aware jailbreak prompts towards applications on GPTs, we discover 60 severe vulnerabilities of these real-world applications with only two queries per vulnerability.","We have reported all found issues and communicated with OpenAI and Meta for bug fixes."],"url":"http://arxiv.org/abs/2407.16667v1"}
{"created":"2024-07-23 17:32:02","title":"A Framework for Pupil Tracking with Event Cameras","abstract":"Saccades are extremely rapid movements of both eyes that occur simultaneously, typically observed when an individual shifts their focus from one object to another. These movements are among the swiftest produced by humans and possess the potential to achieve velocities greater than that of blinks. The peak angular speed of the eye during a saccade can reach as high as 700{\\deg}/s in humans, especially during larger saccades that cover a visual angle of 25{\\deg}. Previous research has demonstrated encouraging outcomes in comprehending neurological conditions through the study of saccades. A necessary step in saccade detection involves accurately identifying the precise location of the pupil within the eye, from which additional information such as gaze angles can be inferred. Conventional frame-based cameras often struggle with the high temporal precision necessary for tracking very fast movements, resulting in motion blur and latency issues. Event cameras, on the other hand, offer a promising alternative by recording changes in the visual scene asynchronously and providing high temporal resolution and low latency. By bridging the gap between traditional computer vision and event-based vision, we present events as frames that can be readily utilized by standard deep learning algorithms. This approach harnesses YOLOv8, a state-of-the-art object detection technology, to process these frames for pupil tracking using the publicly accessible Ev-Eye dataset. Experimental results demonstrate the framework's effectiveness, highlighting its potential applications in neuroscience, ophthalmology, and human-computer interaction.","sentences":["Saccades are extremely rapid movements of both eyes that occur simultaneously, typically observed when an individual shifts their focus from one object to another.","These movements are among the swiftest produced by humans and possess the potential to achieve velocities greater than that of blinks.","The peak angular speed of the eye during a saccade can reach as high as 700{\\deg}/s in humans, especially during larger saccades that cover a visual angle of 25{\\deg}.","Previous research has demonstrated encouraging outcomes in comprehending neurological conditions through the study of saccades.","A necessary step in saccade detection involves accurately identifying the precise location of the pupil within the eye, from which additional information such as gaze angles can be inferred.","Conventional frame-based cameras often struggle with the high temporal precision necessary for tracking very fast movements, resulting in motion blur and latency issues.","Event cameras, on the other hand, offer a promising alternative by recording changes in the visual scene asynchronously and providing high temporal resolution and low latency.","By bridging the gap between traditional computer vision and event-based vision, we present events as frames that can be readily utilized by standard deep learning algorithms.","This approach harnesses YOLOv8, a state-of-the-art object detection technology, to process these frames for pupil tracking using the publicly accessible Ev-Eye dataset.","Experimental results demonstrate the framework's effectiveness, highlighting its potential applications in neuroscience, ophthalmology, and human-computer interaction."],"url":"http://arxiv.org/abs/2407.16665v1"}
{"created":"2024-07-23 17:29:02","title":"Towards scalable efficient on-device ASR with transfer learning","abstract":"Multilingual pretraining for transfer learning significantly boosts the robustness of low-resource monolingual ASR models. This study systematically investigates three main aspects: (a) the impact of transfer learning on model performance during initial training or fine-tuning, (b) the influence of transfer learning across dataset domains and languages, and (c) the effect on rare-word recognition compared to non-rare words. Our finding suggests that RNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word Error Rate (MinWER) loss, consistently reduces Word Error Rates (WER) across languages like Italian and French. WER Reductions (WERR) reach 36.2% and 42.8% compared to monolingual baselines for MLS and in-house datasets. Out-of-domain pretraining leads to 28% higher WERR than in-domain pretraining. Both rare and non-rare words benefit, with rare words showing greater improvements with out-of-domain pretraining, and non-rare words with in-domain pretraining.","sentences":["Multilingual pretraining for transfer learning significantly boosts the robustness of low-resource monolingual ASR models.","This study systematically investigates three main aspects: (a) the impact of transfer learning on model performance during initial training or fine-tuning, (b) the influence of transfer learning across dataset domains and languages, and (c) the effect on rare-word recognition compared to non-rare words.","Our finding suggests that RNNT-loss pretraining, followed by monolingual fine-tuning with Minimum Word Error Rate (MinWER) loss, consistently reduces Word Error Rates (WER) across languages like Italian and French.","WER Reductions (WERR) reach 36.2% and 42.8% compared to monolingual baselines for MLS and in-house datasets.","Out-of-domain pretraining leads to 28% higher WERR than in-domain pretraining.","Both rare and non-rare words benefit, with rare words showing greater improvements with out-of-domain pretraining, and non-rare words with in-domain pretraining."],"url":"http://arxiv.org/abs/2407.16664v1"}
{"created":"2024-07-23 17:26:38","title":"Computable learning of natural hypothesis classes","abstract":"This paper is about the recent notion of computably probably approximately correct learning, which lies between the statistical learning theory where there is no computational requirement on the learner and efficient PAC where the learner must be polynomially bounded. Examples have recently been given of hypothesis classes which are PAC learnable but not computably PAC learnable, but these hypothesis classes are unnatural or non-canonical in the sense that they depend on a numbering of proofs, formulas, or programs. We use the on-a-cone machinery from computability theory to prove that, under mild assumptions such as that the hypothesis class can be computably listable, any natural hypothesis class which is learnable must be computably learnable. Thus the counterexamples given previously are necessarily unnatural.","sentences":["This paper is about the recent notion of computably probably approximately correct learning, which lies between the statistical learning theory where there is no computational requirement on the learner and efficient PAC where the learner must be polynomially bounded.","Examples have recently been given of hypothesis classes which are PAC learnable but not computably PAC learnable, but these hypothesis classes are unnatural or non-canonical in the sense that they depend on a numbering of proofs, formulas, or programs.","We use the on-a-cone machinery from computability theory to prove that, under mild assumptions such as that the hypothesis class can be computably listable, any natural hypothesis class which is learnable must be computably learnable.","Thus the counterexamples given previously are necessarily unnatural."],"url":"http://arxiv.org/abs/2407.16663v1"}
{"created":"2024-07-23 17:21:56","title":"Dynamic Subgraph Matching via Cost-Model-based Vertex Dominance Embeddings (Technical Report)","abstract":"In many real-world applications such as social network analysis, knowledge graph discovery, biological network analytics, and so on, graph data management has become increasingly important and has drawn much attention from the database community. While many graphs (e.g., Twitter, Wikipedia, etc.) are usually involving over time, it is of great importance to study the dynamic subgraph matching (DSM) problem, a fundamental yet challenging graph operator, which continuously monitors subgraph matching results over dynamic graphs with a stream of edge updates. To efficiently tackle the DSM problem, we carefully design a novel vertex dominance embedding approach, which effectively encodes vertex labels that can be incrementally maintained upon graph updates. Inspire by low pruning power for high-degree vertices, we propose a new degree grouping technique over basic subgraph patterns in different degree groups (i.e., groups of star substructures), and devise degree-aware star substructure synopses (DAS^3) to effectively facilitate our designed vertex dominance and range pruning strategies. We develop efficient algorithms to incrementally maintain dynamic graphs and answer DSM queries. Through extensive experiments, we confirm the efficiency of our proposed approaches over both real and synthetic graphs.","sentences":["In many real-world applications such as social network analysis, knowledge graph discovery, biological network analytics, and so on, graph data management has become increasingly important and has drawn much attention from the database community.","While many graphs (e.g., Twitter, Wikipedia, etc.) are usually involving over time, it is of great importance to study the dynamic subgraph matching (DSM) problem, a fundamental yet challenging graph operator, which continuously monitors subgraph matching results over dynamic graphs with a stream of edge updates.","To efficiently tackle the DSM problem, we carefully design a novel vertex dominance embedding approach, which effectively encodes vertex labels that can be incrementally maintained upon graph updates.","Inspire by low pruning power for high-degree vertices, we propose a new degree grouping technique over basic subgraph patterns in different degree groups (i.e., groups of star substructures), and devise degree-aware star substructure synopses (DAS^3) to effectively facilitate our designed vertex dominance and range pruning strategies.","We develop efficient algorithms to incrementally maintain dynamic graphs and answer DSM queries.","Through extensive experiments, we confirm the efficiency of our proposed approaches over both real and synthetic graphs."],"url":"http://arxiv.org/abs/2407.16660v1"}
{"created":"2024-07-23 17:19:23","title":"EgoCVR: An Egocentric Benchmark for Fine-Grained Composed Video Retrieval","abstract":"In Composed Video Retrieval, a video and a textual description which modifies the video content are provided as inputs to the model. The aim is to retrieve the relevant video with the modified content from a database of videos. In this challenging task, the first step is to acquire large-scale training datasets and collect high-quality benchmarks for evaluation. In this work, we introduce EgoCVR, a new evaluation benchmark for fine-grained Composed Video Retrieval using large-scale egocentric video datasets. EgoCVR consists of 2,295 queries that specifically focus on high-quality temporal video understanding. We find that existing Composed Video Retrieval frameworks do not achieve the necessary high-quality temporal video understanding for this task. To address this shortcoming, we adapt a simple training-free method, propose a generic re-ranking framework for Composed Video Retrieval, and demonstrate that this achieves strong results on EgoCVR. Our code and benchmark are freely available at https://github.com/ExplainableML/EgoCVR.","sentences":["In Composed Video Retrieval, a video and a textual description which modifies the video content are provided as inputs to the model.","The aim is to retrieve the relevant video with the modified content from a database of videos.","In this challenging task, the first step is to acquire large-scale training datasets and collect high-quality benchmarks for evaluation.","In this work, we introduce EgoCVR, a new evaluation benchmark for fine-grained Composed Video Retrieval using large-scale egocentric video datasets.","EgoCVR consists of 2,295 queries that specifically focus on high-quality temporal video understanding.","We find that existing Composed Video Retrieval frameworks do not achieve the necessary high-quality temporal video understanding for this task.","To address this shortcoming, we adapt a simple training-free method, propose a generic re-ranking framework for Composed Video Retrieval, and demonstrate that this achieves strong results on EgoCVR.","Our code and benchmark are freely available at https://github.com/ExplainableML/EgoCVR."],"url":"http://arxiv.org/abs/2407.16658v1"}
{"created":"2024-07-23 17:17:05","title":"MovieDreamer: Hierarchical Generation for Coherent Long Visual Sequence","abstract":"Recent advancements in video generation have primarily leveraged diffusion models for short-duration content. However, these approaches often fall short in modeling complex narratives and maintaining character consistency over extended periods, which is essential for long-form video production like movies. We propose MovieDreamer, a novel hierarchical framework that integrates the strengths of autoregressive models with diffusion-based rendering to pioneer long-duration video generation with intricate plot progressions and high visual fidelity. Our approach utilizes autoregressive models for global narrative coherence, predicting sequences of visual tokens that are subsequently transformed into high-quality video frames through diffusion rendering. This method is akin to traditional movie production processes, where complex stories are factorized down into manageable scene capturing. Further, we employ a multimodal script that enriches scene descriptions with detailed character information and visual style, enhancing continuity and character identity across scenes. We present extensive experiments across various movie genres, demonstrating that our approach not only achieves superior visual and narrative quality but also effectively extends the duration of generated content significantly beyond current capabilities. Homepage: https://aim-uofa.github.io/MovieDreamer/.","sentences":["Recent advancements in video generation have primarily leveraged diffusion models for short-duration content.","However, these approaches often fall short in modeling complex narratives and maintaining character consistency over extended periods, which is essential for long-form video production like movies.","We propose MovieDreamer, a novel hierarchical framework that integrates the strengths of autoregressive models with diffusion-based rendering to pioneer long-duration video generation with intricate plot progressions and high visual fidelity.","Our approach utilizes autoregressive models for global narrative coherence, predicting sequences of visual tokens that are subsequently transformed into high-quality video frames through diffusion rendering.","This method is akin to traditional movie production processes, where complex stories are factorized down into manageable scene capturing.","Further, we employ a multimodal script that enriches scene descriptions with detailed character information and visual style, enhancing continuity and character identity across scenes.","We present extensive experiments across various movie genres, demonstrating that our approach not only achieves superior visual and narrative quality but also effectively extends the duration of generated content significantly beyond current capabilities.","Homepage: https://aim-uofa.github.io/MovieDreamer/."],"url":"http://arxiv.org/abs/2407.16655v1"}
{"created":"2024-07-23 17:14:01","title":"Aggregated Attributions for Explanatory Analysis of 3D Segmentation Models","abstract":"Analysis of 3D segmentation models, especially in the context of medical imaging, is often limited to segmentation performance metrics that overlook the crucial aspect of explainability and bias. Currently, effectively explaining these models with saliency maps is challenging due to the high dimensions of input images multiplied by the ever-growing number of segmented class labels. To this end, we introduce Agg^2Exp, a methodology for aggregating fine-grained voxel attributions of the segmentation model's predictions. Unlike classical explanation methods that primarily focus on the local feature attribution, Agg^2Exp enables a more comprehensive global view on the importance of predicted segments in 3D images. Our benchmarking experiments show that gradient-based voxel attributions are more faithful to the model's predictions than perturbation-based explanations. As a concrete use-case, we apply Agg^2Exp to discover knowledge acquired by the Swin UNEt TRansformer model trained on the TotalSegmentator v2 dataset for segmenting anatomical structures in computed tomography medical images. Agg^2Exp facilitates the explanatory analysis of large segmentation models beyond their predictive performance.","sentences":["Analysis of 3D segmentation models, especially in the context of medical imaging, is often limited to segmentation performance metrics that overlook the crucial aspect of explainability and bias.","Currently, effectively explaining these models with saliency maps is challenging due to the high dimensions of input images multiplied by the ever-growing number of segmented class labels.","To this end, we introduce Agg^2Exp, a methodology for aggregating fine-grained voxel attributions of the segmentation model's predictions.","Unlike classical explanation methods that primarily focus on the local feature attribution, Agg^2Exp enables a more comprehensive global view on the importance of predicted segments in 3D images.","Our benchmarking experiments show that gradient-based voxel attributions are more faithful to the model's predictions than perturbation-based explanations.","As a concrete use-case, we apply Agg^2Exp to discover knowledge acquired by the Swin UNEt TRansformer model trained on the TotalSegmentator v2 dataset for segmenting anatomical structures in computed tomography medical images.","Agg^2Exp facilitates the explanatory analysis of large segmentation models beyond their predictive performance."],"url":"http://arxiv.org/abs/2407.16653v1"}
{"created":"2024-07-23 17:02:24","title":"Deformable Convolution Based Road Scene Semantic Segmentation of Fisheye Images in Autonomous Driving","abstract":"This study investigates the effectiveness of modern Deformable Convolutional Neural Networks (DCNNs) for semantic segmentation tasks, particularly in autonomous driving scenarios with fisheye images. These images, providing a wide field of view, pose unique challenges for extracting spatial and geometric information due to dynamic changes in object attributes. Our experiments focus on segmenting the WoodScape fisheye image dataset into ten distinct classes, assessing the Deformable Networks' ability to capture intricate spatial relationships and improve segmentation accuracy. Additionally, we explore different loss functions to address class imbalance issues and compare the performance of conventional CNN architectures with Deformable Convolution-based CNNs, including Vanilla U-Net and Residual U-Net architectures. The significant improvement in mIoU score resulting from integrating Deformable CNNs demonstrates their effectiveness in handling the geometric distortions present in fisheye imagery, exceeding the performance of traditional CNN architectures. This underscores the significant role of Deformable convolution in enhancing semantic segmentation performance for fisheye imagery.","sentences":["This study investigates the effectiveness of modern Deformable Convolutional Neural Networks (DCNNs) for semantic segmentation tasks, particularly in autonomous driving scenarios with fisheye images.","These images, providing a wide field of view, pose unique challenges for extracting spatial and geometric information due to dynamic changes in object attributes.","Our experiments focus on segmenting the WoodScape fisheye image dataset into ten distinct classes, assessing the Deformable Networks' ability to capture intricate spatial relationships and improve segmentation accuracy.","Additionally, we explore different loss functions to address class imbalance issues and compare the performance of conventional CNN architectures with Deformable Convolution-based CNNs, including Vanilla U-Net and Residual U-Net architectures.","The significant improvement in mIoU score resulting from integrating Deformable CNNs demonstrates their effectiveness in handling the geometric distortions present in fisheye imagery, exceeding the performance of traditional CNN architectures.","This underscores the significant role of Deformable convolution in enhancing semantic segmentation performance for fisheye imagery."],"url":"http://arxiv.org/abs/2407.16647v1"}
{"created":"2024-07-23 17:00:09","title":"ExaWorks Software Development Kit: A Robust and Scalable Collection of Interoperable Workflow Technologies","abstract":"Scientific discovery increasingly requires executing heterogeneous scientific workflows on high-performance computing (HPC) platforms. Heterogeneous workflows contain different types of tasks (e.g., simulation, analysis, and learning) that need to be mapped, scheduled, and launched on different computing. That requires a software stack that enables users to code their workflows and automate resource management and workflow execution. Currently, there are many workflow technologies with diverse levels of robustness and capabilities, and users face difficult choices of software that can effectively and efficiently support their use cases on HPC machines, especially when considering the latest exascale platforms. We contributed to addressing this issue by developing the ExaWorks Software Development Kit (SDK). The SDK is a curated collection of workflow technologies engineered following current best practices and specifically designed to work on HPC platforms. We present our experience with (1) curating those technologies, (2) integrating them to provide users with new capabilities, (3) developing a continuous integration platform to test the SDK on DOE HPC platforms, (4) designing a dashboard to publish the results of those tests, and (5) devising an innovative documentation platform to help users to use those technologies. Our experience details the requirements and the best practices needed to curate workflow technologies, and it also serves as a blueprint for the capabilities and services that DOE will have to offer to support a variety of scientific heterogeneous workflows on the newly available exascale HPC platforms.","sentences":["Scientific discovery increasingly requires executing heterogeneous scientific workflows on high-performance computing (HPC) platforms.","Heterogeneous workflows contain different types of tasks (e.g., simulation, analysis, and learning) that need to be mapped, scheduled, and launched on different computing.","That requires a software stack that enables users to code their workflows and automate resource management and workflow execution.","Currently, there are many workflow technologies with diverse levels of robustness and capabilities, and users face difficult choices of software that can effectively and efficiently support their use cases on HPC machines, especially when considering the latest exascale platforms.","We contributed to addressing this issue by developing the ExaWorks Software Development Kit (SDK).","The SDK is a curated collection of workflow technologies engineered following current best practices and specifically designed to work on HPC platforms.","We present our experience with (1) curating those technologies, (2) integrating them to provide users with new capabilities, (3) developing a continuous integration platform to test the SDK on DOE HPC platforms, (4) designing a dashboard to publish the results of those tests, and (5) devising an innovative documentation platform to help users to use those technologies.","Our experience details the requirements and the best practices needed to curate workflow technologies, and it also serves as a blueprint for the capabilities and services that DOE will have to offer to support a variety of scientific heterogeneous workflows on the newly available exascale HPC platforms."],"url":"http://arxiv.org/abs/2407.16646v1"}
{"created":"2024-07-23 16:56:59","title":"A Geometry-Aware Algorithm to Learn Hierarchical Embeddings in Hyperbolic Space","abstract":"Hyperbolic embeddings are a class of representation learning methods that offer competitive performances when data can be abstracted as a tree-like graph. However, in practice, learning hyperbolic embeddings of hierarchical data is difficult due to the different geometry between hyperbolic space and the Euclidean space. To address such difficulties, we first categorize three kinds of illness that harm the performance of the embeddings. Then, we develop a geometry-aware algorithm using a dilation operation and a transitive closure regularization to tackle these illnesses. We empirically validate these techniques and present a theoretical analysis of the mechanism behind the dilation operation. Experiments on synthetic and real-world datasets reveal superior performances of our algorithm.","sentences":["Hyperbolic embeddings are a class of representation learning methods that offer competitive performances when data can be abstracted as a tree-like graph.","However, in practice, learning hyperbolic embeddings of hierarchical data is difficult due to the different geometry between hyperbolic space and the Euclidean space.","To address such difficulties, we first categorize three kinds of illness that harm the performance of the embeddings.","Then, we develop a geometry-aware algorithm using a dilation operation and a transitive closure regularization to tackle these illnesses.","We empirically validate these techniques and present a theoretical analysis of the mechanism behind the dilation operation.","Experiments on synthetic and real-world datasets reveal superior performances of our algorithm."],"url":"http://arxiv.org/abs/2407.16641v1"}
{"created":"2024-07-23 16:55:13","title":"Distortion Recovery: A Two-Stage Method for Guitar Effect Removal","abstract":"Removing audio effects from electric guitar recordings makes it easier for post-production and sound editing. An audio distortion recovery model not only improves the clarity of the guitar sounds but also opens up new opportunities for creative adjustments in mixing and mastering. While progress have been made in creating such models, previous efforts have largely focused on synthetic distortions that may be too simplistic to accurately capture the complexities seen in real-world recordings.   In this paper, we tackle the task by using a dataset of guitar recordings rendered with commercial-grade audio effect VST plugins. Moreover, we introduce a novel two-stage methodology for audio distortion recovery. The idea is to firstly process the audio signal in the Mel-spectrogram domain in the first stage, and then use a neural vocoder to generate the pristine original guitar sound from the processed Mel-spectrogram in the second stage. We report a set of experiments demonstrating the effectiveness of our approach over existing methods, through both subjective and objective evaluation metrics.","sentences":["Removing audio effects from electric guitar recordings makes it easier for post-production and sound editing.","An audio distortion recovery model not only improves the clarity of the guitar sounds but also opens up new opportunities for creative adjustments in mixing and mastering.","While progress have been made in creating such models, previous efforts have largely focused on synthetic distortions that may be too simplistic to accurately capture the complexities seen in real-world recordings.   ","In this paper, we tackle the task by using a dataset of guitar recordings rendered with commercial-grade audio effect VST plugins.","Moreover, we introduce a novel two-stage methodology for audio distortion recovery.","The idea is to firstly process the audio signal in the Mel-spectrogram domain in the first stage, and then use a neural vocoder to generate the pristine original guitar sound from the processed Mel-spectrogram in the second stage.","We report a set of experiments demonstrating the effectiveness of our approach over existing methods, through both subjective and objective evaluation metrics."],"url":"http://arxiv.org/abs/2407.16639v1"}
{"created":"2024-07-23 16:55:04","title":"Unveiling and Mitigating Bias in Audio Visual Segmentation","abstract":"Community researchers have developed a range of advanced audio-visual segmentation models aimed at improving the quality of sounding objects' masks. While masks created by these models may initially appear plausible, they occasionally exhibit anomalies with incorrect grounding logic. We attribute this to real-world inherent preferences and distributions as a simpler signal for learning than the complex audio-visual grounding, which leads to the disregard of important modality information. Generally, the anomalous phenomena are often complex and cannot be directly observed systematically. In this study, we made a pioneering effort with the proper synthetic data to categorize and analyze phenomena as two types \"audio priming bias\" and \"visual prior\" according to the source of anomalies. For audio priming bias, to enhance audio sensitivity to different intensities and semantics, a perception module specifically for audio perceives the latent semantic information and incorporates information into a limited set of queries, namely active queries. Moreover, the interaction mechanism related to such active queries in the transformer decoder is customized to adapt to the need for interaction regulating among audio semantics. For visual prior, multiple contrastive training strategies are explored to optimize the model by incorporating a biased branch, without even changing the structure of the model. During experiments, observation demonstrates the presence and the impact that has been produced by the biases of the existing model. Finally, through experimental evaluation of AVS benchmarks, we demonstrate the effectiveness of our methods in handling both types of biases, achieving competitive performance across all three subsets.","sentences":["Community researchers have developed a range of advanced audio-visual segmentation models aimed at improving the quality of sounding objects' masks.","While masks created by these models may initially appear plausible, they occasionally exhibit anomalies with incorrect grounding logic.","We attribute this to real-world inherent preferences and distributions as a simpler signal for learning than the complex audio-visual grounding, which leads to the disregard of important modality information.","Generally, the anomalous phenomena are often complex and cannot be directly observed systematically.","In this study, we made a pioneering effort with the proper synthetic data to categorize and analyze phenomena as two types \"audio priming bias\" and \"visual prior\" according to the source of anomalies.","For audio priming bias, to enhance audio sensitivity to different intensities and semantics, a perception module specifically for audio perceives the latent semantic information and incorporates information into a limited set of queries, namely active queries.","Moreover, the interaction mechanism related to such active queries in the transformer decoder is customized to adapt to the need for interaction regulating among audio semantics.","For visual prior, multiple contrastive training strategies are explored to optimize the model by incorporating a biased branch, without even changing the structure of the model.","During experiments, observation demonstrates the presence and the impact that has been produced by the biases of the existing model.","Finally, through experimental evaluation of AVS benchmarks, we demonstrate the effectiveness of our methods in handling both types of biases, achieving competitive performance across all three subsets."],"url":"http://arxiv.org/abs/2407.16638v1"}
{"created":"2024-07-23 16:54:28","title":"Course-Correction: Safety Alignment Using Synthetic Preferences","abstract":"The risk of harmful content generated by large language models (LLMs) becomes a critical concern. This paper presents a systematic study on assessing and improving LLMs' capability to perform the task of \\textbf{course-correction}, \\ie, the model can steer away from generating harmful content autonomously. To start with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative assessment and analyze 10 popular LLMs, revealing varying proficiency of current safety-tuned LLMs in course-correction. To improve, we propose fine-tuning LLMs with preference learning, emphasizing the preference for timely course-correction. Using an automated pipeline, we create \\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to teach models the concept of timely course-correction through data-driven preference learning. Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and \\textsc{Qwen2 7B}, show that our method effectively enhances course-correction skills without affecting general performance. Additionally, it effectively improves LLMs' safety, particularly in resisting jailbreak attacks.","sentences":["The risk of harmful content generated by large language models (LLMs) becomes a critical concern.","This paper presents a systematic study on assessing and improving LLMs' capability to perform the task of \\textbf{course-correction}, \\ie, the model can steer away from generating harmful content autonomously.","To start with, we introduce the \\textsc{C$^2$-Eval} benchmark for quantitative assessment and analyze 10 popular LLMs, revealing varying proficiency of current safety-tuned LLMs in course-correction.","To improve, we propose fine-tuning LLMs with preference learning, emphasizing the preference for timely course-correction.","Using an automated pipeline, we create \\textsc{C$^2$-Syn}, a synthetic dataset with 750K pairwise preferences, to teach models the concept of timely course-correction through data-driven preference learning.","Experiments on 2 LLMs, \\textsc{Llama2-Chat 7B} and \\textsc{Qwen2 7B}, show that our method effectively enhances course-correction skills without affecting general performance.","Additionally, it effectively improves LLMs' safety, particularly in resisting jailbreak attacks."],"url":"http://arxiv.org/abs/2407.16637v1"}
{"created":"2024-07-23 16:52:42","title":"Velocity Driven Vision: Asynchronous Sensor Fusion Birds Eye View Models for Autonomous Vehicles","abstract":"Fusing different sensor modalities can be a difficult task, particularly if they are asynchronous. Asynchronisation may arise due to long processing times or improper synchronisation during calibration, and there must exist a way to still utilise this previous information for the purpose of safe driving, and object detection in ego vehicle/ multi-agent trajectory prediction. Difficulties arise in the fact that the sensor modalities have captured information at different times and also at different positions in space. Therefore, they are not spatially nor temporally aligned. This paper will investigate the challenge of radar and LiDAR sensors being asynchronous relative to the camera sensors, for various time latencies. The spatial alignment will be resolved before lifting into BEV space via the transformation of the radar/LiDAR point clouds into the new ego frame coordinate system. Only after this can we concatenate the radar/LiDAR point cloud and lifted camera features. Temporal alignment will be remedied for radar data only, we will implement a novel method of inferring the future radar point positions using the velocity information. Our approach to resolving the issue of sensor asynchrony yields promising results. We demonstrate velocity information can drastically improve IoU for asynchronous datasets, as for a time latency of 360 milliseconds (ms), IoU improves from 49.54 to 53.63. Additionally, for a time latency of 550ms, the camera+radar (C+R) model outperforms the camera+LiDAR (C+L) model by 0.18 IoU. This is an advancement in utilising the often-neglected radar sensor modality, which is less favoured than LiDAR for autonomous driving purposes.","sentences":["Fusing different sensor modalities can be a difficult task, particularly if they are asynchronous.","Asynchronisation may arise due to long processing times or improper synchronisation during calibration, and there must exist a way to still utilise this previous information for the purpose of safe driving, and object detection in ego vehicle/ multi-agent trajectory prediction.","Difficulties arise in the fact that the sensor modalities have captured information at different times and also at different positions in space.","Therefore, they are not spatially nor temporally aligned.","This paper will investigate the challenge of radar and LiDAR sensors being asynchronous relative to the camera sensors, for various time latencies.","The spatial alignment will be resolved before lifting into BEV space via the transformation of the radar/LiDAR point clouds into the new ego frame coordinate system.","Only after this can we concatenate the radar/LiDAR point cloud and lifted camera features.","Temporal alignment will be remedied for radar data only, we will implement a novel method of inferring the future radar point positions using the velocity information.","Our approach to resolving the issue of sensor asynchrony yields promising results.","We demonstrate velocity information can drastically improve IoU for asynchronous datasets, as for a time latency of 360 milliseconds (ms), IoU improves from 49.54 to 53.63.","Additionally, for a time latency of 550ms, the camera+radar (C+R) model outperforms the camera+LiDAR (C+L) model by 0.18 IoU.","This is an advancement in utilising the often-neglected radar sensor modality, which is less favoured than LiDAR for autonomous driving purposes."],"url":"http://arxiv.org/abs/2407.16636v1"}
{"created":"2024-07-23 16:40:39","title":"Efficient Discovery of Actual Causality using Abstraction-Refinement","abstract":"Causality is an influence by which one event contributes to the production of another event, where the cause is partly responsible for the effect, and the effect is partly dependent on the cause. In this paper, we propose a novel and effective method to formally reason about the causal effect of events in engineered systems, with application on finding the root-cause of safety violations in embedded and cyber-physical systems. We are motivated by the notion of actual causality by Halpern and Pearl, which focuses on the causal effect of particular events, rather than type-level causality, which attempts to make general statements about scientific and natural phenomena. Our first contribution is formulating discovery of actual causality in computing systems modeled by a transition systems as an SMT solving problem. Since datasets for causality analysis tend to be large, in order to tackle the scalability problem of automated formal reasoning, our second contribution is a novel technique based on abstraction-refinement that allows identifying actual causes within smaller abstract causal models. We demonstrate the effectiveness of our approach (by several orders of magnitude) using three case studies to find the actual cause of violations of safety in (1) a neural network controller for an mountain car, (2) a controller for a lunar lander obtained by reinforcement learning, and (3) an MPC controller for an F-16 autopilot simulator.","sentences":["Causality is an influence by which one event contributes to the production of another event, where the cause is partly responsible for the effect, and the effect is partly dependent on the cause.","In this paper, we propose a novel and effective method to formally reason about the causal effect of events in engineered systems, with application on finding the root-cause of safety violations in embedded and cyber-physical systems.","We are motivated by the notion of actual causality by Halpern and Pearl, which focuses on the causal effect of particular events, rather than type-level causality, which attempts to make general statements about scientific and natural phenomena.","Our first contribution is formulating discovery of actual causality in computing systems modeled by a transition systems as an SMT solving problem.","Since datasets for causality analysis tend to be large, in order to tackle the scalability problem of automated formal reasoning, our second contribution is a novel technique based on abstraction-refinement that allows identifying actual causes within smaller abstract causal models.","We demonstrate the effectiveness of our approach (by several orders of magnitude) using three case studies to find the actual cause of violations of safety in (1) a neural network controller for an mountain car, (2) a controller for a lunar lander obtained by reinforcement learning, and (3) an MPC controller for an F-16 autopilot simulator."],"url":"http://arxiv.org/abs/2407.16629v1"}
{"created":"2024-07-23 16:35:45","title":"A Tale of Two DL Cities: When Library Tests Meet Compiler","abstract":"Deep Learning (DL) compilers typically load a DL model and optimize it with intermediate representation.Existing DL compiler testing techniques mainly focus on model optimization stages, but rarely explore bug detection at the model loading stage. Effectively testing the model loading stage requires covering diverse usages of each DL operator from various DL libraries, which shares a common objective with DL library testing, indicating that the embedded knowledge in DL library tests is beneficial for testing the model loading stage of DL compilers. In this work, we propose OPERA to extract such domain knowledge from the test inputs for DL libraries. OPERA constructs diverse tests from the various test inputs for DL libraries (including the test inputs documented in DL libraries and those generated by recent fuzzers). In addition, it incorporates a diversity-based test prioritization strategy to migrate and execute those test inputs that are more likely to detect diverse bugs earlier. We considered three sources of tests in DL libraries for migration and used eight frontends from three DL compilers (e.g., TVM, TensorRT, and OpenVINO) for evaluation. OPERA detected 170 previously unknown bugs in total, 90 of which have been confirmed/fixed by developers, demonstrating the effectiveness of such the migration-based idea. The test prioritization strategy in OPERA improves testing efficiency with migrated tests by 11.9%~47.4% on average compared to general test prioritization strategies.","sentences":["Deep Learning (DL) compilers typically load a DL model and optimize it with intermediate representation.","Existing DL compiler testing techniques mainly focus on model optimization stages, but rarely explore bug detection at the model loading stage.","Effectively testing the model loading stage requires covering diverse usages of each DL operator from various DL libraries, which shares a common objective with DL library testing, indicating that the embedded knowledge in DL library tests is beneficial for testing the model loading stage of DL compilers.","In this work, we propose OPERA to extract such domain knowledge from the test inputs for DL libraries.","OPERA constructs diverse tests from the various test inputs for DL libraries (including the test inputs documented in DL libraries and those generated by recent fuzzers).","In addition, it incorporates a diversity-based test prioritization strategy to migrate and execute those test inputs that are more likely to detect diverse bugs earlier.","We considered three sources of tests in DL libraries for migration and used eight frontends from three DL compilers (e.g., TVM, TensorRT, and OpenVINO) for evaluation.","OPERA detected 170 previously unknown bugs in total, 90 of which have been confirmed/fixed by developers, demonstrating the effectiveness of such the migration-based idea.","The test prioritization strategy in OPERA improves testing efficiency with migrated tests by 11.9%~47.4% on average compared to general test prioritization strategies."],"url":"http://arxiv.org/abs/2407.16626v1"}
{"created":"2024-07-23 16:32:49","title":"Semantic Change Characterization with LLMs using Rhetorics","abstract":"Languages continually evolve in response to societal events, resulting in new terms and shifts in meanings. These changes have significant implications for computer applications, including automatic translation and chatbots, making it essential to characterize them accurately. The recent development of LLMs has notably advanced natural language understanding, particularly in sense inference and reasoning. In this paper, we investigate the potential of LLMs in characterizing three types of semantic change: dimension, relation, and orientation. We achieve this by combining LLMs' Chain-of-Thought with rhetorical devices and conducting an experimental assessment of our approach using newly created datasets. Our results highlight the effectiveness of LLMs in capturing and analyzing semantic changes, providing valuable insights to improve computational linguistic applications.","sentences":["Languages continually evolve in response to societal events, resulting in new terms and shifts in meanings.","These changes have significant implications for computer applications, including automatic translation and chatbots, making it essential to characterize them accurately.","The recent development of LLMs has notably advanced natural language understanding, particularly in sense inference and reasoning.","In this paper, we investigate the potential of LLMs in characterizing three types of semantic change: dimension, relation, and orientation.","We achieve this by combining LLMs' Chain-of-Thought with rhetorical devices and conducting an experimental assessment of our approach using newly created datasets.","Our results highlight the effectiveness of LLMs in capturing and analyzing semantic changes, providing valuable insights to improve computational linguistic applications."],"url":"http://arxiv.org/abs/2407.16624v1"}
{"created":"2024-07-23 16:27:24","title":"Motion Accuracy and Computational Effort in QP-based Robot Control","abstract":"Quadratic Programs (QPs) have become a mature technology for the control of robots of all kinds, including humanoid robots. One aspect has been largely overlooked, however, which is the accuracy with which these QPs should be solved. Typical QP solvers aim at providing solutions accurate up to floating point precision ($\\approx10^{-8}$). Considering physical quantities expressed in SI or similar units (meters, radians, etc.), such precision seems completely unrelated to both task requirements and hardware capacity. Typically, humanoid robots never achieve, nor are capable of achieving sub-millimeter precision in manipulation tasks. With this observation in mind, our objectives in this paper are two-fold: first examine how the QP solution accuracy impacts the resulting robot motion accuracy, then evaluate how a reduced solution accuracy requirement can be leveraged to reduce the corresponding computational effort. Numerical experiments with a dynamic simulation of a HRP-4 robot indicate that computational effort can be divided by more than 20 while maintaining the desired motion accuracy.","sentences":["Quadratic Programs (QPs) have become a mature technology for the control of robots of all kinds, including humanoid robots.","One aspect has been largely overlooked, however, which is the accuracy with which these QPs should be solved.","Typical QP solvers aim at providing solutions accurate up to floating point precision ($\\approx10^{-8}$).","Considering physical quantities expressed in SI or similar units (meters, radians, etc.), such precision seems completely unrelated to both task requirements and hardware capacity.","Typically, humanoid robots never achieve, nor are capable of achieving sub-millimeter precision in manipulation tasks.","With this observation in mind, our objectives in this paper are two-fold: first examine how the QP solution accuracy impacts the resulting robot motion accuracy, then evaluate how a reduced solution accuracy requirement can be leveraged to reduce the corresponding computational effort.","Numerical experiments with a dynamic simulation of a HRP-4 robot indicate that computational effort can be divided by more than 20 while maintaining the desired motion accuracy."],"url":"http://arxiv.org/abs/2407.16617v1"}
{"created":"2024-07-23 16:24:29","title":"Implementing engrams from a machine learning perspective: the relevance of a latent space","abstract":"In our previous work, we proposed that engrams in the brain could be biologically implemented as autoencoders over recurrent neural networks. These autoencoders would comprise basic excitatory/inhibitory motifs, with credit assignment deriving from a simple homeostatic criterion. This brief note examines the relevance of the latent space in these autoencoders. We consider the relationship between the dimensionality of these autoencoders and the complexity of the information being encoded. We discuss how observed differences between species in their connectome could be linked to their cognitive capacities. Finally, we link this analysis with a basic but often overlooked fact: human cognition is likely limited by our own brain structure. However, this limitation does not apply to machine learning systems, and we should be aware of the need to learn how to exploit this augmented vision of the nature.","sentences":["In our previous work, we proposed that engrams in the brain could be biologically implemented as autoencoders over recurrent neural networks.","These autoencoders would comprise basic excitatory/inhibitory motifs, with credit assignment deriving from a simple homeostatic criterion.","This brief note examines the relevance of the latent space in these autoencoders.","We consider the relationship between the dimensionality of these autoencoders and the complexity of the information being encoded.","We discuss how observed differences between species in their connectome could be linked to their cognitive capacities.","Finally, we link this analysis with a basic but often overlooked fact: human cognition is likely limited by our own brain structure.","However, this limitation does not apply to machine learning systems, and we should be aware of the need to learn how to exploit this augmented vision of the nature."],"url":"http://arxiv.org/abs/2407.16616v1"}
{"created":"2024-07-23 16:23:04","title":"Lawma: The Power of Specialization for Legal Tasks","abstract":"Annotation and classification of legal text are central components of empirical legal research. Traditionally, these tasks are often delegated to trained research assistants. Motivated by the advances in language modeling, empirical legal scholars are increasingly turning to prompting commercial models, hoping that it will alleviate the significant cost of human annotation. Despite growing use, our understanding of how to best utilize large language models for legal tasks remains limited. We conduct a comprehensive study of 260 legal text classification tasks, nearly all new to the machine learning community. Starting from GPT-4 as a baseline, we show that it has non-trivial but highly varied zero-shot accuracy, often exhibiting performance that may be insufficient for legal work. We then demonstrate that a lightly fine-tuned Llama 3 model vastly outperforms GPT-4 on almost all tasks, typically by double-digit percentage points. We find that larger models respond better to fine-tuning than smaller models. A few tens to hundreds of examples suffice to achieve high classification accuracy. Notably, we can fine-tune a single model on all 260 tasks simultaneously at a small loss in accuracy relative to having a separate model for each task. Our work points to a viable alternative to the predominant practice of prompting commercial models. For concrete legal tasks with some available labeled data, researchers are better off using a fine-tuned open-source model.","sentences":["Annotation and classification of legal text are central components of empirical legal research.","Traditionally, these tasks are often delegated to trained research assistants.","Motivated by the advances in language modeling, empirical legal scholars are increasingly turning to prompting commercial models, hoping that it will alleviate the significant cost of human annotation.","Despite growing use, our understanding of how to best utilize large language models for legal tasks remains limited.","We conduct a comprehensive study of 260 legal text classification tasks, nearly all new to the machine learning community.","Starting from GPT-4 as a baseline, we show that it has non-trivial but highly varied zero-shot accuracy, often exhibiting performance that may be insufficient for legal work.","We then demonstrate that a lightly fine-tuned Llama 3 model vastly outperforms GPT-4 on almost all tasks, typically by double-digit percentage points.","We find that larger models respond better to fine-tuning than smaller models.","A few tens to hundreds of examples suffice to achieve high classification accuracy.","Notably, we can fine-tune a single model on all 260 tasks simultaneously at a small loss in accuracy relative to having a separate model for each task.","Our work points to a viable alternative to the predominant practice of prompting commercial models.","For concrete legal tasks with some available labeled data, researchers are better off using a fine-tuned open-source model."],"url":"http://arxiv.org/abs/2407.16615v1"}
{"created":"2024-07-23 16:21:52","title":"Mobile Technology: A Panacea to Food Insecurity In Nigeria -- A Case Study of SELL HARVEST Application","abstract":"Over time, agriculture is the most consistent activity, and it evolves every day. It contributes to a vast majority of the Gross Domestic Product (GDP) of Nigeria but as ironic as it may be, there is still hunger in significant parts of the country due to low productivity in the agricultural sector and comparison to the geometric population growth. During the first half of 2022, agriculture contributed about 23% of the country's GDP while the industry and services sector had a share of the remaining 77%. This showed that with the high rate of agricultural activities, Nigeria has not achieved food security for the teeming population. and more productivity levels can be attained. Technology can/will assist Nigeria in overcoming global poverty and hunger quicker in both rural and urban areas. Today, there are many types of agricultural technologies available for farmers all over the world to increase productivity. Major technological advancements include indoor vertical farming, automation, robotics, livestock technology, modern greenhouse practices, precision agriculture, artificial intelligence, and blockchain. Mobile phones have one of the highest adoption rates of technologies developed within the last century. Digitalization will bring consumers and farmers closer together to access the shortest supply chain possible and reduce rural poverty and hunger. The paper will review the different agricultural technologies and propose a mobile solution, code Sell Harvest, to make farming more sustainable and secure food.   Keywords: Sell Harvest, Agriculture, Technology, Artificial Intelligence, and Digital Farming.","sentences":["Over time, agriculture is the most consistent activity, and it evolves every day.","It contributes to a vast majority of the Gross Domestic Product (GDP) of Nigeria but as ironic as it may be, there is still hunger in significant parts of the country due to low productivity in the agricultural sector and comparison to the geometric population growth.","During the first half of 2022, agriculture contributed about 23% of the country's GDP while the industry and services sector had a share of the remaining 77%.","This showed that with the high rate of agricultural activities, Nigeria has not achieved food security for the teeming population.","and more productivity levels can be attained.","Technology can/will assist Nigeria in overcoming global poverty and hunger quicker in both rural and urban areas.","Today, there are many types of agricultural technologies available for farmers all over the world to increase productivity.","Major technological advancements include indoor vertical farming, automation, robotics, livestock technology, modern greenhouse practices, precision agriculture, artificial intelligence, and blockchain.","Mobile phones have one of the highest adoption rates of technologies developed within the last century.","Digitalization will bring consumers and farmers closer together to access the shortest supply chain possible and reduce rural poverty and hunger.","The paper will review the different agricultural technologies and propose a mobile solution, code Sell Harvest, to make farming more sustainable and secure food.   ","Keywords: Sell Harvest, Agriculture, Technology, Artificial Intelligence, and Digital Farming."],"url":"http://arxiv.org/abs/2407.16614v1"}
{"created":"2024-07-23 16:20:36","title":"No-brainer: Morphological Computation driven Adaptive Behavior in Soft Robots","abstract":"It is prevalent in contemporary AI and robotics to separately postulate a brain modeled by neural networks and employ it to learn intelligent and adaptive behavior. While this method has worked very well for many types of tasks, it isn't the only type of intelligence that exists in nature. In this work, we study the ways in which intelligent behavior can be created without a separate and explicit brain for robot control, but rather solely as a result of the computation occurring within the physical body of a robot. Specifically, we show that adaptive and complex behavior can be created in voxel-based virtual soft robots by using simple reactive materials that actively change the shape of the robot, and thus its behavior, under different environmental cues. We demonstrate a proof of concept for the idea of closed-loop morphological computation, and show that in our implementation, it enables behavior mimicking logic gates, enabling us to demonstrate how such behaviors may be combined to build up more complex collective behaviors.","sentences":["It is prevalent in contemporary AI and robotics to separately postulate a brain modeled by neural networks and employ it to learn intelligent and adaptive behavior.","While this method has worked very well for many types of tasks, it isn't the only type of intelligence that exists in nature.","In this work, we study the ways in which intelligent behavior can be created without a separate and explicit brain for robot control, but rather solely as a result of the computation occurring within the physical body of a robot.","Specifically, we show that adaptive and complex behavior can be created in voxel-based virtual soft robots by using simple reactive materials that actively change the shape of the robot, and thus its behavior, under different environmental cues.","We demonstrate a proof of concept for the idea of closed-loop morphological computation, and show that in our implementation, it enables behavior mimicking logic gates, enabling us to demonstrate how such behaviors may be combined to build up more complex collective behaviors."],"url":"http://arxiv.org/abs/2407.16613v1"}
{"created":"2024-07-23 16:18:00","title":"Local vs Global continual learning","abstract":"Continual learning is the problem of integrating new information in a model while retaining the knowledge acquired in the past. Despite the tangible improvements achieved in recent years, the problem of continual learning is still an open one. A better understanding of the mechanisms behind the successes and failures of existing continual learning algorithms can unlock the development of new successful strategies. In this work, we view continual learning from the perspective of the multi-task loss approximation, and we compare two alternative strategies, namely local and global approximations. We classify existing continual learning algorithms based on the approximation used, and we assess the practical effects of this distinction in common continual learning settings.Additionally, we study optimal continual learning objectives in the case of local polynomial approximations and we provide examples of existing algorithms implementing the optimal objectives","sentences":["Continual learning is the problem of integrating new information in a model while retaining the knowledge acquired in the past.","Despite the tangible improvements achieved in recent years, the problem of continual learning is still an open one.","A better understanding of the mechanisms behind the successes and failures of existing continual learning algorithms can unlock the development of new successful strategies.","In this work, we view continual learning from the perspective of the multi-task loss approximation, and we compare two alternative strategies, namely local and global approximations.","We classify existing continual learning algorithms based on the approximation used, and we assess the practical effects of this distinction in common continual learning settings.","Additionally, we study optimal continual learning objectives in the case of local polynomial approximations and we provide examples of existing algorithms implementing the optimal objectives"],"url":"http://arxiv.org/abs/2407.16611v1"}
{"created":"2024-07-23 16:13:22","title":"Data Mixture Inference: What do BPE Tokenizers Reveal about their Training Data?","abstract":"The pretraining data of today's strongest language models is opaque. In particular, little is known about the proportions of various domains or languages represented. In this work, we tackle a task which we call data mixture inference, which aims to uncover the distributional make-up of training data. We introduce a novel attack based on a previously overlooked source of information -- byte-pair encoding (BPE) tokenizers, used by the vast majority of modern language models. Our key insight is that the ordered list of merge rules learned by a BPE tokenizer naturally reveals information about the token frequencies in its training data: the first merge is the most common byte pair, the second is the most common pair after merging the first token, and so on. Given a tokenizer's merge list along with data samples for each category of interest, we formulate a linear program that solves for the proportion of each category in the tokenizer's training set. Importantly, to the extent to which tokenizer training data is representative of the pretraining data, we indirectly learn about the pretraining data. In controlled experiments, we show that our attack recovers mixture ratios with high precision for tokenizers trained on known mixtures of natural languages, programming languages, and data sources. We then apply our approach to off-the-shelf tokenizers released with recent LMs. We confirm much publicly disclosed information about these models, and also make several new inferences: GPT-4o's tokenizer is much more multilingual than its predecessors, training on 39% non-English data; Llama3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%). We hope our work sheds light on current design practices for pretraining data, and inspires continued research into data mixture inference for LMs.","sentences":["The pretraining data of today's strongest language models is opaque.","In particular, little is known about the proportions of various domains or languages represented.","In this work, we tackle a task which we call data mixture inference, which aims to uncover the distributional make-up of training data.","We introduce a novel attack based on a previously overlooked source of information -- byte-pair encoding (BPE) tokenizers, used by the vast majority of modern language models.","Our key insight is that the ordered list of merge rules learned by a BPE tokenizer naturally reveals information about the token frequencies in its training data: the first merge is the most common byte pair, the second is the most common pair after merging the first token, and so on.","Given a tokenizer's merge list along with data samples for each category of interest, we formulate a linear program that solves for the proportion of each category in the tokenizer's training set.","Importantly, to the extent to which tokenizer training data is representative of the pretraining data, we indirectly learn about the pretraining data.","In controlled experiments, we show that our attack recovers mixture ratios with high precision for tokenizers trained on known mixtures of natural languages, programming languages, and data sources.","We then apply our approach to off-the-shelf tokenizers released with recent LMs.","We confirm much publicly disclosed information about these models, and also make several new inferences: GPT-4o's tokenizer is much more multilingual than its predecessors, training on 39% non-English data; Llama3 extends GPT-3.5's tokenizer primarily for multilingual (48%) use; GPT-3.5's and Claude's tokenizers are trained on predominantly code (~60%).","We hope our work sheds light on current design practices for pretraining data, and inspires continued research into data mixture inference for LMs."],"url":"http://arxiv.org/abs/2407.16607v1"}
{"created":"2024-07-23 16:11:08","title":"Learning to Play Foosball: System and Baselines","abstract":"This work stages Foosball as a versatile platform for advancing scientific research, particularly in the realm of robot learning. We present an automated Foosball table along with its corresponding simulated counterpart, showcasing a diverse range of challenges through example tasks within the Foosball environment. Initial findings are shared using a simple baseline approach. Foosball constitutes a versatile learning environment with the potential to yield cutting-edge research in various fields of artificial intelligence and machine learning, notably robust learning, while also extending its applicability to industrial robotics and automation setups. To transform our physical Foosball table into a research-friendly system, we augmented it with a 2 degrees of freedom kinematic chain to control the goalkeeper rod as an initial setup with the intention to be extended to the full game as soon as possible. Our experiments reveal that a realistic simulation is essential for mastering complex robotic tasks, yet translating these accomplishments to the real system remains challenging, often accompanied by a performance decline. This emphasizes the critical importance of research in this direction. In this concern, we spotlight the automated Foosball table as an invaluable tool, possessing numerous desirable attributes, to serve as a demanding learning environment for advancing robotics and automation research.","sentences":["This work stages Foosball as a versatile platform for advancing scientific research, particularly in the realm of robot learning.","We present an automated Foosball table along with its corresponding simulated counterpart, showcasing a diverse range of challenges through example tasks within the Foosball environment.","Initial findings are shared using a simple baseline approach.","Foosball constitutes a versatile learning environment with the potential to yield cutting-edge research in various fields of artificial intelligence and machine learning, notably robust learning, while also extending its applicability to industrial robotics and automation setups.","To transform our physical Foosball table into a research-friendly system, we augmented it with a 2 degrees of freedom kinematic chain to control the goalkeeper rod as an initial setup with the intention to be extended to the full game as soon as possible.","Our experiments reveal that a realistic simulation is essential for mastering complex robotic tasks, yet translating these accomplishments to the real system remains challenging, often accompanied by a performance decline.","This emphasizes the critical importance of research in this direction.","In this concern, we spotlight the automated Foosball table as an invaluable tool, possessing numerous desirable attributes, to serve as a demanding learning environment for advancing robotics and automation research."],"url":"http://arxiv.org/abs/2407.16606v1"}
{"created":"2024-07-23 16:06:22","title":"Shared Imagination: LLMs Hallucinate Alike","abstract":"Despite the recent proliferation of large language models (LLMs), their training recipes -- model architecture, pre-training data and optimization algorithm -- are often very similar. This naturally raises the question of the similarity among the resulting models. In this paper, we propose a novel setting, imaginary question answering (IQA), to better understand model similarity. In IQA, we ask one model to generate purely imaginary questions (e.g., on completely made-up concepts in physics) and prompt another model to answer. Surprisingly, despite the total fictionality of these questions, all models can answer each other's questions with remarkable success, suggesting a \"shared imagination space\" in which these models operate during such hallucinations. We conduct a series of investigations into this phenomenon and discuss implications on model homogeneity, hallucination, and computational creativity.","sentences":["Despite the recent proliferation of large language models (LLMs), their training recipes -- model architecture, pre-training data and optimization algorithm -- are often very similar.","This naturally raises the question of the similarity among the resulting models.","In this paper, we propose a novel setting, imaginary question answering (IQA), to better understand model similarity.","In IQA, we ask one model to generate purely imaginary questions (e.g., on completely made-up concepts in physics) and prompt another model to answer.","Surprisingly, despite the total fictionality of these questions, all models can answer each other's questions with remarkable success, suggesting a \"shared imagination space\" in which these models operate during such hallucinations.","We conduct a series of investigations into this phenomenon and discuss implications on model homogeneity, hallucination, and computational creativity."],"url":"http://arxiv.org/abs/2407.16604v1"}
{"created":"2024-07-23 16:04:55","title":"Functional Acceleration for Policy Mirror Descent","abstract":"We apply functional acceleration to the Policy Mirror Descent (PMD) general family of algorithms, which cover a wide range of novel and fundamental methods in Reinforcement Learning (RL). Leveraging duality, we propose a momentum-based PMD update. By taking the functional route, our approach is independent of the policy parametrization and applicable to large-scale optimization, covering previous applications of momentum at the level of policy parameters as a special case. We theoretically analyze several properties of this approach and complement with a numerical ablation study, which serves to illustrate the policy optimization dynamics on the value polytope, relative to different algorithmic design choices in this space. We further characterize numerically several features of the problem setting relevant for functional acceleration, and lastly, we investigate the impact of approximation on their learning mechanics.","sentences":["We apply functional acceleration to the Policy Mirror Descent (PMD) general family of algorithms, which cover a wide range of novel and fundamental methods in Reinforcement Learning (RL).","Leveraging duality, we propose a momentum-based PMD update.","By taking the functional route, our approach is independent of the policy parametrization and applicable to large-scale optimization, covering previous applications of momentum at the level of policy parameters as a special case.","We theoretically analyze several properties of this approach and complement with a numerical ablation study, which serves to illustrate the policy optimization dynamics on the value polytope, relative to different algorithmic design choices in this space.","We further characterize numerically several features of the problem setting relevant for functional acceleration, and lastly, we investigate the impact of approximation on their learning mechanics."],"url":"http://arxiv.org/abs/2407.16602v1"}
{"created":"2024-07-23 16:04:32","title":"Considering dynamical synergy and integrated information; the unusual case of minimum mutual information","abstract":"This brief note considers the problem of estimating temporal synergy and integrated information in dyadic dynamical processes. One of the standard estimators of dynamic synergy is based on the minimal mutual information between sets of elements, however, despite it's increasingly widespread use, the mathematical features of this redundancy function have largely gone unexplored. Here, we show that it has two previously unrecognized limitations: it cannot disambiguate between truly integrated systems and disintegrated systems with first-order autocorrelation. Second, paradoxically, there are some systems that become more synergistic when dis-integrated (as long as first-order autocorrelations are preserved). In these systems, integrated information can decrease while synergy simultaneously increases. We derive conditions under which this occurs and discuss the implications of these findings for past and future work in applied fields such as neuroscience.","sentences":["This brief note considers the problem of estimating temporal synergy and integrated information in dyadic dynamical processes.","One of the standard estimators of dynamic synergy is based on the minimal mutual information between sets of elements, however, despite it's increasingly widespread use, the mathematical features of this redundancy function have largely gone unexplored.","Here, we show that it has two previously unrecognized limitations: it cannot disambiguate between truly integrated systems and disintegrated systems with first-order autocorrelation.","Second, paradoxically, there are some systems that become more synergistic when dis-integrated (as long as first-order autocorrelations are preserved).","In these systems, integrated information can decrease while synergy simultaneously increases.","We derive conditions under which this occurs and discuss the implications of these findings for past and future work in applied fields such as neuroscience."],"url":"http://arxiv.org/abs/2407.16601v1"}
{"created":"2024-07-23 16:03:02","title":"DHGS: Decoupled Hybrid Gaussian Splatting for Driving Scene","abstract":"Existing Gaussian splatting methods struggle to achieve satisfactory novel view synthesis in driving scenes due to the lack of crafty design and geometric constraints of related elements. This paper introduces a novel method called Decoupled Hybrid Gaussian Splatting (DHGS), which aims at promoting the rendering quality of novel view synthesis for driving scenes. The novelty of this work lies in the decoupled and hybrid pixel-level blender for road and non-road layers, without conventional unified differentiable rendering logic for the entire scene, meanwhile maintaining consistent and continuous superimposition through the proposed depth-ordered rendering strategy. Beyond that, an implicit road representation comprised of Signed Distance Field (SDF) is trained to supervise the road surface with subtle geometric attributes. Accompanied by the use of auxiliary transmittance loss and consistency loss, novel images with imperceptible boundary and elevated fidelity are ultimately obtained. Substantial experiments on Waymo dataset prove that DHGS outperforms the state-of-the-art methods.","sentences":["Existing Gaussian splatting methods struggle to achieve satisfactory novel view synthesis in driving scenes due to the lack of crafty design and geometric constraints of related elements.","This paper introduces a novel method called Decoupled Hybrid Gaussian Splatting (DHGS), which aims at promoting the rendering quality of novel view synthesis for driving scenes.","The novelty of this work lies in the decoupled and hybrid pixel-level blender for road and non-road layers, without conventional unified differentiable rendering logic for the entire scene, meanwhile maintaining consistent and continuous superimposition through the proposed depth-ordered rendering strategy.","Beyond that, an implicit road representation comprised of Signed Distance Field (SDF) is trained to supervise the road surface with subtle geometric attributes.","Accompanied by the use of auxiliary transmittance loss and consistency loss, novel images with imperceptible boundary and elevated fidelity are ultimately obtained.","Substantial experiments on Waymo dataset prove that DHGS outperforms the state-of-the-art methods."],"url":"http://arxiv.org/abs/2407.16600v1"}
{"created":"2024-07-23 15:53:17","title":"GenRec: A Flexible Data Generator for Recommendations","abstract":"The scarcity of realistic datasets poses a significant challenge in benchmarking recommender systems and social network analysis methods and techniques. A common and effective solution is to generate synthetic data that simulates realistic interactions. However, although various methods have been proposed, the existing literature still lacks generators that are fully adaptable and allow easy manipulation of the underlying data distributions and structural properties. To address this issue, the present work introduces GenRec, a novel framework for generating synthetic user-item interactions that exhibit realistic and well-known properties observed in recommendation scenarios. The framework is based on a stochastic generative process based on latent factor modeling. Here, the latent factors can be exploited to yield long-tailed preference distributions, and at the same time they characterize subpopulations of users and topic-based item clusters. Notably, the proposed framework is highly flexible and offers a wide range of hyper-parameters for customizing the generation of user-item interactions. The code used to perform the experiments is publicly available at https://anonymous.4open.science/r/GenRec-DED3.","sentences":["The scarcity of realistic datasets poses a significant challenge in benchmarking recommender systems and social network analysis methods and techniques.","A common and effective solution is to generate synthetic data that simulates realistic interactions.","However, although various methods have been proposed, the existing literature still lacks generators that are fully adaptable and allow easy manipulation of the underlying data distributions and structural properties.","To address this issue, the present work introduces GenRec, a novel framework for generating synthetic user-item interactions that exhibit realistic and well-known properties observed in recommendation scenarios.","The framework is based on a stochastic generative process based on latent factor modeling.","Here, the latent factors can be exploited to yield long-tailed preference distributions, and at the same time they characterize subpopulations of users and topic-based item clusters.","Notably, the proposed framework is highly flexible and offers a wide range of hyper-parameters for customizing the generation of user-item interactions.","The code used to perform the experiments is publicly available at https://anonymous.4open.science/r/GenRec-DED3."],"url":"http://arxiv.org/abs/2407.16594v1"}
{"created":"2024-07-23 15:51:46","title":"A Comparative Study on Patient Language across Therapeutic Domains for Effective Patient Voice Classification in Online Health Discussions","abstract":"There exists an invisible barrier between healthcare professionals' perception of a patient's clinical experience and the reality. This barrier may be induced by the environment that hinders patients from sharing their experiences openly with healthcare professionals. As patients are observed to discuss and exchange knowledge more candidly on social media, valuable insights can be leveraged from these platforms. However, the abundance of non-patient posts on social media necessitates filtering out such irrelevant content to distinguish the genuine voices of patients, a task we refer to as patient voice classification. In this study, we analyse the importance of linguistic characteristics in accurately classifying patient voices. Our findings underscore the essential role of linguistic and statistical text similarity analysis in identifying common patterns among patient groups. These results allude to even starker differences in the way patients express themselves at a disease level and across various therapeutic domains. Additionally, we fine-tuned a pre-trained Language Model on the combined datasets with similar linguistic patterns, resulting in a highly accurate automatic patient voice classification. Being the pioneering study on the topic, our focus on extracting authentic patient experiences from social media stands as a crucial step towards advancing healthcare standards and fostering a patient-centric approach.","sentences":["There exists an invisible barrier between healthcare professionals' perception of a patient's clinical experience and the reality.","This barrier may be induced by the environment that hinders patients from sharing their experiences openly with healthcare professionals.","As patients are observed to discuss and exchange knowledge more candidly on social media, valuable insights can be leveraged from these platforms.","However, the abundance of non-patient posts on social media necessitates filtering out such irrelevant content to distinguish the genuine voices of patients, a task we refer to as patient voice classification.","In this study, we analyse the importance of linguistic characteristics in accurately classifying patient voices.","Our findings underscore the essential role of linguistic and statistical text similarity analysis in identifying common patterns among patient groups.","These results allude to even starker differences in the way patients express themselves at a disease level and across various therapeutic domains.","Additionally, we fine-tuned a pre-trained Language Model on the combined datasets with similar linguistic patterns, resulting in a highly accurate automatic patient voice classification.","Being the pioneering study on the topic, our focus on extracting authentic patient experiences from social media stands as a crucial step towards advancing healthcare standards and fostering a patient-centric approach."],"url":"http://arxiv.org/abs/2407.16593v1"}
{"created":"2024-07-23 15:48:36","title":"Real-Time Interactions Between Human Controllers and Remote Devices in Metaverse","abstract":"Supporting real-time interactions between human controllers and remote devices remains a challenging goal in the Metaverse due to the stringent requirements on computing workload, communication throughput, and round-trip latency. In this paper, we establish a novel framework for real-time interactions through the virtual models in the Metaverse. Specifically, we jointly predict the motion of the human controller for 1) proactive rendering in the Metaverse and 2) generating control commands to the real-world remote device in advance. The virtual model is decoupled into two components for rendering and control, respectively. To dynamically adjust the prediction horizons for rendering and control, we develop a two-step human-in-the-loop continuous reinforcement learning approach and use an expert policy to improve the training efficiency. An experimental prototype is built to verify our algorithm with different communication latencies. Compared with the baseline policy without prediction, our proposed method can reduce 1) the Motion-To-Photon (MTP) latency between human motion and rendering feedback and 2) the root mean squared error (RMSE) between human motion and real-world remote devices significantly.","sentences":["Supporting real-time interactions between human controllers and remote devices remains a challenging goal in the Metaverse due to the stringent requirements on computing workload, communication throughput, and round-trip latency.","In this paper, we establish a novel framework for real-time interactions through the virtual models in the Metaverse.","Specifically, we jointly predict the motion of the human controller for 1) proactive rendering in the Metaverse and 2) generating control commands to the real-world remote device in advance.","The virtual model is decoupled into two components for rendering and control, respectively.","To dynamically adjust the prediction horizons for rendering and control, we develop a two-step human-in-the-loop continuous reinforcement learning approach and use an expert policy to improve the training efficiency.","An experimental prototype is built to verify our algorithm with different communication latencies.","Compared with the baseline policy without prediction, our proposed method can reduce 1) the Motion-To-Photon (MTP) latency between human motion and rendering feedback and 2) the root mean squared error (RMSE) between human motion and real-world remote devices significantly."],"url":"http://arxiv.org/abs/2407.16591v1"}
{"created":"2024-07-23 15:40:35","title":"A Faster Branching Algorithm for the Maximum $k$-Defective Clique Problem","abstract":"A $k$-defective clique of an undirected graph $G$ is a subset of its vertices that induces a nearly complete graph with a maximum of $k$ missing edges. The maximum $k$-defective clique problem, which asks for the largest $k$-defective clique from the given graph, is important in many applications, such as social and biological network analysis. In the paper, we propose a new branching algorithm that takes advantage of the structural properties of the $k$-defective clique and uses the efficient maximum clique algorithm as a subroutine. As a result, the algorithm has a better asymptotic running time than the existing ones. We also investigate upper-bounding techniques and propose a new upper bound utilizing the \\textit{conflict relationship} between vertex pairs. Because conflict relationship is common in many graph problems, we believe that this technique can be potentially generalized. Finally, experiments show that our algorithm outperforms state-of-the-art solvers on a wide range of open benchmarks.","sentences":["A $k$-defective clique of an undirected graph $G$ is a subset of its vertices that induces a nearly complete graph with a maximum of $k$ missing edges.","The maximum $k$-defective clique problem, which asks for the largest $k$-defective clique from the given graph, is important in many applications, such as social and biological network analysis.","In the paper, we propose a new branching algorithm that takes advantage of the structural properties of the $k$-defective clique and uses the efficient maximum clique algorithm as a subroutine.","As a result, the algorithm has a better asymptotic running time than the existing ones.","We also investigate upper-bounding techniques and propose a new upper bound utilizing the \\textit{conflict relationship} between vertex pairs.","Because conflict relationship is common in many graph problems, we believe that this technique can be potentially generalized.","Finally, experiments show that our algorithm outperforms state-of-the-art solvers on a wide range of open benchmarks."],"url":"http://arxiv.org/abs/2407.16588v1"}
{"created":"2024-07-23 15:39:07","title":"A Simple Algorithm for Near-Vizing Edge-Coloring in Near-Linear Time","abstract":"We present a simple $(1+\\varepsilon)\\Delta$-edge-coloring algorithm for graphs of maximum degree $\\Delta = \\Omega(\\log n / \\varepsilon)$ with running time $O\\left(m\\,\\log^3 n/\\varepsilon^3\\right)$. Our algorithm improves upon that of [Duan, He, and Zhang; SODA19], which was the first near-linear time algorithm for this problem. While our results are weaker than the current state-of-the-art, our approach is significantly simpler, both in terms of analysis as well as implementation, and may be of practical interest.","sentences":["We present a simple $(1+\\varepsilon)\\Delta$-edge-coloring algorithm for graphs of maximum degree $\\Delta = \\Omega(\\log n / \\varepsilon)$ with running time $O\\left(m\\,\\log^3 n/\\varepsilon^3\\right)$. Our algorithm improves upon that of [Duan, He, and Zhang; SODA19], which was the first near-linear time algorithm for this problem.","While our results are weaker than the current state-of-the-art, our approach is significantly simpler, both in terms of analysis as well as implementation, and may be of practical interest."],"url":"http://arxiv.org/abs/2407.16585v1"}
{"created":"2024-07-23 15:31:26","title":"Exploring Automatic Cryptographic API Misuse Detection in the Era of LLMs","abstract":"While the automated detection of cryptographic API misuses has progressed significantly, its precision diminishes for intricate targets due to the reliance on manually defined patterns. Large Language Models (LLMs), renowned for their contextual understanding, offer a promising avenue to address existing shortcomings. However, applying LLMs in this security-critical domain presents challenges, particularly due to the unreliability stemming from LLMs' stochastic nature and the well-known issue of hallucination. To explore the prevalence of LLMs' unreliable analysis and potential solutions, this paper introduces a systematic evaluation framework to assess LLMs in detecting cryptographic misuses, utilizing a comprehensive dataset encompassing both manually-crafted samples and real-world projects. Our in-depth analysis of 11,940 LLM-generated reports highlights that the inherent instabilities in LLMs can lead to over half of the reports being false positives. Nevertheless, we demonstrate how a constrained problem scope, coupled with LLMs' self-correction capability, significantly enhances the reliability of the detection. The optimized approach achieves a remarkable detection rate of nearly 90%, surpassing traditional methods and uncovering previously unknown misuses in established benchmarks. Moreover, we identify the failure patterns that persistently hinder LLMs' reliability, including both cryptographic knowledge deficiency and code semantics misinterpretation. Guided by these insights, we develop an LLM-based workflow to examine open-source repositories, leading to the discovery of 63 real-world cryptographic misuses. Of these, 46 have been acknowledged by the development community, with 23 currently being addressed and 6 resolved. Reflecting on developers' feedback, we offer recommendations for future research and the development of LLM-based security tools.","sentences":["While the automated detection of cryptographic API misuses has progressed significantly, its precision diminishes for intricate targets due to the reliance on manually defined patterns.","Large Language Models (LLMs), renowned for their contextual understanding, offer a promising avenue to address existing shortcomings.","However, applying LLMs in this security-critical domain presents challenges, particularly due to the unreliability stemming from LLMs' stochastic nature and the well-known issue of hallucination.","To explore the prevalence of LLMs' unreliable analysis and potential solutions, this paper introduces a systematic evaluation framework to assess LLMs in detecting cryptographic misuses, utilizing a comprehensive dataset encompassing both manually-crafted samples and real-world projects.","Our in-depth analysis of 11,940 LLM-generated reports highlights that the inherent instabilities in LLMs can lead to over half of the reports being false positives.","Nevertheless, we demonstrate how a constrained problem scope, coupled with LLMs' self-correction capability, significantly enhances the reliability of the detection.","The optimized approach achieves a remarkable detection rate of nearly 90%, surpassing traditional methods and uncovering previously unknown misuses in established benchmarks.","Moreover, we identify the failure patterns that persistently hinder LLMs' reliability, including both cryptographic knowledge deficiency and code semantics misinterpretation.","Guided by these insights, we develop an LLM-based workflow to examine open-source repositories, leading to the discovery of 63 real-world cryptographic misuses.","Of these, 46 have been acknowledged by the development community, with 23 currently being addressed and 6 resolved.","Reflecting on developers' feedback, we offer recommendations for future research and the development of LLM-based security tools."],"url":"http://arxiv.org/abs/2407.16576v1"}
{"created":"2024-07-23 15:30:08","title":"Timeliness-Fidelity Tradeoff in 3D Scene Representations","abstract":"Real-time three-dimensional (3D) scene representations serve as one of the building blocks that bolster various innovative applications, e.g., digital manufacturing, Virtual/Augmented/Extended/Mixed Reality (VR/AR/XR/MR), and the metaverse. Despite substantial efforts that have been made to real-time communications and computing, real-time 3D scene representations remain a challenging task. This paper investigates the tradeoff between timeliness and fidelity in real-time 3D scene representations. Specifically, we establish a framework to evaluate the impact of communication delay on the tradeoff, where the real-world scenario is monitored by multiple cameras that communicate with an edge server. To improve fidelity for 3D scene representations, we propose to use a single-step Proximal Policy Optimization (PPO) method that leverages the Age of Information (AoI) to decide if the received image needs to be involved in 3D scene representations and rendering. We test our framework and the proposed approach with different well-known 3D scene representation methods. Simulation results reveal that real-time 3D scene representation can be sensitively affected by communication delay, and our proposed method can achieve optimal 3D scene representation results.","sentences":["Real-time three-dimensional (3D) scene representations serve as one of the building blocks that bolster various innovative applications, e.g., digital manufacturing, Virtual/Augmented/Extended/Mixed Reality (VR/AR/XR/MR), and the metaverse.","Despite substantial efforts that have been made to real-time communications and computing, real-time 3D scene representations remain a challenging task.","This paper investigates the tradeoff between timeliness and fidelity in real-time 3D scene representations.","Specifically, we establish a framework to evaluate the impact of communication delay on the tradeoff, where the real-world scenario is monitored by multiple cameras that communicate with an edge server.","To improve fidelity for 3D scene representations, we propose to use a single-step Proximal Policy Optimization (PPO) method that leverages the Age of Information (AoI) to decide if the received image needs to be involved in 3D scene representations and rendering.","We test our framework and the proposed approach with different well-known 3D scene representation methods.","Simulation results reveal that real-time 3D scene representation can be sensitively affected by communication delay, and our proposed method can achieve optimal 3D scene representation results."],"url":"http://arxiv.org/abs/2407.16575v1"}
{"created":"2024-07-23 15:27:37","title":"TLCR: Token-Level Continuous Reward for Fine-grained Reinforcement Learning from Human Feedback","abstract":"Reinforcement Learning from Human Feedback (RLHF) leverages human preference data to train language models to align more closely with human essence. These human preference data, however, are labeled at the sequence level, creating a mismatch between sequence-level preference labels and tokens, which are autoregressively generated from the language model. Although several recent approaches have tried to provide token-level (i.e., dense) rewards for each individual token, these typically rely on predefined discrete reward values (e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying degrees of preference inherent to each token. To address this limitation, we introduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a discriminator trained to distinguish positive and negative tokens, and the confidence of the discriminator is used to assign continuous rewards to each token considering the context. Extensive experiments show that our proposed TLCR leads to consistent performance improvements over previous sequence-level or token-level discrete rewards on open-ended generation benchmarks.","sentences":["Reinforcement Learning from Human Feedback (RLHF) leverages human preference data to train language models to align more closely with human essence.","These human preference data, however, are labeled at the sequence level, creating a mismatch between sequence-level preference labels and tokens, which are autoregressively generated from the language model.","Although several recent approaches have tried to provide token-level (i.e., dense) rewards for each individual token, these typically rely on predefined discrete reward values (e.g., positive: +1, negative: -1, neutral: 0), failing to account for varying degrees of preference inherent to each token.","To address this limitation, we introduce TLCR (Token-Level Continuous Reward) for RLHF, which incorporates a discriminator trained to distinguish positive and negative tokens, and the confidence of the discriminator is used to assign continuous rewards to each token considering the context.","Extensive experiments show that our proposed TLCR leads to consistent performance improvements over previous sequence-level or token-level discrete rewards on open-ended generation benchmarks."],"url":"http://arxiv.org/abs/2407.16574v1"}
{"created":"2024-07-23 15:17:11","title":"Retrieve, Generate, Evaluate: A Case Study for Medical Paraphrases Generation with Small Language Models","abstract":"Recent surge in the accessibility of large language models (LLMs) to the general population can lead to untrackable use of such models for medical-related recommendations. Language generation via LLMs models has two key problems: firstly, they are prone to hallucination and therefore, for any medical purpose they require scientific and factual grounding; secondly, LLMs pose tremendous challenge to computational resources due to their gigantic model size. In this work, we introduce pRAGe, a pipeline for Retrieval Augmented Generation and evaluation of medical paraphrases generation using Small Language Models (SLM). We study the effectiveness of SLMs and the impact of external knowledge base for medical paraphrase generation in French.","sentences":["Recent surge in the accessibility of large language models (LLMs) to the general population can lead to untrackable use of such models for medical-related recommendations.","Language generation via LLMs models has two key problems: firstly, they are prone to hallucination and therefore, for any medical purpose they require scientific and factual grounding; secondly, LLMs pose tremendous challenge to computational resources due to their gigantic model size.","In this work, we introduce pRAGe, a pipeline for Retrieval Augmented Generation and evaluation of medical paraphrases generation using Small Language Models (SLM).","We study the effectiveness of SLMs and the impact of external knowledge base for medical paraphrase generation in French."],"url":"http://arxiv.org/abs/2407.16565v1"}
{"created":"2024-07-23 15:16:18","title":"Audio Prompt Adapter: Unleashing Music Editing Abilities for Text-to-Music with Lightweight Finetuning","abstract":"Text-to-music models allow users to generate nearly realistic musical audio with textual commands. However, editing music audios remains challenging due to the conflicting desiderata of performing fine-grained alterations on the audio while maintaining a simple user interface. To address this challenge, we propose Audio Prompt Adapter (or AP-Adapter), a lightweight addition to pretrained text-to-music models. We utilize AudioMAE to extract features from the input audio, and construct attention-based adapters to feedthese features into the internal layers of AudioLDM2, a diffusion-based text-to-music model. With 22M trainable parameters, AP-Adapter empowers users to harness both global (e.g., genre and timbre) and local (e.g., melody) aspects of music, using the original audio and a short text as inputs. Through objective and subjective studies, we evaluate AP-Adapter on three tasks: timbre transfer, genre transfer, and accompaniment generation. Additionally, we demonstrate its effectiveness on out-of-domain audios containing unseen instruments during training.","sentences":["Text-to-music models allow users to generate nearly realistic musical audio with textual commands.","However, editing music audios remains challenging due to the conflicting desiderata of performing fine-grained alterations on the audio while maintaining a simple user interface.","To address this challenge, we propose Audio Prompt Adapter (or AP-Adapter), a lightweight addition to pretrained text-to-music models.","We utilize AudioMAE to extract features from the input audio, and construct attention-based adapters to feedthese features into the internal layers of AudioLDM2, a diffusion-based text-to-music model.","With 22M trainable parameters, AP-Adapter empowers users to harness both global (e.g., genre and timbre) and local (e.g., melody) aspects of music, using the original audio and a short text as inputs.","Through objective and subjective studies, we evaluate AP-Adapter on three tasks: timbre transfer, genre transfer, and accompaniment generation.","Additionally, we demonstrate its effectiveness on out-of-domain audios containing unseen instruments during training."],"url":"http://arxiv.org/abs/2407.16564v1"}
{"created":"2024-07-23 15:14:39","title":"COALA: A Practical and Vision-Centric Federated Learning Platform","abstract":"We present COALA, a vision-centric Federated Learning (FL) platform, and a suite of benchmarks for practical FL scenarios, which we categorize into three levels: task, data, and model. At the task level, COALA extends support from simple classification to 15 computer vision tasks, including object detection, segmentation, pose estimation, and more. It also facilitates federated multiple-task learning, allowing clients to tackle multiple tasks simultaneously. At the data level, COALA goes beyond supervised FL to benchmark both semi-supervised FL and unsupervised FL. It also benchmarks feature distribution shifts other than commonly considered label distribution shifts. In addition to dealing with static data, it supports federated continual learning for continuously changing data in real-world scenarios. At the model level, COALA benchmarks FL with split models and different models in different clients. COALA platform offers three degrees of customization for these practical FL scenarios, including configuration customization, components customization, and workflow customization. We conduct systematic benchmarking experiments for the practical FL scenarios and highlight potential opportunities for further advancements in FL. Codes are open sourced at https://github.com/SonyResearch/COALA.","sentences":["We present COALA, a vision-centric Federated Learning (FL) platform, and a suite of benchmarks for practical FL scenarios, which we categorize into three levels: task, data, and model.","At the task level, COALA extends support from simple classification to 15 computer vision tasks, including object detection, segmentation, pose estimation, and more.","It also facilitates federated multiple-task learning, allowing clients to tackle multiple tasks simultaneously.","At the data level, COALA goes beyond supervised FL to benchmark both semi-supervised FL and unsupervised FL.","It also benchmarks feature distribution shifts other than commonly considered label distribution shifts.","In addition to dealing with static data, it supports federated continual learning for continuously changing data in real-world scenarios.","At the model level, COALA benchmarks FL with split models and different models in different clients.","COALA platform offers three degrees of customization for these practical FL scenarios, including configuration customization, components customization, and workflow customization.","We conduct systematic benchmarking experiments for the practical FL scenarios and highlight potential opportunities for further advancements in FL.","Codes are open sourced at https://github.com/SonyResearch/COALA."],"url":"http://arxiv.org/abs/2407.16560v1"}
{"created":"2024-07-23 15:12:14","title":"Patched RTC: evaluating LLMs for diverse software development tasks","abstract":"This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel evaluation technique for Large Language Models (LLMs) applied to diverse software development tasks, particularly focusing on \"outer loop\" activities such as bug fixing, code review, and documentation updates. Patched RTC extends the original Round-Trip Correctness method to work with any LLM and downstream task, offering a self-evaluating framework that measures consistency and robustness of model responses without human intervention. The study demonstrates a correlation between Patched RTC scores and task-specific accuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm for open-domain task evaluation. We implement Patched RTC in an open-source framework called patchwork, allowing for transparent evaluation during inference across various patchflows. Experiments comparing GPT-3.5 and GPT-4 models across different software development tasks reveal that Patched RTC effectively distinguishes model performance and task difficulty. The paper also explores the impact of consistency prompts on improving model accuracy, suggesting that Patched RTC can guide prompt refinement and model selection for complex software development workflows.","sentences":["This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel evaluation technique for Large Language Models (LLMs) applied to diverse software development tasks, particularly focusing on \"outer loop\" activities such as bug fixing, code review, and documentation updates.","Patched RTC extends the original Round-Trip Correctness method to work with any LLM and downstream task, offering a self-evaluating framework that measures consistency and robustness of model responses without human intervention.","The study demonstrates a correlation between Patched RTC scores and task-specific accuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm for open-domain task evaluation.","We implement Patched RTC in an open-source framework called patchwork, allowing for transparent evaluation during inference across various patchflows.","Experiments comparing GPT-3.5 and GPT-4 models across different software development tasks reveal that Patched RTC effectively distinguishes model performance and task difficulty.","The paper also explores the impact of consistency prompts on improving model accuracy, suggesting that Patched RTC can guide prompt refinement and model selection for complex software development workflows."],"url":"http://arxiv.org/abs/2407.16557v1"}
{"created":"2024-07-23 15:09:40","title":"DC is all you need: describing ReLU from a signal processing standpoint","abstract":"Non-linear activation functions are crucial in Convolutional Neural Networks. However, until now they have not been well described in the frequency domain. In this work, we study the spectral behavior of ReLU, a popular activation function. We use the ReLU's Taylor expansion to derive its frequency domain behavior. We demonstrate that ReLU introduces higher frequency oscillations in the signal and a constant DC component. Furthermore, we investigate the importance of this DC component, where we demonstrate that it helps the model extract meaningful features related to the input frequency content. We accompany our theoretical derivations with experiments and real-world examples. First, we numerically validate our frequency response model. Then we observe ReLU's spectral behavior on two example models and a real-world one. Finally, we experimentally investigate the role of the DC component introduced by ReLU in the CNN's representations. Our results indicate that the DC helps to converge to a weight configuration that is close to the initial random weights.","sentences":["Non-linear activation functions are crucial in Convolutional Neural Networks.","However, until now they have not been well described in the frequency domain.","In this work, we study the spectral behavior of ReLU, a popular activation function.","We use the ReLU's Taylor expansion to derive its frequency domain behavior.","We demonstrate that ReLU introduces higher frequency oscillations in the signal and a constant DC component.","Furthermore, we investigate the importance of this DC component, where we demonstrate that it helps the model extract meaningful features related to the input frequency content.","We accompany our theoretical derivations with experiments and real-world examples.","First, we numerically validate our frequency response model.","Then we observe ReLU's spectral behavior on two example models and a real-world one.","Finally, we experimentally investigate the role of the DC component introduced by ReLU in the CNN's representations.","Our results indicate that the DC helps to converge to a weight configuration that is close to the initial random weights."],"url":"http://arxiv.org/abs/2407.16556v1"}
{"created":"2024-07-23 15:07:52","title":"Coarse-to-Fine Proposal Refinement Framework for Audio Temporal Forgery Detection and Localization","abstract":"Recently, a novel form of audio partial forgery has posed challenges to its forensics, requiring advanced countermeasures to detect subtle forgery manipulations within long-duration audio. However, existing countermeasures still serve a classification purpose and fail to perform meaningful analysis of the start and end timestamps of partial forgery segments. To address this challenge, we introduce a novel coarse-to-fine proposal refinement framework (CFPRF) that incorporates a frame-level detection network (FDN) and a proposal refinement network (PRN) for audio temporal forgery detection and localization. Specifically, the FDN aims to mine informative inconsistency cues between real and fake frames to obtain discriminative features that are beneficial for roughly indicating forgery regions. The PRN is responsible for predicting confidence scores and regression offsets to refine the coarse-grained proposals derived from the FDN. To learn robust discriminative features, we devise a difference-aware feature learning (DAFL) module guided by contrastive representation learning to enlarge the sensitive differences between different frames induced by minor manipulations. We further design a boundary-aware feature enhancement (BAFE) module to capture the contextual information of multiple transition boundaries and guide the interaction between boundary information and temporal features via a cross-attention mechanism. Extensive experiments show that our CFPRF achieves state-of-the-art performance on various datasets, including LAV-DF, ASVS2019PS, and HAD.","sentences":["Recently, a novel form of audio partial forgery has posed challenges to its forensics, requiring advanced countermeasures to detect subtle forgery manipulations within long-duration audio.","However, existing countermeasures still serve a classification purpose and fail to perform meaningful analysis of the start and end timestamps of partial forgery segments.","To address this challenge, we introduce a novel coarse-to-fine proposal refinement framework (CFPRF) that incorporates a frame-level detection network (FDN) and a proposal refinement network (PRN) for audio temporal forgery detection and localization.","Specifically, the FDN aims to mine informative inconsistency cues between real and fake frames to obtain discriminative features that are beneficial for roughly indicating forgery regions.","The PRN is responsible for predicting confidence scores and regression offsets to refine the coarse-grained proposals derived from the FDN.","To learn robust discriminative features, we devise a difference-aware feature learning (DAFL) module guided by contrastive representation learning to enlarge the sensitive differences between different frames induced by minor manipulations.","We further design a boundary-aware feature enhancement (BAFE) module to capture the contextual information of multiple transition boundaries and guide the interaction between boundary information and temporal features via a cross-attention mechanism.","Extensive experiments show that our CFPRF achieves state-of-the-art performance on various datasets, including LAV-DF, ASVS2019PS, and HAD."],"url":"http://arxiv.org/abs/2407.16554v1"}
{"created":"2024-07-23 15:05:55","title":"MicroEmo: Time-Sensitive Multimodal Emotion Recognition with Micro-Expression Dynamics in Video Dialogues","abstract":"Multimodal Large Language Models (MLLMs) have demonstrated remarkable multimodal emotion recognition capabilities, integrating multimodal cues from visual, acoustic, and linguistic contexts in the video to recognize human emotional states. However, existing methods ignore capturing local facial features of temporal dynamics of micro-expressions and do not leverage the contextual dependencies of the utterance-aware temporal segments in the video, thereby limiting their expected effectiveness to a certain extent. In this work, we propose MicroEmo, a time-sensitive MLLM aimed at directing attention to the local facial micro-expression dynamics and the contextual dependencies of utterance-aware video clips. Our model incorporates two key architectural contributions: (1) a global-local attention visual encoder that integrates global frame-level timestamp-bound image features with local facial features of temporal dynamics of micro-expressions; (2) an utterance-aware video Q-Former that captures multi-scale and contextual dependencies by generating visual token sequences for each utterance segment and for the entire video then combining them. Preliminary qualitative experiments demonstrate that in a new Explainable Multimodal Emotion Recognition (EMER) task that exploits multi-modal and multi-faceted clues to predict emotions in an open-vocabulary (OV) manner, MicroEmo demonstrates its effectiveness compared with the latest methods.","sentences":["Multimodal Large Language Models (MLLMs) have demonstrated remarkable multimodal emotion recognition capabilities, integrating multimodal cues from visual, acoustic, and linguistic contexts in the video to recognize human emotional states.","However, existing methods ignore capturing local facial features of temporal dynamics of micro-expressions and do not leverage the contextual dependencies of the utterance-aware temporal segments in the video, thereby limiting their expected effectiveness to a certain extent.","In this work, we propose MicroEmo, a time-sensitive MLLM aimed at directing attention to the local facial micro-expression dynamics and the contextual dependencies of utterance-aware video clips.","Our model incorporates two key architectural contributions: (1) a global-local attention visual encoder that integrates global frame-level timestamp-bound image features with local facial features of temporal dynamics of micro-expressions; (2) an utterance-aware video Q-Former that captures multi-scale and contextual dependencies by generating visual token sequences for each utterance segment and for the entire video then combining them.","Preliminary qualitative experiments demonstrate that in a new Explainable Multimodal Emotion Recognition (EMER) task that exploits multi-modal and multi-faceted clues to predict emotions in an open-vocabulary (OV) manner, MicroEmo demonstrates its effectiveness compared with the latest methods."],"url":"http://arxiv.org/abs/2407.16552v1"}
{"created":"2024-07-23 15:05:47","title":"Estimating global article processing charges paid to six publishers for open access between 2019 and 2023","abstract":"This study presents estimates of the global expenditure on article processing charges (APCs) paid to six publishers for open access between 2019 and 2023. APCs are fees charged for publishing in some fully open access journals (gold) and in subscription journals to make individual articles open access (hybrid). There is currently no way to systematically track institutional, national or global expenses for open access publishing due to a lack of transparency in APC prices, what articles they are paid for, or who pays them. We therefore curated and used an open dataset of annual APC list prices from Elsevier, Frontiers, MDPI, PLOS, Springer Nature, and Wiley in combination with the number of open access articles from these publishers indexed by OpenAlex to estimate that, globally, a total of \\$8.349 billion (\\$8.968 billion in 2023 US dollars) were spent on APCs between 2019 and 2023. We estimate that in 2023 MDPI (\\$681.6 million), Elsevier (\\$582.8 million) and Springer Nature (\\$546.6) generated the most revenue with APCs. After adjusting for inflation, we also show that annual spending almost tripled from \\$910.3 million in 2019 to \\$2.538 billion in 2023, that hybrid exceed gold fees, and that the median APCs paid are higher than the median listed fees for both gold and hybrid. Our approach addresses major limitations in previous efforts to estimate APCs paid and offers much needed insight into an otherwise opaque aspect of the business of scholarly publishing. We call upon publishers to be more transparent about OA fees.","sentences":["This study presents estimates of the global expenditure on article processing charges (APCs) paid to six publishers for open access between 2019 and 2023.","APCs are fees charged for publishing in some fully open access journals (gold) and in subscription journals to make individual articles open access (hybrid).","There is currently no way to systematically track institutional, national or global expenses for open access publishing due to a lack of transparency in APC prices, what articles they are paid for, or who pays them.","We therefore curated and used an open dataset of annual APC list prices from Elsevier, Frontiers, MDPI, PLOS, Springer Nature, and Wiley in combination with the number of open access articles from these publishers indexed by OpenAlex to estimate that, globally, a total of \\$8.349 billion (\\$8.968 billion in 2023 US dollars) were spent on APCs between 2019 and 2023.","We estimate that in 2023 MDPI (\\$681.6 million), Elsevier (\\$582.8 million) and Springer Nature (\\$546.6) generated the most revenue with APCs.","After adjusting for inflation, we also show that annual spending almost tripled from \\$910.3 million in 2019 to \\$2.538 billion in 2023, that hybrid exceed gold fees, and that the median APCs paid are higher than the median listed fees for both gold and hybrid.","Our approach addresses major limitations in previous efforts to estimate APCs paid and offers much needed insight into an otherwise opaque aspect of the business of scholarly publishing.","We call upon publishers to be more transparent about OA fees."],"url":"http://arxiv.org/abs/2407.16551v1"}
{"created":"2024-07-23 14:53:47","title":"QPT V2: Masked Image Modeling Advances Visual Scoring","abstract":"Quality assessment and aesthetics assessment aim to evaluate the perceived quality and aesthetics of visual content. Current learning-based methods suffer greatly from the scarcity of labeled data and usually perform sub-optimally in terms of generalization. Although masked image modeling (MIM) has achieved noteworthy advancements across various high-level tasks (e.g., classification, detection etc.). In this work, we take on a novel perspective to investigate its capabilities in terms of quality- and aesthetics-awareness. To this end, we propose Quality- and aesthetics-aware pretraining (QPT V2), the first pretraining framework based on MIM that offers a unified solution to quality and aesthetics assessment. To perceive the high-level semantics and fine-grained details, pretraining data is curated. To comprehensively encompass quality- and aesthetics-related factors, degradation is introduced. To capture multi-scale quality and aesthetic information, model structure is modified. Extensive experimental results on 11 downstream benchmarks clearly show the superior performance of QPT V2 in comparison with current state-of-the-art approaches and other pretraining paradigms. Code and models will be released at \\url{https://github.com/KeiChiTse/QPT-V2}.","sentences":["Quality assessment and aesthetics assessment aim to evaluate the perceived quality and aesthetics of visual content.","Current learning-based methods suffer greatly from the scarcity of labeled data and usually perform sub-optimally in terms of generalization.","Although masked image modeling (MIM) has achieved noteworthy advancements across various high-level tasks (e.g., classification, detection etc.).","In this work, we take on a novel perspective to investigate its capabilities in terms of quality- and aesthetics-awareness.","To this end, we propose Quality- and aesthetics-aware pretraining (QPT V2), the first pretraining framework based on MIM that offers a unified solution to quality and aesthetics assessment.","To perceive the high-level semantics and fine-grained details, pretraining data is curated.","To comprehensively encompass quality- and aesthetics-related factors, degradation is introduced.","To capture multi-scale quality and aesthetic information, model structure is modified.","Extensive experimental results on 11 downstream benchmarks clearly show the superior performance of QPT V2 in comparison with current state-of-the-art approaches and other pretraining paradigms.","Code and models will be released at \\url{https://github.com/KeiChiTse/QPT-V2}."],"url":"http://arxiv.org/abs/2407.16541v1"}
{"created":"2024-07-23 14:49:17","title":"Enhancing Encrypted Internet Traffic Classification Through Advanced Data Augmentation Techniques","abstract":"The increasing popularity of online services has made Internet Traffic Classification a critical field of study. However, the rapid development of internet protocols and encryption limits usable data availability. This paper addresses the challenges of classifying encrypted internet traffic, focusing on the scarcity of open-source datasets and limitations of existing ones. We propose two Data Augmentation (DA) techniques to synthetically generate data based on real samples: Average augmentation and MTU augmentation. Both augmentations are aimed to improve the performance of the classifier, each from a different perspective: The Average augmentation aims to increase dataset size by generating new synthetic samples, while the MTU augmentation enhances classifier robustness to varying Maximum Transmission Units (MTUs). Our experiments, conducted on two well-known academic datasets and a commercial dataset, demonstrate the effectiveness of these approaches in improving model performance and mitigating constraints associated with limited and homogeneous datasets. Our findings underscore the potential of data augmentation in addressing the challenges of modern internet traffic classification. Specifically, we show that our augmentation techniques significantly enhance encrypted traffic classification models. This improvement can positively impact user Quality of Experience (QoE) by more accurately classifying traffic as video streaming (e.g., YouTube) or chat (e.g., Google Chat). Additionally, it can enhance Quality of Service (QoS) for file downloading activities (e.g., Google Docs).","sentences":["The increasing popularity of online services has made Internet Traffic Classification a critical field of study.","However, the rapid development of internet protocols and encryption limits usable data availability.","This paper addresses the challenges of classifying encrypted internet traffic, focusing on the scarcity of open-source datasets and limitations of existing ones.","We propose two Data Augmentation (DA) techniques to synthetically generate data based on real samples: Average augmentation and MTU augmentation.","Both augmentations are aimed to improve the performance of the classifier, each from a different perspective: The Average augmentation aims to increase dataset size by generating new synthetic samples, while the MTU augmentation enhances classifier robustness to varying Maximum Transmission Units (MTUs).","Our experiments, conducted on two well-known academic datasets and a commercial dataset, demonstrate the effectiveness of these approaches in improving model performance and mitigating constraints associated with limited and homogeneous datasets.","Our findings underscore the potential of data augmentation in addressing the challenges of modern internet traffic classification.","Specifically, we show that our augmentation techniques significantly enhance encrypted traffic classification models.","This improvement can positively impact user Quality of Experience (QoE) by more accurately classifying traffic as video streaming (e.g., YouTube) or chat (e.g., Google Chat).","Additionally, it can enhance Quality of Service (QoS) for file downloading activities (e.g., Google Docs)."],"url":"http://arxiv.org/abs/2407.16539v1"}
{"created":"2024-07-23 14:47:25","title":"Quantifying the Role of Textual Predictability in Automatic Speech Recognition","abstract":"A long-standing question in automatic speech recognition research is how to attribute errors to the ability of a model to model the acoustics, versus its ability to leverage higher-order context (lexicon, morphology, syntax, semantics). We validate a novel approach which models error rates as a function of relative textual predictability, and yields a single number, $k$, which measures the effect of textual predictability on the recognizer. We use this method to demonstrate that a Wav2Vec 2.0-based model makes greater stronger use of textual context than a hybrid ASR model, in spite of not using an explicit language model, and also use it to shed light on recent results demonstrating poor performance of standard ASR systems on African-American English. We demonstrate that these mostly represent failures of acoustic--phonetic modelling. We show how this approach can be used straightforwardly in diagnosing and improving ASR.","sentences":["A long-standing question in automatic speech recognition research is how to attribute errors to the ability of a model to model the acoustics, versus its ability to leverage higher-order context (lexicon, morphology, syntax, semantics).","We validate a novel approach which models error rates as a function of relative textual predictability, and yields a single number, $k$, which measures the effect of textual predictability on the recognizer.","We use this method to demonstrate that a Wav2Vec 2.0-based model makes greater stronger use of textual context than a hybrid ASR model, in spite of not using an explicit language model, and also use it to shed light on recent results demonstrating poor performance of standard ASR systems on African-American English.","We demonstrate that these mostly represent failures of acoustic--phonetic modelling.","We show how this approach can be used straightforwardly in diagnosing and improving ASR."],"url":"http://arxiv.org/abs/2407.16537v1"}
{"created":"2024-07-23 14:46:07","title":"HAPFI: History-Aware Planning based on Fused Information","abstract":"Embodied Instruction Following (EIF) is a task of planning a long sequence of sub-goals given high-level natural language instructions, such as \"Rinse a slice of lettuce and place on the white table next to the fork\". To successfully execute these long-term horizon tasks, we argue that an agent must consider its past, i.e., historical data, when making decisions in each step. Nevertheless, recent approaches in EIF often neglects the knowledge from historical data and also do not effectively utilize information across the modalities. To this end, we propose History-Aware Planning based on Fused Information (HAPFI), effectively leveraging the historical data from diverse modalities that agents collect while interacting with the environment. Specifically, HAPFI integrates multiple modalities, including historical RGB observations, bounding boxes, sub-goals, and high-level instructions, by effectively fusing modalities via our Mutually Attentive Fusion method. Through experiments with diverse comparisons, we show that an agent utilizing historical multi-modal information surpasses all the compared methods that neglect the historical data in terms of action planning capability, enabling the generation of well-informed action plans for the next step. Moreover, we provided qualitative evidence highlighting the significance of leveraging historical multi-modal data, particularly in scenarios where the agent encounters intermediate failures, showcasing its robust re-planning capabilities.","sentences":["Embodied Instruction Following (EIF) is a task of planning a long sequence of sub-goals given high-level natural language instructions, such as \"Rinse a slice of lettuce and place on the white table next to the fork\".","To successfully execute these long-term horizon tasks, we argue that an agent must consider its past, i.e., historical data, when making decisions in each step.","Nevertheless, recent approaches in EIF often neglects the knowledge from historical data and also do not effectively utilize information across the modalities.","To this end, we propose History-Aware Planning based on Fused Information (HAPFI), effectively leveraging the historical data from diverse modalities that agents collect while interacting with the environment.","Specifically, HAPFI integrates multiple modalities, including historical RGB observations, bounding boxes, sub-goals, and high-level instructions, by effectively fusing modalities via our Mutually Attentive Fusion method.","Through experiments with diverse comparisons, we show that an agent utilizing historical multi-modal information surpasses all the compared methods that neglect the historical data in terms of action planning capability, enabling the generation of well-informed action plans for the next step.","Moreover, we provided qualitative evidence highlighting the significance of leveraging historical multi-modal data, particularly in scenarios where the agent encounters intermediate failures, showcasing its robust re-planning capabilities."],"url":"http://arxiv.org/abs/2407.16533v1"}
{"created":"2024-07-23 14:39:40","title":"Imperfect Vision Encoders: Efficient and Robust Tuning for Vision-Language Models","abstract":"Vision language models (VLMs) demonstrate impressive capabilities in visual question answering and image captioning, acting as a crucial link between visual and language models. However, existing open-source VLMs heavily rely on pretrained and frozen vision encoders (such as CLIP). Despite CLIP's robustness across diverse domains, it still exhibits non-negligible image understanding errors. These errors propagate to the VLM responses, resulting in sub-optimal performance. In our work, we propose an efficient and robust method for updating vision encoders within VLMs. Our approach selectively and locally updates encoders, leading to substantial performance improvements on data where previous mistakes occurred, while maintaining overall robustness. Furthermore, we demonstrate the effectiveness of our method during continual few-shot updates. Theoretical grounding, generality, and computational efficiency characterize our approach.","sentences":["Vision language models (VLMs) demonstrate impressive capabilities in visual question answering and image captioning, acting as a crucial link between visual and language models.","However, existing open-source VLMs heavily rely on pretrained and frozen vision encoders (such as CLIP).","Despite CLIP's robustness across diverse domains, it still exhibits non-negligible image understanding errors.","These errors propagate to the VLM responses, resulting in sub-optimal performance.","In our work, we propose an efficient and robust method for updating vision encoders within VLMs.","Our approach selectively and locally updates encoders, leading to substantial performance improvements on data where previous mistakes occurred, while maintaining overall robustness.","Furthermore, we demonstrate the effectiveness of our method during continual few-shot updates.","Theoretical grounding, generality, and computational efficiency characterize our approach."],"url":"http://arxiv.org/abs/2407.16526v1"}
{"created":"2024-07-23 14:34:38","title":"AMONGAGENTS: Evaluating Large Language Models in the Interactive Text-Based Social Deduction Game","abstract":"Strategic social deduction games serve as valuable testbeds for evaluating the understanding and inference skills of language models, offering crucial insights into social science, artificial intelligence, and strategic gaming. This paper focuses on creating proxies of human behavior in simulated environments, with \\textit{Among Us} utilized as a tool for studying simulated human behavior.   The study introduces a text-based game environment, named AmongAgent, that mirrors the dynamics of \\textit{Among Us}. Players act as crew members aboard a spaceship, tasked with identifying impostors who are sabotaging the ship and eliminating the crew. Within this environment, the behavior of simulated language agents is analyzed. The experiments involve diverse game sequences featuring different configurations of Crewmates and Impostor personality archetypes. Our work demonstrates that state-of-the-art large language models (LLMs) can effectively grasp the game rules and make decisions based on the current context. This work aims to promote further exploration of LLMs in goal-oriented games with incomplete information and complex action spaces, as these settings offer valuable opportunities to assess language model performance in socially driven scenarios.","sentences":["Strategic social deduction games serve as valuable testbeds for evaluating the understanding and inference skills of language models, offering crucial insights into social science, artificial intelligence, and strategic gaming.","This paper focuses on creating proxies of human behavior in simulated environments, with \\textit{Among Us} utilized as a tool for studying simulated human behavior.   ","The study introduces a text-based game environment, named AmongAgent, that mirrors the dynamics of \\textit{Among Us}.","Players act as crew members aboard a spaceship, tasked with identifying impostors who are sabotaging the ship and eliminating the crew.","Within this environment, the behavior of simulated language agents is analyzed.","The experiments involve diverse game sequences featuring different configurations of Crewmates and Impostor personality archetypes.","Our work demonstrates that state-of-the-art large language models (LLMs) can effectively grasp the game rules and make decisions based on the current context.","This work aims to promote further exploration of LLMs in goal-oriented games with incomplete information and complex action spaces, as these settings offer valuable opportunities to assess language model performance in socially driven scenarios."],"url":"http://arxiv.org/abs/2407.16521v1"}
{"created":"2024-07-23 14:31:59","title":"Assessing In-context Learning and Fine-tuning for Topic Classification of German Web Data","abstract":"Researchers in the political and social sciences often rely on classification models to analyze trends in information consumption by examining browsing histories of millions of webpages. Automated scalable methods are necessary due to the impracticality of manual labeling. In this paper, we model the detection of topic-related content as a binary classification task and compare the accuracy of fine-tuned pre-trained encoder models against in-context learning strategies. Using only a few hundred annotated data points per topic, we detect content related to three German policies in a database of scraped webpages. We compare multilingual and monolingual models, as well as zero and few-shot approaches, and investigate the impact of negative sampling strategies and the combination of URL & content-based features. Our results show that a small sample of annotated data is sufficient to train an effective classifier. Fine-tuning encoder-based models yields better results than in-context learning. Classifiers using both URL & content-based features perform best, while using URLs alone provides adequate results when content is unavailable.","sentences":["Researchers in the political and social sciences often rely on classification models to analyze trends in information consumption by examining browsing histories of millions of webpages.","Automated scalable methods are necessary due to the impracticality of manual labeling.","In this paper, we model the detection of topic-related content as a binary classification task and compare the accuracy of fine-tuned pre-trained encoder models against in-context learning strategies.","Using only a few hundred annotated data points per topic, we detect content related to three German policies in a database of scraped webpages.","We compare multilingual and monolingual models, as well as zero and few-shot approaches, and investigate the impact of negative sampling strategies and the combination of URL & content-based features.","Our results show that a small sample of annotated data is sufficient to train an effective classifier.","Fine-tuning encoder-based models yields better results than in-context learning.","Classifiers using both URL & content-based features perform best, while using URLs alone provides adequate results when content is unavailable."],"url":"http://arxiv.org/abs/2407.16516v1"}
{"created":"2024-07-23 14:30:53","title":"Spurious Correlations in Concept Drift: Can Explanatory Interaction Help?","abstract":"Long-running machine learning models face the issue of concept drift (CD), whereby the data distribution changes over time, compromising prediction performance. Updating the model requires detecting drift by monitoring the data and/or the model for unexpected changes. We show that, however, spurious correlations (SCs) can spoil the statistics tracked by detection algorithms. Motivated by this, we introduce ebc-exstream, a novel detector that leverages model explanations to identify potential SCs and human feedback to correct for them. It leverages an entropy-based heuristic to reduce the amount of necessary feedback, cutting annotation costs. Our preliminary experiments on artificially confounded data highlight the promise of ebc-exstream for reducing the impact of SCs on detection.","sentences":["Long-running machine learning models face the issue of concept drift (CD), whereby the data distribution changes over time, compromising prediction performance.","Updating the model requires detecting drift by monitoring the data and/or the model for unexpected changes.","We show that, however, spurious correlations (SCs) can spoil the statistics tracked by detection algorithms.","Motivated by this, we introduce ebc-exstream, a novel detector that leverages model explanations to identify potential SCs and human feedback to correct for them.","It leverages an entropy-based heuristic to reduce the amount of necessary feedback, cutting annotation costs.","Our preliminary experiments on artificially confounded data highlight the promise of ebc-exstream for reducing the impact of SCs on detection."],"url":"http://arxiv.org/abs/2407.16515v1"}
{"created":"2024-07-23 14:30:51","title":"Is 3D Convolution with 5D Tensors Really Necessary for Video Analysis?","abstract":"In this paper, we present a comprehensive study and propose several novel techniques for implementing 3D convolutional blocks using 2D and/or 1D convolutions with only 4D and/or 3D tensors. Our motivation is that 3D convolutions with 5D tensors are computationally very expensive and they may not be supported by some of the edge devices used in real-time applications such as robots. The existing approaches mitigate this by splitting the 3D kernels into spatial and temporal domains, but they still use 3D convolutions with 5D tensors in their implementations. We resolve this issue by introducing some appropriate 4D/3D tensor reshaping as well as new combination techniques for spatial and temporal splits. The proposed implementation methods show significant improvement both in terms of efficiency and accuracy. The experimental results confirm that the proposed spatio-temporal processing structure outperforms the original model in terms of speed and accuracy using only 4D tensors with fewer parameters.","sentences":["In this paper, we present a comprehensive study and propose several novel techniques for implementing 3D convolutional blocks using 2D and/or 1D convolutions with only 4D and/or 3D tensors.","Our motivation is that 3D convolutions with 5D tensors are computationally very expensive and they may not be supported by some of the edge devices used in real-time applications such as robots.","The existing approaches mitigate this by splitting the 3D kernels into spatial and temporal domains, but they still use 3D convolutions with 5D tensors in their implementations.","We resolve this issue by introducing some appropriate 4D/3D tensor reshaping as well as new combination techniques for spatial and temporal splits.","The proposed implementation methods show significant improvement both in terms of efficiency and accuracy.","The experimental results confirm that the proposed spatio-temporal processing structure outperforms the original model in terms of speed and accuracy using only 4D tensors with fewer parameters."],"url":"http://arxiv.org/abs/2407.16514v1"}
{"created":"2024-07-23 14:25:28","title":"DreamVTON: Customizing 3D Virtual Try-on with Personalized Diffusion Models","abstract":"Image-based 3D Virtual Try-ON (VTON) aims to sculpt the 3D human according to person and clothes images, which is data-efficient (i.e., getting rid of expensive 3D data) but challenging. Recent text-to-3D methods achieve remarkable improvement in high-fidelity 3D human generation, demonstrating its potential for 3D virtual try-on. Inspired by the impressive success of personalized diffusion models (e.g., Dreambooth and LoRA) for 2D VTON, it is straightforward to achieve 3D VTON by integrating the personalization technique into the diffusion-based text-to-3D framework. However, employing the personalized module in a pre-trained diffusion model (e.g., StableDiffusion (SD)) would degrade the model's capability for multi-view or multi-domain synthesis, which is detrimental to the geometry and texture optimization guided by Score Distillation Sampling (SDS) loss. In this work, we propose a novel customizing 3D human try-on model, named \\textbf{DreamVTON}, to separately optimize the geometry and texture of the 3D human. Specifically, a personalized SD with multi-concept LoRA is proposed to provide the generative prior about the specific person and clothes, while a Densepose-guided ControlNet is exploited to guarantee consistent prior about body pose across various camera views. Besides, to avoid the inconsistent multi-view priors from the personalized SD dominating the optimization, DreamVTON introduces a template-based optimization mechanism, which employs mask templates for geometry shape learning and normal/RGB templates for geometry/texture details learning. Furthermore, for the geometry optimization phase, DreamVTON integrates a normal-style LoRA into personalized SD to enhance normal map generative prior, facilitating smooth geometry modeling.","sentences":["Image-based 3D Virtual Try-ON (VTON) aims to sculpt the 3D human according to person and clothes images, which is data-efficient (i.e., getting rid of expensive 3D data) but challenging.","Recent text-to-3D methods achieve remarkable improvement in high-fidelity 3D human generation, demonstrating its potential for 3D virtual try-on.","Inspired by the impressive success of personalized diffusion models (e.g., Dreambooth and LoRA) for 2D VTON, it is straightforward to achieve 3D VTON by integrating the personalization technique into the diffusion-based text-to-3D framework.","However, employing the personalized module in a pre-trained diffusion model (e.g., StableDiffusion (SD)) would degrade the model's capability for multi-view or multi-domain synthesis, which is detrimental to the geometry and texture optimization guided by Score Distillation Sampling (SDS) loss.","In this work, we propose a novel customizing 3D human try-on model, named \\textbf{DreamVTON}, to separately optimize the geometry and texture of the 3D human.","Specifically, a personalized SD with multi-concept LoRA is proposed to provide the generative prior about the specific person and clothes, while a Densepose-guided ControlNet is exploited to guarantee consistent prior about body pose across various camera views.","Besides, to avoid the inconsistent multi-view priors from the personalized SD dominating the optimization, DreamVTON introduces a template-based optimization mechanism, which employs mask templates for geometry shape learning and normal/RGB templates for geometry/texture details learning.","Furthermore, for the geometry optimization phase, DreamVTON integrates a normal-style LoRA into personalized SD to enhance normal map generative prior, facilitating smooth geometry modeling."],"url":"http://arxiv.org/abs/2407.16511v1"}
{"created":"2024-07-23 14:24:26","title":"ToDER: Towards Colonoscopy Depth Estimation and Reconstruction with Geometry Constraint Adaptation","abstract":"Visualizing colonoscopy is crucial for medical auxiliary diagnosis to prevent undetected polyps in areas that are not fully observed. Traditional feature-based and depth-based reconstruction approaches usually end up with undesirable results due to incorrect point matching or imprecise depth estimation in realistic colonoscopy videos. Modern deep-based methods often require a sufficient number of ground truth samples, which are generally hard to obtain in optical colonoscopy. To address this issue, self-supervised and domain adaptation methods have been explored. However, these methods neglect geometry constraints and exhibit lower accuracy in predicting detailed depth. We thus propose a novel reconstruction pipeline with a bi-directional adaptation architecture named ToDER to get precise depth estimations. Furthermore, we carefully design a TNet module in our adaptation architecture to yield geometry constraints and obtain better depth quality. Estimated depth is finally utilized to reconstruct a reliable colon model for visualization. Experimental results demonstrate that our approach can precisely predict depth maps in both realistic and synthetic colonoscopy videos compared with other self-supervised and domain adaptation methods. Our method on realistic colonoscopy also shows the great potential for visualizing unobserved regions and preventing misdiagnoses.","sentences":["Visualizing colonoscopy is crucial for medical auxiliary diagnosis to prevent undetected polyps in areas that are not fully observed.","Traditional feature-based and depth-based reconstruction approaches usually end up with undesirable results due to incorrect point matching or imprecise depth estimation in realistic colonoscopy videos.","Modern deep-based methods often require a sufficient number of ground truth samples, which are generally hard to obtain in optical colonoscopy.","To address this issue, self-supervised and domain adaptation methods have been explored.","However, these methods neglect geometry constraints and exhibit lower accuracy in predicting detailed depth.","We thus propose a novel reconstruction pipeline with a bi-directional adaptation architecture named ToDER to get precise depth estimations.","Furthermore, we carefully design a TNet module in our adaptation architecture to yield geometry constraints and obtain better depth quality.","Estimated depth is finally utilized to reconstruct a reliable colon model for visualization.","Experimental results demonstrate that our approach can precisely predict depth maps in both realistic and synthetic colonoscopy videos compared with other self-supervised and domain adaptation methods.","Our method on realistic colonoscopy also shows the great potential for visualizing unobserved regions and preventing misdiagnoses."],"url":"http://arxiv.org/abs/2407.16508v1"}
{"created":"2024-07-23 14:21:27","title":"Language-Based Security for Low-Level MPC","abstract":"Secure Multi-Party Computation (MPC) is an important enabling technology for data privacy in modern distributed applications. Currently, proof methods for low-level MPC protocols are primarily manual and thus tedious and error-prone, and are also non-standardized and unfamiliar to most PL theorists. As a step towards better language support and language-based enforcement, we develop a new staged PL for defining a variety of low-level probabilistic MPC protocols. We also formulate a collection of confidentiality and integrity hyperproperties for our language model that are familiar from information flow, including conditional noninterference, gradual release, and robust declassification. We demonstrate their relation to standard MPC threat models of passive and malicious security, and how they can be leveraged in security verification of protocols. To prove these properties we develop automated tactics in $\\mathbb{F}_2$ that can be integrated with separation logic-style reasoning.","sentences":["Secure Multi-Party Computation (MPC) is an important enabling technology for data privacy in modern distributed applications.","Currently, proof methods for low-level MPC protocols are primarily manual and thus tedious and error-prone, and are also non-standardized and unfamiliar to most PL theorists.","As a step towards better language support and language-based enforcement, we develop a new staged PL for defining a variety of low-level probabilistic MPC protocols.","We also formulate a collection of confidentiality and integrity hyperproperties for our language model that are familiar from information flow, including conditional noninterference, gradual release, and robust declassification.","We demonstrate their relation to standard MPC threat models of passive and malicious security, and how they can be leveraged in security verification of protocols.","To prove these properties we develop automated tactics in $\\mathbb{F}_2$ that can be integrated with separation logic-style reasoning."],"url":"http://arxiv.org/abs/2407.16504v1"}
{"created":"2024-07-23 14:21:00","title":"HDRSplat: Gaussian Splatting for High Dynamic Range 3D Scene Reconstruction from Raw Images","abstract":"The recent advent of 3D Gaussian Splatting (3DGS) has revolutionized the 3D scene reconstruction space enabling high-fidelity novel view synthesis in real-time. However, with the exception of RawNeRF, all prior 3DGS and NeRF-based methods rely on 8-bit tone-mapped Low Dynamic Range (LDR) images for scene reconstruction. Such methods struggle to achieve accurate reconstructions in scenes that require a higher dynamic range. Examples include scenes captured in nighttime or poorly lit indoor spaces having a low signal-to-noise ratio, as well as daylight scenes with shadow regions exhibiting extreme contrast. Our proposed method HDRSplat tailors 3DGS to train directly on 14-bit linear raw images in near darkness which preserves the scenes' full dynamic range and content. Our key contributions are two-fold: Firstly, we propose a linear HDR space-suited loss that effectively extracts scene information from noisy dark regions and nearly saturated bright regions simultaneously, while also handling view-dependent colors without increasing the degree of spherical harmonics. Secondly, through careful rasterization tuning, we implicitly overcome the heavy reliance and sensitivity of 3DGS on point cloud initialization. This is critical for accurate reconstruction in regions of low texture, high depth of field, and low illumination. HDRSplat is the fastest method to date that does 14-bit (HDR) 3D scene reconstruction in $\\le$15 minutes/scene ($\\sim$30x faster than prior state-of-the-art RawNeRF). It also boasts the fastest inference speed at $\\ge$120fps. We further demonstrate the applicability of our HDR scene reconstruction by showcasing various applications like synthetic defocus, dense depth map extraction, and post-capture control of exposure, tone-mapping and view-point.","sentences":["The recent advent of 3D Gaussian Splatting (3DGS) has revolutionized the 3D scene reconstruction space enabling high-fidelity novel view synthesis in real-time.","However, with the exception of RawNeRF, all prior 3DGS and NeRF-based methods rely on 8-bit tone-mapped Low Dynamic Range (LDR) images for scene reconstruction.","Such methods struggle to achieve accurate reconstructions in scenes that require a higher dynamic range.","Examples include scenes captured in nighttime or poorly lit indoor spaces having a low signal-to-noise ratio, as well as daylight scenes with shadow regions exhibiting extreme contrast.","Our proposed method HDRSplat tailors 3DGS to train directly on 14-bit linear raw images in near darkness which preserves the scenes' full dynamic range and content.","Our key contributions are two-fold: Firstly, we propose a linear HDR space-suited loss that effectively extracts scene information from noisy dark regions and nearly saturated bright regions simultaneously, while also handling view-dependent colors without increasing the degree of spherical harmonics.","Secondly, through careful rasterization tuning, we implicitly overcome the heavy reliance and sensitivity of 3DGS on point cloud initialization.","This is critical for accurate reconstruction in regions of low texture, high depth of field, and low illumination.","HDRSplat is the fastest method to date that does 14-bit (HDR) 3D scene reconstruction in $\\le$15 minutes/scene ($\\sim$30x faster than prior state-of-the-art RawNeRF).","It also boasts the fastest inference speed at $\\ge$120fps.","We further demonstrate the applicability of our HDR scene reconstruction by showcasing various applications like synthetic defocus, dense depth map extraction, and post-capture control of exposure, tone-mapping and view-point."],"url":"http://arxiv.org/abs/2407.16503v1"}
{"created":"2024-07-23 14:12:57","title":"Dynamic Retraining-Updating Mean Teacher for Source-Free Object Detection","abstract":"In object detection, unsupervised domain adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain. However, UDA's reliance on labeled source data restricts its adaptability in privacy-related scenarios. This study focuses on source-free object detection (SFOD), which adapts a source-trained detector to an unlabeled target domain without using labeled source data. Recent advancements in self-training, particularly with the Mean Teacher (MT) framework, show promise for SFOD deployment. However, the absence of source supervision significantly compromises the stability of these approaches. We identify two primary issues, (1) uncontrollable degradation of the teacher model due to inopportune updates from the student model, and (2) the student model's tendency to replicate errors from incorrect pseudo labels, leading to it being trapped in a local optimum. Both factors contribute to a detrimental circular dependency, resulting in rapid performance degradation in recent self-training frameworks. To tackle these challenges, we propose the Dynamic Retraining-Updating (DRU) mechanism, which actively manages the student training and teacher updating processes to achieve co-evolutionary training. Additionally, we introduce Historical Student Loss to mitigate the influence of incorrect pseudo labels. Our method achieves state-of-the-art performance in the SFOD setting on multiple domain adaptation benchmarks, comparable to or even surpassing advanced UDA methods. The code will be released at https://github.com/lbktrinh/DRU","sentences":["In object detection, unsupervised domain adaptation (UDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain.","However, UDA's reliance on labeled source data restricts its adaptability in privacy-related scenarios.","This study focuses on source-free object detection (SFOD), which adapts a source-trained detector to an unlabeled target domain without using labeled source data.","Recent advancements in self-training, particularly with the Mean Teacher (MT) framework, show promise for SFOD deployment.","However, the absence of source supervision significantly compromises the stability of these approaches.","We identify two primary issues, (1) uncontrollable degradation of the teacher model due to inopportune updates from the student model, and (2) the student model's tendency to replicate errors from incorrect pseudo labels, leading to it being trapped in a local optimum.","Both factors contribute to a detrimental circular dependency, resulting in rapid performance degradation in recent self-training frameworks.","To tackle these challenges, we propose the Dynamic Retraining-Updating (DRU) mechanism, which actively manages the student training and teacher updating processes to achieve co-evolutionary training.","Additionally, we introduce Historical Student Loss to mitigate the influence of incorrect pseudo labels.","Our method achieves state-of-the-art performance in the SFOD setting on multiple domain adaptation benchmarks, comparable to or even surpassing advanced UDA methods.","The code will be released at https://github.com/lbktrinh/DRU"],"url":"http://arxiv.org/abs/2407.16497v1"}
{"created":"2024-07-23 14:11:12","title":"Articulation Work and Tinkering for Fairness in Machine Learning","abstract":"The field of fair AI aims to counter biased algorithms through computational modelling. However, it faces increasing criticism for perpetuating the use of overly technical and reductionist methods. As a result, novel approaches appear in the field to address more socially-oriented and interdisciplinary (SOI) perspectives on fair AI. In this paper, we take this dynamic as the starting point to study the tension between computer science (CS) and SOI research. By drawing on STS and CSCW theory, we position fair AI research as a matter of 'organizational alignment': what makes research 'doable' is the successful alignment of three levels of work organization (the social world, the laboratory and the experiment). Based on qualitative interviews with CS researchers, we analyze the tasks, resources, and actors required for doable research in the case of fair AI. We find that CS researchers engage with SOI to some extent, but organizational conditions, articulation work, and ambiguities of the social world constrain the doability of SOI research. Based on our findings, we identify and discuss problems for aligning CS and SOI as fair AI continues to evolve.","sentences":["The field of fair AI aims to counter biased algorithms through computational modelling.","However, it faces increasing criticism for perpetuating the use of overly technical and reductionist methods.","As a result, novel approaches appear in the field to address more socially-oriented and interdisciplinary (SOI) perspectives on fair AI.","In this paper, we take this dynamic as the starting point to study the tension between computer science (CS) and SOI research.","By drawing on STS and CSCW theory, we position fair AI research as a matter of 'organizational alignment': what makes research 'doable' is the successful alignment of three levels of work organization (the social world, the laboratory and the experiment).","Based on qualitative interviews with CS researchers, we analyze the tasks, resources, and actors required for doable research in the case of fair AI.","We find that CS researchers engage with SOI to some extent, but organizational conditions, articulation work, and ambiguities of the social world constrain the doability of SOI research.","Based on our findings, we identify and discuss problems for aligning CS and SOI as fair AI continues to evolve."],"url":"http://arxiv.org/abs/2407.16496v1"}
