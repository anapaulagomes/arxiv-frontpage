{"created":"2025-01-06 18:59:57","title":"Gaussian Masked Autoencoders","abstract":"This paper explores Masked Autoencoders (MAE) with Gaussian Splatting. While reconstructive self-supervised learning frameworks such as MAE learns good semantic abstractions, it is not trained for explicit spatial awareness. Our approach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic abstractions and spatial understanding jointly. Like MAE, it reconstructs the image end-to-end in the pixel space, but beyond MAE, it also introduces an intermediate, 3D Gaussian-based representation and renders images via splatting. We show that GMAE can enable various zero-shot learning capabilities of spatial understanding (e.g., figure-ground segmentation, image layering, edge detection, etc.) while preserving the high-level semantics of self-supervised representation quality from MAE. To our knowledge, we are the first to employ Gaussian primitives in an image representation learning framework beyond optimization-based single-scene reconstructions. We believe GMAE will inspire further research in this direction and contribute to developing next-generation techniques for modeling high-fidelity visual data. More details at https://brjathu.github.io/gmae","sentences":["This paper explores Masked Autoencoders (MAE) with Gaussian Splatting.","While reconstructive self-supervised learning frameworks such as MAE learns good semantic abstractions, it is not trained for explicit spatial awareness.","Our approach, named Gaussian Masked Autoencoder, or GMAE, aims to learn semantic abstractions and spatial understanding jointly.","Like MAE, it reconstructs the image end-to-end in the pixel space, but beyond MAE, it also introduces an intermediate, 3D Gaussian-based representation and renders images via splatting.","We show that GMAE can enable various zero-shot learning capabilities of spatial understanding (e.g., figure-ground segmentation, image layering, edge detection, etc.)","while preserving the high-level semantics of self-supervised representation quality from MAE.","To our knowledge, we are the first to employ Gaussian primitives in an image representation learning framework beyond optimization-based single-scene reconstructions.","We believe GMAE will inspire further research in this direction and contribute to developing next-generation techniques for modeling high-fidelity visual data.","More details at https://brjathu.github.io/gmae"],"url":"http://arxiv.org/abs/2501.03229v1"}
{"created":"2025-01-06 18:59:55","title":"LightGNN: Simple Graph Neural Network for Recommendation","abstract":"Graph neural networks (GNNs) have demonstrated superior performance in collaborative recommendation through their ability to conduct high-order representation smoothing, effectively capturing structural information within users' interaction patterns. However, existing GNN paradigms face significant challenges in scalability and robustness when handling large-scale, noisy, and real-world datasets. To address these challenges, we present LightGNN, a lightweight and distillation-based GNN pruning framework designed to substantially reduce model complexity while preserving essential collaboration modeling capabilities. Our LightGNN framework introduces a computationally efficient pruning module that adaptively identifies and removes redundant edges and embedding entries for model compression. The framework is guided by a resource-friendly hierarchical knowledge distillation objective, whose intermediate layer augments the observed graph to maintain performance, particularly in high-rate compression scenarios. Extensive experiments on public datasets demonstrate LightGNN's effectiveness, significantly improving both computational efficiency and recommendation accuracy. Notably, LightGNN achieves an 80% reduction in edge count and 90% reduction in embedding entries while maintaining performance comparable to more complex state-of-the-art baselines. The implementation of our LightGNN framework is available at the github repository: https://github.com/HKUDS/LightGNN.","sentences":["Graph neural networks (GNNs) have demonstrated superior performance in collaborative recommendation through their ability to conduct high-order representation smoothing, effectively capturing structural information within users' interaction patterns.","However, existing GNN paradigms face significant challenges in scalability and robustness when handling large-scale, noisy, and real-world datasets.","To address these challenges, we present LightGNN, a lightweight and distillation-based GNN pruning framework designed to substantially reduce model complexity while preserving essential collaboration modeling capabilities.","Our LightGNN framework introduces a computationally efficient pruning module that adaptively identifies and removes redundant edges and embedding entries for model compression.","The framework is guided by a resource-friendly hierarchical knowledge distillation objective, whose intermediate layer augments the observed graph to maintain performance, particularly in high-rate compression scenarios.","Extensive experiments on public datasets demonstrate LightGNN's effectiveness, significantly improving both computational efficiency and recommendation accuracy.","Notably, LightGNN achieves an 80% reduction in edge count and 90% reduction in embedding entries while maintaining performance comparable to more complex state-of-the-art baselines.","The implementation of our LightGNN framework is available at the github repository: https://github.com/HKUDS/LightGNN."],"url":"http://arxiv.org/abs/2501.03228v1"}
{"created":"2025-01-06 18:59:26","title":"When Should Selfish Miners Double-Spend?","abstract":"Although, both double-spending and selfish-mining attacks have been extensively studied since the ``Bitcoin'' whitepaper of Nakamoto and the ``majority is not enough'' paper of Eyal and Sirer, there has been no rigorous stochastic analysis of an attack that combines the two, except for the complicated MDP models. In this paper, we first combine stubborn and selfish mining attacks, i.e., construct a strategy where the attacker acts stubborn until its private branch reaches a certain length and then switches to act selfish. We provide the optimal stubbornness for each parameter regime. Next, we provide the maximum stubbornness that is still more profitable than honest mining and argue a connection between the level of stubbornness and the $k$-confirmation rule. We show that, at each attack cycle, if the level of stubbornness is higher than $k$, there is a risk of double-spending which comes at no-cost to the adversary. The result can be seen as a guide for picking $k$ in the $k$-confirmation rule in a blockchain design. At each cycle, for a given stubbornness level, we rigorously formulate how great the risk of double-spending is. We provide the minimum double-spend value needed for an attack to be profitable in the regimes where the scheme is less profitable than honest mining. We further modify the attack in the stubborn regime in order to conceal the attack and increase the double-spending probability. Finally, we evaluate the results and provide the optimal and the maximum stubbornness levels for each parameter regime as well as the revenue. As a case study, with Bitcoin's $k=6$ block confirmation rule, we evaluate the revenue and double-spending risk of the attacks for each pool parameter.","sentences":["Although, both double-spending and selfish-mining attacks have been extensively studied since the ``Bitcoin'' whitepaper of Nakamoto and the ``majority is not enough'' paper of Eyal and Sirer, there has been no rigorous stochastic analysis of an attack that combines the two, except for the complicated MDP models.","In this paper, we first combine stubborn and selfish mining attacks, i.e., construct a strategy where the attacker acts stubborn until its private branch reaches a certain length and then switches to act selfish.","We provide the optimal stubbornness for each parameter regime.","Next, we provide the maximum stubbornness that is still more profitable than honest mining and argue a connection between the level of stubbornness and the $k$-confirmation rule.","We show that, at each attack cycle, if the level of stubbornness is higher than $k$, there is a risk of double-spending which comes at no-cost to the adversary.","The result can be seen as a guide for picking $k$ in the $k$-confirmation rule in a blockchain design.","At each cycle, for a given stubbornness level, we rigorously formulate how great the risk of double-spending is.","We provide the minimum double-spend value needed for an attack to be profitable in the regimes where the scheme is less profitable than honest mining.","We further modify the attack in the stubborn regime in order to conceal the attack and increase the double-spending probability.","Finally, we evaluate the results and provide the optimal and the maximum stubbornness levels for each parameter regime as well as the revenue.","As a case study, with Bitcoin's $k=6$ block confirmation rule, we evaluate the revenue and double-spending risk of the attacks for each pool parameter."],"url":"http://arxiv.org/abs/2501.03227v1"}
{"created":"2025-01-06 18:59:13","title":"BoostStep: Boosting mathematical capability of Large Language Models via improved single-step reasoning","abstract":"Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples. However, their potential for improvement is limited by two critical problems within their ICL examples: granularity-mismatch and the ensuing negative-effect noise problem. Specifically, the LLMs are capable of the dividing process yet mostly failed by inaccurate reasoning within a few conquer steps, while the ICL examples retrieved in question-grained sometimes lack relevant steps for a specific challenging reasoning step. Further, this disconnect may hinder the correct reasoning due to its irrelevance. To this end, we focus on improving the reasoning quality within each step and present BoostStep. BoostStep aligns the granularity between the retrieving and reasoning on step grained, and provides highly related ICL examples for each reasoning step with a novel `first-try' strategy. BoostStep provides more relevant examples than the coarse question-grained strategy, enhancing the model reasoning quality within each step steadily. BoostStep is a general and robust reasoning-enhancing method that not only improves standalone reasoning performance but also integrates seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate generation and decision-making. Quantitatively, it improves GPT-4o and Qwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical benchmarks, and 7.5\\% gain combined with MCTS.","sentences":["Cutting-edge large language models (LLMs) demonstrate promising performance in solving complex math problems with a divide-and-conquer pipeline and the assistance of in-context learning (ICL) examples.","However, their potential for improvement is limited by two critical problems within their ICL examples: granularity-mismatch and the ensuing negative-effect noise problem.","Specifically, the LLMs are capable of the dividing process yet mostly failed by inaccurate reasoning within a few conquer steps, while the ICL examples retrieved in question-grained sometimes lack relevant steps for a specific challenging reasoning step.","Further, this disconnect may hinder the correct reasoning due to its irrelevance.","To this end, we focus on improving the reasoning quality within each step and present BoostStep.","BoostStep aligns the granularity between the retrieving and reasoning on step grained, and provides highly related ICL examples for each reasoning step with a novel `first-try' strategy.","BoostStep provides more relevant examples than the coarse question-grained strategy, enhancing the model reasoning quality within each step steadily.","BoostStep is a general and robust reasoning-enhancing method that not only improves standalone reasoning performance but also integrates seamlessly with Monte Carlo Tree Search methods (MCTS) to refine both candidate generation and decision-making.","Quantitatively, it improves GPT-4o and Qwen2.5-Math-72B by 3.6\\% and 2.0\\% respectively on various mathematical benchmarks, and 7.5\\% gain combined with MCTS."],"url":"http://arxiv.org/abs/2501.03226v1"}
{"created":"2025-01-06 18:57:31","title":"Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation","abstract":"The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation.","sentences":["The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation.","However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses.","To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly question creation process.","Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones.","Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions.","We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation."],"url":"http://arxiv.org/abs/2501.03225v1"}
{"created":"2025-01-06 18:57:18","title":"Rate-My-LoRA: Efficient and Adaptive Federated Model Tuning for Cardiac MRI Segmentation","abstract":"Cardiovascular disease (CVD) and cardiac dyssynchrony are major public health problems in the United States. Precise cardiac image segmentation is crucial for extracting quantitative measures that help categorize cardiac dyssynchrony. However, achieving high accuracy often depends on centralizing large datasets from different hospitals, which can be challenging due to privacy concerns. To solve this problem, Federated Learning (FL) is proposed to enable decentralized model training on such data without exchanging sensitive information. However, bandwidth limitations and data heterogeneity remain as significant challenges in conventional FL algorithms. In this paper, we propose a novel efficient and adaptive federate learning method for cardiac segmentation that improves model performance while reducing the bandwidth requirement. Our method leverages the low-rank adaptation (LoRA) to regularize model weight update and reduce communication overhead. We also propose a \\mymethod{} aggregation technique to address data heterogeneity among clients. This technique adaptively penalizes the aggregated weights from different clients by comparing the validation accuracy in each client, allowing better generalization performance and fast local adaptation. In-client and cross-client evaluations on public cardiac MR datasets demonstrate the superiority of our method over other LoRA-based federate learning approaches.","sentences":["Cardiovascular disease (CVD) and cardiac dyssynchrony are major public health problems in the United States.","Precise cardiac image segmentation is crucial for extracting quantitative measures that help categorize cardiac dyssynchrony.","However, achieving high accuracy often depends on centralizing large datasets from different hospitals, which can be challenging due to privacy concerns.","To solve this problem, Federated Learning (FL) is proposed to enable decentralized model training on such data without exchanging sensitive information.","However, bandwidth limitations and data heterogeneity remain as significant challenges in conventional FL algorithms.","In this paper, we propose a novel efficient and adaptive federate learning method for cardiac segmentation that improves model performance while reducing the bandwidth requirement.","Our method leverages the low-rank adaptation (LoRA) to regularize model weight update and reduce communication overhead.","We also propose a \\mymethod{} aggregation technique to address data heterogeneity among clients.","This technique adaptively penalizes the aggregated weights from different clients by comparing the validation accuracy in each client, allowing better generalization performance and fast local adaptation.","In-client and cross-client evaluations on public cardiac MR datasets demonstrate the superiority of our method over other LoRA-based federate learning approaches."],"url":"http://arxiv.org/abs/2501.03223v1"}
{"created":"2025-01-06 18:57:05","title":"Characterizing the Accuracy-Communication-Privacy Trade-off in Distributed Stochastic Convex Optimization","abstract":"We consider the problem of differentially private stochastic convex optimization (DP-SCO) in a distributed setting with $M$ clients, where each of them has a local dataset of $N$ i.i.d. data samples from an underlying data distribution. The objective is to design an algorithm to minimize a convex population loss using a collaborative effort across $M$ clients, while ensuring the privacy of the local datasets. In this work, we investigate the accuracy-communication-privacy trade-off for this problem. We establish matching converse and achievability results using a novel lower bound and a new algorithm for distributed DP-SCO based on Vaidya's plane cutting method. Thus, our results provide a complete characterization of the accuracy-communication-privacy trade-off for DP-SCO in the distributed setting.","sentences":["We consider the problem of differentially private stochastic convex optimization (DP-SCO) in a distributed setting with $M$ clients, where each of them has a local dataset of $N$ i.i.d. data samples from an underlying data distribution.","The objective is to design an algorithm to minimize a convex population loss using a collaborative effort across $M$ clients, while ensuring the privacy of the local datasets.","In this work, we investigate the accuracy-communication-privacy trade-off for this problem.","We establish matching converse and achievability results using a novel lower bound and a new algorithm for distributed DP-SCO based on Vaidya's plane cutting method.","Thus, our results provide a complete characterization of the accuracy-communication-privacy trade-off for DP-SCO in the distributed setting."],"url":"http://arxiv.org/abs/2501.03222v1"}
{"created":"2025-01-06 18:55:59","title":"RW-Net: Enhancing Few-Shot Point Cloud Classification with a Wavelet Transform Projection-based Network","abstract":"In the domain of 3D object classification, a fundamental challenge lies in addressing the scarcity of labeled data, which limits the applicability of traditional data-intensive learning paradigms. This challenge is particularly pronounced in few-shot learning scenarios, where the objective is to achieve robust generalization from minimal annotated samples. To overcome these limitations, it is crucial to identify and leverage the most salient and discriminative features of 3D objects, thereby enhancing learning efficiency and reducing dependency on large-scale labeled datasets. This work introduces RW-Net, a novel framework designed to address the challenges above by integrating Rate-Distortion Explanation (RDE) and wavelet transform into a state-of-the-art projection-based 3D object classification architecture. The proposed method capitalizes on RDE to extract critical features by identifying and preserving the most informative data components while reducing redundancy. This process ensures the retention of essential information for effective decision-making, optimizing the model's ability to learn from limited data. Complementing RDE, incorporating the wavelet transform further enhances the framework's capability to generalize in low-data regimes. By emphasizing low-frequency components of the input data, the wavelet transform captures fundamental geometric and structural attributes of 3D objects. These attributes are instrumental in mitigating overfitting and improving the robustness of the learned representations across diverse tasks and domains. To validate the effectiveness of our RW-Net, we conduct extensive experiments on three datasets: ModelNet40, ModelNet40-C, and ScanObjectNN for few-shot 3D object classification. The results demonstrate that our approach achieves state-of-the-art performance and exhibits superior generalization and robustness in few-shot learning scenarios.","sentences":["In the domain of 3D object classification, a fundamental challenge lies in addressing the scarcity of labeled data, which limits the applicability of traditional data-intensive learning paradigms.","This challenge is particularly pronounced in few-shot learning scenarios, where the objective is to achieve robust generalization from minimal annotated samples.","To overcome these limitations, it is crucial to identify and leverage the most salient and discriminative features of 3D objects, thereby enhancing learning efficiency and reducing dependency on large-scale labeled datasets.","This work introduces RW-Net, a novel framework designed to address the challenges above by integrating Rate-Distortion Explanation (RDE) and wavelet transform into a state-of-the-art projection-based 3D object classification architecture.","The proposed method capitalizes on RDE to extract critical features by identifying and preserving the most informative data components while reducing redundancy.","This process ensures the retention of essential information for effective decision-making, optimizing the model's ability to learn from limited data.","Complementing RDE, incorporating the wavelet transform further enhances the framework's capability to generalize in low-data regimes.","By emphasizing low-frequency components of the input data, the wavelet transform captures fundamental geometric and structural attributes of 3D objects.","These attributes are instrumental in mitigating overfitting and improving the robustness of the learned representations across diverse tasks and domains.","To validate the effectiveness of our RW-Net, we conduct extensive experiments on three datasets: ModelNet40, ModelNet40-C, and ScanObjectNN for few-shot 3D object classification.","The results demonstrate that our approach achieves state-of-the-art performance and exhibits superior generalization and robustness in few-shot learning scenarios."],"url":"http://arxiv.org/abs/2501.03221v1"}
{"created":"2025-01-06 18:55:52","title":"ProTracker: Probabilistic Integration for Robust and Accurate Point Tracking","abstract":"In this paper, we propose ProTracker, a novel framework for robust and accurate long-term dense tracking of arbitrary points in videos. The key idea of our method is incorporating probabilistic integration to refine multiple predictions from both optical flow and semantic features for robust short-term and long-term tracking. Specifically, we integrate optical flow estimations in a probabilistic manner, producing smooth and accurate trajectories by maximizing the likelihood of each prediction. To effectively re-localize challenging points that disappear and reappear due to occlusion, we further incorporate long-term feature correspondence into our flow predictions for continuous trajectory generation. Extensive experiments show that ProTracker achieves the state-of-the-art performance among unsupervised and self-supervised approaches, and even outperforms supervised methods on several benchmarks. Our code and model will be publicly available upon publication.","sentences":["In this paper, we propose ProTracker, a novel framework for robust and accurate long-term dense tracking of arbitrary points in videos.","The key idea of our method is incorporating probabilistic integration to refine multiple predictions from both optical flow and semantic features for robust short-term and long-term tracking.","Specifically, we integrate optical flow estimations in a probabilistic manner, producing smooth and accurate trajectories by maximizing the likelihood of each prediction.","To effectively re-localize challenging points that disappear and reappear due to occlusion, we further incorporate long-term feature correspondence into our flow predictions for continuous trajectory generation.","Extensive experiments show that ProTracker achieves the state-of-the-art performance among unsupervised and self-supervised approaches, and even outperforms supervised methods on several benchmarks.","Our code and model will be publicly available upon publication."],"url":"http://arxiv.org/abs/2501.03220v1"}
{"created":"2025-01-06 18:55:10","title":"Dispider: Enabling Video LLMs with Active Real-Time Interaction via Disentangled Perception, Decision, and Reaction","abstract":"Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly. Unlike offline video LLMs, which analyze the entire video before answering questions, active real-time interaction requires three capabilities: 1) Perception: real-time video monitoring and interaction capturing. 2) Decision: raising proactive interaction in proper situations, 3) Reaction: continuous interaction with users. However, inherent conflicts exist among the desired capabilities. The Decision and Reaction require a contrary Perception scale and grain, and the autoregressive decoding blocks the real-time Perception and Decision during the Reaction. To unify the conflicted capabilities within a harmonious system, we present Dispider, a system that disentangles Perception, Decision, and Reaction. Dispider features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction. Once the interaction is triggered, an asynchronous interaction module provides detailed responses, while the processing module continues to monitor the video in the meantime. Our disentangled and asynchronous design ensures timely, contextually accurate, and computationally efficient responses, making Dispider ideal for active real-time interaction for long-duration video streams. Experiments show that Dispider not only maintains strong performance in conventional video QA tasks, but also significantly surpasses previous online models in streaming scenario responses, thereby validating the effectiveness of our architecture. The code and model are released at \\url{https://github.com/Mark12Ding/Dispider}.","sentences":["Active Real-time interaction with video LLMs introduces a new paradigm for human-computer interaction, where the model not only understands user intent but also responds while continuously processing streaming video on the fly.","Unlike offline video LLMs, which analyze the entire video before answering questions, active real-time interaction requires three capabilities: 1) Perception: real-time video monitoring and interaction capturing.","2) Decision: raising proactive interaction in proper situations, 3) Reaction: continuous interaction with users.","However, inherent conflicts exist among the desired capabilities.","The Decision and Reaction require a contrary Perception scale and grain, and the autoregressive decoding blocks the real-time Perception and Decision during the Reaction.","To unify the conflicted capabilities within a harmonious system, we present Dispider, a system that disentangles Perception, Decision, and Reaction.","Dispider features a lightweight proactive streaming video processing module that tracks the video stream and identifies optimal moments for interaction.","Once the interaction is triggered, an asynchronous interaction module provides detailed responses, while the processing module continues to monitor the video in the meantime.","Our disentangled and asynchronous design ensures timely, contextually accurate, and computationally efficient responses, making Dispider ideal for active real-time interaction for long-duration video streams.","Experiments show that Dispider not only maintains strong performance in conventional video QA tasks, but also significantly surpasses previous online models in streaming scenario responses, thereby validating the effectiveness of our architecture.","The code and model are released at \\url{https://github.com/Mark12Ding/Dispider}."],"url":"http://arxiv.org/abs/2501.03218v1"}
{"created":"2025-01-06 18:46:53","title":"Leveraging Explainable AI for LLM Text Attribution: Differentiating Human-Written and Multiple LLMs-Generated Text","abstract":"The development of Generative AI Large Language Models (LLMs) raised the alarm regarding identifying content produced through generative AI or humans. In one case, issues arise when students heavily rely on such tools in a manner that can affect the development of their writing or coding skills. Other issues of plagiarism also apply. This study aims to support efforts to detect and identify textual content generated using LLM tools. We hypothesize that LLMs-generated text is detectable by machine learning (ML), and investigate ML models that can recognize and differentiate texts generated by multiple LLMs tools. We leverage several ML and Deep Learning (DL) algorithms such as Random Forest (RF), and Recurrent Neural Networks (RNN), and utilized Explainable Artificial Intelligence (XAI) to understand the important features in attribution. Our method is divided into 1) binary classification to differentiate between human-written and AI-text, and 2) multi classification, to differentiate between human-written text and the text generated by the five different LLM tools (ChatGPT, LLaMA, Google Bard, Claude, and Perplexity). Results show high accuracy in the multi and binary classification. Our model outperformed GPTZero with 98.5\\% accuracy to 78.3\\%. Notably, GPTZero was unable to recognize about 4.2\\% of the observations, but our model was able to recognize the complete test dataset. XAI results showed that understanding feature importance across different classes enables detailed author/source profiles. Further, aiding in attribution and supporting plagiarism detection by highlighting unique stylistic and structural elements ensuring robust content originality verification.","sentences":["The development of Generative AI Large Language Models (LLMs) raised the alarm regarding identifying content produced through generative AI or humans.","In one case, issues arise when students heavily rely on such tools in a manner that can affect the development of their writing or coding skills.","Other issues of plagiarism also apply.","This study aims to support efforts to detect and identify textual content generated using LLM tools.","We hypothesize that LLMs-generated text is detectable by machine learning (ML), and investigate ML models that can recognize and differentiate texts generated by multiple LLMs tools.","We leverage several ML and Deep Learning (DL) algorithms such as Random Forest (RF), and Recurrent Neural Networks (RNN), and utilized Explainable Artificial Intelligence (XAI) to understand the important features in attribution.","Our method is divided into 1) binary classification to differentiate between human-written and AI-text, and 2) multi classification, to differentiate between human-written text and the text generated by the five different LLM tools (ChatGPT, LLaMA, Google Bard, Claude, and Perplexity).","Results show high accuracy in the multi and binary classification.","Our model outperformed GPTZero with 98.5\\% accuracy to 78.3\\%.","Notably, GPTZero was unable to recognize about 4.2\\% of the observations, but our model was able to recognize the complete test dataset.","XAI results showed that understanding feature importance across different classes enables detailed author/source profiles.","Further, aiding in attribution and supporting plagiarism detection by highlighting unique stylistic and structural elements ensuring robust content originality verification."],"url":"http://arxiv.org/abs/2501.03212v1"}
{"created":"2025-01-06 18:44:48","title":"Simulation of entanglement based quantum networks for performance characterization","abstract":"Entanglement-based networks (EBNs) enable general-purpose quantum communication by combining entanglement and its swapping in a sequence that addresses the challenges of achieving long distance communication with high fidelity associated with quantum technologies. In this context, entanglement distribution refers to the process by which two nodes in a quantum network share an entangled state, serving as a fundamental resource for communication. In this paper, we study the performance of entanglement distribution mechanisms over a physical topology comprising end nodes and quantum switches, which are crucial for constructing large-scale links. To this end, we implemented a switch-based topology in NetSquid and conducted a series of simulation experiments to gain insight into practical and realistic quantum network engineering challenges. These challenges include, on the one hand, aspects related to quantum technology, such as memory technology, gate durations, and noise; and, on the other hand, factors associated with the distribution process, such as the number of switches, distances, purification, and error correction. All these factors significantly impact the end-to-end fidelity across a path, which supports communication between two quantum nodes. We use these experiments to derive some guidelines towards the design and configuration of future EBNs.","sentences":["Entanglement-based networks (EBNs) enable general-purpose quantum communication by combining entanglement and its swapping in a sequence that addresses the challenges of achieving long distance communication with high fidelity associated with quantum technologies.","In this context, entanglement distribution refers to the process by which two nodes in a quantum network share an entangled state, serving as a fundamental resource for communication.","In this paper, we study the performance of entanglement distribution mechanisms over a physical topology comprising end nodes and quantum switches, which are crucial for constructing large-scale links.","To this end, we implemented a switch-based topology in NetSquid and conducted a series of simulation experiments to gain insight into practical and realistic quantum network engineering challenges.","These challenges include, on the one hand, aspects related to quantum technology, such as memory technology, gate durations, and noise; and, on the other hand, factors associated with the distribution process, such as the number of switches, distances, purification, and error correction.","All these factors significantly impact the end-to-end fidelity across a path, which supports communication between two quantum nodes.","We use these experiments to derive some guidelines towards the design and configuration of future EBNs."],"url":"http://arxiv.org/abs/2501.03210v1"}
{"created":"2025-01-06 18:34:20","title":"Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning and Explainable AI for Academic Integrity","abstract":"This study seeks to enhance academic integrity by providing tools to detect AI-generated content in student work using advanced technologies. The findings promote transparency and accountability, helping educators maintain ethical standards and supporting the responsible integration of AI in education. A key contribution of this work is the generation of the CyberHumanAI dataset, which has 1000 observations, 500 of which are written by humans and the other 500 produced by ChatGPT. We evaluate various machine learning (ML) and deep learning (DL) algorithms on the CyberHumanAI dataset comparing human-written and AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT). Results demonstrate that traditional ML algorithms, specifically XGBoost and Random Forest, achieve high performance (83% and 81% accuracies respectively). Results also show that classifying shorter content seems to be more challenging than classifying longer content. Further, using Explainable Artificial Intelligence (XAI) we identify discriminative features influencing the ML model's predictions, where human-written content tends to use a practical language (e.g., use and allow). Meanwhile AI-generated text is characterized by more abstract and formal terms (e.g., realm and employ). Finally, a comparative analysis with GPTZero show that our narrowly focused, simple, and fine-tuned model can outperform generalized systems like GPTZero. The proposed model achieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when tasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a tendency to classify challenging and small-content cases as either mixed or unrecognized while our proposed model showed a more balanced performance across the three classes.","sentences":["This study seeks to enhance academic integrity by providing tools to detect AI-generated content in student work using advanced technologies.","The findings promote transparency and accountability, helping educators maintain ethical standards and supporting the responsible integration of AI in education.","A key contribution of this work is the generation of the CyberHumanAI dataset, which has 1000 observations, 500 of which are written by humans and the other 500 produced by ChatGPT.","We evaluate various machine learning (ML) and deep learning (DL) algorithms on the CyberHumanAI dataset comparing human-written and AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT).","Results demonstrate that traditional ML algorithms, specifically XGBoost and Random Forest, achieve high performance (83% and 81% accuracies respectively).","Results also show that classifying shorter content seems to be more challenging than classifying longer content.","Further, using Explainable Artificial Intelligence (XAI) we identify discriminative features influencing the ML model's predictions, where human-written content tends to use a practical language (e.g., use and allow).","Meanwhile AI-generated text is characterized by more abstract and formal terms (e.g., realm and employ).","Finally, a comparative analysis with GPTZero show that our narrowly focused, simple, and fine-tuned model can outperform generalized systems like GPTZero.","The proposed model achieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy when tasked to classify Pure AI, Pure Human, and mixed class.","GPTZero showed a tendency to classify challenging and small-content cases as either mixed or unrecognized while our proposed model showed a more balanced performance across the three classes."],"url":"http://arxiv.org/abs/2501.03203v1"}
{"created":"2025-01-06 18:28:04","title":"The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input","abstract":"We introduce FACTS Grounding, an online leaderboard and associated benchmark that evaluates language models' ability to generate text that is factually accurate with respect to given context in the user prompt. In our benchmark, each prompt includes a user request and a full document, with a maximum length of 32k tokens, requiring long-form responses. The long-form responses are required to be fully grounded in the provided context document while fulfilling the user request. Models are evaluated using automated judge models in two phases: (1) responses are disqualified if they do not fulfill the user request; (2) they are judged as accurate if the response is fully grounded in the provided document. The automated judge models were comprehensively evaluated against a held-out test-set to pick the best prompt template, and the final factuality score is an aggregate of multiple judge models to mitigate evaluation bias. The FACTS Grounding leaderboard will be actively maintained over time, and contains both public and private splits to allow for external participation while guarding the integrity of the leaderboard. It can be found at https://www.kaggle.com/facts-leaderboard.","sentences":["We introduce FACTS Grounding, an online leaderboard and associated benchmark that evaluates language models' ability to generate text that is factually accurate with respect to given context in the user prompt.","In our benchmark, each prompt includes a user request and a full document, with a maximum length of 32k tokens, requiring long-form responses.","The long-form responses are required to be fully grounded in the provided context document while fulfilling the user request.","Models are evaluated using automated judge models in two phases: (1) responses are disqualified if they do not fulfill the user request; (2) they are judged as accurate if the response is fully grounded in the provided document.","The automated judge models were comprehensively evaluated against a held-out test-set to pick the best prompt template, and the final factuality score is an aggregate of multiple judge models to mitigate evaluation bias.","The FACTS Grounding leaderboard will be actively maintained over time, and contains both public and private splits to allow for external participation while guarding the integrity of the leaderboard.","It can be found at https://www.kaggle.com/facts-leaderboard."],"url":"http://arxiv.org/abs/2501.03200v1"}
{"created":"2025-01-06 18:06:37","title":"CLIX: Cross-Lingual Explanations of Idiomatic Expressions","abstract":"Automated definition generation systems have been proposed to support vocabulary expansion for language learners. The main barrier to the success of these systems is that learners often struggle to understand definitions due to the presence of potentially unfamiliar words and grammar, particularly when non-standard language is involved. To address these challenges, we propose CLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions. We explore the capabilities of current NLP models for this task, and observe that while it remains challenging, large language models show promise. Finally, we perform a detailed error analysis to highlight the key challenges that need to be addressed before we can reliably incorporate these systems into educational tools.","sentences":["Automated definition generation systems have been proposed to support vocabulary expansion for language learners.","The main barrier to the success of these systems is that learners often struggle to understand definitions due to the presence of potentially unfamiliar words and grammar, particularly when non-standard language is involved.","To address these challenges, we propose CLIX, the task of Cross-Lingual explanations of Idiomatic eXpressions.","We explore the capabilities of current NLP models for this task, and observe that while it remains challenging, large language models show promise.","Finally, we perform a detailed error analysis to highlight the key challenges that need to be addressed before we can reliably incorporate these systems into educational tools."],"url":"http://arxiv.org/abs/2501.03191v1"}
{"created":"2025-01-06 18:05:35","title":"Multimodal Machine Learning Can Predict Videoconference Fluidity and Enjoyment","abstract":"Videoconferencing is now a frequent mode of communication in both professional and informal settings, yet it often lacks the fluidity and enjoyment of in-person conversation. This study leverages multimodal machine learning to predict moments of negative experience in videoconferencing. We sampled thousands of short clips from the RoomReader corpus, extracting audio embeddings, facial actions, and body motion features to train models for identifying low conversational fluidity, low enjoyment, and classifying conversational events (backchanneling, interruption, or gap). Our best models achieved an ROC-AUC of up to 0.87 on hold-out videoconference sessions, with domain-general audio features proving most critical. This work demonstrates that multimodal audio-video signals can effectively predict high-level subjective conversational outcomes. In addition, this is a contribution to research on videoconferencing user experience by showing that multimodal machine learning can be used to identify rare moments of negative user experience for further study or mitigation.","sentences":["Videoconferencing is now a frequent mode of communication in both professional and informal settings, yet it often lacks the fluidity and enjoyment of in-person conversation.","This study leverages multimodal machine learning to predict moments of negative experience in videoconferencing.","We sampled thousands of short clips from the RoomReader corpus, extracting audio embeddings, facial actions, and body motion features to train models for identifying low conversational fluidity, low enjoyment, and classifying conversational events (backchanneling, interruption, or gap).","Our best models achieved an ROC-AUC of up to 0.87 on hold-out videoconference sessions, with domain-general audio features proving most critical.","This work demonstrates that multimodal audio-video signals can effectively predict high-level subjective conversational outcomes.","In addition, this is a contribution to research on videoconferencing user experience by showing that multimodal machine learning can be used to identify rare moments of negative user experience for further study or mitigation."],"url":"http://arxiv.org/abs/2501.03190v1"}
{"created":"2025-01-06 18:04:20","title":"Turn-based Multi-Agent Reinforcement Learning Model Checking","abstract":"In this paper, we propose a novel approach for verifying the compliance of turn-based multi-agent reinforcement learning (TMARL) agents with complex requirements in stochastic multiplayer games. Our method overcomes the limitations of existing verification approaches, which are inadequate for dealing with TMARL agents and not scalable to large games with multiple agents. Our approach relies on tight integration of TMARL and a verification technique referred to as model checking. We demonstrate the effectiveness and scalability of our technique through experiments in different types of environments. Our experiments show that our method is suited to verify TMARL agents and scales better than naive monolithic model checking.","sentences":["In this paper, we propose a novel approach for verifying the compliance of turn-based multi-agent reinforcement learning (TMARL) agents with complex requirements in stochastic multiplayer games.","Our method overcomes the limitations of existing verification approaches, which are inadequate for dealing with TMARL agents and not scalable to large games with multiple agents.","Our approach relies on tight integration of TMARL and a verification technique referred to as model checking.","We demonstrate the effectiveness and scalability of our technique through experiments in different types of environments.","Our experiments show that our method is suited to verify TMARL agents and scales better than naive monolithic model checking."],"url":"http://arxiv.org/abs/2501.03187v1"}
{"created":"2025-01-06 17:49:00","title":"Scalable Forward-Forward Algorithm","abstract":"We propose a scalable Forward-Forward (FF) algorithm that eliminates the need for backpropagation by training each layer separately. Unlike backpropagation, FF avoids backward gradients and can be more modular and memory efficient, making it appealing for large networks. We extend FF to modern convolutional architectures, such as MobileNetV3 and ResNet18, by introducing a new way to compute losses for convolutional layers. Experiments show that our method achieves performance comparable to standard backpropagation. Furthermore, when we divide the network into blocks, such as the residual blocks in ResNet, and apply backpropagation only within each block, but not across blocks, our hybrid design tends to outperform backpropagation baselines while maintaining a similar training speed. Finally, we present experiments on small datasets and transfer learning that confirm the adaptability of our method.","sentences":["We propose a scalable Forward-Forward (FF) algorithm that eliminates the need for backpropagation by training each layer separately.","Unlike backpropagation, FF avoids backward gradients and can be more modular and memory efficient, making it appealing for large networks.","We extend FF to modern convolutional architectures, such as MobileNetV3 and ResNet18, by introducing a new way to compute losses for convolutional layers.","Experiments show that our method achieves performance comparable to standard backpropagation.","Furthermore, when we divide the network into blocks, such as the residual blocks in ResNet, and apply backpropagation only within each block, but not across blocks, our hybrid design tends to outperform backpropagation baselines while maintaining a similar training speed.","Finally, we present experiments on small datasets and transfer learning that confirm the adaptability of our method."],"url":"http://arxiv.org/abs/2501.03176v1"}
{"created":"2025-01-06 17:48:53","title":"Hamiltonian dynamics of Boolean networks","abstract":"This article examines the impact of Hamiltonian dynamics on the interaction graph of Boolean networks. Three types of dynamics are considered: maximum height, Hamiltonian cycle, and an intermediate dynamic between these two. The study addresses how these dynamics influence the connectivity of the graph and the existence of variables that depend on all other variables in the system. Additionally, a family of regulatory Boolean networks capable of describing these three Hamiltonian behaviors is introduced, highlighting their specific properties and limitations. The results provide theoretical tools for modeling complex systems and contribute to the understanding of dynamic interactions in Boolean networks.","sentences":["This article examines the impact of Hamiltonian dynamics on the interaction graph of Boolean networks.","Three types of dynamics are considered: maximum height, Hamiltonian cycle, and an intermediate dynamic between these two.","The study addresses how these dynamics influence the connectivity of the graph and the existence of variables that depend on all other variables in the system.","Additionally, a family of regulatory Boolean networks capable of describing these three Hamiltonian behaviors is introduced, highlighting their specific properties and limitations.","The results provide theoretical tools for modeling complex systems and contribute to the understanding of dynamic interactions in Boolean networks."],"url":"http://arxiv.org/abs/2501.03175v1"}
{"created":"2025-01-06 17:43:26","title":"MObI: Multimodal Object Inpainting Using Diffusion Models","abstract":"Safety-critical applications, such as autonomous driving, require extensive multimodal data for rigorous testing. Methods based on synthetic data are gaining prominence due to the cost and complexity of gathering real-world data but require a high degree of realism and controllability in order to be useful. This paper introduces MObI, a novel framework for Multimodal Object Inpainting that leverages a diffusion model to create realistic and controllable object inpaintings across perceptual modalities, demonstrated for both camera and lidar simultaneously. Using a single reference RGB image, MObI enables objects to be seamlessly inserted into existing multimodal scenes at a 3D location specified by a bounding box, while maintaining semantic consistency and multimodal coherence. Unlike traditional inpainting methods that rely solely on edit masks, our 3D bounding box conditioning gives objects accurate spatial positioning and realistic scaling. As a result, our approach can be used to insert novel objects flexibly into multimodal scenes, providing significant advantages for testing perception models.","sentences":["Safety-critical applications, such as autonomous driving, require extensive multimodal data for rigorous testing.","Methods based on synthetic data are gaining prominence due to the cost and complexity of gathering real-world data but require a high degree of realism and controllability in order to be useful.","This paper introduces MObI, a novel framework for Multimodal Object Inpainting that leverages a diffusion model to create realistic and controllable object inpaintings across perceptual modalities, demonstrated for both camera and lidar simultaneously.","Using a single reference RGB image, MObI enables objects to be seamlessly inserted into existing multimodal scenes at a 3D location specified by a bounding box, while maintaining semantic consistency and multimodal coherence.","Unlike traditional inpainting methods that rely solely on edit masks, our 3D bounding box conditioning gives objects accurate spatial positioning and realistic scaling.","As a result, our approach can be used to insert novel objects flexibly into multimodal scenes, providing significant advantages for testing perception models."],"url":"http://arxiv.org/abs/2501.03173v1"}
{"created":"2025-01-06 17:42:29","title":"GLiREL -- Generalist Model for Zero-Shot Relation Extraction","abstract":"We introduce GLiREL (Generalist Lightweight model for zero-shot Relation Extraction), an efficient architecture and training paradigm for zero-shot relation classification. Inspired by recent advancements in zero-shot named entity recognition, this work presents an approach to efficiently and accurately predict zero-shot relationship labels between multiple entities in a single forward pass. Experiments using the FewRel and WikiZSL benchmarks demonstrate that our approach achieves state-of-the-art results on the zero-shot relation classification task. In addition, we contribute a protocol for synthetically-generating datasets with diverse relation labels.","sentences":["We introduce GLiREL (Generalist Lightweight model for zero-shot Relation Extraction), an efficient architecture and training paradigm for zero-shot relation classification.","Inspired by recent advancements in zero-shot named entity recognition, this work presents an approach to efficiently and accurately predict zero-shot relationship labels between multiple entities in a single forward pass.","Experiments using the FewRel and WikiZSL benchmarks demonstrate that our approach achieves state-of-the-art results on the zero-shot relation classification task.","In addition, we contribute a protocol for synthetically-generating datasets with diverse relation labels."],"url":"http://arxiv.org/abs/2501.03172v1"}
{"created":"2025-01-06 17:36:09","title":"Semantic Captioning: Benchmark Dataset and Graph-Aware Few-Shot In-Context Learning for SQL2Text","abstract":"Large Language Models (LLMs) have demonstrated remarkable performance in various NLP tasks, including semantic parsing, which trans lates natural language into formal code representations. However, the reverse process, translating code into natural language, termed semantic captioning, has received less attention. This task is becoming increasingly important as LLMs are integrated into platforms for code generation, security analysis, and educational purposes. In this paper, we focus on the captioning of SQL query (SQL2Text) to address the critical need for understanding and explaining SQL queries in an era where LLM-generated code poses potential security risks. We repurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt using GPT-4o to generate multiple additional utterances, which enhances the robustness of the datasets for the reverse task. We conduct our experiments using in-context learning (ICL) based on different sample selection methods, emphasizing smaller, more computationally efficient LLMs. Our findings demonstrate that leveraging the inherent graph properties of SQL for ICL sample selection significantly outperforms random selection by up to 39% on BLEU score and provides better results than alternative methods. Dataset and codes are published: \\url{https://github.com/aliwister/ast-icl}.","sentences":["Large Language Models (LLMs) have demonstrated remarkable performance in various NLP tasks, including semantic parsing, which trans lates natural language into formal code representations.","However, the reverse process, translating code into natural language, termed semantic captioning, has received less attention.","This task is becoming increasingly important as LLMs are integrated into platforms for code generation, security analysis, and educational purposes.","In this paper, we focus on the captioning of SQL query (SQL2Text) to address the critical need for understanding and explaining SQL queries in an era where LLM-generated code poses potential security risks.","We repurpose Text2SQL datasets for SQL2Text by introducing an iterative ICL prompt using GPT-4o to generate multiple additional utterances, which enhances the robustness of the datasets for the reverse task.","We conduct our experiments using in-context learning (ICL) based on different sample selection methods, emphasizing smaller, more computationally efficient LLMs.","Our findings demonstrate that leveraging the inherent graph properties of SQL for ICL sample selection significantly outperforms random selection by up to 39% on BLEU score and provides better results than alternative methods.","Dataset and codes are published: \\url{https://github.com/aliwister/ast-icl}."],"url":"http://arxiv.org/abs/2501.03166v1"}
{"created":"2025-01-06 17:31:36","title":"Deep-Relative-Trust-Based Diffusion for Decentralized Deep Learning","abstract":"Decentralized learning strategies allow a collection of agents to learn efficiently from local data sets without the need for central aggregation or orchestration. Current decentralized learning paradigms typically rely on an averaging mechanism to encourage agreement in the parameter space. We argue that in the context of deep neural networks, which are often over-parameterized, encouraging consensus of the neural network outputs, as opposed to their parameters can be more appropriate. This motivates the development of a new decentralized learning algorithm, termed DRT diffusion, based on deep relative trust (DRT), a recently introduced similarity measure for neural networks. We provide convergence analysis for the proposed strategy, and numerically establish its benefit to generalization, especially with sparse topologies, in an image classification task.","sentences":["Decentralized learning strategies allow a collection of agents to learn efficiently from local data sets without the need for central aggregation or orchestration.","Current decentralized learning paradigms typically rely on an averaging mechanism to encourage agreement in the parameter space.","We argue that in the context of deep neural networks, which are often over-parameterized, encouraging consensus of the neural network outputs, as opposed to their parameters can be more appropriate.","This motivates the development of a new decentralized learning algorithm, termed DRT diffusion, based on deep relative trust (DRT), a recently introduced similarity measure for neural networks.","We provide convergence analysis for the proposed strategy, and numerically establish its benefit to generalization, especially with sparse topologies, in an image classification task."],"url":"http://arxiv.org/abs/2501.03162v1"}
{"created":"2025-01-06 17:28:43","title":"Statistical Reconstruction For Anisotropic X-ray Dark-Field Tomography","abstract":"Anisotropic X-ray Dark-Field Tomography (AXDT) is a novel imaging technology that enables the extraction of fiber structures on the micrometer scale, far smaller than standard X-ray Computed Tomography (CT) setups. Directional and structural information is relevant in medical diagnostics and material testing. Compared to existing solutions, AXDT could prove a viable alternative. Reconstruction methods in AXDT have so far been driven by practicality. Improved methods could make AXDT more accessible. We contribute numerically stable implementations and validation of advanced statistical reconstruction methods that incorporate the statistical noise behavior of the imaging system. We further provide a new statistical reconstruction formulation that retains the advanced noise assumptions of the imaging setup while being efficient and easy to optimize. Finally, we provide a detailed analysis of the optimization behavior for all models regarding AXDT. Our experiments show that statistical reconstruction outperforms the previously used model, and particularly the noise performance is superior. While the previously proposed statistical method is effective, it is computationally expensive, and our newly proposed formulation proves highly efficient with identical performance. Our theoretical analysis opens the possibility to new and more advanced reconstruction algorithms, which in turn enable future research in AXDT.","sentences":["Anisotropic X-ray Dark-Field Tomography (AXDT) is a novel imaging technology that enables the extraction of fiber structures on the micrometer scale, far smaller than standard X-ray Computed Tomography (CT) setups.","Directional and structural information is relevant in medical diagnostics and material testing.","Compared to existing solutions, AXDT could prove a viable alternative.","Reconstruction methods in AXDT have so far been driven by practicality.","Improved methods could make AXDT more accessible.","We contribute numerically stable implementations and validation of advanced statistical reconstruction methods that incorporate the statistical noise behavior of the imaging system.","We further provide a new statistical reconstruction formulation that retains the advanced noise assumptions of the imaging setup while being efficient and easy to optimize.","Finally, we provide a detailed analysis of the optimization behavior for all models regarding AXDT.","Our experiments show that statistical reconstruction outperforms the previously used model, and particularly the noise performance is superior.","While the previously proposed statistical method is effective, it is computationally expensive, and our newly proposed formulation proves highly efficient with identical performance.","Our theoretical analysis opens the possibility to new and more advanced reconstruction algorithms, which in turn enable future research in AXDT."],"url":"http://arxiv.org/abs/2501.03160v1"}
{"created":"2025-01-06 17:20:15","title":"A Faster Algorithm for Constrained Correlation Clustering","abstract":"In the Correlation Clustering problem we are given $n$ nodes, and a preference for each pair of nodes indicating whether we prefer the two endpoints to be in the same cluster or not. The output is a clustering inducing the minimum number of violated preferences. In certain cases, however, the preference between some pairs may be too important to be violated. The constrained version of this problem specifies pairs of nodes that must be in the same cluster as well as pairs that must not be in the same cluster (hard constraints). The output clustering has to satisfy all hard constraints while minimizing the number of violated preferences.   Constrained Correlation Clustering is APX-Hard and has been approximated within a factor 3 by van Zuylen et al. [SODA '07] using $\\Omega(n^{3\\omega})$ time. In this work, using a more combinatorial approach, we show how to approximate this problem significantly faster at the cost of a slightly weaker approximation factor. In particular, our algorithm runs in $\\widetilde{O}(n^3)$ time and approximates Constrained Correlation Clustering within a factor 16.   To achieve our result we need properties guaranteed by a particular influential algorithm for (unconstrained) Correlation Clustering, the CC-PIVOT algorithm. This algorithm chooses a pivot node $u$, creates a cluster containing $u$ and all its preferred nodes, and recursively solves the rest of the problem. As a byproduct of our work, we provide a derandomization of the CC-PIVOT algorithm that still achieves the 3-approximation; furthermore, we show that there exist instances where no ordering of the pivots can give a $(3-\\varepsilon)$-approximation, for any constant $\\varepsilon$.   Finally, we introduce a node-weighted version of Correlation Clustering, which can be approximated within factor 3 using our insights on Constrained Correlation Clustering.","sentences":["In the Correlation Clustering problem we are given $n$ nodes, and a preference for each pair of nodes indicating whether we prefer the two endpoints to be in the same cluster or not.","The output is a clustering inducing the minimum number of violated preferences.","In certain cases, however, the preference between some pairs may be too important to be violated.","The constrained version of this problem specifies pairs of nodes that must be in the same cluster as well as pairs that must not be in the same cluster (hard constraints).","The output clustering has to satisfy all hard constraints while minimizing the number of violated preferences.   ","Constrained Correlation Clustering is APX-Hard and has been approximated within a factor 3 by van Zuylen et al.","[SODA '07] using $\\Omega(n^{3\\omega})$ time.","In this work, using a more combinatorial approach, we show how to approximate this problem significantly faster at the cost of a slightly weaker approximation factor.","In particular, our algorithm runs in $\\widetilde{O}(n^3)$ time and approximates Constrained Correlation Clustering within a factor 16.   ","To achieve our result we need properties guaranteed by a particular influential algorithm for (unconstrained) Correlation Clustering, the CC-PIVOT algorithm.","This algorithm chooses a pivot node $u$, creates a cluster containing $u$ and all its preferred nodes, and recursively solves the rest of the problem.","As a byproduct of our work, we provide a derandomization of the CC-PIVOT algorithm that still achieves the 3-approximation; furthermore, we show that there exist instances where no ordering of the pivots can give a $(3-\\varepsilon)$-approximation, for any constant $\\varepsilon$.   Finally, we introduce a node-weighted version of Correlation Clustering, which can be approximated within factor 3 using our insights on Constrained Correlation Clustering."],"url":"http://arxiv.org/abs/2501.03154v1"}
{"created":"2025-01-06 17:19:27","title":"Segment Anything Model for Zero-shot Single Particle Tracking in Liquid Phase Transmission Electron Microscopy","abstract":"Liquid phase transmission electron microscopy (LPTEM) offers an unparalleled combination of spatial and temporal resolution, making it a promising tool for single particle tracking at the nanoscale. However, the absence of a standardized framework for identifying and tracking nanoparticles in noisy LPTEM videos has impeded progress in the field to develop this technique as a single particle tracking tool. To address this, we leveraged Segment Anything Model 2 (SAM 2), released by Meta, which is a foundation model developed for segmenting videos and images. Here, we demonstrate that SAM 2 can successfully segment LPTEM videos in a zero-shot manner and without requiring fine-tuning. Building on this capability, we introduce SAM4EM, a comprehensive framework that integrates promptable video segmentation with particle tracking and statistical analysis, providing an end-to-end LPTEM analysis framework for single particle tracking. SAM4EM achieves nearly 50-fold higher accuracy in segmenting and analyzing LPTEM videos compared to state-of-the-art methods, paving the way for broader applications of LPTEM in nanoscale imaging.","sentences":["Liquid phase transmission electron microscopy (LPTEM) offers an unparalleled combination of spatial and temporal resolution, making it a promising tool for single particle tracking at the nanoscale.","However, the absence of a standardized framework for identifying and tracking nanoparticles in noisy LPTEM videos has impeded progress in the field to develop this technique as a single particle tracking tool.","To address this, we leveraged Segment Anything Model 2 (SAM 2), released by Meta, which is a foundation model developed for segmenting videos and images.","Here, we demonstrate that SAM 2 can successfully segment LPTEM videos in a zero-shot manner and without requiring fine-tuning.","Building on this capability, we introduce SAM4EM, a comprehensive framework that integrates promptable video segmentation with particle tracking and statistical analysis, providing an end-to-end LPTEM analysis framework for single particle tracking.","SAM4EM achieves nearly 50-fold higher accuracy in segmenting and analyzing LPTEM videos compared to state-of-the-art methods, paving the way for broader applications of LPTEM in nanoscale imaging."],"url":"http://arxiv.org/abs/2501.03153v1"}
{"created":"2025-01-06 17:19:19","title":"The Scaling Law for LoRA Base on Mutual Information Upper Bound","abstract":"LoRA (Low-Rank Adaptation) is a widely used model fine-tuning method. In fine-tuning, the law among model performance, model parameters, and data complexity has been a focal issue in the field. Existing methods often leverage external metrics (such as cross-entropy or perplexity) to evaluate model performance. In the fine-tuning process for large models, two types of knowledge are typically involved: the frozen, general knowledge acquired by the model during pre-training and the new knowledge learned through the LoRA module from the current data. Generally, the less LoRA's learned knowledge relies on the large model, the more it captures the specific knowledge of new data, thereby enhancing its adaptability to new tasks. However, external metrics do not readily capture the dependency relationship between these two types of knowledge. Therefore, we designed an internal metric based on the Mutual Information Upper Bound (MIUB) theory to investigate the scaling law of large-model LoRA fine-tuning. In our experiments, we validated this approach on benchmark datasets, using the Llama3-8B and Phi3-3B models. The results show that the proposed MIUB metric aligns more accurately and stably with the scaling law of LoRA fine-tuning compared to cross-entropy and perplexity.","sentences":["LoRA (Low-Rank Adaptation) is a widely used model fine-tuning method.","In fine-tuning, the law among model performance, model parameters, and data complexity has been a focal issue in the field.","Existing methods often leverage external metrics (such as cross-entropy or perplexity) to evaluate model performance.","In the fine-tuning process for large models, two types of knowledge are typically involved: the frozen, general knowledge acquired by the model during pre-training and the new knowledge learned through the LoRA module from the current data.","Generally, the less LoRA's learned knowledge relies on the large model, the more it captures the specific knowledge of new data, thereby enhancing its adaptability to new tasks.","However, external metrics do not readily capture the dependency relationship between these two types of knowledge.","Therefore, we designed an internal metric based on the Mutual Information Upper Bound (MIUB) theory to investigate the scaling law of large-model LoRA fine-tuning.","In our experiments, we validated this approach on benchmark datasets, using the Llama3-8B and Phi3-3B models.","The results show that the proposed MIUB metric aligns more accurately and stably with the scaling law of LoRA fine-tuning compared to cross-entropy and perplexity."],"url":"http://arxiv.org/abs/2501.03152v1"}
{"created":"2025-01-06 17:18:47","title":"Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches","abstract":"Generative artificial intelligence (AI) systems based on large-scale pretrained foundation models (PFMs) such as vision-language models, large language models (LLMs), diffusion models and vision-language-action (VLA) models have demonstrated the ability to solve complex and truly non-trivial AI problems in a wide variety of domains and contexts. Multimodal large language models (MLLMs), in particular, learn from vast and diverse data sources, allowing rich and nuanced representations of the world and, thereby, providing extensive capabilities, including the ability to reason, engage in meaningful dialog; collaborate with humans and other agents to jointly solve complex problems; and understand social and emotional aspects of humans. Despite this impressive feat, the cognitive abilities of state-of-the-art LLMs trained on large-scale datasets are still superficial and brittle. Consequently, generic LLMs are severely limited in their generalist capabilities. A number of foundational problems -- embodiment, symbol grounding, causality and memory -- are required to be addressed for LLMs to attain human-level general intelligence. These concepts are more aligned with human cognition and provide LLMs with inherent human-like cognitive properties that support the realization of physically-plausible, semantically meaningful, flexible and more generalizable knowledge and intelligence. In this work, we discuss the aforementioned foundational issues and survey state-of-the art approaches for implementing these concepts in LLMs. Specifically, we discuss how the principles of embodiment, symbol grounding, causality and memory can be leveraged toward the attainment of artificial general intelligence (AGI) in an organic manner.","sentences":["Generative artificial intelligence (AI) systems based on large-scale pretrained foundation models (PFMs) such as vision-language models, large language models (LLMs), diffusion models and vision-language-action (VLA) models have demonstrated the ability to solve complex and truly non-trivial AI problems in a wide variety of domains and contexts.","Multimodal large language models (MLLMs), in particular, learn from vast and diverse data sources, allowing rich and nuanced representations of the world and, thereby, providing extensive capabilities, including the ability to reason, engage in meaningful dialog; collaborate with humans and other agents to jointly solve complex problems; and understand social and emotional aspects of humans.","Despite this impressive feat, the cognitive abilities of state-of-the-art LLMs trained on large-scale datasets are still superficial and brittle.","Consequently, generic LLMs are severely limited in their generalist capabilities.","A number of foundational problems -- embodiment, symbol grounding, causality and memory -- are required to be addressed for LLMs to attain human-level general intelligence.","These concepts are more aligned with human cognition and provide LLMs with inherent human-like cognitive properties that support the realization of physically-plausible, semantically meaningful, flexible and more generalizable knowledge and intelligence.","In this work, we discuss the aforementioned foundational issues and survey state-of-the art approaches for implementing these concepts in LLMs.","Specifically, we discuss how the principles of embodiment, symbol grounding, causality and memory can be leveraged toward the attainment of artificial general intelligence (AGI) in an organic manner."],"url":"http://arxiv.org/abs/2501.03151v1"}
{"created":"2025-01-06 17:12:19","title":"Geometry Restoration and Dewarping of Camera-Captured Document Images","abstract":"This research focuses on developing a method for restoring the topology of digital images of paper documents captured by a camera, using algorithms for detection, segmentation, geometry restoration, and dewarping. Our methodology employs deep learning (DL) for document outline detection, followed by computer vision (CV) to create a topological 2D grid using cubic polynomial interpolation and correct nonlinear distortions by remapping the image. Using classical CV methods makes the document topology restoration process more efficient and faster, as it requires significantly fewer computational resources and memory. We developed a new pipeline for automatic document dewarping and reconstruction, along with a framework and annotated dataset to demonstrate its efficiency. Our experiments confirm the promise of our methodology and its superiority over existing benchmarks (including mobile apps and popular DL solutions, such as RectiNet, DocGeoNet, and DocTr++) both visually and in terms of document readability via Optical Character Recognition (OCR) and geometry restoration metrics. This paves the way for creating high-quality digital copies of paper documents and enhancing the efficiency of OCR systems. Project page: https://github.com/HorizonParadox/DRCCBI","sentences":["This research focuses on developing a method for restoring the topology of digital images of paper documents captured by a camera, using algorithms for detection, segmentation, geometry restoration, and dewarping.","Our methodology employs deep learning (DL) for document outline detection, followed by computer vision (CV) to create a topological 2D grid using cubic polynomial interpolation and correct nonlinear distortions by remapping the image.","Using classical CV methods makes the document topology restoration process more efficient and faster, as it requires significantly fewer computational resources and memory.","We developed a new pipeline for automatic document dewarping and reconstruction, along with a framework and annotated dataset to demonstrate its efficiency.","Our experiments confirm the promise of our methodology and its superiority over existing benchmarks (including mobile apps and popular DL solutions, such as RectiNet, DocGeoNet, and DocTr++) both visually and in terms of document readability via Optical Character Recognition (OCR) and geometry restoration metrics.","This paves the way for creating high-quality digital copies of paper documents and enhancing the efficiency of OCR systems.","Project page: https://github.com/HorizonParadox/DRCCBI"],"url":"http://arxiv.org/abs/2501.03145v1"}
{"created":"2025-01-06 17:07:44","title":"Co-Activation Graph Analysis of Safety-Verified and Explainable Deep Reinforcement Learning Policies","abstract":"Deep reinforcement learning (RL) policies can demonstrate unsafe behaviors and are challenging to interpret. To address these challenges, we combine RL policy model checking--a technique for determining whether RL policies exhibit unsafe behaviors--with co-activation graph analysis--a method that maps neural network inner workings by analyzing neuron activation patterns--to gain insight into the safe RL policy's sequential decision-making. This combination lets us interpret the RL policy's inner workings for safe decision-making. We demonstrate its applicability in various experiments.","sentences":["Deep reinforcement learning (RL) policies can demonstrate unsafe behaviors and are challenging to interpret.","To address these challenges, we combine RL policy model checking--a technique for determining whether RL policies exhibit unsafe behaviors--with co-activation graph analysis--a method that maps neural network inner workings by analyzing neuron activation patterns--to gain insight into the safe RL policy's sequential decision-making.","This combination lets us interpret the RL policy's inner workings for safe decision-making.","We demonstrate its applicability in various experiments."],"url":"http://arxiv.org/abs/2501.03142v1"}
{"created":"2025-01-06 17:04:26","title":"Foundations of Platform-Assisted Auctions","abstract":"Today, many auctions are carried out with the help of intermediary platforms like Google and eBay. We refer to such auctions as platform-assisted auctions.Traditionally, the auction theory literature mainly focuses on designing auctions that incentivize the buyers to bid truthfully,assuming that the platform always faithfully implements the auction. In practice, however, the platforms have been found to manipulate the auctions to earn more profit, resulting in high-profile anti-trust lawsuits. We propose a new model for studying platform-assisted auctions in the permissionless setting. We explore whether it is possible to design a dream auction in thisnew model, such that honest behavior is the utility-maximizing strategy for each individual buyer, the platform, the seller, as well as platform-seller or platform-buyer coalitions.Through a collection of feasibility and infeasibility results,we carefully characterize the mathematical landscape of platform-assisted auctions. We show how cryptography can lend to the design of an efficient platform-assisted auction with dream properties. Although a line of works have also used MPC or the blockchain to remove the reliance on a trusted auctioneer, our work is distinct in nature in several dimensions.First, we initiate a systematic exploration of the game theoretic implications when the service providers are strategic and can collude with sellers or buyers. Second, we observe that the full simulation paradigm is too stringent and leads to high asymptotical costs. Specifically, because every player has a different private outcomein an auction protocol, running any generic MPC protocol among the players would incur at least $n^2$ total cost. We propose a new notion of simulation calledutility-dominated emulation.Under this new notion, we showhow to design efficient auction protocols with quasilinear efficiency.","sentences":["Today, many auctions are carried out with the help of intermediary platforms like Google and eBay.","We refer to such auctions as platform-assisted auctions.","Traditionally, the auction theory literature mainly focuses on designing auctions that incentivize the buyers to bid truthfully,assuming that the platform always faithfully implements the auction.","In practice, however, the platforms have been found to manipulate the auctions to earn more profit, resulting in high-profile anti-trust lawsuits.","We propose a new model for studying platform-assisted auctions in the permissionless setting.","We explore whether it is possible to design a dream auction in thisnew model, such that honest behavior is the utility-maximizing strategy for each individual buyer, the platform, the seller, as well as platform-seller or platform-buyer coalitions.","Through a collection of feasibility and infeasibility results,we carefully characterize the mathematical landscape of platform-assisted auctions.","We show how cryptography can lend to the design of an efficient platform-assisted auction with dream properties.","Although a line of works have also used MPC or the blockchain to remove the reliance on a trusted auctioneer, our work is distinct in nature in several dimensions.","First, we initiate a systematic exploration of the game theoretic implications when the service providers are strategic and can collude with sellers or buyers.","Second, we observe that the full simulation paradigm is too stringent and leads to high asymptotical costs.","Specifically, because every player has a different private outcomein an auction protocol, running any generic MPC protocol among the players would incur at least $n^2$ total cost.","We propose a new notion of simulation calledutility-dominated emulation.","Under this new notion, we showhow to design efficient auction protocols with quasilinear efficiency."],"url":"http://arxiv.org/abs/2501.03141v1"}
{"created":"2025-01-06 17:01:45","title":"VicSim: Enhancing Victim Simulation with Emotional and Linguistic Fidelity","abstract":"Scenario-based training has been widely adopted in many public service sectors. Recent advancements in Large Language Models (LLMs) have shown promise in simulating diverse personas to create these training scenarios. However, little is known about how LLMs can be developed to simulate victims for scenario-based training purposes. In this paper, we introduce VicSim (victim simulator), a novel model that addresses three key dimensions of user simulation: informational faithfulness, emotional dynamics, and language style (e.g., grammar usage). We pioneer the integration of scenario-based victim modeling with GAN-based training workflow and key-information-based prompting, aiming to enhance the realism of simulated victims. Our adversarial training approach teaches the discriminator to recognize grammar and emotional cues as reliable indicators of synthetic content. According to evaluations by human raters, the VicSim model outperforms GPT-4 in terms of human-likeness.","sentences":["Scenario-based training has been widely adopted in many public service sectors.","Recent advancements in Large Language Models (LLMs) have shown promise in simulating diverse personas to create these training scenarios.","However, little is known about how LLMs can be developed to simulate victims for scenario-based training purposes.","In this paper, we introduce VicSim (victim simulator), a novel model that addresses three key dimensions of user simulation: informational faithfulness, emotional dynamics, and language style (e.g., grammar usage).","We pioneer the integration of scenario-based victim modeling with GAN-based training workflow and key-information-based prompting, aiming to enhance the realism of simulated victims.","Our adversarial training approach teaches the discriminator to recognize grammar and emotional cues as reliable indicators of synthetic content.","According to evaluations by human raters, the VicSim model outperforms GPT-4 in terms of human-likeness."],"url":"http://arxiv.org/abs/2501.03139v1"}
{"created":"2025-01-06 16:50:35","title":"Communication Bounds for the Distributed Experts Problem","abstract":"In this work, we study the experts problem in the distributed setting where an expert's cost needs to be aggregated across multiple servers. Our study considers various communication models such as the message-passing model and the broadcast model, along with multiple aggregation functions, such as summing and taking the $\\ell_p$ norm of an expert's cost across servers. We propose the first communication-efficient protocols that achieve near-optimal regret in these settings, even against a strong adversary who can choose the inputs adaptively. Additionally, we give a conditional lower bound showing that the communication of our protocols is nearly optimal. Finally, we implement our protocols and demonstrate empirical savings on the HPO-B benchmarks.","sentences":["In this work, we study the experts problem in the distributed setting where an expert's cost needs to be aggregated across multiple servers.","Our study considers various communication models such as the message-passing model and the broadcast model, along with multiple aggregation functions, such as summing and taking the $\\ell_p$ norm of an expert's cost across servers.","We propose the first communication-efficient protocols that achieve near-optimal regret in these settings, even against a strong adversary who can choose the inputs adaptively.","Additionally, we give a conditional lower bound showing that the communication of our protocols is nearly optimal.","Finally, we implement our protocols and demonstrate empirical savings on the HPO-B benchmarks."],"url":"http://arxiv.org/abs/2501.03132v1"}
{"created":"2025-01-06 16:48:30","title":"Learning DAGs and Root Causes from Time-Series Data","abstract":"We introduce DAG-TFRC, a novel method for learning directed acyclic graphs (DAGs) from time series with few root causes. By this, we mean that the data are generated by a small number of events at certain, unknown nodes and time points under a structural vector autoregression model. For such data, we (i) learn the DAGs representing both the instantaneous and time-lagged dependencies between nodes, and (ii) discover the location and time of the root causes. For synthetic data with few root causes, DAG-TFRC shows superior performance in accuracy and runtime over prior work, scaling up to thousands of nodes. Experiments on simulated and real-world financial data demonstrate the viability of our sparse root cause assumption. On S&P 500 data, DAG-TFRC successfully clusters stocks by sectors and discovers major stock movements as root causes.","sentences":["We introduce DAG-TFRC, a novel method for learning directed acyclic graphs (DAGs) from time series with few root causes.","By this, we mean that the data are generated by a small number of events at certain, unknown nodes and time points under a structural vector autoregression model.","For such data, we (i) learn the DAGs representing both the instantaneous and time-lagged dependencies between nodes, and (ii) discover the location and time of the root causes.","For synthetic data with few root causes, DAG-TFRC shows superior performance in accuracy and runtime over prior work, scaling up to thousands of nodes.","Experiments on simulated and real-world financial data demonstrate the viability of our sparse root cause assumption.","On S&P 500 data, DAG-TFRC successfully clusters stocks by sectors and discovers major stock movements as root causes."],"url":"http://arxiv.org/abs/2501.03130v1"}
{"created":"2025-01-06 16:34:46","title":"CrowdProve: Community Proving for ZK Rollups","abstract":"Zero-Knowledge (ZK) rollups have become a popular solution for scaling blockchain systems, offering improved transaction throughput and reduced costs by aggregating Layer 2 transactions and submitting them as a single batch to a Layer 1 blockchain. However, the computational burden of generating validity proofs, a key feature of ZK rollups, presents significant challenges in terms of performance and decentralization. Current solutions rely on centralized infrastructure to handle the computational tasks, limiting the scalability and decentralization of rollup systems.   This paper proposes CrowdProve, a prover orchestration layer for outsourcing computation to unreliable commodity hardware run by a broad community of small provers. We apply CrowdProve to proving transaction batches for a popular ZK rollup.   Through our experimental evaluation, we demonstrate that community proving can achieve performance comparable to, and in some cases better than, existing centralized deployments. Our results show that even systems utilizing modest hardware configurations can match the performance of centralized solutions, making community-based proof generation a viable and cost-effective alternative. CrowdProve allows both the rollup operator and community participants to benefit: the operator reduces infrastructure costs by leveraging idle community hardware, while community provers are compensated for their contributions.","sentences":["Zero-Knowledge (ZK) rollups have become a popular solution for scaling blockchain systems, offering improved transaction throughput and reduced costs by aggregating Layer 2 transactions and submitting them as a single batch to a Layer 1 blockchain.","However, the computational burden of generating validity proofs, a key feature of ZK rollups, presents significant challenges in terms of performance and decentralization.","Current solutions rely on centralized infrastructure to handle the computational tasks, limiting the scalability and decentralization of rollup systems.   ","This paper proposes CrowdProve, a prover orchestration layer for outsourcing computation to unreliable commodity hardware run by a broad community of small provers.","We apply CrowdProve to proving transaction batches for a popular ZK rollup.   ","Through our experimental evaluation, we demonstrate that community proving can achieve performance comparable to, and in some cases better than, existing centralized deployments.","Our results show that even systems utilizing modest hardware configurations can match the performance of centralized solutions, making community-based proof generation a viable and cost-effective alternative.","CrowdProve allows both the rollup operator and community participants to benefit: the operator reduces infrastructure costs by leveraging idle community hardware, while community provers are compensated for their contributions."],"url":"http://arxiv.org/abs/2501.03126v1"}
{"created":"2025-01-06 16:31:45","title":"PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models","abstract":"Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs' performance systematically. To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBench can be a robust bench for advancing research on PRM evaluation and development.","sentences":["Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process.","Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios.","However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs' performance systematically.","To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs.","PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity.","In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs.","These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research.","We hope PRMBench can be a robust bench for advancing research on PRM evaluation and development."],"url":"http://arxiv.org/abs/2501.03124v1"}
{"created":"2025-01-06 16:29:46","title":"Normalizing Batch Normalization for Long-Tailed Recognition","abstract":"In real-world scenarios, the number of training samples across classes usually subjects to a long-tailed distribution. The conventionally trained network may achieve unexpected inferior performance on the rare class compared to the frequent class. Most previous works attempt to rectify the network bias from the data-level or from the classifier-level. Differently, in this paper, we identify that the bias towards the frequent class may be encoded into features, i.e., the rare-specific features which play a key role in discriminating the rare class are much weaker than the frequent-specific features. Based on such an observation, we introduce a simple yet effective approach, normalizing the parameters of Batch Normalization (BN) layer to explicitly rectify the feature bias. To achieve this end, we represent the Weight/Bias parameters of a BN layer as a vector, normalize it into a unit one and multiply the unit vector by a scalar learnable parameter. Through decoupling the direction and magnitude of parameters in BN layer to learn, the Weight/Bias exhibits a more balanced distribution and thus the strength of features becomes more even. Extensive experiments on various long-tailed recognition benchmarks (i.e., CIFAR-10/100-LT, ImageNet-LT and iNaturalist 2018) show that our method outperforms previous state-of-the-arts remarkably. The code and checkpoints are available at https://github.com/yuxiangbao/NBN.","sentences":["In real-world scenarios, the number of training samples across classes usually subjects to a long-tailed distribution.","The conventionally trained network may achieve unexpected inferior performance on the rare class compared to the frequent class.","Most previous works attempt to rectify the network bias from the data-level or from the classifier-level.","Differently, in this paper, we identify that the bias towards the frequent class may be encoded into features, i.e., the rare-specific features which play a key role in discriminating the rare class are much weaker than the frequent-specific features.","Based on such an observation, we introduce a simple yet effective approach, normalizing the parameters of Batch Normalization (BN) layer to explicitly rectify the feature bias.","To achieve this end, we represent the Weight/Bias parameters of a BN layer as a vector, normalize it into a unit one and multiply the unit vector by a scalar learnable parameter.","Through decoupling the direction and magnitude of parameters in BN layer to learn, the Weight/Bias exhibits a more balanced distribution and thus the strength of features becomes more even.","Extensive experiments on various long-tailed recognition benchmarks (i.e., CIFAR-10/100-LT, ImageNet-LT and iNaturalist 2018) show that our method outperforms previous state-of-the-arts remarkably.","The code and checkpoints are available at https://github.com/yuxiangbao/NBN."],"url":"http://arxiv.org/abs/2501.03122v1"}
{"created":"2025-01-06 16:29:19","title":"Distributed and heterogeneous tensor-vector contraction algorithms for high performance computing","abstract":"The tensor-vector contraction (TVC) is the most memory-bound operation of its class and a core component of the higher order power method (HOPM). This paper brings distributed-memory parallelization to a native TVC algorithm for dense tensors that overall remains oblivious to contraction mode, tensor splitting and tensor order. Similarly, we propose a novel distributed HOPM, namely dHOPM3, that can save up to one order of magnitude of streamed memory and is about twice as costly in terms of data movement as a distributed TVC operation (dTVC) when using task-based parallelization. The numerical experiments carried out in this work on three different architectures featuring multi-core and accelerated systems confirm that the performance of dTVC and dHOPM3 remains relatively close to the peak system memory bandwidth (50%-80%, depending on the architecture) and on par with STREAM reference values. On strong scalability scenarios, our native multi-core implementations of these two algorithms can achieve similar and sometimes even greater performance figures than those based upon state-of-the-art CUDA batched kernels. Finally, we demonstrate that both computation and communication can benefit from mixed precision arithmetic also in cases where the hardware does not support low precision data types natively.","sentences":["The tensor-vector contraction (TVC) is the most memory-bound operation of its class and a core component of the higher order power method (HOPM).","This paper brings distributed-memory parallelization to a native TVC algorithm for dense tensors that overall remains oblivious to contraction mode, tensor splitting and tensor order.","Similarly, we propose a novel distributed HOPM, namely dHOPM3, that can save up to one order of magnitude of streamed memory and is about twice as costly in terms of data movement as a distributed TVC operation (dTVC) when using task-based parallelization.","The numerical experiments carried out in this work on three different architectures featuring multi-core and accelerated systems confirm that the performance of dTVC and dHOPM3 remains relatively close to the peak system memory bandwidth (50%-80%, depending on the architecture) and on par with STREAM reference values.","On strong scalability scenarios, our native multi-core implementations of these two algorithms can achieve similar and sometimes even greater performance figures than those based upon state-of-the-art CUDA batched kernels.","Finally, we demonstrate that both computation and communication can benefit from mixed precision arithmetic also in cases where the hardware does not support low precision data types natively."],"url":"http://arxiv.org/abs/2501.03121v1"}
{"created":"2025-01-06 16:28:47","title":"CAT: Content-Adaptive Image Tokenization","abstract":"Most existing image tokenizers encode images into a fixed number of tokens or patches, overlooking the inherent variability in image complexity. To address this, we introduce Content-Adaptive Tokenizer (CAT), which dynamically adjusts representation capacity based on the image content and encodes simpler images into fewer tokens. We design a caption-based evaluation system that leverages large language models (LLMs) to predict content complexity and determine the optimal compression ratio for a given image, taking into account factors critical to human perception. Trained on images with diverse compression ratios, CAT demonstrates robust performance in image reconstruction. We also utilize its variable-length latent representations to train Diffusion Transformers (DiTs) for ImageNet generation. By optimizing token allocation, CAT improves the FID score over fixed-ratio baselines trained with the same flops and boosts the inference throughput by 18.5%.","sentences":["Most existing image tokenizers encode images into a fixed number of tokens or patches, overlooking the inherent variability in image complexity.","To address this, we introduce Content-Adaptive Tokenizer (CAT), which dynamically adjusts representation capacity based on the image content and encodes simpler images into fewer tokens.","We design a caption-based evaluation system that leverages large language models (LLMs) to predict content complexity and determine the optimal compression ratio for a given image, taking into account factors critical to human perception.","Trained on images with diverse compression ratios, CAT demonstrates robust performance in image reconstruction.","We also utilize its variable-length latent representations to train Diffusion Transformers (DiTs) for ImageNet generation.","By optimizing token allocation, CAT improves the FID score over fixed-ratio baselines trained with the same flops and boosts the inference throughput by 18.5%."],"url":"http://arxiv.org/abs/2501.03120v1"}
{"created":"2025-01-06 16:27:53","title":"From Models to Network Topologies: A Topology Inference Attack in Decentralized Federated Learning","abstract":"Federated Learning (FL) is widely recognized as a privacy-preserving machine learning paradigm due to its model-sharing mechanism that avoids direct data exchange. However, model training inevitably leaves exploitable traces that can be used to infer sensitive information. In Decentralized FL (DFL), the overlay topology significantly influences its models' convergence, robustness, and security. This study explores the feasibility of inferring the overlay topology of DFL systems based solely on model behavior, introducing a novel Topology Inference Attack. A taxonomy of topology inference attacks is proposed, categorizing them by the attacker's capabilities and knowledge. Practical attack strategies are developed for different scenarios, and quantitative experiments are conducted to identify key factors influencing the attack effectiveness. Experimental results demonstrate that analyzing only the public models of individual nodes can accurately infer the DFL topology, underscoring the risk of sensitive information leakage in DFL systems. This finding offers valuable insights for improving privacy preservation in decentralized learning environments.","sentences":["Federated Learning (FL) is widely recognized as a privacy-preserving machine learning paradigm due to its model-sharing mechanism that avoids direct data exchange.","However, model training inevitably leaves exploitable traces that can be used to infer sensitive information.","In Decentralized FL (DFL), the overlay topology significantly influences its models' convergence, robustness, and security.","This study explores the feasibility of inferring the overlay topology of DFL systems based solely on model behavior, introducing a novel Topology Inference Attack.","A taxonomy of topology inference attacks is proposed, categorizing them by the attacker's capabilities and knowledge.","Practical attack strategies are developed for different scenarios, and quantitative experiments are conducted to identify key factors influencing the attack effectiveness.","Experimental results demonstrate that analyzing only the public models of individual nodes can accurately infer the DFL topology, underscoring the risk of sensitive information leakage in DFL systems.","This finding offers valuable insights for improving privacy preservation in decentralized learning environments."],"url":"http://arxiv.org/abs/2501.03119v1"}
{"created":"2025-01-06 16:26:44","title":"TEE-based Key-Value Stores: a Survey","abstract":"Key-Value Stores (KVSs) are No-SQL databases that store data as key-value pairs and have gained popularity due to their simplicity, scalability, and fast retrieval capabilities. However, storing sensitive data in KVSs requires strong security properties to prevent data leakage and unauthorized tampering. While software (SW)-based encryption techniques are commonly used to maintain data confidentiality and integrity, they suffer from several drawbacks. They strongly assume trust in the hosting system stack and do not secure data during processing unless using performance-heavy techniques (e.g., homomorphic encryption). Alternatively, Trusted Execution Environments (TEEs) provide a solution that enforces the confidentiality and integrity of code and data at the CPU level, allowing users to build trusted applications in an untrusted environment. They also secure data in use by providing an encapsulated processing environment called enclave. Nevertheless, TEEs come with their own set of drawbacks, including performance issues due to memory size limitations and CPU context switching. This paper examines the state of the art in TEE-based confidential KVSs and highlights common design strategies used in KVSs to leverage TEE security features while overcoming their inherent limitations. This work aims to provide a comprehensive understanding of the use of TEEs in KVSs and to identify research directions for future work.","sentences":["Key-Value Stores (KVSs) are No-SQL databases that store data as key-value pairs and have gained popularity due to their simplicity, scalability, and fast retrieval capabilities.","However, storing sensitive data in KVSs requires strong security properties to prevent data leakage and unauthorized tampering.","While software (SW)-based encryption techniques are commonly used to maintain data confidentiality and integrity, they suffer from several drawbacks.","They strongly assume trust in the hosting system stack and do not secure data during processing unless using performance-heavy techniques (e.g., homomorphic encryption).","Alternatively, Trusted Execution Environments (TEEs) provide a solution that enforces the confidentiality and integrity of code and data at the CPU level, allowing users to build trusted applications in an untrusted environment.","They also secure data in use by providing an encapsulated processing environment called enclave.","Nevertheless, TEEs come with their own set of drawbacks, including performance issues due to memory size limitations and CPU context switching.","This paper examines the state of the art in TEE-based confidential KVSs and highlights common design strategies used in KVSs to leverage TEE security features while overcoming their inherent limitations.","This work aims to provide a comprehensive understanding of the use of TEEs in KVSs and to identify research directions for future work."],"url":"http://arxiv.org/abs/2501.03118v1"}
{"created":"2025-01-06 16:20:54","title":"Balancing Efficiency and Expressiveness: Subgraph GNNs with Walk-Based Centrality","abstract":"We propose an expressive and efficient approach that combines the strengths of two prominent extensions of Graph Neural Networks (GNNs): Subgraph GNNs and Structural Encodings (SEs). Our approach leverages walk-based centrality measures, both as a powerful form of SE and also as a subgraph selection strategy for Subgraph GNNs. By drawing a connection to perturbation analysis, we highlight the effectiveness of centrality-based sampling, and show it significantly reduces the computational burden associated with Subgraph GNNs. Further, we combine our efficient Subgraph GNN with SEs derived from the calculated centrality and demonstrate this hybrid approach, dubbed HyMN, gains in discriminative power. HyMN effectively addresses the expressiveness limitations of Message Passing Neural Networks (MPNNs) while mitigating the computational costs of Subgraph GNNs. Through a series of experiments on synthetic and real-world tasks, we show it outperforms other subgraph sampling approaches while being competitive with full-bag Subgraph GNNs and other state-of-the-art approaches with a notably reduced runtime.","sentences":["We propose an expressive and efficient approach that combines the strengths of two prominent extensions of Graph Neural Networks (GNNs): Subgraph GNNs and Structural Encodings (SEs).","Our approach leverages walk-based centrality measures, both as a powerful form of SE and also as a subgraph selection strategy for Subgraph GNNs.","By drawing a connection to perturbation analysis, we highlight the effectiveness of centrality-based sampling, and show it significantly reduces the computational burden associated with Subgraph GNNs.","Further, we combine our efficient Subgraph GNN with SEs derived from the calculated centrality and demonstrate this hybrid approach, dubbed HyMN, gains in discriminative power.","HyMN effectively addresses the expressiveness limitations of Message Passing Neural Networks (MPNNs) while mitigating the computational costs of Subgraph GNNs.","Through a series of experiments on synthetic and real-world tasks, we show it outperforms other subgraph sampling approaches while being competitive with full-bag Subgraph GNNs and other state-of-the-art approaches with a notably reduced runtime."],"url":"http://arxiv.org/abs/2501.03113v1"}
{"created":"2025-01-06 16:20:44","title":"LangFair: A Python Package for Assessing Bias and Fairness in Large Language Model Use Cases","abstract":"Large Language Models (LLMs) have been observed to exhibit bias in numerous ways, potentially creating or worsening outcomes for specific groups identified by protected attributes such as sex, race, sexual orientation, or age. To help address this gap, we introduce LangFair, an open-source Python package that aims to equip LLM practitioners with the tools to evaluate bias and fairness risks relevant to their specific use cases. The package offers functionality to easily generate evaluation datasets, comprised of LLM responses to use-case-specific prompts, and subsequently calculate applicable metrics for the practitioner's use case. To guide in metric selection, LangFair offers an actionable decision framework.","sentences":["Large Language Models (LLMs) have been observed to exhibit bias in numerous ways, potentially creating or worsening outcomes for specific groups identified by protected attributes such as sex, race, sexual orientation, or age.","To help address this gap, we introduce LangFair, an open-source Python package that aims to equip LLM practitioners with the tools to evaluate bias and fairness risks relevant to their specific use cases.","The package offers functionality to easily generate evaluation datasets, comprised of LLM responses to use-case-specific prompts, and subsequently calculate applicable metrics for the practitioner's use case.","To guide in metric selection, LangFair offers an actionable decision framework."],"url":"http://arxiv.org/abs/2501.03112v1"}
{"created":"2025-01-06 16:19:59","title":"Assessing the impact of external factors on the occurrence of emergencies","abstract":"This study investigates the impact of 19 external factors, related to weather, road traffic conditions, air quality, and time, on the occurrence of emergencies using historical data provided by the dispatch center of the Centre Hospitalier Universitaire Vaudois (CHUV). This center is responsible for managing Emergency Medical Service (EMS) resources in the majority of the French-speaking part of Switzerland. First, classical statistical methods, such as correlation, Chi-squared test, Student's $t$-test, and information value, are employed to identify dependencies between the occurrence of emergencies and the considered parameters. Additionally, SHapley Additive exPlanations (SHAP) values and permutation importance are computed using eXtreme Gradient Boosting (XGBoost) and Multilayer Perceptron (MLP) models. The results indicate that the hour of the day, along with correlated parameters, plays a crucial role in the occurrence of emergencies. Conversely, other factors do not significantly influence emergency occurrences. Subsequently, a simplified model that considers only the hour of the day is compared with our XGBoost and MLP models. These comparisons reveal no significant difference between the three models in terms of performance, supporting the use of the basic model in this context. These observations provide valuable insights for EMS resource relocation strategies, benefit predictive modeling efforts, and inform decision-making in the context of EMS. The implications extend to enhancing EMS quality, making this research essential.","sentences":["This study investigates the impact of 19 external factors, related to weather, road traffic conditions, air quality, and time, on the occurrence of emergencies using historical data provided by the dispatch center of the Centre Hospitalier Universitaire Vaudois (CHUV).","This center is responsible for managing Emergency Medical Service (EMS) resources in the majority of the French-speaking part of Switzerland.","First, classical statistical methods, such as correlation, Chi-squared test, Student's $t$-test, and information value, are employed to identify dependencies between the occurrence of emergencies and the considered parameters.","Additionally, SHapley Additive exPlanations (SHAP) values and permutation importance are computed using eXtreme Gradient Boosting (XGBoost) and Multilayer Perceptron (MLP) models.","The results indicate that the hour of the day, along with correlated parameters, plays a crucial role in the occurrence of emergencies.","Conversely, other factors do not significantly influence emergency occurrences.","Subsequently, a simplified model that considers only the hour of the day is compared with our XGBoost and MLP models.","These comparisons reveal no significant difference between the three models in terms of performance, supporting the use of the basic model in this context.","These observations provide valuable insights for EMS resource relocation strategies, benefit predictive modeling efforts, and inform decision-making in the context of EMS.","The implications extend to enhancing EMS quality, making this research essential."],"url":"http://arxiv.org/abs/2501.03111v1"}
{"created":"2025-01-06 16:09:22","title":"MVP: Multimodal Emotion Recognition based on Video and Physiological Signals","abstract":"Human emotions entail a complex set of behavioral, physiological and cognitive changes. Current state-of-the-art models fuse the behavioral and physiological components using classic machine learning, rather than recent deep learning techniques. We propose to fill this gap, designing the Multimodal for Video and Physio (MVP) architecture, streamlined to fuse video and physiological signals. Differently then others approaches, MVP exploits the benefits of attention to enable the use of long input sequences (1-2 minutes). We have studied video and physiological backbones for inputting long sequences and evaluated our method with respect to the state-of-the-art. Our results show that MVP outperforms former methods for emotion recognition based on facial videos, EDA, and ECG/PPG.","sentences":["Human emotions entail a complex set of behavioral, physiological and cognitive changes.","Current state-of-the-art models fuse the behavioral and physiological components using classic machine learning, rather than recent deep learning techniques.","We propose to fill this gap, designing the Multimodal for Video and Physio (MVP) architecture, streamlined to fuse video and physiological signals.","Differently then others approaches, MVP exploits the benefits of attention to enable the use of long input sequences (1-2 minutes).","We have studied video and physiological backbones for inputting long sequences and evaluated our method with respect to the state-of-the-art.","Our results show that MVP outperforms former methods for emotion recognition based on facial videos, EDA, and ECG/PPG."],"url":"http://arxiv.org/abs/2501.03103v1"}
{"created":"2025-01-06 16:08:12","title":"Enhancing Multirotor Drone Efficiency: Exploring Minimum Energy Consumption Rate of Forward Flight under Varying Payload","abstract":"Multirotor unmanned aerial vehicle is a prevailing type of aircraft with wide real-world applications. Energy efficiency is a critical aspect of its performance, determining the range and duration of the missions that can be performed. In this study, we show both analytically and numerically that the optimum of a key energy efficiency index in forward flight, namely energy per meter traveled per unit mass, is a constant under different vehicle mass (including payload). Note that this relationship is only true under the optimal forward velocity that minimizes the energy consumption (under different mass), but not under arbitrary velocity. The study is based on a previously developed model capturing the first-principle energy dynamics of the multirotor, and a key step is to prove that the pitch angle under optimal velocity is a constant. By employing both analytical derivation and validation studies, the research provides critical insights into the optimization of multirotor energy efficiency, and facilitate the development of flight control strategies to extend mission duration and range.","sentences":["Multirotor unmanned aerial vehicle is a prevailing type of aircraft with wide real-world applications.","Energy efficiency is a critical aspect of its performance, determining the range and duration of the missions that can be performed.","In this study, we show both analytically and numerically that the optimum of a key energy efficiency index in forward flight, namely energy per meter traveled per unit mass, is a constant under different vehicle mass (including payload).","Note that this relationship is only true under the optimal forward velocity that minimizes the energy consumption (under different mass), but not under arbitrary velocity.","The study is based on a previously developed model capturing the first-principle energy dynamics of the multirotor, and a key step is to prove that the pitch angle under optimal velocity is a constant.","By employing both analytical derivation and validation studies, the research provides critical insights into the optimization of multirotor energy efficiency, and facilitate the development of flight control strategies to extend mission duration and range."],"url":"http://arxiv.org/abs/2501.03102v1"}
{"created":"2025-01-06 15:56:54","title":"Early Perspectives on the Digital Europe Programme","abstract":"A new Digital Europe Programme (DEP), a funding instrument for development and innovation, was established in the European Union (EU) in 2021. The paper makes an empirical inquiry into the projects funded through the DEP. According to the results, the projects align well with the DEP's strategic focus on cyber security, artificial intelligence, high-performance computing, innovation hubs, small- and medium-sized enterprises, and education. Most of the projects have received an equal amount of national and EU funding. Although national origins of participating organizations do not explain the amounts of funding granted, there is a rather strong tendency for national organizations to primarily collaborate with other national organizations. Finally, information about the technological domains addressed and the economic sectors involved provides decent explanatory power for statistically explaining the funding amounts granted. With these results and the accompanying discussion, the paper contributes to the timely debate about innovation, technology development, and industrial policy in Europe.","sentences":["A new Digital Europe Programme (DEP), a funding instrument for development and innovation, was established in the European Union (EU) in 2021.","The paper makes an empirical inquiry into the projects funded through the DEP.","According to the results, the projects align well with the DEP's strategic focus on cyber security, artificial intelligence, high-performance computing, innovation hubs, small- and medium-sized enterprises, and education.","Most of the projects have received an equal amount of national and EU funding.","Although national origins of participating organizations do not explain the amounts of funding granted, there is a rather strong tendency for national organizations to primarily collaborate with other national organizations.","Finally, information about the technological domains addressed and the economic sectors involved provides decent explanatory power for statistically explaining the funding amounts granted.","With these results and the accompanying discussion, the paper contributes to the timely debate about innovation, technology development, and industrial policy in Europe."],"url":"http://arxiv.org/abs/2501.03098v1"}
{"created":"2025-01-06 15:55:31","title":"Self-directed online information search can affect policy attitudes: a randomized encouragement design with digital behavioral data","abstract":"The abundance of information sources in our digital environment makes it difficult to study how such information shapes individuals' attitudes towards current policies. Our study investigates how self-directed online search in a naturalistic setting through three randomized controlled experiments with 791 German participants on three topical policy issues: basic child support, renewable energy transition, and cannabis legalization. Participants' online browsing was passively tracked, and changes in their attitudes were measured. By encouraging participants to seek online information, this study enhances ecological validity compared to traditional experiments that expose subjects to predetermined content. Significant attitude shifts were observed for child support and cannabis legalization, but not for renewable energy transition. Some findings suggest that the specificity and granularity of policy topics may affect whether and how online information shapes political views, providing insights into the nuanced impact of online information seeking on policy attitudes. By exploring participant's searches and visits, we depict the behavioral patterns that emerge on from our encouragement. Our experimental approach lays the groundwork for future research to advance understanding of the media effect within the dynamic online information landscape.","sentences":["The abundance of information sources in our digital environment makes it difficult to study how such information shapes individuals' attitudes towards current policies.","Our study investigates how self-directed online search in a naturalistic setting through three randomized controlled experiments with 791 German participants on three topical policy issues: basic child support, renewable energy transition, and cannabis legalization.","Participants' online browsing was passively tracked, and changes in their attitudes were measured.","By encouraging participants to seek online information, this study enhances ecological validity compared to traditional experiments that expose subjects to predetermined content.","Significant attitude shifts were observed for child support and cannabis legalization, but not for renewable energy transition.","Some findings suggest that the specificity and granularity of policy topics may affect whether and how online information shapes political views, providing insights into the nuanced impact of online information seeking on policy attitudes.","By exploring participant's searches and visits, we depict the behavioral patterns that emerge on from our encouragement.","Our experimental approach lays the groundwork for future research to advance understanding of the media effect within the dynamic online information landscape."],"url":"http://arxiv.org/abs/2501.03097v1"}
{"created":"2025-01-06 15:51:29","title":"A Novel Structure-Agnostic Multi-Objective Approach for Weight-Sharing Compression in Deep Neural Networks","abstract":"Deep neural networks suffer from storing millions and billions of weights in memory post-training, making challenging memory-intensive models to deploy on embedded devices. The weight-sharing technique is one of the popular compression approaches that use fewer weight values and share across specific connections in the network. In this paper, we propose a multi-objective evolutionary algorithm (MOEA) based compression framework independent of neural network architecture, dimension, task, and dataset. We use uniformly sized bins to quantize network weights into a single codebook (lookup table) for efficient weight representation. Using MOEA, we search for Pareto optimal $k$ bins by optimizing two objectives. Then, we apply the iterative merge technique to non-dominated Pareto frontier solutions by combining neighboring bins without degrading performance to decrease the number of bins and increase the compression ratio. Our approach is model- and layer-independent, meaning the weights are mixed in the clusters from any layer, and the uniform quantization method used in this work has $O(N)$ complexity instead of non-uniform quantization methods such as k-means with $O(Nkt)$ complexity. In addition, we use the center of clusters as the shared weight values instead of retraining shared weights, which is computationally expensive. The advantage of using evolutionary multi-objective optimization is that it can obtain non-dominated Pareto frontier solutions with respect to performance and shared weights. The experimental results show that we can reduce the neural network memory by $13.72 \\sim14.98 \\times$ on CIFAR-10, $11.61 \\sim 12.99\\times$ on CIFAR-100, and $7.44 \\sim 8.58\\times$ on ImageNet showcasing the effectiveness of the proposed deep neural network compression framework.","sentences":["Deep neural networks suffer from storing millions and billions of weights in memory post-training, making challenging memory-intensive models to deploy on embedded devices.","The weight-sharing technique is one of the popular compression approaches that use fewer weight values and share across specific connections in the network.","In this paper, we propose a multi-objective evolutionary algorithm (MOEA) based compression framework independent of neural network architecture, dimension, task, and dataset.","We use uniformly sized bins to quantize network weights into a single codebook (lookup table) for efficient weight representation.","Using MOEA, we search for Pareto optimal $k$ bins by optimizing two objectives.","Then, we apply the iterative merge technique to non-dominated Pareto frontier solutions by combining neighboring bins without degrading performance to decrease the number of bins and increase the compression ratio.","Our approach is model- and layer-independent, meaning the weights are mixed in the clusters from any layer, and the uniform quantization method used in this work has $O(N)$ complexity instead of non-uniform quantization methods such as k-means with $O(Nkt)$ complexity.","In addition, we use the center of clusters as the shared weight values instead of retraining shared weights, which is computationally expensive.","The advantage of using evolutionary multi-objective optimization is that it can obtain non-dominated Pareto frontier solutions with respect to performance and shared weights.","The experimental results show that we can reduce the neural network memory by $13.72 \\sim14.98 \\times$ on CIFAR-10, $11.61 \\sim 12.99\\times$ on CIFAR-100, and $7.44 \\sim 8.58\\times$ on ImageNet showcasing the effectiveness of the proposed deep neural network compression framework."],"url":"http://arxiv.org/abs/2501.03095v1"}
{"created":"2025-01-06 15:41:52","title":"Sentiment-guided Commonsense-aware Response Generation for Mental Health Counseling","abstract":"The crisis of mental health issues is escalating. Effective counseling serves as a critical lifeline for individuals suffering from conditions like PTSD, stress, etc. Therapists forge a crucial therapeutic bond with clients, steering them towards positivity. Unfortunately, the massive shortage of professionals, high costs, and mental health stigma pose significant barriers to consulting therapists. As a substitute, Virtual Mental Health Assistants (VMHAs) have emerged in the digital healthcare space. However, most existing VMHAs lack the commonsense to understand the nuanced sentiments of clients to generate effective responses. To this end, we propose EmpRes, a novel sentiment-guided mechanism incorporating commonsense awareness for generating responses. By leveraging foundation models and harnessing commonsense knowledge, EmpRes aims to generate responses that effectively shape the client's sentiment towards positivity. We evaluate the performance of EmpRes on HOPE, a benchmark counseling dataset, and observe a remarkable performance improvement compared to the existing baselines across a suite of qualitative and quantitative metrics. Moreover, our extensive empirical analysis and human evaluation show that the generation ability of EmpRes is well-suited and, in some cases, surpasses the gold standard. Further, we deploy EmpRes as a chat interface for users seeking mental health support. We address the deployed system's effectiveness through an exhaustive user study with a significant positive response. Our findings show that 91% of users find the system effective, 80% express satisfaction, and over 85.45% convey a willingness to continue using the interface and recommend it to others, demonstrating the practical applicability of EmpRes in addressing the pressing challenges of mental health support, emphasizing user feedback, and ethical considerations in a real-world context.","sentences":["The crisis of mental health issues is escalating.","Effective counseling serves as a critical lifeline for individuals suffering from conditions like PTSD, stress, etc.","Therapists forge a crucial therapeutic bond with clients, steering them towards positivity.","Unfortunately, the massive shortage of professionals, high costs, and mental health stigma pose significant barriers to consulting therapists.","As a substitute, Virtual Mental Health Assistants (VMHAs) have emerged in the digital healthcare space.","However, most existing VMHAs lack the commonsense to understand the nuanced sentiments of clients to generate effective responses.","To this end, we propose EmpRes, a novel sentiment-guided mechanism incorporating commonsense awareness for generating responses.","By leveraging foundation models and harnessing commonsense knowledge, EmpRes aims to generate responses that effectively shape the client's sentiment towards positivity.","We evaluate the performance of EmpRes on HOPE, a benchmark counseling dataset, and observe a remarkable performance improvement compared to the existing baselines across a suite of qualitative and quantitative metrics.","Moreover, our extensive empirical analysis and human evaluation show that the generation ability of EmpRes is well-suited and, in some cases, surpasses the gold standard.","Further, we deploy EmpRes as a chat interface for users seeking mental health support.","We address the deployed system's effectiveness through an exhaustive user study with a significant positive response.","Our findings show that 91% of users find the system effective, 80% express satisfaction, and over 85.45% convey a willingness to continue using the interface and recommend it to others, demonstrating the practical applicability of EmpRes in addressing the pressing challenges of mental health support, emphasizing user feedback, and ethical considerations in a real-world context."],"url":"http://arxiv.org/abs/2501.03088v1"}
{"created":"2025-01-06 15:31:10","title":"Personalized Fashion Recommendation with Image Attributes and Aesthetics Assessment","abstract":"Personalized fashion recommendation is a difficult task because 1) the decisions are highly correlated with users' aesthetic appetite, which previous work frequently overlooks, and 2) many new items are constantly rolling out that cause strict cold-start problems in the popular identity (ID)-based recommendation methods. These new items are critical to recommend because of trend-driven consumerism. In this work, we aim to provide more accurate personalized fashion recommendations and solve the cold-start problem by converting available information, especially images, into two attribute graphs focusing on optimized image utilization and noise-reducing user modeling. Compared with previous methods that separate image and text as two components, the proposed method combines image and text information to create a richer attributes graph. Capitalizing on the advancement of large language and vision models, we experiment with extracting fine-grained attributes efficiently and as desired using two different prompts. Preliminary experiments on the IQON3000 dataset have shown that the proposed method achieves competitive accuracy compared with baselines.","sentences":["Personalized fashion recommendation is a difficult task because 1) the decisions are highly correlated with users' aesthetic appetite, which previous work frequently overlooks, and 2) many new items are constantly rolling out that cause strict cold-start problems in the popular identity (ID)-based recommendation methods.","These new items are critical to recommend because of trend-driven consumerism.","In this work, we aim to provide more accurate personalized fashion recommendations and solve the cold-start problem by converting available information, especially images, into two attribute graphs focusing on optimized image utilization and noise-reducing user modeling.","Compared with previous methods that separate image and text as two components, the proposed method combines image and text information to create a richer attributes graph.","Capitalizing on the advancement of large language and vision models, we experiment with extracting fine-grained attributes efficiently and as desired using two different prompts.","Preliminary experiments on the IQON3000 dataset have shown that the proposed method achieves competitive accuracy compared with baselines."],"url":"http://arxiv.org/abs/2501.03085v1"}
{"created":"2025-01-06 15:20:26","title":"Wheel-GINS: A GNSS/INS Integrated Navigation System with a Wheel-mounted IMU","abstract":"A long-term accurate and robust localization system is essential for mobile robots to operate efficiently outdoors. Recent studies have shown the significant advantages of the wheel-mounted inertial measurement unit (Wheel-IMU)-based dead reckoning system. However, it still drifts over extended periods because of the absence of external correction signals. To achieve the goal of long-term accurate localization, we propose Wheel-GINS, a Global Navigation Satellite System (GNSS)/inertial navigation system (INS) integrated navigation system using a Wheel-IMU. Wheel-GINS fuses the GNSS position measurement with the Wheel-IMU via an extended Kalman filter to limit the long-term error drift and provide continuous state estimation when the GNSS signal is blocked. Considering the specificities of the GNSS/Wheel-IMU integration, we conduct detailed modeling and online estimation of the Wheel-IMU installation parameters, including the Wheel-IMU leverarm and mounting angle and the wheel radius error. Experimental results have shown that Wheel-GINS outperforms the traditional GNSS/Odometer/INS integrated navigation system during GNSS outages. At the same time, Wheel-GINS can effectively estimate the Wheel-IMU installation parameters online and, consequently, improve the localization accuracy and practicality of the system. The source code of our implementation is publicly available (https://github.com/i2Nav-WHU/Wheel-GINS).","sentences":["A long-term accurate and robust localization system is essential for mobile robots to operate efficiently outdoors.","Recent studies have shown the significant advantages of the wheel-mounted inertial measurement unit (Wheel-IMU)-based dead reckoning system.","However, it still drifts over extended periods because of the absence of external correction signals.","To achieve the goal of long-term accurate localization, we propose Wheel-GINS, a Global Navigation Satellite System (GNSS)/inertial navigation system (INS) integrated navigation system using a Wheel-IMU.","Wheel-GINS fuses the GNSS position measurement with the Wheel-IMU via an extended Kalman filter to limit the long-term error drift and provide continuous state estimation when the GNSS signal is blocked.","Considering the specificities of the GNSS/Wheel-IMU integration, we conduct detailed modeling and online estimation of the Wheel-IMU installation parameters, including the Wheel-IMU leverarm and mounting angle and the wheel radius error.","Experimental results have shown that Wheel-GINS outperforms the traditional GNSS/Odometer/INS integrated navigation system during GNSS outages.","At the same time, Wheel-GINS can effectively estimate the Wheel-IMU installation parameters online and, consequently, improve the localization accuracy and practicality of the system.","The source code of our implementation is publicly available (https://github.com/i2Nav-WHU/Wheel-GINS)."],"url":"http://arxiv.org/abs/2501.03079v1"}
{"created":"2025-01-06 15:20:22","title":"Qinco2: Vector Compression and Search with Improved Implicit Neural Codebooks","abstract":"Vector quantization is a fundamental technique for compression and large-scale nearest neighbor search. For high-accuracy operating points, multi-codebook quantization associates data vectors with one element from each of multiple codebooks. An example is residual quantization (RQ), which iteratively quantizes the residual error of previous steps. Dependencies between the different parts of the code are, however, ignored in RQ, which leads to suboptimal rate-distortion performance. QINCo recently addressed this inefficiency by using a neural network to determine the quantization codebook in RQ based on the vector reconstruction from previous steps. In this paper we introduce QINCo2 which extends and improves QINCo with (i) improved vector encoding using codeword pre-selection and beam-search, (ii) a fast approximate decoder leveraging codeword pairs to establish accurate short-lists for search, and (iii) an optimized training procedure and network architecture. We conduct experiments on four datasets to evaluate QINCo2 for vector compression and billion-scale nearest neighbor search. We obtain outstanding results in both settings, improving the state-of-the-art reconstruction MSE by 34% for 16-byte vector compression on BigANN, and search accuracy by 24% with 8-byte encodings on Deep1M.","sentences":["Vector quantization is a fundamental technique for compression and large-scale nearest neighbor search.","For high-accuracy operating points, multi-codebook quantization associates data vectors with one element from each of multiple codebooks.","An example is residual quantization (RQ), which iteratively quantizes the residual error of previous steps.","Dependencies between the different parts of the code are, however, ignored in RQ, which leads to suboptimal rate-distortion performance.","QINCo recently addressed this inefficiency by using a neural network to determine the quantization codebook in RQ based on the vector reconstruction from previous steps.","In this paper we introduce QINCo2 which extends and improves QINCo with (i) improved vector encoding using codeword pre-selection and beam-search, (ii) a fast approximate decoder leveraging codeword pairs to establish accurate short-lists for search, and (iii) an optimized training procedure and network architecture.","We conduct experiments on four datasets to evaluate QINCo2 for vector compression and billion-scale nearest neighbor search.","We obtain outstanding results in both settings, improving the state-of-the-art reconstruction MSE by 34% for 16-byte vector compression on BigANN, and search accuracy by 24% with 8-byte encodings on Deep1M."],"url":"http://arxiv.org/abs/2501.03078v1"}
{"created":"2025-01-06 15:17:44","title":"RIS-Driven Resource Allocation Strategies for Diverse Network Environments: A Comprehensive Review","abstract":"This comprehensive survey examines how Reconfigurable Intelligent Surfaces (RIS) revolutionize resource allocation in various network frameworks. It begins by establishing a theoretical foundation with an overview of RIS technologies, including passive RIS, active RIS, and Simultaneously Transmitting and Reflecting RIS (STAR-RIS). The core of the survey focuses on RIS's role in optimizing resource allocation within Single-Input Multiple-Output (SIMO), Multiple-Input Single-Output (MISO), and Multiple-Input Multiple-Output (MIMO) systems. It further explores RIS integration in complex network environments, such as Heterogeneous Wireless Networks (HetNets) and Non-Orthogonal Multiple Access (NOMA) frameworks. Additionally, the survey investigates RIS applications in advanced communication domains like Terahertz (THz) networks, Vehicular Communication (VC), and Unmanned Aerial Vehicle (UAV) communications, highlighting the synergy between RIS and Artificial Intelligence (AI) for enhanced network efficiency. Summary tables provide comparative insights into various schemes. The survey concludes with lessons learned, future research directions, and challenges, emphasizing critical open issues.","sentences":["This comprehensive survey examines how Reconfigurable Intelligent Surfaces (RIS) revolutionize resource allocation in various network frameworks.","It begins by establishing a theoretical foundation with an overview of RIS technologies, including passive RIS, active RIS, and Simultaneously Transmitting and Reflecting RIS (STAR-RIS).","The core of the survey focuses on RIS's role in optimizing resource allocation within Single-Input Multiple-Output (SIMO), Multiple-Input Single-Output (MISO), and Multiple-Input Multiple-Output (MIMO) systems.","It further explores RIS integration in complex network environments, such as Heterogeneous Wireless Networks (HetNets) and Non-Orthogonal Multiple Access (NOMA) frameworks.","Additionally, the survey investigates RIS applications in advanced communication domains like Terahertz (THz) networks, Vehicular Communication (VC), and Unmanned Aerial Vehicle (UAV) communications, highlighting the synergy between RIS and Artificial Intelligence (AI) for enhanced network efficiency.","Summary tables provide comparative insights into various schemes.","The survey concludes with lessons learned, future research directions, and challenges, emphasizing critical open issues."],"url":"http://arxiv.org/abs/2501.03075v1"}
{"created":"2025-01-06 15:11:24","title":"AIF-SFDA: Autonomous Information Filter-driven Source-Free Domain Adaptation for Medical Image Segmentation","abstract":"Decoupling domain-variant information (DVI) from domain-invariant information (DII) serves as a prominent strategy for mitigating domain shifts in the practical implementation of deep learning algorithms. However, in medical settings, concerns surrounding data collection and privacy often restrict access to both training and test data, hindering the empirical decoupling of information by existing methods. To tackle this issue, we propose an Autonomous Information Filter-driven Source-free Domain Adaptation (AIF-SFDA) algorithm, which leverages a frequency-based learnable information filter to autonomously decouple DVI and DII. Information Bottleneck (IB) and Self-supervision (SS) are incorporated to optimize the learnable frequency filter. The IB governs the information flow within the filter to diminish redundant DVI, while SS preserves DII in alignment with the specific task and image modality. Thus, the autonomous information filter can overcome domain shifts relying solely on target data. A series of experiments covering various medical image modalities and segmentation tasks were conducted to demonstrate the benefits of AIF-SFDA through comparisons with leading algorithms and ablation studies. The code is available at https://github.com/JingHuaMan/AIF-SFDA.","sentences":["Decoupling domain-variant information (DVI) from domain-invariant information (DII) serves as a prominent strategy for mitigating domain shifts in the practical implementation of deep learning algorithms.","However, in medical settings, concerns surrounding data collection and privacy often restrict access to both training and test data, hindering the empirical decoupling of information by existing methods.","To tackle this issue, we propose an Autonomous Information Filter-driven Source-free Domain Adaptation (AIF-SFDA) algorithm, which leverages a frequency-based learnable information filter to autonomously decouple DVI and DII.","Information Bottleneck (IB) and Self-supervision (SS) are incorporated to optimize the learnable frequency filter.","The IB governs the information flow within the filter to diminish redundant DVI, while SS preserves DII in alignment with the specific task and image modality.","Thus, the autonomous information filter can overcome domain shifts relying solely on target data.","A series of experiments covering various medical image modalities and segmentation tasks were conducted to demonstrate the benefits of AIF-SFDA through comparisons with leading algorithms and ablation studies.","The code is available at https://github.com/JingHuaMan/AIF-SFDA."],"url":"http://arxiv.org/abs/2501.03074v1"}
{"created":"2025-01-06 15:10:22","title":"Retrieval-Augmented TLAPS Proof Generation with Large Language Models","abstract":"We present a novel approach to automated proof generation for the TLA+ Proof System (TLAPS) using Large Language Models (LLMs). Our method combines two key components: a sub-proof obligation generation phase that breaks down complex proof obligations into simpler sub-obligations, and a proof generation phase that leverages Retrieval-Augmented Generation with verified proof examples. We evaluate our approach using proof obligations from varying complexity levels of proof obligations, spanning from fundamental arithmetic properties to the properties of algorithms. Our experiments demonstrate that while the method successfully generates valid proofs for intermediate-complexity obligations, it faces limitations with more complex theorems. These results indicate that our approach can effectively assist in proof development for certain classes of properties, contributing to the broader goal of integrating LLMs into formal verification workflows.","sentences":["We present a novel approach to automated proof generation for the TLA+ Proof System (TLAPS) using Large Language Models (LLMs).","Our method combines two key components: a sub-proof obligation generation phase that breaks down complex proof obligations into simpler sub-obligations, and a proof generation phase that leverages Retrieval-Augmented Generation with verified proof examples.","We evaluate our approach using proof obligations from varying complexity levels of proof obligations, spanning from fundamental arithmetic properties to the properties of algorithms.","Our experiments demonstrate that while the method successfully generates valid proofs for intermediate-complexity obligations, it faces limitations with more complex theorems.","These results indicate that our approach can effectively assist in proof development for certain classes of properties, contributing to the broader goal of integrating LLMs into formal verification workflows."],"url":"http://arxiv.org/abs/2501.03073v1"}
{"created":"2025-01-06 15:04:58","title":"SGLDBench: A Benchmark Suite for Stress-Guided Lightweight 3D Designs","abstract":"We introduce the Stress-Guided Lightweight Design Benchmark (SGLDBench), a comprehensive benchmark suite to apply and evaluate material layout strategies for generating stiff lightweight designs in 3D domains. SGLDBench provides a seamlessly integrated simulation and analysis framework, providing six reference strategies accompanied by a scalable multigrid elasticity solver to efficiently execute these strategies and validate the stiffness of their results. This facilitates systematic analysis and comparison of design strategies regarding the mechanical properties they achieve. SGLDBench enables the evaluation of diverse settings of load conditions and, through the tight integration of the solver, enables support for high-resolution designs and stiffness analysis. Moreover, SGLDBench emphasizes visual analysis to explore relations between the geometric structure of a design and the distribution of stresses, providing insights into the specific properties and behaviors of different design strategies. SGLDBenchs' specific features are highlighted in several experiments, by comparing the results of reference strategies with respect to geometric and mechanical properties.","sentences":["We introduce the Stress-Guided Lightweight Design Benchmark (SGLDBench), a comprehensive benchmark suite to apply and evaluate material layout strategies for generating stiff lightweight designs in 3D domains.","SGLDBench provides a seamlessly integrated simulation and analysis framework, providing six reference strategies accompanied by a scalable multigrid elasticity solver to efficiently execute these strategies and validate the stiffness of their results.","This facilitates systematic analysis and comparison of design strategies regarding the mechanical properties they achieve.","SGLDBench enables the evaluation of diverse settings of load conditions and, through the tight integration of the solver, enables support for high-resolution designs and stiffness analysis.","Moreover, SGLDBench emphasizes visual analysis to explore relations between the geometric structure of a design and the distribution of stresses, providing insights into the specific properties and behaviors of different design strategies.","SGLDBenchs' specific features are highlighted in several experiments, by comparing the results of reference strategies with respect to geometric and mechanical properties."],"url":"http://arxiv.org/abs/2501.03068v1"}
{"created":"2025-01-06 15:04:45","title":"Design and implementation of tools to build an ontology of Security Requirements for Internet of Medical Things","abstract":"When developing devices, architectures and services for the Internet of Medical Things (IoMT) world, manufacturers or integrators must be aware of the security requirements expressed by both laws and specifications. To provide tools guiding through these requirements and to assure a third party of the correct compliance, an ontology charting the relevant laws and specifications (for the European context) is very useful. We here address the development of this ontology. Due to the very high number and size of the considered specification documents, we have put in place a methodology and tools to simplify the transition from natural text to an ontology. The first step is a manual highlighting of relevant concepts in the corpus, then a manual translation to XML/XSD is operated. We have developed a tool allowing us to convert this semi-structured data into an ontology. Because the different specifications use similar but different wording, our approach favors the creation of similar instances in the ontology. To improve the ontology simplification through instance merging, we consider the use of LLMs. The responses of the LLMs are compared against our manually defined correct responses. The quality of the responses of the automated system does not prove to be good enough to be trusted blindly, and should only be used as a starting point for a manual correction.","sentences":["When developing devices, architectures and services for the Internet of Medical Things (IoMT) world, manufacturers or integrators must be aware of the security requirements expressed by both laws and specifications.","To provide tools guiding through these requirements and to assure a third party of the correct compliance, an ontology charting the relevant laws and specifications (for the European context) is very useful.","We here address the development of this ontology.","Due to the very high number and size of the considered specification documents, we have put in place a methodology and tools to simplify the transition from natural text to an ontology.","The first step is a manual highlighting of relevant concepts in the corpus, then a manual translation to XML/XSD is operated.","We have developed a tool allowing us to convert this semi-structured data into an ontology.","Because the different specifications use similar but different wording, our approach favors the creation of similar instances in the ontology.","To improve the ontology simplification through instance merging, we consider the use of LLMs.","The responses of the LLMs are compared against our manually defined correct responses.","The quality of the responses of the automated system does not prove to be good enough to be trusted blindly, and should only be used as a starting point for a manual correction."],"url":"http://arxiv.org/abs/2501.03067v1"}
{"created":"2025-01-06 15:02:30","title":"Trust Modeling in Counseling Conversations: A Benchmark Study","abstract":"In mental health counseling, a variety of earlier studies have focused on dialogue modeling. However, most of these studies give limited to no emphasis on the quality of interaction between a patient and a therapist. The therapeutic bond between a patient and a therapist directly correlates with effective mental health counseling. It involves developing the patient's trust on the therapist over the course of counseling. To assess the therapeutic bond in counseling, we introduce trust as a therapist-assistive metric. Our definition of trust involves patients' willingness and openness to express themselves and, consequently, receive better care. We conceptualize it as a dynamic trajectory observable through textual interactions during the counseling. To facilitate trust modeling, we present MENTAL-TRUST, a novel counseling dataset comprising manual annotation of 212 counseling sessions with first-of-its-kind seven expert-verified ordinal trust levels. We project our problem statement as an ordinal classification task for trust quantification and propose a new benchmark, TrustBench, comprising a suite of classical and state-of-the-art language models on MENTAL-TRUST. We evaluate the performance across a suite of metrics and lay out an exhaustive set of findings. Our study aims to unfold how trust evolves in therapeutic interactions.","sentences":["In mental health counseling, a variety of earlier studies have focused on dialogue modeling.","However, most of these studies give limited to no emphasis on the quality of interaction between a patient and a therapist.","The therapeutic bond between a patient and a therapist directly correlates with effective mental health counseling.","It involves developing the patient's trust on the therapist over the course of counseling.","To assess the therapeutic bond in counseling, we introduce trust as a therapist-assistive metric.","Our definition of trust involves patients' willingness and openness to express themselves and, consequently, receive better care.","We conceptualize it as a dynamic trajectory observable through textual interactions during the counseling.","To facilitate trust modeling, we present MENTAL-TRUST, a novel counseling dataset comprising manual annotation of 212 counseling sessions with first-of-its-kind seven expert-verified ordinal trust levels.","We project our problem statement as an ordinal classification task for trust quantification and propose a new benchmark, TrustBench, comprising a suite of classical and state-of-the-art language models on MENTAL-TRUST.","We evaluate the performance across a suite of metrics and lay out an exhaustive set of findings.","Our study aims to unfold how trust evolves in therapeutic interactions."],"url":"http://arxiv.org/abs/2501.03064v1"}
{"created":"2025-01-06 14:49:26","title":"Through-The-Mask: Mask-based Motion Trajectories for Image-to-Video Generation","abstract":"We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description. While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object motion, especially in multi-object scenarios. To address these limitations, we propose a two-stage compositional framework that decomposes I2V generation into: (i) An explicit intermediate representation generation stage, followed by (ii) A video generation stage that is conditioned on this representation. Our key innovation is the introduction of a mask-based motion trajectory as an intermediate representation, that captures both semantic object information and motion, enabling an expressive but compact representation of motion and semantics. To incorporate the learned representation in the second stage, we utilize object-level attention objectives. Specifically, we consider a spatial, per-object, masked-cross attention objective, integrating object-specific prompts into corresponding latent space regions and a masked spatio-temporal self-attention objective, ensuring frame-to-frame consistency for each object. We evaluate our method on challenging benchmarks with multi-object and high-motion scenarios and empirically demonstrate that the proposed method achieves state-of-the-art results in temporal coherence, motion realism, and text-prompt faithfulness. Additionally, we introduce \\benchmark, a new challenging benchmark for single-object and multi-object I2V generation, and demonstrate our method's superiority on this benchmark. Project page is available at https://guyyariv.github.io/TTM/.","sentences":["We consider the task of Image-to-Video (I2V) generation, which involves transforming static images into realistic video sequences based on a textual description.","While recent advancements produce photorealistic outputs, they frequently struggle to create videos with accurate and consistent object motion, especially in multi-object scenarios.","To address these limitations, we propose a two-stage compositional framework that decomposes I2V generation into: (i)","An explicit intermediate representation generation stage, followed by (ii) A video generation stage that is conditioned on this representation.","Our key innovation is the introduction of a mask-based motion trajectory as an intermediate representation, that captures both semantic object information and motion, enabling an expressive but compact representation of motion and semantics.","To incorporate the learned representation in the second stage, we utilize object-level attention objectives.","Specifically, we consider a spatial, per-object, masked-cross attention objective, integrating object-specific prompts into corresponding latent space regions and a masked spatio-temporal self-attention objective, ensuring frame-to-frame consistency for each object.","We evaluate our method on challenging benchmarks with multi-object and high-motion scenarios and empirically demonstrate that the proposed method achieves state-of-the-art results in temporal coherence, motion realism, and text-prompt faithfulness.","Additionally, we introduce \\benchmark, a new challenging benchmark for single-object and multi-object I2V generation, and demonstrate our method's superiority on this benchmark.","Project page is available at https://guyyariv.github.io/TTM/."],"url":"http://arxiv.org/abs/2501.03059v1"}
{"created":"2025-01-06 14:48:30","title":"Survival Analysis Revisited: Understanding and Unifying Poisson, Exponential, and Cox Models in Fall Risk Analysis","abstract":"This paper explores foundational and applied aspects of survival analysis, using fall risk assessment as a case study. It revisits key time-related probability distributions and statistical methods, including logistic regression, Poisson regression, Exponential regression, and the Cox Proportional Hazards model, offering a unified perspective on their relationships within the survival analysis framework. A contribution of this work is the step-by-step derivation and clarification of the relationships among these models, particularly demonstrating that Poisson regression in the survival context is a specific case of the Cox model. These insights address gaps in understanding and reinforce the simplicity and interpretability of survival models. The paper also emphasizes the practical utility of survival analysis by connecting theoretical insights with real-world applications. In the context of fall detection, it demonstrates how these models can simultaneously predict fall risk, analyze contributing factors, and estimate time-to-event outcomes within a single streamlined framework. In contrast, advanced deep learning methods often require complex post-hoc interpretation and separate training for different tasks particularly when working with structured numerical data. This highlights the enduring relevance of classical statistical frameworks and makes survival models especially valuable in healthcare settings, where explainability and robustness are critical. By unifying foundational concepts and offering a cohesive perspective on time-to-event analysis, this work serves as an accessible resource for understanding survival models and applying them effectively to diverse analytical challenges.","sentences":["This paper explores foundational and applied aspects of survival analysis, using fall risk assessment as a case study.","It revisits key time-related probability distributions and statistical methods, including logistic regression, Poisson regression, Exponential regression, and the Cox Proportional Hazards model, offering a unified perspective on their relationships within the survival analysis framework.","A contribution of this work is the step-by-step derivation and clarification of the relationships among these models, particularly demonstrating that Poisson regression in the survival context is a specific case of the Cox model.","These insights address gaps in understanding and reinforce the simplicity and interpretability of survival models.","The paper also emphasizes the practical utility of survival analysis by connecting theoretical insights with real-world applications.","In the context of fall detection, it demonstrates how these models can simultaneously predict fall risk, analyze contributing factors, and estimate time-to-event outcomes within a single streamlined framework.","In contrast, advanced deep learning methods often require complex post-hoc interpretation and separate training for different tasks particularly when working with structured numerical data.","This highlights the enduring relevance of classical statistical frameworks and makes survival models especially valuable in healthcare settings, where explainability and robustness are critical.","By unifying foundational concepts and offering a cohesive perspective on time-to-event analysis, this work serves as an accessible resource for understanding survival models and applying them effectively to diverse analytical challenges."],"url":"http://arxiv.org/abs/2501.03058v1"}
{"created":"2025-01-06 14:41:45","title":"To Analyze and Regulate Human-in-the-loop Learning for Congestion Games","abstract":"In congestion games, selfish users behave myopically to crowd to the shortest paths, and the social planner designs mechanisms to regulate such selfish routing through information or payment incentives. However, such mechanism design requires the knowledge of time-varying traffic conditions and it is the users themselves to learn and report past road experiences to the social planner (e.g., Waze or Google Maps). When congestion games meet mobile crowdsourcing, it is critical to incentivize selfish users to explore non-shortest paths in the best exploitation-exploration trade-off. First, we consider a simple but fundamental parallel routing network with one deterministic path and multiple stochastic paths for users with an average arrival probability $\\lambda$. We prove that the current myopic routing policy (widely used in Waze and Google Maps) misses both exploration (when strong hazard belief) and exploitation (when weak hazard belief) as compared to the social optimum. Due to the myopic policy's under-exploration, we prove that the caused price of anarchy (PoA) is larger than \\(\\frac{1}{1-\\rho^{\\frac{1}{\\lambda}}}\\), which can be arbitrarily large as discount factor \\(\\rho\\rightarrow1\\). To mitigate such huge efficiency loss, we propose a novel selective information disclosure (SID) mechanism: we only reveal the latest traffic information to users when they intend to over-explore stochastic paths upon arrival, while hiding such information when they want to under-explore. We prove that our mechanism successfully reduces PoA to be less than~\\(2\\). Besides the parallel routing network, we further extend our mechanism and PoA results to any linear path graphs with multiple intermediate nodes.","sentences":["In congestion games, selfish users behave myopically to crowd to the shortest paths, and the social planner designs mechanisms to regulate such selfish routing through information or payment incentives.","However, such mechanism design requires the knowledge of time-varying traffic conditions and it is the users themselves to learn and report past road experiences to the social planner (e.g., Waze or Google Maps).","When congestion games meet mobile crowdsourcing, it is critical to incentivize selfish users to explore non-shortest paths in the best exploitation-exploration trade-off.","First, we consider a simple but fundamental parallel routing network with one deterministic path and multiple stochastic paths for users with an average arrival probability $\\lambda$. We prove that the current myopic routing policy (widely used in Waze and Google Maps) misses both exploration (when strong hazard belief) and exploitation (when weak hazard belief) as compared to the social optimum.","Due to the myopic policy's under-exploration, we prove that the caused price of anarchy (PoA) is larger than \\(\\frac{1}{1-\\rho^{\\frac{1}{\\lambda}}}\\), which can be arbitrarily large as discount factor \\(\\rho\\rightarrow1\\).","To mitigate such huge efficiency loss, we propose a novel selective information disclosure (SID) mechanism: we only reveal the latest traffic information to users when they intend to over-explore stochastic paths upon arrival, while hiding such information when they want to under-explore.","We prove that our mechanism successfully reduces PoA to be less than~\\(2\\).","Besides the parallel routing network, we further extend our mechanism and PoA results to any linear path graphs with multiple intermediate nodes."],"url":"http://arxiv.org/abs/2501.03055v1"}
{"created":"2025-01-06 14:40:49","title":"A Passive Mechanical Add-on for Treadmill Exercise (P-MATE) in Stroke Rehabilitation","abstract":"Robotic rehabilitation can deliver high-dose gait therapy and improve motor function after a stroke. However, for many devices, high costs and lengthy setup times limit clinical adoption. Thus, we designed, built, and evaluated the Passive Mechanical Add-on for Treadmill Exercise (P-MATE), a low-cost passive end-effector add-on for treadmills that couples the movement of the paretic and non-paretic legs via a reciprocating system of elastic cables and pulleys. Two human-device mechanical interfaces were designed to attach the elastic cables to the user. The P-MATE and two interface prototypes were tested with a physical therapist and eight unimpaired participants. Biomechanical data, including kinematics and interaction forces, were collected alongside standardized questionnaires to assess usability and user experience. Both interfaces were quick and easy to attach, though user experience differed, highlighting the need for personalization. We also identified areas for future improvement, including pretension adjustments, tendon derailing prevention, and understanding long-term impacts on user gait. Our preliminary findings underline the potential of the P-MATE to provide effective, accessible, and sustainable stroke gait rehabilitation.","sentences":["Robotic rehabilitation can deliver high-dose gait therapy and improve motor function after a stroke.","However, for many devices, high costs and lengthy setup times limit clinical adoption.","Thus, we designed, built, and evaluated the Passive Mechanical Add-on for Treadmill Exercise (P-MATE), a low-cost passive end-effector add-on for treadmills that couples the movement of the paretic and non-paretic legs via a reciprocating system of elastic cables and pulleys.","Two human-device mechanical interfaces were designed to attach the elastic cables to the user.","The P-MATE and two interface prototypes were tested with a physical therapist and eight unimpaired participants.","Biomechanical data, including kinematics and interaction forces, were collected alongside standardized questionnaires to assess usability and user experience.","Both interfaces were quick and easy to attach, though user experience differed, highlighting the need for personalization.","We also identified areas for future improvement, including pretension adjustments, tendon derailing prevention, and understanding long-term impacts on user gait.","Our preliminary findings underline the potential of the P-MATE to provide effective, accessible, and sustainable stroke gait rehabilitation."],"url":"http://arxiv.org/abs/2501.03054v1"}
{"created":"2025-01-06 14:27:41","title":"ChronoSense: Exploring Temporal Understanding in Large Language Models with Time Intervals of Events","abstract":"Large Language Models (LLMs) have achieved remarkable success in various NLP tasks, yet they still face significant challenges in reasoning and arithmetic. Temporal reasoning, a critical component of natural language understanding, has raised increasing research attention. However, comprehensive testing of Allen's interval relations (e.g., before, after, during) -- a fundamental framework for temporal relationships -- remains underexplored. To fill this gap, we present ChronoSense, a new benchmark for evaluating LLMs' temporal understanding. It includes 16 tasks, focusing on identifying the Allen relation between two temporal events and temporal arithmetic, using both abstract events and real-world data from Wikidata. We assess the performance of seven recent LLMs using this benchmark and the results indicate that models handle Allen relations, even symmetrical ones, quite differently. Moreover, the findings suggest that the models may rely on memorization to answer time-related questions. Overall, the models' low performance highlights the need for improved temporal understanding in LLMs and ChronoSense offers a robust framework for future research in this area. Our dataset and the source code are available at https://github.com/duyguislakoglu/chronosense.","sentences":["Large Language Models (LLMs) have achieved remarkable success in various NLP tasks, yet they still face significant challenges in reasoning and arithmetic.","Temporal reasoning, a critical component of natural language understanding, has raised increasing research attention.","However, comprehensive testing of Allen's interval relations (e.g., before, after, during) -- a fundamental framework for temporal relationships -- remains underexplored.","To fill this gap, we present ChronoSense, a new benchmark for evaluating LLMs' temporal understanding.","It includes 16 tasks, focusing on identifying the Allen relation between two temporal events and temporal arithmetic, using both abstract events and real-world data from Wikidata.","We assess the performance of seven recent LLMs using this benchmark and the results indicate that models handle Allen relations, even symmetrical ones, quite differently.","Moreover, the findings suggest that the models may rely on memorization to answer time-related questions.","Overall, the models' low performance highlights the need for improved temporal understanding in LLMs and ChronoSense offers a robust framework for future research in this area.","Our dataset and the source code are available at https://github.com/duyguislakoglu/chronosense."],"url":"http://arxiv.org/abs/2501.03040v1"}
{"created":"2025-01-06 14:26:00","title":"Piano Transcription by Hierarchical Language Modeling with Pretrained Roll-based Encoders","abstract":"Automatic Music Transcription (AMT), aiming to get musical notes from raw audio, typically uses frame-level systems with piano-roll outputs or language model (LM)-based systems with note-level predictions. However, frame-level systems require manual thresholding, while the LM-based systems struggle with long sequences. In this paper, we propose a hybrid method combining pre-trained roll-based encoders with an LM decoder to leverage the strengths of both methods. Besides, our approach employs a hierarchical prediction strategy, first predicting onset and pitch, then velocity, and finally offset. The hierarchical prediction strategy reduces computational costs by breaking down long sequences into different hierarchies. Evaluated on two benchmark roll-based encoders, our method outperforms traditional piano-roll outputs 0.01 and 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a performance-enhancing plug-in for arbitrary roll-based music transcription encoder. We release the code of this work at https://github.com/yongyizang/AMT_train.","sentences":["Automatic Music Transcription (AMT), aiming to get musical notes from raw audio, typically uses frame-level systems with piano-roll outputs or language model (LM)-based systems with note-level predictions.","However, frame-level systems require manual thresholding, while the LM-based systems struggle with long sequences.","In this paper, we propose a hybrid method combining pre-trained roll-based encoders with an LM decoder to leverage the strengths of both methods.","Besides, our approach employs a hierarchical prediction strategy, first predicting onset and pitch, then velocity, and finally offset.","The hierarchical prediction strategy reduces computational costs by breaking down long sequences into different hierarchies.","Evaluated on two benchmark roll-based encoders, our method outperforms traditional piano-roll outputs 0.01 and 0.022 in onset-offset-velocity F1 score, demonstrating its potential as a performance-enhancing plug-in for arbitrary roll-based music transcription encoder.","We release the code of this work at https://github.com/yongyizang/AMT_train."],"url":"http://arxiv.org/abs/2501.03038v1"}
{"created":"2025-01-06 14:23:02","title":"Quantization Meets Reasoning: Exploring LLM Low-Bit Quantization Degradation for Mathematical Reasoning","abstract":"Large language models have achieved significant advancements in complex mathematical reasoning benchmarks, such as MATH. However, their substantial computational requirements present challenges for practical deployment. Model quantization has emerged as an effective strategy to reduce memory usage and computational costs by employing lower precision and bit-width representations. In this study, we systematically evaluate the impact of quantization on mathematical reasoning tasks. We introduce a multidimensional evaluation framework that qualitatively assesses specific capability dimensions and conduct quantitative analyses on the step-by-step outputs of various quantization methods. Our results demonstrate that quantization differentially affects numerical computation and reasoning planning abilities, identifying key areas where quantized models experience performance degradation.","sentences":["Large language models have achieved significant advancements in complex mathematical reasoning benchmarks, such as MATH.","However, their substantial computational requirements present challenges for practical deployment.","Model quantization has emerged as an effective strategy to reduce memory usage and computational costs by employing lower precision and bit-width representations.","In this study, we systematically evaluate the impact of quantization on mathematical reasoning tasks.","We introduce a multidimensional evaluation framework that qualitatively assesses specific capability dimensions and conduct quantitative analyses on the step-by-step outputs of various quantization methods.","Our results demonstrate that quantization differentially affects numerical computation and reasoning planning abilities, identifying key areas where quantized models experience performance degradation."],"url":"http://arxiv.org/abs/2501.03035v1"}
{"created":"2025-01-06 14:09:35","title":"Putnam's Critical and Explanatory Tendencies Interpreted from a Machine Learning Perspective","abstract":"Making sense of theory choice in normal and across extraordinary science is central to philosophy of science. The emergence of machine learning models has the potential to act as a wrench in the gears of current debates. In this paper, I will attempt to reconstruct the main movements that lead to and came out of Putnam's critical and explanatory tendency distinction, argue for the biconditional necessity of the tendencies, and conceptualize that wrench through a machine learning interpretation of my claim.","sentences":["Making sense of theory choice in normal and across extraordinary science is central to philosophy of science.","The emergence of machine learning models has the potential to act as a wrench in the gears of current debates.","In this paper, I will attempt to reconstruct the main movements that lead to and came out of Putnam's critical and explanatory tendency distinction, argue for the biconditional necessity of the tendencies, and conceptualize that wrench through a machine learning interpretation of my claim."],"url":"http://arxiv.org/abs/2501.03026v1"}
