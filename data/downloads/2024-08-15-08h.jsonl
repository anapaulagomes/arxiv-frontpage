{"created":"2024-08-14 17:59:32","title":"Knowledge Distillation with Refined Logits","abstract":"Recent research on knowledge distillation has increasingly focused on logit distillation because of its simplicity, effectiveness, and versatility in model compression. In this paper, we introduce Refined Logit Distillation (RLD) to address the limitations of current logit distillation methods. Our approach is motivated by the observation that even high-performing teacher models can make incorrect predictions, creating a conflict between the standard distillation loss and the cross-entropy loss. This conflict can undermine the consistency of the student model's learning objectives. Previous attempts to use labels to empirically correct teacher predictions may undermine the class correlation. In contrast, our RLD employs labeling information to dynamically refine teacher logits. In this way, our method can effectively eliminate misleading information from the teacher while preserving crucial class correlations, thus enhancing the value and efficiency of distilled knowledge. Experimental results on CIFAR-100 and ImageNet demonstrate its superiority over existing methods. The code is provided at \\text{https://github.com/zju-SWJ/RLD}.","sentences":["Recent research on knowledge distillation has increasingly focused on logit distillation because of its simplicity, effectiveness, and versatility in model compression.","In this paper, we introduce Refined Logit Distillation (RLD) to address the limitations of current logit distillation methods.","Our approach is motivated by the observation that even high-performing teacher models can make incorrect predictions, creating a conflict between the standard distillation loss and the cross-entropy loss.","This conflict can undermine the consistency of the student model's learning objectives.","Previous attempts to use labels to empirically correct teacher predictions may undermine the class correlation.","In contrast, our RLD employs labeling information to dynamically refine teacher logits.","In this way, our method can effectively eliminate misleading information from the teacher while preserving crucial class correlations, thus enhancing the value and efficiency of distilled knowledge.","Experimental results on CIFAR-100 and ImageNet demonstrate its superiority over existing methods.","The code is provided at \\text{https://github.com/zju-SWJ/RLD}."],"url":"http://arxiv.org/abs/2408.07703v1"}
{"created":"2024-08-14 17:59:04","title":"The Death of Schema Linking? Text-to-SQL in the Age of Well-Reasoned Language Models","abstract":"Schema linking is a crucial step in Text-to-SQL pipelines, which translate natural language queries into SQL. The goal of schema linking is to retrieve relevant tables and columns (signal) while disregarding irrelevant ones (noise). However, imperfect schema linking can often exclude essential columns needed for accurate query generation. In this work, we revisit the need for schema linking when using the latest generation of large language models (LLMs). We find empirically that newer models are adept at identifying relevant schema elements during generation, without the need for explicit schema linking. This allows Text-to-SQL pipelines to bypass schema linking entirely and instead pass the full database schema to the LLM, eliminating the risk of excluding necessary information. Furthermore, as alternatives to schema linking, we propose techniques that improve Text-to-SQL accuracy without compromising on essential schema information. Our approach achieves 71.83\\% execution accuracy on the BIRD benchmark, ranking first at the time of submission.","sentences":["Schema linking is a crucial step in Text-to-SQL pipelines, which translate natural language queries into SQL.","The goal of schema linking is to retrieve relevant tables and columns (signal) while disregarding irrelevant ones (noise).","However, imperfect schema linking can often exclude essential columns needed for accurate query generation.","In this work, we revisit the need for schema linking when using the latest generation of large language models (LLMs).","We find empirically that newer models are adept at identifying relevant schema elements during generation, without the need for explicit schema linking.","This allows Text-to-SQL pipelines to bypass schema linking entirely and instead pass the full database schema to the LLM, eliminating the risk of excluding necessary information.","Furthermore, as alternatives to schema linking, we propose techniques that improve Text-to-SQL accuracy without compromising on essential schema information.","Our approach achieves 71.83\\% execution accuracy on the BIRD benchmark, ranking first at the time of submission."],"url":"http://arxiv.org/abs/2408.07702v1"}
{"created":"2024-08-14 17:53:13","title":"Quantifying over Optimum Answer Sets","abstract":"Answer Set Programming with Quantifiers (ASP(Q)) has been introduced to provide a natural extension of ASP modeling to problems in the polynomial hierarchy (PH). However, ASP(Q) lacks a method for encoding in an elegant and compact way problems requiring a polynomial number of calls to an oracle in $\\Sigma_n^p$ (that is, problems in $\\Delta_{n+1}^p$). Such problems include, in particular, optimization problems. In this paper we propose an extension of ASP(Q), in which component programs may contain weak constraints. Weak constraints can be used both for expressing local optimization within quantified component programs and for modeling global optimization criteria. We showcase the modeling capabilities of the new formalism through various application scenarios. Further, we study its computational properties obtaining complexity results and unveiling non-obvious characteristics of ASP(Q) programs with weak constraints.","sentences":["Answer Set Programming with Quantifiers (ASP(Q)) has been introduced to provide a natural extension of ASP modeling to problems in the polynomial hierarchy (PH).","However, ASP(Q) lacks a method for encoding in an elegant and compact way problems requiring a polynomial number of calls to an oracle in $\\Sigma_n^p$ (that is, problems in $\\Delta_{n+1}^p$).","Such problems include, in particular, optimization problems.","In this paper we propose an extension of ASP(Q), in which component programs may contain weak constraints.","Weak constraints can be used both for expressing local optimization within quantified component programs and for modeling global optimization criteria.","We showcase the modeling capabilities of the new formalism through various application scenarios.","Further, we study its computational properties obtaining complexity results and unveiling non-obvious characteristics of ASP(Q) programs with weak constraints."],"url":"http://arxiv.org/abs/2408.07697v1"}
{"created":"2024-08-14 17:50:27","title":"End-to-end Semantic-centric Video-based Multimodal Affective Computing","abstract":"In the pathway toward Artificial General Intelligence (AGI), understanding human's affection is essential to enhance machine's cognition abilities. For achieving more sensual human-AI interaction, Multimodal Affective Computing (MAC) in human-spoken videos has attracted increasing attention. However, previous methods are mainly devoted to designing multimodal fusion algorithms, suffering from two issues: semantic imbalance caused by diverse pre-processing operations and semantic mismatch raised by inconsistent affection content contained in different modalities comparing with the multimodal ground truth. Besides, the usage of manual features extractors make they fail in building end-to-end pipeline for multiple MAC downstream tasks. To address above challenges, we propose a novel end-to-end framework named SemanticMAC to compute multimodal semantic-centric affection for human-spoken videos. We firstly employ pre-trained Transformer model in multimodal data pre-processing and design Affective Perceiver module to capture unimodal affective information. Moreover, we present a semantic-centric approach to unify multimodal representation learning in three ways, including gated feature interaction, multi-task pseudo label generation, and intra-/inter-sample contrastive learning. Finally, SemanticMAC effectively learn specific- and shared-semantic representations in the guidance of semantic-centric labels. Extensive experimental results demonstrate that our approach surpass the state-of-the-art methods on 7 public datasets in four MAC downstream tasks.","sentences":["In the pathway toward Artificial General Intelligence (AGI), understanding human's affection is essential to enhance machine's cognition abilities.","For achieving more sensual human-AI interaction, Multimodal Affective Computing (MAC) in human-spoken videos has attracted increasing attention.","However, previous methods are mainly devoted to designing multimodal fusion algorithms, suffering from two issues: semantic imbalance caused by diverse pre-processing operations and semantic mismatch raised by inconsistent affection content contained in different modalities comparing with the multimodal ground truth.","Besides, the usage of manual features extractors make they fail in building end-to-end pipeline for multiple MAC downstream tasks.","To address above challenges, we propose a novel end-to-end framework named SemanticMAC to compute multimodal semantic-centric affection for human-spoken videos.","We firstly employ pre-trained Transformer model in multimodal data pre-processing and design Affective Perceiver module to capture unimodal affective information.","Moreover, we present a semantic-centric approach to unify multimodal representation learning in three ways, including gated feature interaction, multi-task pseudo label generation, and intra-/inter-sample contrastive learning.","Finally, SemanticMAC effectively learn specific- and shared-semantic representations in the guidance of semantic-centric labels.","Extensive experimental results demonstrate that our approach surpass the state-of-the-art methods on 7 public datasets in four MAC downstream tasks."],"url":"http://arxiv.org/abs/2408.07694v1"}
{"created":"2024-08-14 17:45:13","title":"Detecting Near-Duplicate Face Images","abstract":"Near-duplicate images are often generated when applying repeated photometric and geometric transformations that produce imperceptible variants of the original image. Consequently, a deluge of near-duplicates can be circulated online posing copyright infringement concerns. The concerns are more severe when biometric data is altered through such nuanced transformations. In this work, we address the challenge of near-duplicate detection in face images by, firstly, identifying the original image from a set of near-duplicates and, secondly, deducing the relationship between the original image and the near-duplicates. We construct a tree-like structure, called an Image Phylogeny Tree (IPT) using a graph-theoretic approach to estimate the relationship, i.e., determine the sequence in which they have been generated. We further extend our method to create an ensemble of IPTs known as Image Phylogeny Forests (IPFs). We rigorously evaluate our method to demonstrate robustness across other modalities, unseen transformations by latest generative models and IPT configurations, thereby significantly advancing the state-of-the-art performance by 42% on IPF reconstruction accuracy.","sentences":["Near-duplicate images are often generated when applying repeated photometric and geometric transformations that produce imperceptible variants of the original image.","Consequently, a deluge of near-duplicates can be circulated online posing copyright infringement concerns.","The concerns are more severe when biometric data is altered through such nuanced transformations.","In this work, we address the challenge of near-duplicate detection in face images by, firstly, identifying the original image from a set of near-duplicates and, secondly, deducing the relationship between the original image and the near-duplicates.","We construct a tree-like structure, called an Image Phylogeny Tree (IPT) using a graph-theoretic approach to estimate the relationship, i.e., determine the sequence in which they have been generated.","We further extend our method to create an ensemble of IPTs known as Image Phylogeny Forests (IPFs).","We rigorously evaluate our method to demonstrate robustness across other modalities, unseen transformations by latest generative models and IPT configurations, thereby significantly advancing the state-of-the-art performance by 42% on IPF reconstruction accuracy."],"url":"http://arxiv.org/abs/2408.07689v1"}
{"created":"2024-08-14 17:41:45","title":"RSD-DOG : A New Image Descriptor based on Second Order Derivatives","abstract":"This paper introduces the new and powerful image patch descriptor based on second order image statistics/derivatives. Here, the image patch is treated as a 3D surface with intensity being the 3rd dimension. The considered 3D surface has a rich set of second order features/statistics such as ridges, valleys, cliffs and so on, that can be easily captured by using the difference of rotating semi Gaussian filters. The originality of this method is based on successfully combining the response of the directional filters with that of the Difference of Gaussian (DOG) approach. The obtained descriptor shows a good discriminative power when dealing with the variations in illumination, scale, rotation, blur, viewpoint and compression. The experiments on image matching, demonstrates the advantage of the obtained descriptor when compared to its first order counterparts such as SIFT, DAISY, GLOH, GIST and LIDRIC.","sentences":["This paper introduces the new and powerful image patch descriptor based on second order image statistics/derivatives.","Here, the image patch is treated as a 3D surface with intensity being the 3rd dimension.","The considered 3D surface has a rich set of second order features/statistics such as ridges, valleys, cliffs and so on, that can be easily captured by using the difference of rotating semi Gaussian filters.","The originality of this method is based on successfully combining the response of the directional filters with that of the Difference of Gaussian (DOG) approach.","The obtained descriptor shows a good discriminative power when dealing with the variations in illumination, scale, rotation, blur, viewpoint and compression.","The experiments on image matching, demonstrates the advantage of the obtained descriptor when compared to its first order counterparts such as SIFT, DAISY, GLOH, GIST and LIDRIC."],"url":"http://arxiv.org/abs/2408.07687v1"}
{"created":"2024-08-14 17:37:05","title":"Auto-bidding and Auctions in Online Advertising: A Survey","abstract":"In this survey, we summarize recent developments in research fueled by the growing adoption of automated bidding strategies in online advertising. We explore the challenges and opportunities that have arisen as markets embrace this autobidding and cover a range of topics in this area, including bidding algorithms, equilibrium analysis and efficiency of common auction formats, and optimal auction design.","sentences":["In this survey, we summarize recent developments in research fueled by the growing adoption of automated bidding strategies in online advertising.","We explore the challenges and opportunities that have arisen as markets embrace this autobidding and cover a range of topics in this area, including bidding algorithms, equilibrium analysis and efficiency of common auction formats, and optimal auction design."],"url":"http://arxiv.org/abs/2408.07685v1"}
{"created":"2024-08-14 17:35:25","title":"Composing Automatic Differentiation with Custom Derivatives of Higher-Order Functions","abstract":"Recent theoretical work on automatic differentiation (autodiff) has focused on characteristics such as correctness and efficiency while assuming that all derivatives are automatically generated by autodiff using program transformation, with the exception of a fixed set of derivatives for primitive operations. However, in practice this assumption is insufficient: the programmer often needs to provide custom derivatives for composite functions to achieve efficiency and numerical stability. In this work, we start from the untyped lambda calculus with a reverse-mode autodiff operator, extend it with an operator to attach manual derivatives, and demonstrate its utility via several examples.","sentences":["Recent theoretical work on automatic differentiation (autodiff) has focused on characteristics such as correctness and efficiency while assuming that all derivatives are automatically generated by autodiff using program transformation, with the exception of a fixed set of derivatives for primitive operations.","However, in practice this assumption is insufficient: the programmer often needs to provide custom derivatives for composite functions to achieve efficiency and numerical stability.","In this work, we start from the untyped lambda calculus with a reverse-mode autodiff operator, extend it with an operator to attach manual derivatives, and demonstrate its utility via several examples."],"url":"http://arxiv.org/abs/2408.07683v1"}
{"created":"2024-08-14 17:28:58","title":"A Spitting Image: Modular Superpixel Tokenization in Vision Transformers","abstract":"Vision Transformer (ViT) architectures traditionally employ a grid-based approach to tokenization independent of the semantic content of an image. We propose a modular superpixel tokenization strategy which decouples tokenization and feature extraction; a shift from contemporary approaches where these are treated as an undifferentiated whole. Using on-line content-aware tokenization and scale- and shape-invariant positional embeddings, we perform experiments and ablations that contrast our approach with patch-based tokenization and randomized partitions as baselines. We show that our method significantly improves the faithfulness of attributions, gives pixel-level granularity on zero-shot unsupervised dense prediction tasks, while maintaining predictive performance in classification tasks. Our approach provides a modular tokenization framework commensurable with standard architectures, extending the space of ViTs to a larger class of semantically-rich models.","sentences":["Vision Transformer (ViT) architectures traditionally employ a grid-based approach to tokenization independent of the semantic content of an image.","We propose a modular superpixel tokenization strategy which decouples tokenization and feature extraction; a shift from contemporary approaches where these are treated as an undifferentiated whole.","Using on-line content-aware tokenization and scale- and shape-invariant positional embeddings, we perform experiments and ablations that contrast our approach with patch-based tokenization and randomized partitions as baselines.","We show that our method significantly improves the faithfulness of attributions, gives pixel-level granularity on zero-shot unsupervised dense prediction tasks, while maintaining predictive performance in classification tasks.","Our approach provides a modular tokenization framework commensurable with standard architectures, extending the space of ViTs to a larger class of semantically-rich models."],"url":"http://arxiv.org/abs/2408.07680v1"}
{"created":"2024-08-14 17:23:12","title":"Enhanced Detection of Conversational Mental Manipulation Through Advanced Prompting Techniques","abstract":"This study presents a comprehensive, long-term project to explore the effectiveness of various prompting techniques in detecting dialogical mental manipulation. We implement Chain-of-Thought prompting with Zero-Shot and Few-Shot settings on a binary mental manipulation detection task, building upon existing work conducted with Zero-Shot and Few- Shot prompting. Our primary objective is to decipher why certain prompting techniques display superior performance, so as to craft a novel framework tailored for detection of mental manipulation. Preliminary findings suggest that advanced prompting techniques may not be suitable for more complex models, if they are not trained through example-based learning.","sentences":["This study presents a comprehensive, long-term project to explore the effectiveness of various prompting techniques in detecting dialogical mental manipulation.","We implement Chain-of-Thought prompting with Zero-Shot and Few-Shot settings on a binary mental manipulation detection task, building upon existing work conducted with Zero-Shot and Few- Shot prompting.","Our primary objective is to decipher why certain prompting techniques display superior performance, so as to craft a novel framework tailored for detection of mental manipulation.","Preliminary findings suggest that advanced prompting techniques may not be suitable for more complex models, if they are not trained through example-based learning."],"url":"http://arxiv.org/abs/2408.07676v1"}
{"created":"2024-08-14 17:22:41","title":"G$^2$V$^2$former: Graph Guided Video Vision Transformer for Face Anti-Spoofing","abstract":"In videos containing spoofed faces, we may uncover the spoofing evidence based on either photometric or dynamic abnormality, even a combination of both. Prevailing face anti-spoofing (FAS) approaches generally concentrate on the single-frame scenario, however, purely photometric-driven methods overlook the dynamic spoofing clues that may be exposed over time. This may lead FAS systems to conclude incorrect judgments, especially in cases where it is easily distinguishable in terms of dynamics but challenging to discern in terms of photometrics. To this end, we propose the Graph Guided Video Vision Transformer (G$^2$V$^2$former), which combines faces with facial landmarks for photometric and dynamic feature fusion. We factorize the attention into space and time, and fuse them via a spatiotemporal block. Specifically, we design a novel temporal attention called Kronecker temporal attention, which has a wider receptive field, and is beneficial for capturing dynamic information. Moreover, we leverage the low-semantic motion of facial landmarks to guide the high-semantic change of facial expressions based on the motivation that regions containing landmarks may reveal more dynamic clues. Extensive experiments on nine benchmark datasets demonstrate that our method achieves superior performance under various scenarios. The codes will be released soon.","sentences":["In videos containing spoofed faces, we may uncover the spoofing evidence based on either photometric or dynamic abnormality, even a combination of both.","Prevailing face anti-spoofing (FAS) approaches generally concentrate on the single-frame scenario, however, purely photometric-driven methods overlook the dynamic spoofing clues that may be exposed over time.","This may lead FAS systems to conclude incorrect judgments, especially in cases where it is easily distinguishable in terms of dynamics but challenging to discern in terms of photometrics.","To this end, we propose the Graph Guided Video Vision Transformer (G$^2$V$^2$former), which combines faces with facial landmarks for photometric and dynamic feature fusion.","We factorize the attention into space and time, and fuse them via a spatiotemporal block.","Specifically, we design a novel temporal attention called Kronecker temporal attention, which has a wider receptive field, and is beneficial for capturing dynamic information.","Moreover, we leverage the low-semantic motion of facial landmarks to guide the high-semantic change of facial expressions based on the motivation that regions containing landmarks may reveal more dynamic clues.","Extensive experiments on nine benchmark datasets demonstrate that our method achieves superior performance under various scenarios.","The codes will be released soon."],"url":"http://arxiv.org/abs/2408.07675v1"}
{"created":"2024-08-14 17:16:50","title":"Deep Learning: a Heuristic Three-stage Mechanism for Grid Searches to Optimize the Future Risk Prediction of Breast Cancer Metastasis Using EHR-based Clinical Data","abstract":"A grid search, at the cost of training and testing a large number of models, is an effective way to optimize the prediction performance of deep learning models. A challenging task concerning grid search is the time management. Without a good time management scheme, a grid search can easily be set off as a mission that will not finish in our lifetime. In this study, we introduce a heuristic three-stage mechanism for managing the running time of low-budget grid searches, and the sweet-spot grid search (SSGS) and randomized grid search (RGS) strategies for improving model prediction performance, in predicting the 5-year, 10-year, and 15-year risk of breast cancer metastasis. We develop deep feedforward neural network (DFNN) models and optimize them through grid searches. We conduct eight cycles of grid searches by applying our three-stage mechanism and SSGS and RGS strategies. We conduct various SHAP analyses including unique ones that interpret the importance of the DFNN-model hyperparameters. Our results show that grid search can greatly improve model prediction. The grid searches we conducted improved the risk prediction of 5-year, 10-year, and 15-year breast cancer metastasis by 18.6%, 16.3%, and 17.3% respectively, over the average performance of all corresponding models we trained. We not only demonstrate best model performance but also characterize grid searches from various aspects such as their capabilities of discovering decent models and the unit grid search time. The three-stage mechanism worked effectively. It made our low-budget grid searches feasible and manageable, and in the meantime helped improve model prediction performance. Our SHAP analyses identified both clinical risk factors important for the prediction of future risk of breast cancer metastasis, and DFNN-model hyperparameters important to the prediction of performance scores.","sentences":["A grid search, at the cost of training and testing a large number of models, is an effective way to optimize the prediction performance of deep learning models.","A challenging task concerning grid search is the time management.","Without a good time management scheme, a grid search can easily be set off as a mission that will not finish in our lifetime.","In this study, we introduce a heuristic three-stage mechanism for managing the running time of low-budget grid searches, and the sweet-spot grid search (SSGS) and randomized grid search (RGS) strategies for improving model prediction performance, in predicting the 5-year, 10-year, and 15-year risk of breast cancer metastasis.","We develop deep feedforward neural network (DFNN) models and optimize them through grid searches.","We conduct eight cycles of grid searches by applying our three-stage mechanism and SSGS and RGS strategies.","We conduct various SHAP analyses including unique ones that interpret the importance of the DFNN-model hyperparameters.","Our results show that grid search can greatly improve model prediction.","The grid searches we conducted improved the risk prediction of 5-year, 10-year, and 15-year breast cancer metastasis by 18.6%, 16.3%, and 17.3% respectively, over the average performance of all corresponding models we trained.","We not only demonstrate best model performance but also characterize grid searches from various aspects such as their capabilities of discovering decent models and the unit grid search time.","The three-stage mechanism worked effectively.","It made our low-budget grid searches feasible and manageable, and in the meantime helped improve model prediction performance.","Our SHAP analyses identified both clinical risk factors important for the prediction of future risk of breast cancer metastasis, and DFNN-model hyperparameters important to the prediction of performance scores."],"url":"http://arxiv.org/abs/2408.07673v1"}
{"created":"2024-08-14 17:11:41","title":"NeuroEvolution algorithms applied in the designing process of biohybrid actuators","abstract":"Soft robots diverge from traditional rigid robotics, offering unique advantages in adaptability, safety, and human-robot interaction. In some cases, soft robots can be powered by biohybrid actuators and the design process of these systems is far from straightforward. We analyse here two algorithms that may assist the design of these systems, namely, NEAT (NeuroEvolution of Augmented Topologies) and HyperNEAT (Hypercube-based NeuroEvolution of Augmented Topologies). These algorithms exploit the evolution of the structure of actuators encoded through neural networks. To evaluate these algorithms, we compare them with a similar approach using the Age Fitness Pareto Optimization (AFPO) algorithm, with a focus on assessing the maximum displacement achieved by the discovered biohybrid morphologies. Additionally, we investigate the effects of optimization against both the volume of these morphologies and the distance they can cover. To further accelerate the computational process, the proposed methodology is implemented in a client-server setting; so, the most demanding calculations can be executed on specialized and efficient hardware. The results indicate that the HyperNEAT-based approach excels in identifying morphologies with minimal volumes that still achieve satisfactory displacement targets.","sentences":["Soft robots diverge from traditional rigid robotics, offering unique advantages in adaptability, safety, and human-robot interaction.","In some cases, soft robots can be powered by biohybrid actuators and the design process of these systems is far from straightforward.","We analyse here two algorithms that may assist the design of these systems, namely, NEAT (NeuroEvolution of Augmented Topologies) and HyperNEAT (Hypercube-based NeuroEvolution of Augmented Topologies).","These algorithms exploit the evolution of the structure of actuators encoded through neural networks.","To evaluate these algorithms, we compare them with a similar approach using the Age Fitness Pareto Optimization (AFPO) algorithm, with a focus on assessing the maximum displacement achieved by the discovered biohybrid morphologies.","Additionally, we investigate the effects of optimization against both the volume of these morphologies and the distance they can cover.","To further accelerate the computational process, the proposed methodology is implemented in a client-server setting; so, the most demanding calculations can be executed on specialized and efficient hardware.","The results indicate that the HyperNEAT-based approach excels in identifying morphologies with minimal volumes that still achieve satisfactory displacement targets."],"url":"http://arxiv.org/abs/2408.07671v1"}
{"created":"2024-08-14 16:58:48","title":"Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities","abstract":"Model merging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation. As model merging becomes increasingly prevalent across various fields, it is crucial to understand the available model merging techniques comprehensively. However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques. This survey provides a comprehensive overview of model merging methods and theories, their applications in various domains and settings, and future research directions. Specifically, we first propose a new taxonomic approach that exhaustively discusses existing model merging methods. Secondly, we discuss the application of model merging techniques in large language models, multimodal large language models, and 10+ machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc. Finally, we highlight the remaining challenges of model merging and discuss future research directions. A comprehensive list of papers about model merging is available at \\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.","sentences":["Model merging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation.","As model merging becomes increasingly prevalent across various fields, it is crucial to understand the available model merging techniques comprehensively.","However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques.","This survey provides a comprehensive overview of model merging methods and theories, their applications in various domains and settings, and future research directions.","Specifically, we first propose a new taxonomic approach that exhaustively discusses existing model merging methods.","Secondly, we discuss the application of model merging techniques in large language models, multimodal large language models, and 10+ machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc.","Finally, we highlight the remaining challenges of model merging and discuss future research directions.","A comprehensive list of papers about model merging is available at \\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}."],"url":"http://arxiv.org/abs/2408.07666v1"}
{"created":"2024-08-14 16:55:06","title":"Spoken Stereoset: On Evaluating Social Bias Toward Speaker in Speech Large Language Models","abstract":"Warning: This paper may contain texts with uncomfortable content.   Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech. However, these models often exhibit biases due to the nature of their training data. Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases. This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs. By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases. Our experiments reveal significant insights into their performance and bias levels. The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies.","sentences":["Warning: This paper may contain texts with uncomfortable content.   ","Large Language Models (LLMs) have achieved remarkable performance in various tasks, including those involving multimodal data like speech.","However, these models often exhibit biases due to the nature of their training data.","Recently, more Speech Large Language Models (SLLMs) have emerged, underscoring the urgent need to address these biases.","This study introduces Spoken Stereoset, a dataset specifically designed to evaluate social biases in SLLMs.","By examining how different models respond to speech from diverse demographic groups, we aim to identify these biases.","Our experiments reveal significant insights into their performance and bias levels.","The findings indicate that while most models show minimal bias, some still exhibit slightly stereotypical or anti-stereotypical tendencies."],"url":"http://arxiv.org/abs/2408.07665v1"}
{"created":"2024-08-14 16:51:21","title":"Alignment-Enhanced Decoding:Defending via Token-Level Adaptive Refining of Probability Distributions","abstract":"Large language models are susceptible to jailbreak attacks, which can result in the generation of harmful content. While prior defenses mitigate these risks by perturbing or inspecting inputs, they ignore competing objectives, the underlying cause of alignment failures. In this paper, we propose Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive decoding to address the root causes of jailbreak issues. We first define the Competitive Index to quantify alignment failures and utilize feedback from self-evaluation to compute post-alignment logits. Then, AED adaptively combines AED and post-alignment logits with the original logits to obtain harmless and helpful distributions. Consequently, our method enhances safety alignment while maintaining helpfulness. We conduct experiments across five models and four common jailbreaks, with the results validating the effectiveness of our approach. Code is available at https://github.com/GIGABaozi/AED.git.","sentences":["Large language models are susceptible to jailbreak attacks, which can result in the generation of harmful content.","While prior defenses mitigate these risks by perturbing or inspecting inputs, they ignore competing objectives, the underlying cause of alignment failures.","In this paper, we propose Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive decoding to address the root causes of jailbreak issues.","We first define the Competitive Index to quantify alignment failures and utilize feedback from self-evaluation to compute post-alignment logits.","Then, AED adaptively combines AED and post-alignment logits with the original logits to obtain harmless and helpful distributions.","Consequently, our method enhances safety alignment while maintaining helpfulness.","We conduct experiments across five models and four common jailbreaks, with the results validating the effectiveness of our approach.","Code is available at https://github.com/GIGABaozi/AED.git."],"url":"http://arxiv.org/abs/2408.07663v1"}
{"created":"2024-08-14 16:49:25","title":"Interpretable Graph Neural Networks for Heterogeneous Tabular Data","abstract":"Many machine learning algorithms for tabular data produce black-box models, which prevent users from understanding the rationale behind the model predictions. In their unconstrained form, graph neural networks fall into this category, and they have further limited abilities to handle heterogeneous data. To overcome these limitations, an approach is proposed, called IGNH (Interpretable Graph Neural Network for Heterogeneous tabular data), which handles both categorical and numerical features, while constraining the learning process to generate exact feature attributions together with the predictions. A large-scale empirical investigation is presented, showing that the feature attributions provided by IGNH align with Shapley values that are computed post hoc. Furthermore, the results show that IGNH outperforms two powerful machine learning algorithms for tabular data, Random Forests and TabNet, while reaching a similar level of performance as XGBoost.","sentences":["Many machine learning algorithms for tabular data produce black-box models, which prevent users from understanding the rationale behind the model predictions.","In their unconstrained form, graph neural networks fall into this category, and they have further limited abilities to handle heterogeneous data.","To overcome these limitations, an approach is proposed, called IGNH (Interpretable Graph Neural Network for Heterogeneous tabular data), which handles both categorical and numerical features, while constraining the learning process to generate exact feature attributions together with the predictions.","A large-scale empirical investigation is presented, showing that the feature attributions provided by IGNH align with Shapley values that are computed post hoc.","Furthermore, the results show that IGNH outperforms two powerful machine learning algorithms for tabular data, Random Forests and TabNet, while reaching a similar level of performance as XGBoost."],"url":"http://arxiv.org/abs/2408.07661v1"}
{"created":"2024-08-14 16:29:57","title":"Development of simulation model for Single Carrier Transceiver for Nanosatellite","abstract":"CubeSat is a nanosatellite concept emerged from a paper published by Stanford University and with their low cost nature and extreme feasibility , more started researching on nano satellites. New technology emerged , paving path to many academics and small vendors to create their own CubeSat models .   This nanosatellite requires a transceiver to maintain its communication between it's systems and the ground station, which helps it navigate and collects data gained from its programmed functions. This transceiver system consists mainly of a transmitter and a receiver. The transmitter manages sending data from the satellite to ground station while the receiver captures the data and instruction sent from the ground station to the satellite. These systems were built using separate digital communication devices in the beginning, with many critical limitations with respect to the space and scalability of the modules to be attached and the programmability of hardware materials and the concept of system-on -board emerged. Meanwhile, As the size of electronic devices minimized with the research conducted, FPGA (Filed Programmable Logic Array) was introduced as an architecture to be used in various applications and research needs. The reason FPGA was mention was for the fact that it provides flexibility in the designing of transceiver ed with design and prototypes implementation at a low cost competitional electronic ware and the system -on chip Concept was introduced . This research describes the development a system-on-chip transceiver model for nanosatellites which contains a single carrier . Keywords-Single Carrier, transceiver, system C, FPGA","sentences":["CubeSat is a nanosatellite concept emerged from a paper published by Stanford University and with their low cost nature and extreme feasibility , more started researching on nano satellites.","New technology emerged , paving path to many academics and small vendors to create their own CubeSat models .   ","This nanosatellite requires a transceiver to maintain its communication between it's systems and the ground station, which helps it navigate and collects data gained from its programmed functions.","This transceiver system consists mainly of a transmitter and a receiver.","The transmitter manages sending data from the satellite to ground station while the receiver captures the data and instruction sent from the ground station to the satellite.","These systems were built using separate digital communication devices in the beginning, with many critical limitations with respect to the space and scalability of the modules to be attached and the programmability of hardware materials and the concept of system-on -board emerged.","Meanwhile, As the size of electronic devices minimized with the research conducted, FPGA (Filed Programmable Logic Array) was introduced as an architecture to be used in various applications and research needs.","The reason FPGA was mention was for the fact that it provides flexibility in the designing of transceiver ed with design and prototypes implementation at a low cost competitional electronic ware and the system -on chip Concept was introduced .","This research describes the development a system-on-chip transceiver model for nanosatellites which contains a single carrier .","Keywords-Single Carrier, transceiver, system C, FPGA"],"url":"http://arxiv.org/abs/2408.07655v1"}
{"created":"2024-08-14 16:29:07","title":"Graph Triple Attention Network: A Decoupled Perspective","abstract":"Graph Transformers (GTs) have recently achieved significant success in the graph domain by effectively capturing both long-range dependencies and graph inductive biases. However, these methods face two primary challenges: (1) multi-view chaos, which results from coupling multi-view information (positional, structural, attribute), thereby impeding flexible usage and the interpretability of the propagation process. (2) local-global chaos, which arises from coupling local message passing with global attention, leading to issues of overfitting and over-globalizing. To address these challenges, we propose a high-level decoupled perspective of GTs, breaking them down into three components and two interaction levels: positional attention, structural attention, and attribute attention, alongside local and global interaction. Based on this decoupled perspective, we design a decoupled graph triple attention network named DeGTA, which separately computes multi-view attentions and adaptively integrates multi-view local and global information. This approach offers three key advantages: enhanced interpretability, flexible design, and adaptive integration of local and global information. Through extensive experiments, DeGTA achieves state-of-the-art performance across various datasets and tasks, including node classification and graph classification. Comprehensive ablation studies demonstrate that decoupling is essential for improving performance and enhancing interpretability. Our code is available at: https://github.com/wangxiaotang0906/DeGTA","sentences":["Graph Transformers (GTs) have recently achieved significant success in the graph domain by effectively capturing both long-range dependencies and graph inductive biases.","However, these methods face two primary challenges: (1) multi-view chaos, which results from coupling multi-view information (positional, structural, attribute), thereby impeding flexible usage and the interpretability of the propagation process.","(2) local-global chaos, which arises from coupling local message passing with global attention, leading to issues of overfitting and over-globalizing.","To address these challenges, we propose a high-level decoupled perspective of GTs, breaking them down into three components and two interaction levels: positional attention, structural attention, and attribute attention, alongside local and global interaction.","Based on this decoupled perspective, we design a decoupled graph triple attention network named DeGTA, which separately computes multi-view attentions and adaptively integrates multi-view local and global information.","This approach offers three key advantages: enhanced interpretability, flexible design, and adaptive integration of local and global information.","Through extensive experiments, DeGTA achieves state-of-the-art performance across various datasets and tasks, including node classification and graph classification.","Comprehensive ablation studies demonstrate that decoupling is essential for improving performance and enhancing interpretability.","Our code is available at: https://github.com/wangxiaotang0906/DeGTA"],"url":"http://arxiv.org/abs/2408.07654v1"}
{"created":"2024-08-14 16:27:16","title":"The Semantics of Metapropramming in Prolog","abstract":"This paper describes a semantics for pure Prolog programs with negation that provides meaning to metaprograms. Metaprograms are programs that construct and use data structures as programs. In Prolog a primary mataprogramming construct is the use of a variable as a literal in the body of a clause. The traditional Prolog 3-line metainterpreter is another example of a metaprogram. The account given here also supplies a meaning for clauses that have a variable as head, even though most Prolog systems do not support such clauses. This semantics naturally includes such programs, giving them their intuitive meaning.   Ideas from M. Denecker and his colleagues form the basis of this approach. The key idea is to notice that if we give meanings to all propositional programs and treat Prolog rules with variables as the set of their ground instances, then we can give meanings to all programs. We must treat Prolog rules (which may be metarules) as templates for generating ground propositional rules, and not as first-order formulas, which they may not be. We use parameterized inductive definitions to give propositional models to Prolog programs, in which the propositions are expressions. Then the set of expressions of a propositional model determine a first-order Herbrand Model, providing a first-order logical semantics for all (pure) Prolog programs, including metaprograms.   We give examples to show the applicability of this theory. We also demonstrate how this theory makes proofs of some important properties of metaprograms very straightforward.","sentences":["This paper describes a semantics for pure Prolog programs with negation that provides meaning to metaprograms.","Metaprograms are programs that construct and use data structures as programs.","In Prolog a primary mataprogramming construct is the use of a variable as a literal in the body of a clause.","The traditional Prolog 3-line metainterpreter is another example of a metaprogram.","The account given here also supplies a meaning for clauses that have a variable as head, even though most Prolog systems do not support such clauses.","This semantics naturally includes such programs, giving them their intuitive meaning.   Ideas from M. Denecker and his colleagues form the basis of this approach.","The key idea is to notice that if we give meanings to all propositional programs and treat Prolog rules with variables as the set of their ground instances, then we can give meanings to all programs.","We must treat Prolog rules (which may be metarules) as templates for generating ground propositional rules, and not as first-order formulas, which they may not be.","We use parameterized inductive definitions to give propositional models to Prolog programs, in which the propositions are expressions.","Then the set of expressions of a propositional model determine a first-order Herbrand Model, providing a first-order logical semantics for all (pure) Prolog programs, including metaprograms.   ","We give examples to show the applicability of this theory.","We also demonstrate how this theory makes proofs of some important properties of metaprograms very straightforward."],"url":"http://arxiv.org/abs/2408.07652v1"}
{"created":"2024-08-14 16:21:28","title":"Exact Trajectory Similarity Search With N-tree: An Efficient Metric Index for kNN and Range Queries","abstract":"Similarity search is the problem of finding in a collection of objects those that are similar to a given query object. It is a fundamental problem in modern applications and the objects considered may be as diverse as locations in space, text documents, images, twitter messages, or trajectories of moving objects.   In this paper we are motivated by the latter application. Trajectories are recorded movements of mobile objects such as vehicles, animals, public transportation, or parts of the human body. We propose a novel distance function called DistanceAvg to capture the similarity of such movements. To be practical, it is necessary to provide indexing for this distance measure.   Fortunately we do not need to start from scratch. A generic and unifying approach is metric space, which organizes the set of objects solely by a distance (similarity) function with certain natural properties. Our function DistanceAvg is a metric.   Although metric indexes have been studied for decades and many such structures are available, they do not offer the best performance with trajectories. In this paper we propose a new design, which outperforms the best existing indexes for kNN queries and is equally good for range queries. It is especially suitable for expensive distance functions as they occur in trajectory similarity search. In many applications, kNN queries are more practical than range queries as it may be difficult to determine an appropriate search radius. Our index provides exact result sets for the given distance function.","sentences":["Similarity search is the problem of finding in a collection of objects those that are similar to a given query object.","It is a fundamental problem in modern applications and the objects considered may be as diverse as locations in space, text documents, images, twitter messages, or trajectories of moving objects.   ","In this paper we are motivated by the latter application.","Trajectories are recorded movements of mobile objects such as vehicles, animals, public transportation, or parts of the human body.","We propose a novel distance function called DistanceAvg to capture the similarity of such movements.","To be practical, it is necessary to provide indexing for this distance measure.   ","Fortunately we do not need to start from scratch.","A generic and unifying approach is metric space, which organizes the set of objects solely by a distance (similarity) function with certain natural properties.","Our function DistanceAvg is a metric.   ","Although metric indexes have been studied for decades and many such structures are available, they do not offer the best performance with trajectories.","In this paper we propose a new design, which outperforms the best existing indexes for kNN queries and is equally good for range queries.","It is especially suitable for expensive distance functions as they occur in trajectory similarity search.","In many applications, kNN queries are more practical than range queries as it may be difficult to determine an appropriate search radius.","Our index provides exact result sets for the given distance function."],"url":"http://arxiv.org/abs/2408.07650v1"}
{"created":"2024-08-14 16:19:18","title":"See It All: Contextualized Late Aggregation for 3D Dense Captioning","abstract":"3D dense captioning is a task to localize objects in a 3D scene and generate descriptive sentences for each object. Recent approaches in 3D dense captioning have adopted transformer encoder-decoder frameworks from object detection to build an end-to-end pipeline without hand-crafted components. However, these approaches struggle with contradicting objectives where a single query attention has to simultaneously view both the tightly localized object regions and contextual environment. To overcome this challenge, we introduce SIA (See-It-All), a transformer pipeline that engages in 3D dense captioning with a novel paradigm called late aggregation. SIA simultaneously decodes two sets of queries-context query and instance query. The instance query focuses on localization and object attribute descriptions, while the context query versatilely captures the region-of-interest of relationships between multiple objects or with the global scene, then aggregated afterwards (i.e., late aggregation) via simple distance-based measures. To further enhance the quality of contextualized caption generation, we design a novel aggregator to generate a fully informed caption based on the surrounding context, the global environment, and object instances. Extensive experiments on two of the most widely-used 3D dense captioning datasets demonstrate that our proposed method achieves a significant improvement over prior methods.","sentences":["3D dense captioning is a task to localize objects in a 3D scene and generate descriptive sentences for each object.","Recent approaches in 3D dense captioning have adopted transformer encoder-decoder frameworks from object detection to build an end-to-end pipeline without hand-crafted components.","However, these approaches struggle with contradicting objectives where a single query attention has to simultaneously view both the tightly localized object regions and contextual environment.","To overcome this challenge, we introduce SIA (See-It-All), a transformer pipeline that engages in 3D dense captioning with a novel paradigm called late aggregation.","SIA simultaneously decodes two sets of queries-context query and instance query.","The instance query focuses on localization and object attribute descriptions, while the context query versatilely captures the region-of-interest of relationships between multiple objects or with the global scene, then aggregated afterwards (i.e., late aggregation) via simple distance-based measures.","To further enhance the quality of contextualized caption generation, we design a novel aggregator to generate a fully informed caption based on the surrounding context, the global environment, and object instances.","Extensive experiments on two of the most widely-used 3D dense captioning datasets demonstrate that our proposed method achieves a significant improvement over prior methods."],"url":"http://arxiv.org/abs/2408.07648v1"}
{"created":"2024-08-14 16:18:51","title":"Adaptive Behavioral AI: Reinforcement Learning to Enhance Pharmacy Services","abstract":"Pharmacies are critical in healthcare systems, particularly in low- and middle-income countries. Procuring pharmacists with the right behavioral interventions or nudges can enhance their skills, public health awareness, and pharmacy inventory management, ensuring access to essential medicines that ultimately benefit their patients. We introduce a reinforcement learning operational system to deliver personalized behavioral interventions through mobile health applications. We illustrate its potential by discussing a series of initial experiments run with SwipeRx, an all-in-one app for pharmacists, including B2B e-commerce, in Indonesia. The proposed method has broader applications extending beyond pharmacy operations to optimize healthcare delivery.","sentences":["Pharmacies are critical in healthcare systems, particularly in low- and middle-income countries.","Procuring pharmacists with the right behavioral interventions or nudges can enhance their skills, public health awareness, and pharmacy inventory management, ensuring access to essential medicines that ultimately benefit their patients.","We introduce a reinforcement learning operational system to deliver personalized behavioral interventions through mobile health applications.","We illustrate its potential by discussing a series of initial experiments run with SwipeRx, an all-in-one app for pharmacists, including B2B e-commerce, in Indonesia.","The proposed method has broader applications extending beyond pharmacy operations to optimize healthcare delivery."],"url":"http://arxiv.org/abs/2408.07647v1"}
{"created":"2024-08-14 16:16:51","title":"SigmaRL: A Sample-Efficient and Generalizable Multi-Agent Reinforcement Learning Framework for Motion Planning","abstract":"This paper introduces an open-source, decentralized framework named SigmaRL, designed to enhance both sample efficiency and generalization of multi-agent Reinforcement Learning (RL) for motion planning of connected and automated vehicles. Most RL agents exhibit a limited capacity to generalize, often focusing narrowly on specific scenarios, and are usually evaluated in similar or even the same scenarios seen during training. Various methods have been proposed to address these challenges, including experience replay and regularization. However, how observation design in RL affects sample efficiency and generalization remains an under-explored area. We address this gap by proposing five strategies to design information-dense observations, focusing on general features that are applicable to most traffic scenarios. We train our RL agents using these strategies on an intersection and evaluate their generalization through numerical experiments across completely unseen traffic scenarios, including a new intersection, an on-ramp, and a roundabout. Incorporating these information-dense observations reduces training times to under one hour on a single CPU, and the evaluation results reveal that our RL agents can effectively zero-shot generalize. Code: github.com/cas-lab-munich/SigmaRL","sentences":["This paper introduces an open-source, decentralized framework named SigmaRL, designed to enhance both sample efficiency and generalization of multi-agent Reinforcement Learning (RL) for motion planning of connected and automated vehicles.","Most RL agents exhibit a limited capacity to generalize, often focusing narrowly on specific scenarios, and are usually evaluated in similar or even the same scenarios seen during training.","Various methods have been proposed to address these challenges, including experience replay and regularization.","However, how observation design in RL affects sample efficiency and generalization remains an under-explored area.","We address this gap by proposing five strategies to design information-dense observations, focusing on general features that are applicable to most traffic scenarios.","We train our RL agents using these strategies on an intersection and evaluate their generalization through numerical experiments across completely unseen traffic scenarios, including a new intersection, an on-ramp, and a roundabout.","Incorporating these information-dense observations reduces training times to under one hour on a single CPU, and the evaluation results reveal that our RL agents can effectively zero-shot generalize.","Code: github.com/cas-lab-munich/SigmaRL"],"url":"http://arxiv.org/abs/2408.07644v1"}
{"created":"2024-08-14 16:13:03","title":"Boosting Unconstrained Face Recognition with Targeted Style Adversary","abstract":"While deep face recognition models have demonstrated remarkable performance, they often struggle on the inputs from domains beyond their training data. Recent attempts aim to expand the training set by relying on computationally expensive and inherently challenging image-space augmentation of image generation modules. In an orthogonal direction, we present a simple yet effective method to expand the training data by interpolating between instance-level feature statistics across labeled and unlabeled sets. Our method, dubbed Targeted Style Adversary (TSA), is motivated by two observations: (i) the input domain is reflected in feature statistics, and (ii) face recognition model performance is influenced by style information. Shifting towards an unlabeled style implicitly synthesizes challenging training instances. We devise a recognizability metric to constraint our framework to preserve the inherent identity-related information of labeled instances. The efficacy of our method is demonstrated through evaluations on unconstrained benchmarks, outperforming or being on par with its competitors while offering nearly a 70\\% improvement in training speed and 40\\% less memory consumption.","sentences":["While deep face recognition models have demonstrated remarkable performance, they often struggle on the inputs from domains beyond their training data.","Recent attempts aim to expand the training set by relying on computationally expensive and inherently challenging image-space augmentation of image generation modules.","In an orthogonal direction, we present a simple yet effective method to expand the training data by interpolating between instance-level feature statistics across labeled and unlabeled sets.","Our method, dubbed Targeted Style Adversary (TSA), is motivated by two observations: (i) the input domain is reflected in feature statistics, and (ii) face recognition model performance is influenced by style information.","Shifting towards an unlabeled style implicitly synthesizes challenging training instances.","We devise a recognizability metric to constraint our framework to preserve the inherent identity-related information of labeled instances.","The efficacy of our method is demonstrated through evaluations on unconstrained benchmarks, outperforming or being on par with its competitors while offering nearly a 70\\% improvement in training speed and 40\\% less memory consumption."],"url":"http://arxiv.org/abs/2408.07642v1"}
{"created":"2024-08-14 15:56:27","title":"Towards Fair and Rigorous Evaluations: Hyperparameter Optimization for Top-N Recommendation Task with Implicit Feedback","abstract":"The widespread use of the internet has led to an overwhelming amount of data, which has resulted in the problem of information overload. Recommender systems have emerged as a solution to this problem by providing personalized recommendations to users based on their preferences and historical data. However, as recommendation models become increasingly complex, finding the best hyperparameter combination for different models has become a challenge. The high-dimensional hyperparameter search space poses numerous challenges for researchers, and failure to disclose hyperparameter settings may impede the reproducibility of research results. In this paper, we investigate the Top-N implicit recommendation problem and focus on optimizing the benchmark recommendation algorithm commonly used in comparative experiments using hyperparameter optimization algorithms. We propose a research methodology that follows the principles of a fair comparison, employing seven types of hyperparameter search algorithms to fine-tune six common recommendation algorithms on three datasets. We have identified the most suitable hyperparameter search algorithms for various recommendation algorithms on different types of datasets as a reference for later study. This study contributes to algorithmic research in recommender systems based on hyperparameter optimization, providing a fair basis for comparison.","sentences":["The widespread use of the internet has led to an overwhelming amount of data, which has resulted in the problem of information overload.","Recommender systems have emerged as a solution to this problem by providing personalized recommendations to users based on their preferences and historical data.","However, as recommendation models become increasingly complex, finding the best hyperparameter combination for different models has become a challenge.","The high-dimensional hyperparameter search space poses numerous challenges for researchers, and failure to disclose hyperparameter settings may impede the reproducibility of research results.","In this paper, we investigate the Top-N implicit recommendation problem and focus on optimizing the benchmark recommendation algorithm commonly used in comparative experiments using hyperparameter optimization algorithms.","We propose a research methodology that follows the principles of a fair comparison, employing seven types of hyperparameter search algorithms to fine-tune six common recommendation algorithms on three datasets.","We have identified the most suitable hyperparameter search algorithms for various recommendation algorithms on different types of datasets as a reference for later study.","This study contributes to algorithmic research in recommender systems based on hyperparameter optimization, providing a fair basis for comparison."],"url":"http://arxiv.org/abs/2408.07630v1"}
{"created":"2024-08-14 15:55:31","title":"Optimizing HIV Patient Engagement with Reinforcement Learning in Resource-Limited Settings","abstract":"By providing evidence-based clinical decision support, digital tools and electronic health records can revolutionize patient management, especially in resource-poor settings where fewer health workers are available and often need more training. When these tools are integrated with AI, they can offer personalized support and adaptive interventions, effectively connecting community health workers (CHWs) and healthcare facilities. The CHARM (Community Health Access & Resource Management) app is an AI-native mobile app for CHWs. Developed through a joint partnership of Causal Foundry (CF) and mothers2mothers (m2m), CHARM empowers CHWs, mainly local women, by streamlining case management, enhancing learning, and improving communication. This paper details CHARM's development, integration, and upcoming reinforcement learning-based adaptive interventions, all aimed at enhancing health worker engagement, efficiency, and patient outcomes, thereby enhancing CHWs' capabilities and community health.","sentences":["By providing evidence-based clinical decision support, digital tools and electronic health records can revolutionize patient management, especially in resource-poor settings where fewer health workers are available and often need more training.","When these tools are integrated with AI, they can offer personalized support and adaptive interventions, effectively connecting community health workers (CHWs) and healthcare facilities.","The CHARM (Community Health Access & Resource Management) app is an AI-native mobile app for CHWs.","Developed through a joint partnership of Causal Foundry (CF) and mothers2mothers (m2m), CHARM empowers CHWs, mainly local women, by streamlining case management, enhancing learning, and improving communication.","This paper details CHARM's development, integration, and upcoming reinforcement learning-based adaptive interventions, all aimed at enhancing health worker engagement, efficiency, and patient outcomes, thereby enhancing CHWs' capabilities and community health."],"url":"http://arxiv.org/abs/2408.07629v1"}
{"created":"2024-08-14 15:54:00","title":"Embodied Biocomputing Sequential Circuits with Data Processing and Storage for Neurons-on-a-chip","abstract":"With conventional silicon-based computing approaching its physical and efficiency limits, biocomputing emerges as a promising alternative. This approach utilises biomaterials such as DNA and neurons as an interesting alternative to data processing and storage. This study explores the potential of neuronal biocomputing to rival silicon-based systems. We explore neuronal logic gates and sequential circuits that mimic conventional computer architectures. Through mathematical modelling, optimisation, and computer simulation, we demonstrate the operational capabilities of neuronal sequential circuits. These circuits include a neuronal NAND gate, SR Latch flip-flop, and D flip-flop memory units. Our approach involves manipulating neuron communication, synaptic conductance, spike buffers, neuron types, and specific neuronal network topology designs. The experiments demonstrate the practicality of encoding binary information using patterns of neuronal activity and overcoming synchronization difficulties with neuronal buffers and inhibition strategies. Our results confirm the effectiveness and scalability of neuronal logic circuits, showing that they maintain a stable metabolic burden even in complex data storage configurations. Our study not only demonstrates the concept of embodied biocomputing by manipulating neuronal properties for digital signal processing but also establishes the foundation for cutting-edge biocomputing technologies. Our designs open up possibilities for using neurons as energy-efficient computing solutions. These solutions have the potential to become an alternate to silicon-based systems by providing a carbon-neutral, biologically feasible alternative.","sentences":["With conventional silicon-based computing approaching its physical and efficiency limits, biocomputing emerges as a promising alternative.","This approach utilises biomaterials such as DNA and neurons as an interesting alternative to data processing and storage.","This study explores the potential of neuronal biocomputing to rival silicon-based systems.","We explore neuronal logic gates and sequential circuits that mimic conventional computer architectures.","Through mathematical modelling, optimisation, and computer simulation, we demonstrate the operational capabilities of neuronal sequential circuits.","These circuits include a neuronal NAND gate, SR Latch flip-flop, and D flip-flop memory units.","Our approach involves manipulating neuron communication, synaptic conductance, spike buffers, neuron types, and specific neuronal network topology designs.","The experiments demonstrate the practicality of encoding binary information using patterns of neuronal activity and overcoming synchronization difficulties with neuronal buffers and inhibition strategies.","Our results confirm the effectiveness and scalability of neuronal logic circuits, showing that they maintain a stable metabolic burden even in complex data storage configurations.","Our study not only demonstrates the concept of embodied biocomputing by manipulating neuronal properties for digital signal processing but also establishes the foundation for cutting-edge biocomputing technologies.","Our designs open up possibilities for using neurons as energy-efficient computing solutions.","These solutions have the potential to become an alternate to silicon-based systems by providing a carbon-neutral, biologically feasible alternative."],"url":"http://arxiv.org/abs/2408.07628v1"}
{"created":"2024-08-14 15:44:56","title":"Battery GraphNets : Relational Learning for Lithium-ion Batteries(LiBs) Life Estimation","abstract":"Battery life estimation is critical for optimizing battery performance and guaranteeing minimal degradation for better efficiency and reliability of battery-powered systems. The existing methods to predict the Remaining Useful Life(RUL) of Lithium-ion Batteries (LiBs) neglect the relational dependencies of the battery parameters to model the nonlinear degradation trajectories. We present the Battery GraphNets framework that jointly learns to incorporate a discrete dependency graph structure between battery parameters to capture the complex interactions and the graph-learning algorithm to model the intrinsic battery degradation for RUL prognosis. The proposed method outperforms several popular methods by a significant margin on publicly available battery datasets and achieves SOTA performance. We report the ablation studies to support the efficacy of our approach.","sentences":["Battery life estimation is critical for optimizing battery performance and guaranteeing minimal degradation for better efficiency and reliability of battery-powered systems.","The existing methods to predict the Remaining Useful Life(RUL) of Lithium-ion Batteries (LiBs) neglect the relational dependencies of the battery parameters to model the nonlinear degradation trajectories.","We present the Battery GraphNets framework that jointly learns to incorporate a discrete dependency graph structure between battery parameters to capture the complex interactions and the graph-learning algorithm to model the intrinsic battery degradation for RUL prognosis.","The proposed method outperforms several popular methods by a significant margin on publicly available battery datasets and achieves SOTA performance.","We report the ablation studies to support the efficacy of our approach."],"url":"http://arxiv.org/abs/2408.07624v1"}
{"created":"2024-08-14 15:44:51","title":"Latent Anomaly Detection Through Density Matrices","abstract":"This paper introduces a novel anomaly detection framework that combines the robust statistical principles of density-estimation-based anomaly detection methods with the representation-learning capabilities of deep learning models. The method originated from this framework is presented in two different versions: a shallow approach employing a density-estimation model based on adaptive Fourier features and density matrices, and a deep approach that integrates an autoencoder to learn a low-dimensional representation of the data. By estimating the density of new samples, both methods are able to find normality scores. The methods can be seamlessly integrated into an end-to-end architecture and optimized using gradient-based optimization techniques. To evaluate their performance, extensive experiments were conducted on various benchmark datasets. The results demonstrate that both versions of the method can achieve comparable or superior performance when compared to other state-of-the-art methods. Notably, the shallow approach performs better on datasets with fewer dimensions, while the autoencoder-based approach shows improved performance on datasets with higher dimensions.","sentences":["This paper introduces a novel anomaly detection framework that combines the robust statistical principles of density-estimation-based anomaly detection methods with the representation-learning capabilities of deep learning models.","The method originated from this framework is presented in two different versions: a shallow approach employing a density-estimation model based on adaptive Fourier features and density matrices, and a deep approach that integrates an autoencoder to learn a low-dimensional representation of the data.","By estimating the density of new samples, both methods are able to find normality scores.","The methods can be seamlessly integrated into an end-to-end architecture and optimized using gradient-based optimization techniques.","To evaluate their performance, extensive experiments were conducted on various benchmark datasets.","The results demonstrate that both versions of the method can achieve comparable or superior performance when compared to other state-of-the-art methods.","Notably, the shallow approach performs better on datasets with fewer dimensions, while the autoencoder-based approach shows improved performance on datasets with higher dimensions."],"url":"http://arxiv.org/abs/2408.07623v1"}
{"created":"2024-08-14 15:44:00","title":"Information-Set Decoding for Convolutional Codes","abstract":"In this paper, we present a framework for generic decoding of convolutional codes, which allows us to do cryptanalysis of code-based systems that use convolutional codes. We then apply this framework to information set decoding, study success probabilities and give tools to choose variables. Finally, we use this to attack two cryptosystems based on convolutional codes. In the first, our code recovered about 74% of errors in less than 10 hours each, and in the second case, we give experimental evidence that 80% of the errors can be recovered in times corresponding to about 60 bits of operational security, with some instances being significantly lower.","sentences":["In this paper, we present a framework for generic decoding of convolutional codes, which allows us to do cryptanalysis of code-based systems that use convolutional codes.","We then apply this framework to information set decoding, study success probabilities and give tools to choose variables.","Finally, we use this to attack two cryptosystems based on convolutional codes.","In the first, our code recovered about 74% of errors in less than 10 hours each, and in the second case, we give experimental evidence that 80% of the errors can be recovered in times corresponding to about 60 bits of operational security, with some instances being significantly lower."],"url":"http://arxiv.org/abs/2408.07621v1"}
{"created":"2024-08-14 15:31:15","title":"Prophet Inequalities: Competing with the Top $\\ell$ Items is Easy","abstract":"We explore a novel variant of the classical prophet inequality problem, where the values of a sequence of items are drawn i.i.d. from some distribution, and an online decision maker must select one item irrevocably. We establish that the competitive ratio between the expected optimal performance of the online decision maker compared to that of a prophet, who uses the average of the top $\\ell$ items, must be greater than $\\ell/c_{\\ell}$, with $c_{\\ell}$ the solution to an integral equation. We prove that this lower bound is larger than $1-1/(\\exp(\\ell)-1)$. This implies that the bound converges exponentially fast to $1$ as $\\ell$ grows. In particular, the bound for $\\ell=2$ is $2/c_{2} \\approx 0.966$ which is much closer to $1$ than the classical bound of $0.745$ for $\\ell=1$. Additionally, the proposed algorithm can be extended to a more general scenario, where the decision maker is permitted to select $k$ items. This subsumes the $k$ multi-unit i.i.d. prophet problem and provides the current best asymptotic guarantees, as well as enables broader understanding in the more general framework. Finally, we prove a nearly tight competitive ratio when only static threshold policies are allowed.","sentences":["We explore a novel variant of the classical prophet inequality problem, where the values of a sequence of items are drawn i.i.d.","from some distribution, and an online decision maker must select one item irrevocably.","We establish that the competitive ratio between the expected optimal performance of the online decision maker compared to that of a prophet, who uses the average of the top $\\ell$ items, must be greater than $\\ell/c_{\\ell}$, with $c_{\\ell}$ the solution to an integral equation.","We prove that this lower bound is larger than $1-1/(\\exp(\\ell)-1)$.","This implies that the bound converges exponentially fast to $1$ as $\\ell$ grows.","In particular, the bound for $\\ell=2$ is $2/c_{2} \\approx 0.966$ which is much closer to $1$ than the classical bound of $0.745$ for $\\ell=1$. Additionally, the proposed algorithm can be extended to a more general scenario, where the decision maker is permitted to select $k$ items.","This subsumes the $k$ multi-unit i.i.d. prophet problem and provides the current best asymptotic guarantees, as well as enables broader understanding in the more general framework.","Finally, we prove a nearly tight competitive ratio when only static threshold policies are allowed."],"url":"http://arxiv.org/abs/2408.07616v1"}
{"created":"2024-08-14 15:28:28","title":"Practical Considerations for Differential Privacy","abstract":"Differential privacy is the gold standard for statistical data release. Used by governments, companies, and academics, its mathematically rigorous guarantees and worst-case assumptions on the strength and knowledge of attackers make it a robust and compelling framework for reasoning about privacy. However, even with landmark successes, differential privacy has not achieved widespread adoption in everyday data use and data protection. In this work we examine some of the practical obstacles that stand in the way.","sentences":["Differential privacy is the gold standard for statistical data release.","Used by governments, companies, and academics, its mathematically rigorous guarantees and worst-case assumptions on the strength and knowledge of attackers make it a robust and compelling framework for reasoning about privacy.","However, even with landmark successes, differential privacy has not achieved widespread adoption in everyday data use and data protection.","In this work we examine some of the practical obstacles that stand in the way."],"url":"http://arxiv.org/abs/2408.07614v1"}
{"created":"2024-08-14 15:26:10","title":"Rethinking the Key Factors for the Generalization of Remote Sensing Stereo Matching Networks","abstract":"Stereo matching, a critical step of 3D reconstruction, has fully shifted towards deep learning due to its strong feature representation of remote sensing images. However, ground truth for stereo matching task relies on expensive airborne LiDAR data, thus making it difficult to obtain enough samples for supervised learning. To improve the generalization ability of stereo matching networks on cross-domain data from different sensors and scenarios, in this paper, we dedicate to study key training factors from three perspectives. (1) For the selection of training dataset, it is important to select data with similar regional target distribution as the test set instead of utilizing data from the same sensor. (2) For model structure, cascaded structure that flexibly adapts to different sizes of features is preferred. (3) For training manner, unsupervised methods generalize better than supervised methods, and we design an unsupervised early-stop strategy to help retain the best model with pre-trained weights as the basis. Extensive experiments are conducted to support the previous findings, on the basis of which we present an unsupervised stereo matching network with good generalization performance. We release the source code and the datasets at https://github.com/Elenairene/RKF_RSSM to reproduce the results and encourage future work.","sentences":["Stereo matching, a critical step of 3D reconstruction, has fully shifted towards deep learning due to its strong feature representation of remote sensing images.","However, ground truth for stereo matching task relies on expensive airborne LiDAR data, thus making it difficult to obtain enough samples for supervised learning.","To improve the generalization ability of stereo matching networks on cross-domain data from different sensors and scenarios, in this paper, we dedicate to study key training factors from three perspectives.","(1) For the selection of training dataset, it is important to select data with similar regional target distribution as the test set instead of utilizing data from the same sensor.","(2) For model structure, cascaded structure that flexibly adapts to different sizes of features is preferred.","(3) For training manner, unsupervised methods generalize better than supervised methods, and we design an unsupervised early-stop strategy to help retain the best model with pre-trained weights as the basis.","Extensive experiments are conducted to support the previous findings, on the basis of which we present an unsupervised stereo matching network with good generalization performance.","We release the source code and the datasets at https://github.com/Elenairene/RKF_RSSM to reproduce the results and encourage future work."],"url":"http://arxiv.org/abs/2408.07613v1"}
{"created":"2024-08-14 15:19:16","title":"WeKnow-RAG: An Adaptive Approach for Retrieval-Augmented Generation Integrating Web Search and Knowledge Graphs","abstract":"Large Language Models (LLMs) have greatly contributed to the development of adaptive intelligent agents and are positioned as an important way to achieve Artificial General Intelligence (AGI). However, LLMs are prone to produce factually incorrect information and often produce \"phantom\" content that undermines their reliability, which poses a serious challenge for their deployment in real-world scenarios. Enhancing LLMs by combining external databases and information retrieval mechanisms is an effective path. To address the above challenges, we propose a new approach called WeKnow-RAG, which integrates Web search and Knowledge Graphs into a \"Retrieval-Augmented Generation (RAG)\" system. First, the accuracy and reliability of LLM responses are improved by combining the structured representation of Knowledge Graphs with the flexibility of dense vector retrieval. WeKnow-RAG then utilizes domain-specific knowledge graphs to satisfy a variety of queries and domains, thereby improving performance on factual information and complex reasoning tasks by employing multi-stage web page retrieval techniques using both sparse and dense retrieval methods. Our approach effectively balances the efficiency and accuracy of information retrieval, thus improving the overall retrieval process. Finally, we also integrate a self-assessment mechanism for the LLM to evaluate the trustworthiness of the answers it generates. Our approach proves its outstanding effectiveness in a wide range of offline experiments and online submissions.","sentences":["Large Language Models (LLMs) have greatly contributed to the development of adaptive intelligent agents and are positioned as an important way to achieve Artificial General Intelligence (AGI).","However, LLMs are prone to produce factually incorrect information and often produce \"phantom\" content that undermines their reliability, which poses a serious challenge for their deployment in real-world scenarios.","Enhancing LLMs by combining external databases and information retrieval mechanisms is an effective path.","To address the above challenges, we propose a new approach called WeKnow-RAG, which integrates Web search and Knowledge Graphs into a \"Retrieval-Augmented Generation (RAG)\" system.","First, the accuracy and reliability of LLM responses are improved by combining the structured representation of Knowledge Graphs with the flexibility of dense vector retrieval.","WeKnow-RAG then utilizes domain-specific knowledge graphs to satisfy a variety of queries and domains, thereby improving performance on factual information and complex reasoning tasks by employing multi-stage web page retrieval techniques using both sparse and dense retrieval methods.","Our approach effectively balances the efficiency and accuracy of information retrieval, thus improving the overall retrieval process.","Finally, we also integrate a self-assessment mechanism for the LLM to evaluate the trustworthiness of the answers it generates.","Our approach proves its outstanding effectiveness in a wide range of offline experiments and online submissions."],"url":"http://arxiv.org/abs/2408.07611v1"}
{"created":"2024-08-14 15:17:33","title":"Modernizing an Operational Real-time Tsunami Simulator to Support Diverse Hardware Platforms","abstract":"To issue early warnings and rapidly initiate disaster responses after tsunami damage, various tsunami inundation forecast systems have been deployed worldwide. Japan's Cabinet Office operates a forecast system that utilizes supercomputers to perform tsunami propagation and inundation simulation in real time. Although this real-time approach is able to produce significantly more accurate forecasts than the conventional database-driven approach, its wider adoption was hindered because it was specifically developed for vector supercomputers. In this paper, we migrate the simulation code to modern CPUs and GPUs in a minimally invasive manner to reduce the testing and maintenance costs. A directive-based approach is employed to retain the structure of the original code while achieving performance portability, and hardware-specific optimizations including load balance improvement for GPUs are applied. The migrated code runs efficiently on recent CPUs, GPUs and vector processors: a six-hour tsunami simulation using over 47 million cells completes in less than 2.5 minutes on 32 Intel Sapphire Rapids CPUs and 1.5 minutes on 32 NVIDIA H100 GPUs. These results demonstrate that the code enables broader access to accurate tsunami inundation forecasts.","sentences":["To issue early warnings and rapidly initiate disaster responses after tsunami damage, various tsunami inundation forecast systems have been deployed worldwide.","Japan's Cabinet Office operates a forecast system that utilizes supercomputers to perform tsunami propagation and inundation simulation in real time.","Although this real-time approach is able to produce significantly more accurate forecasts than the conventional database-driven approach, its wider adoption was hindered because it was specifically developed for vector supercomputers.","In this paper, we migrate the simulation code to modern CPUs and GPUs in a minimally invasive manner to reduce the testing and maintenance costs.","A directive-based approach is employed to retain the structure of the original code while achieving performance portability, and hardware-specific optimizations including load balance improvement for GPUs are applied.","The migrated code runs efficiently on recent CPUs, GPUs and vector processors: a six-hour tsunami simulation using over 47 million cells completes in less than 2.5 minutes on 32 Intel Sapphire Rapids CPUs and 1.5 minutes on 32 NVIDIA H100 GPUs.","These results demonstrate that the code enables broader access to accurate tsunami inundation forecasts."],"url":"http://arxiv.org/abs/2408.07609v1"}
{"created":"2024-08-14 15:10:13","title":"Panacea+: Panoramic and Controllable Video Generation for Autonomous Driving","abstract":"The field of autonomous driving increasingly demands high-quality annotated video training data. In this paper, we propose Panacea+, a powerful and universally applicable framework for generating video data in driving scenes. Built upon the foundation of our previous work, Panacea, Panacea+ adopts a multi-view appearance noise prior mechanism and a super-resolution module for enhanced consistency and increased resolution. Extensive experiments show that the generated video samples from Panacea+ greatly benefit a wide range of tasks on different datasets, including 3D object tracking, 3D object detection, and lane detection tasks on the nuScenes and Argoverse 2 dataset. These results strongly prove Panacea+ to be a valuable data generation framework for autonomous driving.","sentences":["The field of autonomous driving increasingly demands high-quality annotated video training data.","In this paper, we propose Panacea+, a powerful and universally applicable framework for generating video data in driving scenes.","Built upon the foundation of our previous work, Panacea, Panacea+ adopts a multi-view appearance noise prior mechanism and a super-resolution module for enhanced consistency and increased resolution.","Extensive experiments show that the generated video samples from Panacea+ greatly benefit a wide range of tasks on different datasets, including 3D object tracking, 3D object detection, and lane detection tasks on the nuScenes and Argoverse 2 dataset.","These results strongly prove Panacea+ to be a valuable data generation framework for autonomous driving."],"url":"http://arxiv.org/abs/2408.07605v1"}
{"created":"2024-08-14 15:00:27","title":"Disentangle and denoise: Tackling context misalignment for video moment retrieval","abstract":"Video Moment Retrieval, which aims to locate in-context video moments according to a natural language query, is an essential task for cross-modal grounding. Existing methods focus on enhancing the cross-modal interactions between all moments and the textual description for video understanding. However, constantly interacting with all locations is unreasonable because of uneven semantic distribution across the timeline and noisy visual backgrounds. This paper proposes a cross-modal Context Denoising Network (CDNet) for accurate moment retrieval by disentangling complex correlations and denoising irrelevant dynamics.Specifically, we propose a query-guided semantic disentanglement (QSD) to decouple video moments by estimating alignment levels according to the global and fine-grained correlation. A Context-aware Dynamic Denoisement (CDD) is proposed to enhance understanding of aligned spatial-temporal details by learning a group of query-relevant offsets. Extensive experiments on public benchmarks demonstrate that the proposed CDNet achieves state-of-the-art performances.","sentences":["Video Moment Retrieval, which aims to locate in-context video moments according to a natural language query, is an essential task for cross-modal grounding.","Existing methods focus on enhancing the cross-modal interactions between all moments and the textual description for video understanding.","However, constantly interacting with all locations is unreasonable because of uneven semantic distribution across the timeline and noisy visual backgrounds.","This paper proposes a cross-modal Context Denoising Network (CDNet) for accurate moment retrieval by disentangling complex correlations and denoising irrelevant dynamics.","Specifically, we propose a query-guided semantic disentanglement (QSD) to decouple video moments by estimating alignment levels according to the global and fine-grained correlation.","A Context-aware Dynamic Denoisement (CDD) is proposed to enhance understanding of aligned spatial-temporal details by learning a group of query-relevant offsets.","Extensive experiments on public benchmarks demonstrate that the proposed CDNet achieves state-of-the-art performances."],"url":"http://arxiv.org/abs/2408.07600v1"}
{"created":"2024-08-14 14:59:20","title":"Assessing the Role of Lexical Semantics in Cross-lingual Transfer through Controlled Manipulations","abstract":"While cross-linguistic model transfer is effective in many settings, there is still limited understanding of the conditions under which it works. In this paper, we focus on assessing the role of lexical semantics in cross-lingual transfer, as we compare its impact to that of other language properties. Examining each language property individually, we systematically analyze how differences between English and a target language influence the capacity to align the language with an English pretrained representation space. We do so by artificially manipulating the English sentences in ways that mimic specific characteristics of the target language, and reporting the effect of each manipulation on the quality of alignment with the representation space. We show that while properties such as the script or word order only have a limited impact on alignment quality, the degree of lexical matching between the two languages, which we define using a measure of translation entropy, greatly affects it.","sentences":["While cross-linguistic model transfer is effective in many settings, there is still limited understanding of the conditions under which it works.","In this paper, we focus on assessing the role of lexical semantics in cross-lingual transfer, as we compare its impact to that of other language properties.","Examining each language property individually, we systematically analyze how differences between English and a target language influence the capacity to align the language with an English pretrained representation space.","We do so by artificially manipulating the English sentences in ways that mimic specific characteristics of the target language, and reporting the effect of each manipulation on the quality of alignment with the representation space.","We show that while properties such as the script or word order only have a limited impact on alignment quality, the degree of lexical matching between the two languages, which we define using a measure of translation entropy, greatly affects it."],"url":"http://arxiv.org/abs/2408.07599v1"}
{"created":"2024-08-14 14:50:08","title":"Progressive Radiance Distillation for Inverse Rendering with Gaussian Splatting","abstract":"We propose progressive radiance distillation, an inverse rendering method that combines physically-based rendering with Gaussian-based radiance field rendering using a distillation progress map. Taking multi-view images as input, our method starts from a pre-trained radiance field guidance, and distills physically-based light and material parameters from the radiance field using an image-fitting process. The distillation progress map is initialized to a small value, which favors radiance field rendering. During early iterations when fitted light and material parameters are far from convergence, the radiance field fallback ensures the sanity of image loss gradients and avoids local minima that attracts under-fit states. As fitted parameters converge, the physical model gradually takes over and the distillation progress increases correspondingly. In presence of light paths unmodeled by the physical model, the distillation progress never finishes on affected pixels and the learned radiance field stays in the final rendering. With this designed tolerance for physical model limitations, we prevent unmodeled color components from leaking into light and material parameters, alleviating relighting artifacts. Meanwhile, the remaining radiance field compensates for the limitations of the physical model, guaranteeing high-quality novel views synthesis. Experimental results demonstrate that our method significantly outperforms state-of-the-art techniques quality-wise in both novel view synthesis and relighting. The idea of progressive radiance distillation is not limited to Gaussian splatting. We show that it also has positive effects for prominently specular scenes when adapted to a mesh-based inverse rendering method.","sentences":["We propose progressive radiance distillation, an inverse rendering method that combines physically-based rendering with Gaussian-based radiance field rendering using a distillation progress map.","Taking multi-view images as input, our method starts from a pre-trained radiance field guidance, and distills physically-based light and material parameters from the radiance field using an image-fitting process.","The distillation progress map is initialized to a small value, which favors radiance field rendering.","During early iterations when fitted light and material parameters are far from convergence, the radiance field fallback ensures the sanity of image loss gradients and avoids local minima that attracts under-fit states.","As fitted parameters converge, the physical model gradually takes over and the distillation progress increases correspondingly.","In presence of light paths unmodeled by the physical model, the distillation progress never finishes on affected pixels and the learned radiance field stays in the final rendering.","With this designed tolerance for physical model limitations, we prevent unmodeled color components from leaking into light and material parameters, alleviating relighting artifacts.","Meanwhile, the remaining radiance field compensates for the limitations of the physical model, guaranteeing high-quality novel views synthesis.","Experimental results demonstrate that our method significantly outperforms state-of-the-art techniques quality-wise in both novel view synthesis and relighting.","The idea of progressive radiance distillation is not limited to Gaussian splatting.","We show that it also has positive effects for prominently specular scenes when adapted to a mesh-based inverse rendering method."],"url":"http://arxiv.org/abs/2408.07595v1"}
{"created":"2024-08-14 14:49:25","title":"Crossover Designs in Software Engineering Experiments: Review of the State of Analysis","abstract":"Experimentation is an essential method for causal inference in any empirical discipline. Crossover-design experiments are common in Software Engineering (SE) research. In these, subjects apply more than one treatment in different orders. This design increases the amount of obtained data and deals with subject variability but introduces threats to internal validity like the learning and carryover effect. Vegas et al. reviewed the state of practice for crossover designs in SE research and provided guidelines on how to address its threats during data analysis while still harnessing its benefits. In this paper, we reflect on the impact of these guidelines and review the state of analysis of crossover design experiments in SE publications between 2015 and March 2024. To this end, by conducting a forward snowballing of the guidelines, we survey 136 publications reporting 67 crossover-design experiments and evaluate their data analysis against the provided guidelines. The results show that the validity of data analyses has improved compared to the original state of analysis. Still, despite the explicit guidelines, only 29.5% of all threats to validity were addressed properly. While the maturation and the optimal sequence threats are properly addressed in 35.8% and 38.8% of all studies in our sample respectively, the carryover threat is only modeled in about 3% of the observed cases. The lack of adherence to the analysis guidelines threatens the validity of the conclusions drawn from crossover design experiments","sentences":["Experimentation is an essential method for causal inference in any empirical discipline.","Crossover-design experiments are common in Software Engineering (SE) research.","In these, subjects apply more than one treatment in different orders.","This design increases the amount of obtained data and deals with subject variability but introduces threats to internal validity like the learning and carryover effect.","Vegas et al. reviewed the state of practice for crossover designs in SE research and provided guidelines on how to address its threats during data analysis while still harnessing its benefits.","In this paper, we reflect on the impact of these guidelines and review the state of analysis of crossover design experiments in SE publications between 2015 and March 2024.","To this end, by conducting a forward snowballing of the guidelines, we survey 136 publications reporting 67 crossover-design experiments and evaluate their data analysis against the provided guidelines.","The results show that the validity of data analyses has improved compared to the original state of analysis.","Still, despite the explicit guidelines, only 29.5% of all threats to validity were addressed properly.","While the maturation and the optimal sequence threats are properly addressed in 35.8% and 38.8% of all studies in our sample respectively, the carryover threat is only modeled in about 3% of the observed cases.","The lack of adherence to the analysis guidelines threatens the validity of the conclusions drawn from crossover design experiments"],"url":"http://arxiv.org/abs/2408.07594v1"}
{"created":"2024-08-14 14:41:51","title":"Creating Data Art: Authentic Learning and Visualisation Exhibition","abstract":"We present an authentic learning task designed for computing students, centred on the creation of data-art visualisations from chosen datasets for a public exhibition. This exhibition was showcased in the cinema foyer for two weeks in June, providing a real-world platform for students to display their work. Over the course of two years, we implemented this active learning task with two different cohorts of students. In this paper, we share our experiences and insights from these activities, highlighting the impact on student engagement and learning outcomes. We also provide a detailed description of the seven individual tasks that learners must perform: topic and data selection and analysis, research and art inspiration, design conceptualisation, proposed solution, visualisation creation, exhibition curation, and reflection. By integrating these tasks, students not only develop technical skills but also gain practical experience in presenting their work to a public audience, bridging the gap between academic learning and professional practice.","sentences":["We present an authentic learning task designed for computing students, centred on the creation of data-art visualisations from chosen datasets for a public exhibition.","This exhibition was showcased in the cinema foyer for two weeks in June, providing a real-world platform for students to display their work.","Over the course of two years, we implemented this active learning task with two different cohorts of students.","In this paper, we share our experiences and insights from these activities, highlighting the impact on student engagement and learning outcomes.","We also provide a detailed description of the seven individual tasks that learners must perform: topic and data selection and analysis, research and art inspiration, design conceptualisation, proposed solution, visualisation creation, exhibition curation, and reflection.","By integrating these tasks, students not only develop technical skills but also gain practical experience in presenting their work to a public audience, bridging the gap between academic learning and professional practice."],"url":"http://arxiv.org/abs/2408.07590v1"}
{"created":"2024-08-14 14:36:28","title":"FedQUIT: On-Device Federated Unlearning via a Quasi-Competent Virtual Teacher","abstract":"Federated Learning (FL) promises better privacy guarantees for individuals' data when machine learning models are collaboratively trained. When an FL participant exercises its right to be forgotten, i.e., to detach from the FL framework it has participated and to remove its past contributions to the global model, the FL solution should perform all the necessary steps to make it possible without sacrificing the overall performance of the global model, which is not supported in state-of-the-art related solutions nowadays. In this paper, we propose FedQUIT, a novel algorithm that uses knowledge distillation to scrub the contribution of the forgetting data from an FL global model while preserving its generalization ability. FedQUIT directly works on clients' devices and does not require sharing additional information if compared with a regular FL process, nor does it assume the availability of publicly available proxy data. Our solution is efficient, effective, and applicable in both centralized and federated settings. Our experimental results show that, on average, FedQUIT requires less than 2.5% additional communication rounds to recover generalization performances after unlearning, obtaining a sanitized global model whose predictions are comparable to those of a global model that has never seen the data to be forgotten.","sentences":["Federated Learning (FL) promises better privacy guarantees for individuals' data when machine learning models are collaboratively trained.","When an FL participant exercises its right to be forgotten, i.e., to detach from the FL framework it has participated and to remove its past contributions to the global model, the FL solution should perform all the necessary steps to make it possible without sacrificing the overall performance of the global model, which is not supported in state-of-the-art related solutions nowadays.","In this paper, we propose FedQUIT, a novel algorithm that uses knowledge distillation to scrub the contribution of the forgetting data from an FL global model while preserving its generalization ability.","FedQUIT directly works on clients' devices and does not require sharing additional information if compared with a regular FL process, nor does it assume the availability of publicly available proxy data.","Our solution is efficient, effective, and applicable in both centralized and federated settings.","Our experimental results show that, on average, FedQUIT requires less than 2.5% additional communication rounds to recover generalization performances after unlearning, obtaining a sanitized global model whose predictions are comparable to those of a global model that has never seen the data to be forgotten."],"url":"http://arxiv.org/abs/2408.07587v1"}
{"created":"2024-08-14 14:28:11","title":"Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey","abstract":"With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles. The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.","sentences":["With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction.","One field benefiting greatly from these advancements is cybersecurity.","In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols.","This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems.","The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research.","The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field.","The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others.","Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles.","The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more.","Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development."],"url":"http://arxiv.org/abs/2408.07583v1"}
{"created":"2024-08-14 14:23:12","title":"TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases","abstract":"While adversarial robustness in computer vision is a mature research field, fewer researchers have tackled the evasion attacks against tabular deep learning, and even fewer investigated robustification mechanisms and reliable defenses. We hypothesize that this lag in the research on tabular adversarial attacks is in part due to the lack of standardized benchmarks. To fill this gap, we propose TabularBench, the first comprehensive benchmark of robustness of tabular deep learning classification models. We evaluated adversarial robustness with CAA, an ensemble of gradient and search attacks which was recently demonstrated as the most effective attack against a tabular model. In addition to our open benchmark (https://github.com/serval-uni-lu/tabularbench) where we welcome submissions of new models and defenses, we implement 7 robustification mechanisms inspired by state-of-the-art defenses in computer vision and propose the largest benchmark of robust tabular deep learning over 200 models across five critical scenarios in finance, healthcare and security. We curated real datasets for each use case, augmented with hundreds of thousands of realistic synthetic inputs, and trained and assessed our models with and without data augmentations. We open-source our library that provides API access to all our pre-trained robust tabular models, and the largest datasets of real and synthetic tabular inputs. Finally, we analyze the impact of various defenses on the robustness and provide actionable insights to design new defenses and robustification mechanisms.","sentences":["While adversarial robustness in computer vision is a mature research field, fewer researchers have tackled the evasion attacks against tabular deep learning, and even fewer investigated robustification mechanisms and reliable defenses.","We hypothesize that this lag in the research on tabular adversarial attacks is in part due to the lack of standardized benchmarks.","To fill this gap, we propose TabularBench, the first comprehensive benchmark of robustness of tabular deep learning classification models.","We evaluated adversarial robustness with CAA, an ensemble of gradient and search attacks which was recently demonstrated as the most effective attack against a tabular model.","In addition to our open benchmark (https://github.com/serval-uni-lu/tabularbench) where we welcome submissions of new models and defenses, we implement 7 robustification mechanisms inspired by state-of-the-art defenses in computer vision and propose the largest benchmark of robust tabular deep learning over 200 models across five critical scenarios in finance, healthcare and security.","We curated real datasets for each use case, augmented with hundreds of thousands of realistic synthetic inputs, and trained and assessed our models with and without data augmentations.","We open-source our library that provides API access to all our pre-trained robust tabular models, and the largest datasets of real and synthetic tabular inputs.","Finally, we analyze the impact of various defenses on the robustness and provide actionable insights to design new defenses and robustification mechanisms."],"url":"http://arxiv.org/abs/2408.07579v1"}
{"created":"2024-08-14 14:18:51","title":"A Nested Graph Reinforcement Learning-based Decision-making Strategy for Eco-platooning","abstract":"Platooning technology is renowned for its precise vehicle control, traffic flow optimization, and energy efficiency enhancement. However, in large-scale mixed platoons, vehicle heterogeneity and unpredictable traffic conditions lead to virtual bottlenecks. These bottlenecks result in reduced traffic throughput and increased energy consumption within the platoon. To address these challenges, we introduce a decision-making strategy based on nested graph reinforcement learning. This strategy improves collaborative decision-making, ensuring energy efficiency and alleviating congestion. We propose a theory of nested traffic graph representation that maps dynamic interactions between vehicles and platoons in non-Euclidean spaces. By incorporating spatio-temporal weighted graph into a multi-head attention mechanism, we further enhance the model's capacity to process both local and global data. Additionally, we have developed a nested graph reinforcement learning framework to enhance the self-iterative learning capabilities of platooning. Using the I-24 dataset, we designed and conducted comparative algorithm experiments, generalizability testing, and permeability ablation experiments, thereby validating the proposed strategy's effectiveness. Compared to the baseline, our strategy increases throughput by 10% and decreases energy use by 9%. Specifically, increasing the penetration rate of CAVs significantly enhances traffic throughput, though it also increases energy consumption.","sentences":["Platooning technology is renowned for its precise vehicle control, traffic flow optimization, and energy efficiency enhancement.","However, in large-scale mixed platoons, vehicle heterogeneity and unpredictable traffic conditions lead to virtual bottlenecks.","These bottlenecks result in reduced traffic throughput and increased energy consumption within the platoon.","To address these challenges, we introduce a decision-making strategy based on nested graph reinforcement learning.","This strategy improves collaborative decision-making, ensuring energy efficiency and alleviating congestion.","We propose a theory of nested traffic graph representation that maps dynamic interactions between vehicles and platoons in non-Euclidean spaces.","By incorporating spatio-temporal weighted graph into a multi-head attention mechanism, we further enhance the model's capacity to process both local and global data.","Additionally, we have developed a nested graph reinforcement learning framework to enhance the self-iterative learning capabilities of platooning.","Using the I-24 dataset, we designed and conducted comparative algorithm experiments, generalizability testing, and permeability ablation experiments, thereby validating the proposed strategy's effectiveness.","Compared to the baseline, our strategy increases throughput by 10% and decreases energy use by 9%.","Specifically, increasing the penetration rate of CAVs significantly enhances traffic throughput, though it also increases energy consumption."],"url":"http://arxiv.org/abs/2408.07578v1"}
{"created":"2024-08-14 14:16:52","title":"MetaSeg: MetaFormer-based Global Contexts-aware Network for Efficient Semantic Segmentation","abstract":"Beyond the Transformer, it is important to explore how to exploit the capacity of the MetaFormer, an architecture that is fundamental to the performance improvements of the Transformer. Previous studies have exploited it only for the backbone network. Unlike previous studies, we explore the capacity of the Metaformer architecture more extensively in the semantic segmentation task. We propose a powerful semantic segmentation network, MetaSeg, which leverages the Metaformer architecture from the backbone to the decoder. Our MetaSeg shows that the MetaFormer architecture plays a significant role in capturing the useful contexts for the decoder as well as for the backbone. In addition, recent segmentation methods have shown that using a CNN-based backbone for extracting the spatial information and a decoder for extracting the global information is more effective than using a transformer-based backbone with a CNN-based decoder. This motivates us to adopt the CNN-based backbone using the MetaFormer block and design our MetaFormer-based decoder, which consists of a novel self-attention module to capture the global contexts. To consider both the global contexts extraction and the computational efficiency of the self-attention for semantic segmentation, we propose a Channel Reduction Attention (CRA) module that reduces the channel dimension of the query and key into the one dimension. In this way, our proposed MetaSeg outperforms the previous state-of-the-art methods with more efficient computational costs on popular semantic segmentation and a medical image segmentation benchmark, including ADE20K, Cityscapes, COCO-stuff, and Synapse. The code is available at \\url{https://github.com/hyunwoo137/MetaSeg}.","sentences":["Beyond the Transformer, it is important to explore how to exploit the capacity of the MetaFormer, an architecture that is fundamental to the performance improvements of the Transformer.","Previous studies have exploited it only for the backbone network.","Unlike previous studies, we explore the capacity of the Metaformer architecture more extensively in the semantic segmentation task.","We propose a powerful semantic segmentation network, MetaSeg, which leverages the Metaformer architecture from the backbone to the decoder.","Our MetaSeg shows that the MetaFormer architecture plays a significant role in capturing the useful contexts for the decoder as well as for the backbone.","In addition, recent segmentation methods have shown that using a CNN-based backbone for extracting the spatial information and a decoder for extracting the global information is more effective than using a transformer-based backbone with a CNN-based decoder.","This motivates us to adopt the CNN-based backbone using the MetaFormer block and design our MetaFormer-based decoder, which consists of a novel self-attention module to capture the global contexts.","To consider both the global contexts extraction and the computational efficiency of the self-attention for semantic segmentation, we propose a Channel Reduction Attention (CRA) module that reduces the channel dimension of the query and key into the one dimension.","In this way, our proposed MetaSeg outperforms the previous state-of-the-art methods with more efficient computational costs on popular semantic segmentation and a medical image segmentation benchmark, including ADE20K, Cityscapes, COCO-stuff, and Synapse.","The code is available at \\url{https://github.com/hyunwoo137/MetaSeg}."],"url":"http://arxiv.org/abs/2408.07576v1"}
{"created":"2024-08-14 14:16:02","title":"A General Framework for Constraint-based Causal Learning","abstract":"By representing any constraint-based causal learning algorithm via a placeholder property, we decompose the correctness condition into a part relating the distribution and the true causal graph, and a part that depends solely on the distribution. This provides a general framework to obtain correctness conditions for causal learning, and has the following implications. We provide exact correctness conditions for the PC algorithm, which are then related to correctness conditions of some other existing causal discovery algorithms. We show that the sparsest Markov representation condition is the weakest correctness condition resulting from existing notions of minimality for maximal ancestral graphs and directed acyclic graphs. We also reason that additional knowledge than just Pearl-minimality is necessary for causal learning beyond faithfulness.","sentences":["By representing any constraint-based causal learning algorithm via a placeholder property, we decompose the correctness condition into a part relating the distribution and the true causal graph, and a part that depends solely on the distribution.","This provides a general framework to obtain correctness conditions for causal learning, and has the following implications.","We provide exact correctness conditions for the PC algorithm, which are then related to correctness conditions of some other existing causal discovery algorithms.","We show that the sparsest Markov representation condition is the weakest correctness condition resulting from existing notions of minimality for maximal ancestral graphs and directed acyclic graphs.","We also reason that additional knowledge than just Pearl-minimality is necessary for causal learning beyond faithfulness."],"url":"http://arxiv.org/abs/2408.07575v1"}
{"created":"2024-08-14 14:06:13","title":"Multi-task Heterogeneous Graph Learning on Electronic Health Records","abstract":"Learning electronic health records (EHRs) has received emerging attention because of its capability to facilitate accurate medical diagnosis. Since the EHRs contain enriched information specifying complex interactions between entities, modeling EHRs with graphs is shown to be effective in practice. The EHRs, however, present a great degree of heterogeneity, sparsity, and complexity, which hamper the performance of most of the models applied to them. Moreover, existing approaches modeling EHRs often focus on learning the representations for a single task, overlooking the multi-task nature of EHR analysis problems and resulting in limited generalizability across different tasks. In view of these limitations, we propose a novel framework for EHR modeling, namely MulT-EHR (Multi-Task EHR), which leverages a heterogeneous graph to mine the complex relations and model the heterogeneity in the EHRs. To mitigate the large degree of noise, we introduce a denoising module based on the causal inference framework to adjust for severe confounding effects and reduce noise in the EHR data. Additionally, since our model adopts a single graph neural network for simultaneous multi-task prediction, we design a multi-task learning module to leverage the inter-task knowledge to regularize the training process. Extensive empirical studies on MIMIC-III and MIMIC-IV datasets validate that the proposed method consistently outperforms the state-of-the-art designs in four popular EHR analysis tasks -- drug recommendation, and predictions of the length of stay, mortality, and readmission. Thorough ablation studies demonstrate the robustness of our method upon variations to key components and hyperparameters.","sentences":["Learning electronic health records (EHRs) has received emerging attention because of its capability to facilitate accurate medical diagnosis.","Since the EHRs contain enriched information specifying complex interactions between entities, modeling EHRs with graphs is shown to be effective in practice.","The EHRs, however, present a great degree of heterogeneity, sparsity, and complexity, which hamper the performance of most of the models applied to them.","Moreover, existing approaches modeling EHRs often focus on learning the representations for a single task, overlooking the multi-task nature of EHR analysis problems and resulting in limited generalizability across different tasks.","In view of these limitations, we propose a novel framework for EHR modeling, namely MulT-EHR (Multi-Task EHR), which leverages a heterogeneous graph to mine the complex relations and model the heterogeneity in the EHRs.","To mitigate the large degree of noise, we introduce a denoising module based on the causal inference framework to adjust for severe confounding effects and reduce noise in the EHR data.","Additionally, since our model adopts a single graph neural network for simultaneous multi-task prediction, we design a multi-task learning module to leverage the inter-task knowledge to regularize the training process.","Extensive empirical studies on MIMIC-III and MIMIC-IV datasets validate that the proposed method consistently outperforms the state-of-the-art designs in four popular EHR analysis tasks -- drug recommendation, and predictions of the length of stay, mortality, and readmission.","Thorough ablation studies demonstrate the robustness of our method upon variations to key components and hyperparameters."],"url":"http://arxiv.org/abs/2408.07569v1"}
{"created":"2024-08-14 13:58:56","title":"Multilayer Network of Cardiovascular Diseases and Depression via Multipartite Projection","abstract":"There is a significant comorbidity between cardiovascular diseases (CVD) and depression that is highly predictive of poor clinical outcome. Yet, its underlying biological pathways remain challenging to decipher, presumably due to its non-linear associations across multiple mechanisms. Mutual information provides a framework to analyze such intricacies. In this study, we proposed a multipartite projection method based on mutual information correlations to construct multilayer disease networks. We applied the method to a cross-sectional dataset from a wave of the Young Finns Study. This dataset assesses CVD and depression, along with related risk factors and two omics of biomarkers: metabolites and lipids. Instead of directly correlating CVD-related phenotypes and depressive symptoms, we extended the notion of bipartite networks to create a multipartite network that connects these phenotype and symptom variables to intermediate biological variables. Projecting from these intermediate variables results in a weighted multilayer network, where each link between CVD and depression variables is marked by its `layer' (i.e., metabolome or lipidome). Using this projection method, we identified potential mediating biomarkers that connect CVD to depression. These biomarkers thus may play significant roles in the biological pathways of CVD-depression comorbidity. Additionally, the projected network highlights sex and BMI as the most important risk factors, or confounders, associated with the comorbidity. Our method can generalize to any number of omics layers and disease phenotypes, offering a truly system-level overview of biological pathways contributing to comorbidity.","sentences":["There is a significant comorbidity between cardiovascular diseases (CVD) and depression that is highly predictive of poor clinical outcome.","Yet, its underlying biological pathways remain challenging to decipher, presumably due to its non-linear associations across multiple mechanisms.","Mutual information provides a framework to analyze such intricacies.","In this study, we proposed a multipartite projection method based on mutual information correlations to construct multilayer disease networks.","We applied the method to a cross-sectional dataset from a wave of the Young Finns Study.","This dataset assesses CVD and depression, along with related risk factors and two omics of biomarkers: metabolites and lipids.","Instead of directly correlating CVD-related phenotypes and depressive symptoms, we extended the notion of bipartite networks to create a multipartite network that connects these phenotype and symptom variables to intermediate biological variables.","Projecting from these intermediate variables results in a weighted multilayer network, where each link between CVD and depression variables is marked by its `layer' (i.e., metabolome or lipidome).","Using this projection method, we identified potential mediating biomarkers that connect CVD to depression.","These biomarkers thus may play significant roles in the biological pathways of CVD-depression comorbidity.","Additionally, the projected network highlights sex and BMI as the most important risk factors, or confounders, associated with the comorbidity.","Our method can generalize to any number of omics layers and disease phenotypes, offering a truly system-level overview of biological pathways contributing to comorbidity."],"url":"http://arxiv.org/abs/2408.07562v1"}
{"created":"2024-08-14 13:43:59","title":"Sonic: Fast and Transferable Data Poisoning on Clustering Algorithms","abstract":"Data poisoning attacks on clustering algorithms have received limited attention, with existing methods struggling to scale efficiently as dataset sizes and feature counts increase. These attacks typically require re-clustering the entire dataset multiple times to generate predictions and assess the attacker's objectives, significantly hindering their scalability. This paper addresses these limitations by proposing Sonic, a novel genetic data poisoning attack that leverages incremental and scalable clustering algorithms, e.g., FISHDBC, as surrogates to accelerate poisoning attacks against graph-based and density-based clustering methods, such as HDBSCAN. We empirically demonstrate the effectiveness and efficiency of Sonic in poisoning the target clustering algorithms. We then conduct a comprehensive analysis of the factors affecting the scalability and transferability of poisoning attacks against clustering algorithms, and we conclude by examining the robustness of hyperparameters in our attack strategy Sonic.","sentences":["Data poisoning attacks on clustering algorithms have received limited attention, with existing methods struggling to scale efficiently as dataset sizes and feature counts increase.","These attacks typically require re-clustering the entire dataset multiple times to generate predictions and assess the attacker's objectives, significantly hindering their scalability.","This paper addresses these limitations by proposing Sonic, a novel genetic data poisoning attack that leverages incremental and scalable clustering algorithms, e.g., FISHDBC, as surrogates to accelerate poisoning attacks against graph-based and density-based clustering methods, such as HDBSCAN.","We empirically demonstrate the effectiveness and efficiency of Sonic in poisoning the target clustering algorithms.","We then conduct a comprehensive analysis of the factors affecting the scalability and transferability of poisoning attacks against clustering algorithms, and we conclude by examining the robustness of hyperparameters in our attack strategy Sonic."],"url":"http://arxiv.org/abs/2408.07558v1"}
{"created":"2024-08-14 13:43:22","title":"PolyCL: Contrastive Learning for Polymer Representation Learning via Explicit and Implicit Augmentations","abstract":"Polymers play a crucial role in a wide array of applications due to their diverse and tunable properties. Establishing the relationship between polymer representations and their properties is crucial to the computational design and screening of potential polymers via machine learning. The quality of the representation significantly influences the effectiveness of these computational methods. Here, we present a self-supervised contrastive learning paradigm, PolyCL, for learning high-quality polymer representation without the need for labels. Our model combines explicit and implicit augmentation strategies for improved learning performance. The results demonstrate that our model achieves either better, or highly competitive, performances on transfer learning tasks as a feature extractor without an overcomplicated training strategy or hyperparameter optimisation. Further enhancing the efficacy of our model, we conducted extensive analyses on various augmentation combinations used in contrastive learning. This led to identifying the most effective combination to maximise PolyCL's performance.","sentences":["Polymers play a crucial role in a wide array of applications due to their diverse and tunable properties.","Establishing the relationship between polymer representations and their properties is crucial to the computational design and screening of potential polymers via machine learning.","The quality of the representation significantly influences the effectiveness of these computational methods.","Here, we present a self-supervised contrastive learning paradigm, PolyCL, for learning high-quality polymer representation without the need for labels.","Our model combines explicit and implicit augmentation strategies for improved learning performance.","The results demonstrate that our model achieves either better, or highly competitive, performances on transfer learning tasks as a feature extractor without an overcomplicated training strategy or hyperparameter optimisation.","Further enhancing the efficacy of our model, we conducted extensive analyses on various augmentation combinations used in contrastive learning.","This led to identifying the most effective combination to maximise PolyCL's performance."],"url":"http://arxiv.org/abs/2408.07556v1"}
{"created":"2024-08-14 13:36:17","title":"PeriodWave: Multi-Period Flow Matching for High-Fidelity Waveform Generation","abstract":"Recently, universal waveform generation tasks have been investigated conditioned on various out-of-distribution scenarios. Although GAN-based methods have shown their strength in fast waveform generation, they are vulnerable to train-inference mismatch scenarios such as two-stage text-to-speech. Meanwhile, diffusion-based models have shown their powerful generative performance in other domains; however, they stay out of the limelight due to slow inference speed in waveform generation tasks. Above all, there is no generator architecture that can explicitly disentangle the natural periodic features of high-resolution waveform signals. In this paper, we propose PeriodWave, a novel universal waveform generation model. First, we introduce a period-aware flow matching estimator that can capture the periodic features of the waveform signal when estimating the vector fields. Additionally, we utilize a multi-period estimator that avoids overlaps to capture different periodic features of waveform signals. Although increasing the number of periods can improve the performance significantly, this requires more computational costs. To reduce this issue, we also propose a single period-conditional universal estimator that can feed-forward parallel by period-wise batch inference. Additionally, we utilize discrete wavelet transform to losslessly disentangle the frequency information of waveform signals for high-frequency modeling, and introduce FreeU to reduce the high-frequency noise for waveform generation. The experimental results demonstrated that our model outperforms the previous models both in Mel-spectrogram reconstruction and text-to-speech tasks. All source code will be available at \\url{https://github.com/sh-lee-prml/PeriodWave}.","sentences":["Recently, universal waveform generation tasks have been investigated conditioned on various out-of-distribution scenarios.","Although GAN-based methods have shown their strength in fast waveform generation, they are vulnerable to train-inference mismatch scenarios such as two-stage text-to-speech.","Meanwhile, diffusion-based models have shown their powerful generative performance in other domains; however, they stay out of the limelight due to slow inference speed in waveform generation tasks.","Above all, there is no generator architecture that can explicitly disentangle the natural periodic features of high-resolution waveform signals.","In this paper, we propose PeriodWave, a novel universal waveform generation model.","First, we introduce a period-aware flow matching estimator that can capture the periodic features of the waveform signal when estimating the vector fields.","Additionally, we utilize a multi-period estimator that avoids overlaps to capture different periodic features of waveform signals.","Although increasing the number of periods can improve the performance significantly, this requires more computational costs.","To reduce this issue, we also propose a single period-conditional universal estimator that can feed-forward parallel by period-wise batch inference.","Additionally, we utilize discrete wavelet transform to losslessly disentangle the frequency information of waveform signals for high-frequency modeling, and introduce FreeU to reduce the high-frequency noise for waveform generation.","The experimental results demonstrated that our model outperforms the previous models both in Mel-spectrogram reconstruction and text-to-speech tasks.","All source code will be available at \\url{https://github.com/sh-lee-prml/PeriodWave}."],"url":"http://arxiv.org/abs/2408.07547v1"}
{"created":"2024-08-14 13:31:32","title":"$\u03c7$SPN: Characteristic Interventional Sum-Product Networks for Causal Inference in Hybrid Domains","abstract":"Causal inference in hybrid domains, characterized by a mixture of discrete and continuous variables, presents a formidable challenge. We take a step towards this direction and propose Characteristic Interventional Sum-Product Network ($\\chi$SPN) that is capable of estimating interventional distributions in presence of random variables drawn from mixed distributions. $\\chi$SPN uses characteristic functions in the leaves of an interventional SPN (iSPN) thereby providing a unified view for discrete and continuous random variables through the Fourier-Stieltjes transform of the probability measures. A neural network is used to estimate the parameters of the learned iSPN using the intervened data. Our experiments on 3 synthetic heterogeneous datasets suggest that $\\chi$SPN can effectively capture the interventional distributions for both discrete and continuous variables while being expressive and causally adequate. We also show that $\\chi$SPN generalize to multiple interventions while being trained only on a single intervention data.","sentences":["Causal inference in hybrid domains, characterized by a mixture of discrete and continuous variables, presents a formidable challenge.","We take a step towards this direction and propose Characteristic Interventional Sum-Product Network ($\\chi$SPN) that is capable of estimating interventional distributions in presence of random variables drawn from mixed distributions.","$\\chi$SPN uses characteristic functions in the leaves of an interventional SPN (iSPN) thereby providing a unified view for discrete and continuous random variables through the Fourier-Stieltjes transform of the probability measures.","A neural network is used to estimate the parameters of the learned iSPN using the intervened data.","Our experiments on 3 synthetic heterogeneous datasets suggest that $\\chi$SPN can effectively capture the interventional distributions for both discrete and continuous variables while being expressive and causally adequate.","We also show that $\\chi$SPN generalize to multiple interventions while being trained only on a single intervention data."],"url":"http://arxiv.org/abs/2408.07545v1"}
{"created":"2024-08-14 13:27:02","title":"Planning with OWL-DL Ontologies (Extended Version)","abstract":"We introduce ontology-mediated planning, in which planning problems are combined with an ontology. Our formalism differs from existing ones in that we focus on a strong separation of the formalisms for describing planning problems and ontologies, which are only losely coupled by an interface. Moreover, we present a black-box algorithm that supports the full expressive power of OWL DL. This goes beyond what existing approaches combining automated planning with ontologies can do, which only support limited description logics such as DL-Lite and description logics that are Horn. Our main algorithm relies on rewritings of the ontology-mediated planning specifications into PDDL, so that existing planning systems can be used to solve them. The algorithm relies on justifications, which allows for a generic approach that is independent of the expressivity of the ontology language. However, dedicated optimizations for computing justifications need to be implemented to enable an efficient rewriting procedure. We evaluated our implementation on benchmark sets from several domains. The evaluation shows that our procedure works in practice and that tailoring the reasoning procedure has significant impact on the performance.","sentences":["We introduce ontology-mediated planning, in which planning problems are combined with an ontology.","Our formalism differs from existing ones in that we focus on a strong separation of the formalisms for describing planning problems and ontologies, which are only losely coupled by an interface.","Moreover, we present a black-box algorithm that supports the full expressive power of OWL DL.","This goes beyond what existing approaches combining automated planning with ontologies can do, which only support limited description logics such as DL-Lite and description logics that are Horn.","Our main algorithm relies on rewritings of the ontology-mediated planning specifications into PDDL, so that existing planning systems can be used to solve them.","The algorithm relies on justifications, which allows for a generic approach that is independent of the expressivity of the ontology language.","However, dedicated optimizations for computing justifications need to be implemented to enable an efficient rewriting procedure.","We evaluated our implementation on benchmark sets from several domains.","The evaluation shows that our procedure works in practice and that tailoring the reasoning procedure has significant impact on the performance."],"url":"http://arxiv.org/abs/2408.07544v1"}
{"created":"2024-08-14 13:23:43","title":"MathScape: Evaluating MLLMs in multimodal Math Scenarios through a Hierarchical Benchmark","abstract":"With the development of Multimodal Large Language Models (MLLMs), the evaluation of multimodal models in the context of mathematical problems has become a valuable research field. Multimodal visual-textual mathematical reasoning serves as a critical indicator for evaluating the comprehension and complex multi-step quantitative reasoning abilities of MLLMs. However, previous multimodal math benchmarks have not sufficiently integrated visual and textual information. To address this gap, we proposed MathScape, a new benchmark that emphasizes the understanding and application of combined visual and textual information. MathScape is designed to evaluate photo-based math problem scenarios, assessing the theoretical understanding and application ability of MLLMs through a categorical hierarchical approach. We conduct a multi-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark is challenging even for the most sophisticated models. By analyzing the evaluation results, we identify the limitations of MLLMs, offering valuable insights for enhancing model performance.","sentences":["With the development of Multimodal Large Language Models (MLLMs), the evaluation of multimodal models in the context of mathematical problems has become a valuable research field.","Multimodal visual-textual mathematical reasoning serves as a critical indicator for evaluating the comprehension and complex multi-step quantitative reasoning abilities of MLLMs.","However, previous multimodal math benchmarks have not sufficiently integrated visual and textual information.","To address this gap, we proposed MathScape, a new benchmark that emphasizes the understanding and application of combined visual and textual information.","MathScape is designed to evaluate photo-based math problem scenarios, assessing the theoretical understanding and application ability of MLLMs through a categorical hierarchical approach.","We conduct a multi-dimensional evaluation on 11 advanced MLLMs, revealing that our benchmark is challenging even for the most sophisticated models.","By analyzing the evaluation results, we identify the limitations of MLLMs, offering valuable insights for enhancing model performance."],"url":"http://arxiv.org/abs/2408.07543v1"}
{"created":"2024-08-14 13:22:14","title":"New Curriculum, New Chance -- Retrieval Augmented Generation for Lesson Planning in Ugandan Secondary Schools. Prototype Quality Evaluation","abstract":"Introduction: Poor educational quality in Secondary Schools is still regarded as one of the major struggles in 21st century Uganda - especially in rural areas. Research identifies several problems, including low quality or absent teacher lesson planning. As the government pushes towards the implementation of a new curriculum, exiting lesson plans become obsolete and the problem is worsened. Using a Retrieval Augmented Generation approach, we developed a prototype that generates customized lesson plans based on the government-accredited textbooks. This helps teachers create lesson plans more efficiently and with better quality, ensuring they are fully aligned the new curriculum and the competence-based learning approach.   Methods: The prototype was created using Cohere LLM and Sentence Embeddings, and LangChain Framework - and thereafter made available on a public website. Vector stores were trained for three new curriculum textbooks (ICT, Mathematics, History), all at Secondary 1 Level. Twenty-four lessons plans were generated following a pseudo-random generation protocol, based on the suggested periods in the textbooks. The lesson plans were analyzed regarding their technical quality by three independent raters following the Lesson Plan Analysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically designed for East Africa and competence-based curriculums.   Results: Evaluation of 24 lesson plans using the LPAP resulted in an average quality of between 75 and 80%, corresponding to \"very good lesson plan\". None of the lesson plans scored below 65%, although one lesson plan could be argued to have been missing the topic. In conclusion, the quality of the generated lesson plans is at least comparable, if not better, than those created by humans, as demonstrated in a study in Rwanda, whereby no lesson plan even reached the benchmark of 50%.","sentences":["Introduction: Poor educational quality in Secondary Schools is still regarded as one of the major struggles in 21st century Uganda - especially in rural areas.","Research identifies several problems, including low quality or absent teacher lesson planning.","As the government pushes towards the implementation of a new curriculum, exiting lesson plans become obsolete and the problem is worsened.","Using a Retrieval Augmented Generation approach, we developed a prototype that generates customized lesson plans based on the government-accredited textbooks.","This helps teachers create lesson plans more efficiently and with better quality, ensuring they are fully aligned the new curriculum and the competence-based learning approach.   ","Methods: The prototype was created using Cohere LLM and Sentence Embeddings, and LangChain Framework - and thereafter made available on a public website.","Vector stores were trained for three new curriculum textbooks (ICT, Mathematics, History), all at Secondary 1 Level.","Twenty-four lessons plans were generated following a pseudo-random generation protocol, based on the suggested periods in the textbooks.","The lesson plans were analyzed regarding their technical quality by three independent raters following the Lesson Plan Analysis Protocol (LPAP) by Ndihokubwayo et al. (2022) that is specifically designed for East Africa and competence-based curriculums.   ","Results: Evaluation of 24 lesson plans using the LPAP resulted in an average quality of between 75 and 80%, corresponding to \"very good lesson plan\".","None of the lesson plans scored below 65%, although one lesson plan could be argued to have been missing the topic.","In conclusion, the quality of the generated lesson plans is at least comparable, if not better, than those created by humans, as demonstrated in a study in Rwanda, whereby no lesson plan even reached the benchmark of 50%."],"url":"http://arxiv.org/abs/2408.07542v1"}
{"created":"2024-08-14 13:20:52","title":"DifuzCam: Replacing Camera Lens with a Mask and a Diffusion Model","abstract":"The flat lensless camera design reduces the camera size and weight significantly. In this design, the camera lens is replaced by another optical element that interferes with the incoming light. The image is recovered from the raw sensor measurements using a reconstruction algorithm. Yet, the quality of the reconstructed images is not satisfactory. To mitigate this, we propose utilizing a pre-trained diffusion model with a control network and a learned separable transformation for reconstruction. This allows us to build a prototype flat camera with high-quality imaging, presenting state-of-the-art results in both terms of quality and perceptuality. We demonstrate its ability to leverage also textual descriptions of the captured scene to further enhance reconstruction. Our reconstruction method which leverages the strong capabilities of a pre-trained diffusion model can be used in other imaging systems for improved reconstruction results.","sentences":["The flat lensless camera design reduces the camera size and weight significantly.","In this design, the camera lens is replaced by another optical element that interferes with the incoming light.","The image is recovered from the raw sensor measurements using a reconstruction algorithm.","Yet, the quality of the reconstructed images is not satisfactory.","To mitigate this, we propose utilizing a pre-trained diffusion model with a control network and a learned separable transformation for reconstruction.","This allows us to build a prototype flat camera with high-quality imaging, presenting state-of-the-art results in both terms of quality and perceptuality.","We demonstrate its ability to leverage also textual descriptions of the captured scene to further enhance reconstruction.","Our reconstruction method which leverages the strong capabilities of a pre-trained diffusion model can be used in other imaging systems for improved reconstruction results."],"url":"http://arxiv.org/abs/2408.07541v1"}
{"created":"2024-08-14 13:17:42","title":"3D Gaussian Editing with A Single Image","abstract":"The modeling and manipulation of 3D scenes captured from the real world are pivotal in various applications, attracting growing research interest. While previous works on editing have achieved interesting results through manipulating 3D meshes, they often require accurately reconstructed meshes to perform editing, which limits their application in 3D content generation. To address this gap, we introduce a novel single-image-driven 3D scene editing approach based on 3D Gaussian Splatting, enabling intuitive manipulation via directly editing the content on a 2D image plane. Our method learns to optimize the 3D Gaussians to align with an edited version of the image rendered from a user-specified viewpoint of the original scene. To capture long-range object deformation, we introduce positional loss into the optimization process of 3D Gaussian Splatting and enable gradient propagation through reparameterization. To handle occluded 3D Gaussians when rendering from the specified viewpoint, we build an anchor-based structure and employ a coarse-to-fine optimization strategy capable of handling long-range deformation while maintaining structural stability. Furthermore, we design a novel masking strategy to adaptively identify non-rigid deformation regions for fine-scale modeling. Extensive experiments show the effectiveness of our method in handling geometric details, long-range, and non-rigid deformation, demonstrating superior editing flexibility and quality compared to previous approaches.","sentences":["The modeling and manipulation of 3D scenes captured from the real world are pivotal in various applications, attracting growing research interest.","While previous works on editing have achieved interesting results through manipulating 3D meshes, they often require accurately reconstructed meshes to perform editing, which limits their application in 3D content generation.","To address this gap, we introduce a novel single-image-driven 3D scene editing approach based on 3D Gaussian Splatting, enabling intuitive manipulation via directly editing the content on a 2D image plane.","Our method learns to optimize the 3D Gaussians to align with an edited version of the image rendered from a user-specified viewpoint of the original scene.","To capture long-range object deformation, we introduce positional loss into the optimization process of 3D Gaussian Splatting and enable gradient propagation through reparameterization.","To handle occluded 3D Gaussians when rendering from the specified viewpoint, we build an anchor-based structure and employ a coarse-to-fine optimization strategy capable of handling long-range deformation while maintaining structural stability.","Furthermore, we design a novel masking strategy to adaptively identify non-rigid deformation regions for fine-scale modeling.","Extensive experiments show the effectiveness of our method in handling geometric details, long-range, and non-rigid deformation, demonstrating superior editing flexibility and quality compared to previous approaches."],"url":"http://arxiv.org/abs/2408.07540v1"}
{"created":"2024-08-14 13:17:41","title":"Cross-aware Early Fusion with Stage-divided Vision and Language Transformer Encoders for Referring Image Segmentation","abstract":"Referring segmentation aims to segment a target object related to a natural language expression. Key challenges of this task are understanding the meaning of complex and ambiguous language expressions and determining the relevant regions in the image with multiple objects by referring to the expression. Recent models have focused on the early fusion with the language features at the intermediate stage of the vision encoder, but these approaches have a limitation that the language features cannot refer to the visual information. To address this issue, this paper proposes a novel architecture, Cross-aware early fusion with stage-divided Vision and Language Transformer encoders (CrossVLT), which allows both language and vision encoders to perform the early fusion for improving the ability of the cross-modal context modeling. Unlike previous methods, our method enables the vision and language features to refer to each other's information at each stage to mutually enhance the robustness of both encoders. Furthermore, unlike the conventional scheme that relies solely on the high-level features for the cross-modal alignment, we introduce a feature-based alignment scheme that enables the low-level to high-level features of the vision and language encoders to engage in the cross-modal alignment. By aligning the intermediate cross-modal features in all encoder stages, this scheme leads to effective cross-modal fusion. In this way, the proposed approach is simple but effective for referring image segmentation, and it outperforms the previous state-of-the-art methods on three public benchmarks.","sentences":["Referring segmentation aims to segment a target object related to a natural language expression.","Key challenges of this task are understanding the meaning of complex and ambiguous language expressions and determining the relevant regions in the image with multiple objects by referring to the expression.","Recent models have focused on the early fusion with the language features at the intermediate stage of the vision encoder, but these approaches have a limitation that the language features cannot refer to the visual information.","To address this issue, this paper proposes a novel architecture, Cross-aware early fusion with stage-divided Vision and Language Transformer encoders (CrossVLT), which allows both language and vision encoders to perform the early fusion for improving the ability of the cross-modal context modeling.","Unlike previous methods, our method enables the vision and language features to refer to each other's information at each stage to mutually enhance the robustness of both encoders.","Furthermore, unlike the conventional scheme that relies solely on the high-level features for the cross-modal alignment, we introduce a feature-based alignment scheme that enables the low-level to high-level features of the vision and language encoders to engage in the cross-modal alignment.","By aligning the intermediate cross-modal features in all encoder stages, this scheme leads to effective cross-modal fusion.","In this way, the proposed approach is simple but effective for referring image segmentation, and it outperforms the previous state-of-the-art methods on three public benchmarks."],"url":"http://arxiv.org/abs/2408.07539v1"}
{"created":"2024-08-14 13:14:27","title":"Usefulness of data flow diagrams and large language models for security threat validation: a registered report","abstract":"The arrival of recent cybersecurity standards has raised the bar for security assessments in organizations, but existing techniques don't always scale well. Threat analysis and risk assessment are used to identify security threats for new or refactored systems. Still, there is a lack of definition-of-done, so identified threats have to be validated which slows down the analysis. Existing literature has focused on the overall performance of threat analysis, but no previous work has investigated how deep must the analysts dig into the material before they can effectively validate the identified security threats. We propose a controlled experiment with practitioners to investigate whether some analysis material (like LLM-generated advice) is better than none and whether more material (the system's data flow diagram and LLM-generated advice) is better than some material. In addition, we present key findings from running a pilot with 41 MSc students, which are used to improve the study design. Finally, we also provide an initial replication package, including experimental material and data analysis scripts and a plan to extend it to include new materials based on the final data collection campaign with practitioners (e.g., pre-screening questions).","sentences":["The arrival of recent cybersecurity standards has raised the bar for security assessments in organizations, but existing techniques don't always scale well.","Threat analysis and risk assessment are used to identify security threats for new or refactored systems.","Still, there is a lack of definition-of-done, so identified threats have to be validated which slows down the analysis.","Existing literature has focused on the overall performance of threat analysis, but no previous work has investigated how deep must the analysts dig into the material before they can effectively validate the identified security threats.","We propose a controlled experiment with practitioners to investigate whether some analysis material (like LLM-generated advice) is better than none and whether more material (the system's data flow diagram and LLM-generated advice) is better than some material.","In addition, we present key findings from running a pilot with 41 MSc students, which are used to improve the study design.","Finally, we also provide an initial replication package, including experimental material and data analysis scripts and a plan to extend it to include new materials based on the final data collection campaign with practitioners (e.g., pre-screening questions)."],"url":"http://arxiv.org/abs/2408.07537v1"}
{"created":"2024-08-14 13:14:22","title":"Context-aware Container Orchestration in Serverless Edge Computing","abstract":"Adopting serverless computing to edge networks benefits end-users from the pay-as-you-use billing model and flexible scaling of applications. This paradigm extends the boundaries of edge computing and remarkably improves the quality of services. However, due to the heterogeneous nature of computing and bandwidth resources in edge networks, it is challenging to dynamically allocate different resources while adapting to the burstiness and high concurrency in serverless workloads. This article focuses on serverless function provisioning in edge networks to optimize end-to-end latency, where the challenge lies in jointly allocating wireless bandwidth and computing resources among heterogeneous computing nodes. To address this challenge, We devised a context-aware learning framework that adaptively orchestrates a wide spectrum of resources and jointly considers them to avoid resource fragmentation. Extensive simulation results justified that the proposed algorithm reduces over 95% of converge time while the end-to-end delay is comparable to the state of the art.","sentences":["Adopting serverless computing to edge networks benefits end-users from the pay-as-you-use billing model and flexible scaling of applications.","This paradigm extends the boundaries of edge computing and remarkably improves the quality of services.","However, due to the heterogeneous nature of computing and bandwidth resources in edge networks, it is challenging to dynamically allocate different resources while adapting to the burstiness and high concurrency in serverless workloads.","This article focuses on serverless function provisioning in edge networks to optimize end-to-end latency, where the challenge lies in jointly allocating wireless bandwidth and computing resources among heterogeneous computing nodes.","To address this challenge, We devised a context-aware learning framework that adaptively orchestrates a wide spectrum of resources and jointly considers them to avoid resource fragmentation.","Extensive simulation results justified that the proposed algorithm reduces over 95% of converge time while the end-to-end delay is comparable to the state of the art."],"url":"http://arxiv.org/abs/2408.07536v1"}
{"created":"2024-08-14 13:04:34","title":"Information-Theoretic Measures on Lattices for High-Order Interactions","abstract":"Traditional models reliant solely on pairwise associations often prove insufficient in capturing the complex statistical structure inherent in multivariate data. Yet existing methods for identifying information shared among groups of $d>3$ variables are often intractable; asymmetric around a target variable; or unable to consider all factorisations of the joint probability distribution. Here, we present a framework that systematically derives high-order measures using lattice and operator function pairs, whereby the lattice captures the algebraic relational structure of the variables and the operator function computes measures over the lattice. We show that many existing information-theoretic high-order measures can be derived by using divergences as operator functions on sublattices of the partition lattice, thus preventing the accurate quantification of all interactions for $d>3$. Similarly, we show that using the KL divergence as the operator function also leads to unwanted cancellation of interactions for $d>3$. To characterise all interactions among $d$ variables, we introduce the Streitberg information defined on the full partition lattice using generalisations of the KL divergence as operator functions. We validate our results numerically on synthetic data, and illustrate the use of the Streitberg information through applications to stock market returns and neural electrophysiology data.","sentences":["Traditional models reliant solely on pairwise associations often prove insufficient in capturing the complex statistical structure inherent in multivariate data.","Yet existing methods for identifying information shared among groups of $d>3$ variables are often intractable; asymmetric around a target variable; or unable to consider all factorisations of the joint probability distribution.","Here, we present a framework that systematically derives high-order measures using lattice and operator function pairs, whereby the lattice captures the algebraic relational structure of the variables and the operator function computes measures over the lattice.","We show that many existing information-theoretic high-order measures can be derived by using divergences as operator functions on sublattices of the partition lattice, thus preventing the accurate quantification of all interactions for $d>3$. Similarly, we show that using the KL divergence as the operator function also leads to unwanted cancellation of interactions for $d>3$. To characterise all interactions among $d$ variables, we introduce the Streitberg information defined on the full partition lattice using generalisations of the KL divergence as operator functions.","We validate our results numerically on synthetic data, and illustrate the use of the Streitberg information through applications to stock market returns and neural electrophysiology data."],"url":"http://arxiv.org/abs/2408.07533v1"}
{"created":"2024-08-14 13:03:41","title":"Development of a Multi-Agent Clinical Decision Support System for Korean Triage and Acuity Scale (KTAS)-Based Triage and Treatment Planning in Emergency Departments","abstract":"Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide. While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making. This study presents an LLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management.   We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM, orchestrated by CrewAI and Langchain. The system comprises four AI agents emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED Coordinator. It incorporates the Korean Triage and Acuity Scale (KTAS) for triage assessment and integrates with the RxNorm API for medication management.   The model was evaluated using the Asclepius dataset, with performance assessed by a clinical emergency medicine specialist. The CDSS demonstrated high accuracy in triage decision-making compared to the baseline of a single-agent system. Furthermore, the system exhibited strong performance in critical areas, including primary diagnosis, critical findings identification, disposition decision-making, treatment planning, and resource allocation.   Our multi-agent CDSS demonstrates significant potential for supporting comprehensive emergency care management. By leveraging state-of-the-art AI technologies, this system offers a scalable and adaptable tool that could enhance emergency medical care delivery, potentially alleviating ED overcrowding and improving patient outcomes. This work contributes to the growing field of AI applications in emergency medicine and offers a promising direction for future research and clinical implementation.","sentences":["Emergency department (ED) overcrowding and the complexity of rapid decision-making in critical care settings pose significant challenges to healthcare systems worldwide.","While clinical decision support systems (CDSS) have shown promise, the integration of large language models (LLMs) offers new possibilities for enhancing triage accuracy and clinical decision-making.","This study presents an LLM-driven CDSS designed to assist ED physicians and nurses in patient triage, treatment planning, and overall emergency care management.   ","We developed a multi-agent CDSS utilizing Llama-3-70b as the base LLM, orchestrated by CrewAI and Langchain.","The system comprises four AI agents emulating key ED roles: Triage Nurse, Emergency Physician, Pharmacist, and ED Coordinator.","It incorporates the Korean Triage and Acuity Scale (KTAS) for triage assessment and integrates with the RxNorm API for medication management.   ","The model was evaluated using the Asclepius dataset, with performance assessed by a clinical emergency medicine specialist.","The CDSS demonstrated high accuracy in triage decision-making compared to the baseline of a single-agent system.","Furthermore, the system exhibited strong performance in critical areas, including primary diagnosis, critical findings identification, disposition decision-making, treatment planning, and resource allocation.   ","Our multi-agent CDSS demonstrates significant potential for supporting comprehensive emergency care management.","By leveraging state-of-the-art AI technologies, this system offers a scalable and adaptable tool that could enhance emergency medical care delivery, potentially alleviating ED overcrowding and improving patient outcomes.","This work contributes to the growing field of AI applications in emergency medicine and offers a promising direction for future research and clinical implementation."],"url":"http://arxiv.org/abs/2408.07531v1"}
{"created":"2024-08-14 13:03:31","title":"Towards Real-time Video Compressive Sensing on Mobile Devices","abstract":"Video Snapshot Compressive Imaging (SCI) uses a low-speed 2D camera to capture high-speed scenes as snapshot compressed measurements, followed by a reconstruction algorithm to retrieve the high-speed video frames. The fast evolving mobile devices and existing high-performance video SCI reconstruction algorithms motivate us to develop mobile reconstruction methods for real-world applications. Yet, it is still challenging to deploy previous reconstruction algorithms on mobile devices due to the complex inference process, let alone real-time mobile reconstruction. To the best of our knowledge, there is no video SCI reconstruction model designed to run on the mobile devices. Towards this end, in this paper, we present an effective approach for video SCI reconstruction, dubbed MobileSCI, which can run at real-time speed on the mobile devices for the first time. Specifically, we first build a U-shaped 2D convolution-based architecture, which is much more efficient and mobile-friendly than previous state-of-the-art reconstruction methods. Besides, an efficient feature mixing block, based on the channel splitting and shuffling mechanisms, is introduced as a novel bottleneck block of our proposed MobileSCI to alleviate the computational burden. Finally, a customized knowledge distillation strategy is utilized to further improve the reconstruction quality. Extensive results on both simulated and real data show that our proposed MobileSCI can achieve superior reconstruction quality with high efficiency on the mobile devices. Particularly, we can reconstruct a 256 X 256 X 8 snapshot compressed measurement with real-time performance (about 35 FPS) on an iPhone 15. Code is available at https://github.com/mcao92/MobileSCI.","sentences":["Video Snapshot Compressive Imaging (SCI) uses a low-speed 2D camera to capture high-speed scenes as snapshot compressed measurements, followed by a reconstruction algorithm to retrieve the high-speed video frames.","The fast evolving mobile devices and existing high-performance video SCI reconstruction algorithms motivate us to develop mobile reconstruction methods for real-world applications.","Yet, it is still challenging to deploy previous reconstruction algorithms on mobile devices due to the complex inference process, let alone real-time mobile reconstruction.","To the best of our knowledge, there is no video SCI reconstruction model designed to run on the mobile devices.","Towards this end, in this paper, we present an effective approach for video SCI reconstruction, dubbed MobileSCI, which can run at real-time speed on the mobile devices for the first time.","Specifically, we first build a U-shaped 2D convolution-based architecture, which is much more efficient and mobile-friendly than previous state-of-the-art reconstruction methods.","Besides, an efficient feature mixing block, based on the channel splitting and shuffling mechanisms, is introduced as a novel bottleneck block of our proposed MobileSCI to alleviate the computational burden.","Finally, a customized knowledge distillation strategy is utilized to further improve the reconstruction quality.","Extensive results on both simulated and real data show that our proposed MobileSCI can achieve superior reconstruction quality with high efficiency on the mobile devices.","Particularly, we can reconstruct a 256 X 256 X 8 snapshot compressed measurement with real-time performance (about 35 FPS) on an iPhone 15.","Code is available at https://github.com/mcao92/MobileSCI."],"url":"http://arxiv.org/abs/2408.07530v1"}
{"created":"2024-08-14 13:02:20","title":"Evidential Graph Contrastive Alignment for Source-Free Blending-Target Domain Adaptation","abstract":"In this paper, we firstly tackle a more realistic Domain Adaptation (DA) setting: Source-Free Blending-Target Domain Adaptation (SF-BTDA), where we can not access to source domain data while facing mixed multiple target domains without any domain labels in prior. Compared to existing DA scenarios, SF-BTDA generally faces the co-existence of different label shifts in different targets, along with noisy target pseudo labels generated from the source model. In this paper, we propose a new method called Evidential Contrastive Alignment (ECA) to decouple the blending target domain and alleviate the effect from noisy target pseudo labels. First, to improve the quality of pseudo target labels, we propose a calibrated evidential learning module to iteratively improve both the accuracy and certainty of the resulting model and adaptively generate high-quality pseudo target labels. Second, we design a graph contrastive learning with the domain distance matrix and confidence-uncertainty criterion, to minimize the distribution gap of samples of a same class in the blended target domains, which alleviates the co-existence of different label shifts in blended targets. We conduct a new benchmark based on three standard DA datasets and ECA outperforms other methods with considerable gains and achieves comparable results compared with those that have domain labels or source data in prior.","sentences":["In this paper, we firstly tackle a more realistic Domain Adaptation (DA) setting: Source-Free Blending-Target Domain Adaptation (SF-BTDA), where we can not access to source domain data while facing mixed multiple target domains without any domain labels in prior.","Compared to existing DA scenarios, SF-BTDA generally faces the co-existence of different label shifts in different targets, along with noisy target pseudo labels generated from the source model.","In this paper, we propose a new method called Evidential Contrastive Alignment (ECA) to decouple the blending target domain and alleviate the effect from noisy target pseudo labels.","First, to improve the quality of pseudo target labels, we propose a calibrated evidential learning module to iteratively improve both the accuracy and certainty of the resulting model and adaptively generate high-quality pseudo target labels.","Second, we design a graph contrastive learning with the domain distance matrix and confidence-uncertainty criterion, to minimize the distribution gap of samples of a same class in the blended target domains, which alleviates the co-existence of different label shifts in blended targets.","We conduct a new benchmark based on three standard DA datasets and ECA outperforms other methods with considerable gains and achieves comparable results compared with those that have domain labels or source data in prior."],"url":"http://arxiv.org/abs/2408.07527v1"}
{"created":"2024-08-14 13:01:30","title":"Learning-based Models for Vulnerability Detection: An Extensive Study","abstract":"Though many deep learning-based models have made great progress in vulnerability detection, we have no good understanding of these models, which limits the further advancement of model capability, understanding of the mechanism of model detection, and efficiency and safety of practical application of models. In this paper, we extensively and comprehensively investigate two types of state-of-the-art learning-based approaches (sequence-based and graph-based) by conducting experiments on a recently built large-scale dataset. We investigate seven research questions from five dimensions, namely model capabilities, model interpretation, model stability, ease of use of model, and model economy. We experimentally demonstrate the priority of sequence-based models and the limited abilities of both LLM (ChatGPT) and graph-based models. We explore the types of vulnerability that learning-based models skilled in and reveal the instability of the models though the input is subtlely semantical-equivalently changed. We empirically explain what the models have learned. We summarize the pre-processing as well as requirements for easily using the models. Finally, we initially induce the vital information for economically and safely practical usage of these models.","sentences":["Though many deep learning-based models have made great progress in vulnerability detection, we have no good understanding of these models, which limits the further advancement of model capability, understanding of the mechanism of model detection, and efficiency and safety of practical application of models.","In this paper, we extensively and comprehensively investigate two types of state-of-the-art learning-based approaches (sequence-based and graph-based) by conducting experiments on a recently built large-scale dataset.","We investigate seven research questions from five dimensions, namely model capabilities, model interpretation, model stability, ease of use of model, and model economy.","We experimentally demonstrate the priority of sequence-based models and the limited abilities of both LLM (ChatGPT) and graph-based models.","We explore the types of vulnerability that learning-based models skilled in and reveal the instability of the models though the input is subtlely semantical-equivalently changed.","We empirically explain what the models have learned.","We summarize the pre-processing as well as requirements for easily using the models.","Finally, we initially induce the vital information for economically and safely practical usage of these models."],"url":"http://arxiv.org/abs/2408.07526v1"}
{"created":"2024-08-14 13:00:24","title":"Dinkel: Testing Graph Database Engines via State-Aware Query Generation","abstract":"Graph database management systems (GDBMSs) store and manipulate graph data and form a core part of many data-driven applications. To ensure their reliability, several approaches have been proposed to test GDBMSs by generating queries in Cypher, the most popular graph query language. However, Cypher allows queries with complicated state changes and data dependencies, which existing approaches do not support and thus fail to generate valid, complex queries, thereby missing many bugs in GDBMSs.   In this paper, we propose a novel state-aware testing approach to generate complex Cypher queries for GDBMSs. Our approach models two kinds of graph state, query context and graph schema. Query context describes the available Cypher variables and their corresponding scopes, whereas graph schema summarizes the manipulated graph labels and properties. While generating Cypher queries, we modify the graph states on the fly to ensure each clause within the query can reference the correct state information. In this way, our approach can generate Cypher queries with multiple state changes and complicated data dependencies while retaining high query validity. We implemented this approach as a fully automatic GDBMS testing framework, Dinkel, and evaluated it on three popular open-source GDBMSs, namely Neo4j, RedisGraph, and Apache AGE. In total, Dinkel found 60 bugs, among which 58 were confirmed and 51 fixed. Our evaluation results show that Dinkel can effectively generate complex queries with high validity (93.43%). Compared to existing approaches, Dinkel can cover over 60% more code and find more bugs within the 48-hour testing campaign. We expect Dinkel's powerful test-case generation to benefit GDBMS testing and help strengthen the reliability of GDBMSs.","sentences":["Graph database management systems (GDBMSs) store and manipulate graph data and form a core part of many data-driven applications.","To ensure their reliability, several approaches have been proposed to test GDBMSs by generating queries in Cypher, the most popular graph query language.","However, Cypher allows queries with complicated state changes and data dependencies, which existing approaches do not support and thus fail to generate valid, complex queries, thereby missing many bugs in GDBMSs.   ","In this paper, we propose a novel state-aware testing approach to generate complex Cypher queries for GDBMSs.","Our approach models two kinds of graph state, query context and graph schema.","Query context describes the available Cypher variables and their corresponding scopes, whereas graph schema summarizes the manipulated graph labels and properties.","While generating Cypher queries, we modify the graph states on the fly to ensure each clause within the query can reference the correct state information.","In this way, our approach can generate Cypher queries with multiple state changes and complicated data dependencies while retaining high query validity.","We implemented this approach as a fully automatic GDBMS testing framework, Dinkel, and evaluated it on three popular open-source GDBMSs, namely Neo4j, RedisGraph, and Apache AGE.","In total, Dinkel found 60 bugs, among which 58 were confirmed and 51 fixed.","Our evaluation results show that Dinkel can effectively generate complex queries with high validity (93.43%).","Compared to existing approaches, Dinkel can cover over 60% more code and find more bugs within the 48-hour testing campaign.","We expect Dinkel's powerful test-case generation to benefit GDBMS testing and help strengthen the reliability of GDBMSs."],"url":"http://arxiv.org/abs/2408.07525v1"}
{"created":"2024-08-14 12:58:22","title":"Fast Inference for Probabilistic Answer Set Programs via the Residual Program","abstract":"When we want to compute the probability of a query from a Probabilistic Answer Set Program, some parts of a program may not influence the probability of a query, but they impact on the size of the grounding. Identifying and removing them is crucial to speed up the computation. Algorithms for SLG resolution offer the possibility of returning the residual program which can be used for computing answer sets for normal programs that do have a total well-founded model. The residual program does not contain the parts of the program that do not influence the probability. In this paper, we propose to exploit the residual program for performing inference. Empirical results on graph datasets show that the approach leads to significantly faster inference.","sentences":["When we want to compute the probability of a query from a Probabilistic Answer Set Program, some parts of a program may not influence the probability of a query, but they impact on the size of the grounding.","Identifying and removing them is crucial to speed up the computation.","Algorithms for SLG resolution offer the possibility of returning the residual program which can be used for computing answer sets for normal programs that do have a total well-founded model.","The residual program does not contain the parts of the program that do not influence the probability.","In this paper, we propose to exploit the residual program for performing inference.","Empirical results on graph datasets show that the approach leads to significantly faster inference."],"url":"http://arxiv.org/abs/2408.07524v1"}
{"created":"2024-08-14 12:56:17","title":"Optimising MFCC parameters for the automatic detection of respiratory diseases","abstract":"Voice signals originating from the respiratory tract are utilized as valuable acoustic biomarkers for the diagnosis and assessment of respiratory diseases. Among the employed acoustic features, Mel Frequency Cepstral Coefficients (MFCC) is widely used for automatic analysis, with MFCC extraction commonly relying on default parameters. However, no comprehensive study has systematically investigated the impact of MFCC extraction parameters on respiratory disease diagnosis. In this study, we address this gap by examining the effects of key parameters, namely the number of coefficients, frame length, and hop length between frames, on respiratory condition examination. Our investigation uses four datasets: the Cambridge COVID-19 Sound database, the Coswara dataset, the Saarbrucken Voice Disorders (SVD) database, and a TACTICAS dataset. The Support Vector Machine (SVM) is employed as the classifier, given its widespread adoption and efficacy. Our findings indicate that the accuracy of MFCC decreases as hop length increases, and the optimal number of coefficients is observed to be approximately 30. The performance of MFCC varies with frame length across the datasets: for the COVID-19 datasets (Cambridge COVID-19 Sound database and Coswara dataset), performance declines with longer frame lengths, while for the SVD dataset, performance improves with increasing frame length (from 50 ms to 500 ms). Furthermore, we investigate the optimized combination of these parameters and observe substantial enhancements in accuracy. Compared to the worst combination, the SVM model achieves an accuracy of 81.1%, 80.6%, and 71.7%, with improvements of 19.6%, 16.10%, and 14.90% for the Cambridge COVID-19 Sound database, the Coswara dataset, and the SVD dataset respectively.","sentences":["Voice signals originating from the respiratory tract are utilized as valuable acoustic biomarkers for the diagnosis and assessment of respiratory diseases.","Among the employed acoustic features, Mel Frequency Cepstral Coefficients (MFCC) is widely used for automatic analysis, with MFCC extraction commonly relying on default parameters.","However, no comprehensive study has systematically investigated the impact of MFCC extraction parameters on respiratory disease diagnosis.","In this study, we address this gap by examining the effects of key parameters, namely the number of coefficients, frame length, and hop length between frames, on respiratory condition examination.","Our investigation uses four datasets: the Cambridge COVID-19 Sound database, the Coswara dataset, the Saarbrucken Voice Disorders (SVD) database, and a TACTICAS dataset.","The Support Vector Machine (SVM) is employed as the classifier, given its widespread adoption and efficacy.","Our findings indicate that the accuracy of MFCC decreases as hop length increases, and the optimal number of coefficients is observed to be approximately 30.","The performance of MFCC varies with frame length across the datasets: for the COVID-19 datasets (Cambridge COVID-19 Sound database and Coswara dataset), performance declines with longer frame lengths, while for the SVD dataset, performance improves with increasing frame length (from 50 ms to 500 ms).","Furthermore, we investigate the optimized combination of these parameters and observe substantial enhancements in accuracy.","Compared to the worst combination, the SVM model achieves an accuracy of 81.1%, 80.6%, and 71.7%, with improvements of 19.6%, 16.10%, and 14.90% for the Cambridge COVID-19 Sound database, the Coswara dataset, and the SVD dataset respectively."],"url":"http://arxiv.org/abs/2408.07522v1"}
{"created":"2024-08-14 12:54:26","title":"Optimising Dynamic Traffic Distribution for Urban Networks with Answer Set Programming","abstract":"Answer Set Programming (ASP) has demonstrated its potential as an effective tool for concisely representing and reasoning about real-world problems. In this paper, we present an application in which ASP has been successfully used in the context of dynamic traffic distribution for urban networks, within a more general framework devised for solving such a real-world problem. In particular, ASP has been employed for the computation of the \"optimal\" routes for all the vehicles in the network. We also provide an empirical analysis of the performance of the whole framework, and of its part in which ASP is employed, on two European urban areas, which shows the viability of the framework and the contribution ASP can give.","sentences":["Answer Set Programming (ASP) has demonstrated its potential as an effective tool for concisely representing and reasoning about real-world problems.","In this paper, we present an application in which ASP has been successfully used in the context of dynamic traffic distribution for urban networks, within a more general framework devised for solving such a real-world problem.","In particular, ASP has been employed for the computation of the \"optimal\" routes for all the vehicles in the network.","We also provide an empirical analysis of the performance of the whole framework, and of its part in which ASP is employed, on two European urban areas, which shows the viability of the framework and the contribution ASP can give."],"url":"http://arxiv.org/abs/2408.07521v1"}
{"created":"2024-08-14 12:52:13","title":"Whitening Consistently Improves Self-Supervised Learning","abstract":"Self-supervised learning (SSL) has been shown to be a powerful approach for learning visual representations. In this study, we propose incorporating ZCA whitening as the final layer of the encoder in self-supervised learning to enhance the quality of learned features by normalizing and decorrelating them. Although whitening has been utilized in SSL in previous works, its potential to universally improve any SSL model has not been explored. We demonstrate that adding whitening as the last layer of SSL pretrained encoders is independent of the self-supervised learning method and encoder architecture, thus it improves performance for a wide range of SSL methods across multiple encoder architectures and datasets. Our experiments show that whitening is capable of improving linear and k-NN probing accuracy by 1-5%. Additionally, we propose metrics that allow for a comprehensive analysis of the learned features, provide insights into the quality of the representations and help identify collapse patterns.","sentences":["Self-supervised learning (SSL) has been shown to be a powerful approach for learning visual representations.","In this study, we propose incorporating ZCA whitening as the final layer of the encoder in self-supervised learning to enhance the quality of learned features by normalizing and decorrelating them.","Although whitening has been utilized in SSL in previous works, its potential to universally improve any SSL model has not been explored.","We demonstrate that adding whitening as the last layer of SSL pretrained encoders is independent of the self-supervised learning method and encoder architecture, thus it improves performance for a wide range of SSL methods across multiple encoder architectures and datasets.","Our experiments show that whitening is capable of improving linear and k-NN probing accuracy by 1-5%.","Additionally, we propose metrics that allow for a comprehensive analysis of the learned features, provide insights into the quality of the representations and help identify collapse patterns."],"url":"http://arxiv.org/abs/2408.07519v1"}
{"created":"2024-08-14 12:49:58","title":"Advancing Spatio-Temporal Processing in Spiking Neural Networks through Adaptation","abstract":"Efficient implementations of spiking neural networks on neuromorphic hardware promise orders of magnitude less power consumption than their non-spiking counterparts. The standard neuron model for spike-based computation on such neuromorphic systems has long been the leaky integrate-and-fire (LIF) neuron. As a promising advancement, a computationally light augmentation of the LIF neuron model with an adaptation mechanism experienced a recent upswing in popularity, caused by demonstrations of its superior performance on spatio-temporal processing tasks. The root of the superiority of these so-called adaptive LIF neurons however, is not well understood. In this article, we thoroughly analyze the dynamical, computational, and learning properties of adaptive LIF neurons and networks thereof. We find that the frequently observed stability problems during training of such networks can be overcome by applying an alternative discretization method that results in provably better stability properties than the commonly used Euler-Forward method. With this discretization, we achieved a new state-of-the-art performance on common event-based benchmark datasets. We also show that the superiority of networks of adaptive LIF neurons extends to the prediction and generation of complex time series. Our further analysis of the computational properties of networks of adaptive LIF neurons shows that they are particularly well suited to exploit the spatio-temporal structure of input sequences. Furthermore, these networks are surprisingly robust to shifts of the mean input strength and input spike rate, even when these shifts were not observed during training. As a consequence, high-performance networks can be obtained without any normalization techniques such as batch normalization or batch-normalization through time.","sentences":["Efficient implementations of spiking neural networks on neuromorphic hardware promise orders of magnitude less power consumption than their non-spiking counterparts.","The standard neuron model for spike-based computation on such neuromorphic systems has long been the leaky integrate-and-fire (LIF) neuron.","As a promising advancement, a computationally light augmentation of the LIF neuron model with an adaptation mechanism experienced a recent upswing in popularity, caused by demonstrations of its superior performance on spatio-temporal processing tasks.","The root of the superiority of these so-called adaptive LIF neurons however, is not well understood.","In this article, we thoroughly analyze the dynamical, computational, and learning properties of adaptive LIF neurons and networks thereof.","We find that the frequently observed stability problems during training of such networks can be overcome by applying an alternative discretization method that results in provably better stability properties than the commonly used Euler-Forward method.","With this discretization, we achieved a new state-of-the-art performance on common event-based benchmark datasets.","We also show that the superiority of networks of adaptive LIF neurons extends to the prediction and generation of complex time series.","Our further analysis of the computational properties of networks of adaptive LIF neurons shows that they are particularly well suited to exploit the spatio-temporal structure of input sequences.","Furthermore, these networks are surprisingly robust to shifts of the mean input strength and input spike rate, even when these shifts were not observed during training.","As a consequence, high-performance networks can be obtained without any normalization techniques such as batch normalization or batch-normalization through time."],"url":"http://arxiv.org/abs/2408.07517v1"}
{"created":"2024-08-14 12:49:50","title":"DIffSteISR: Harnessing Diffusion Prior for Superior Real-world Stereo Image Super-Resolution","abstract":"We introduce DiffSteISR, a pioneering framework for reconstructing real-world stereo images. DiffSteISR utilizes the powerful prior knowledge embedded in pre-trained text-to-image model to efficiently recover the lost texture details in low-resolution stereo images. Specifically, DiffSteISR implements a time-aware stereo cross attention with temperature adapter (TASCATA) to guide the diffusion process, ensuring that the generated left and right views exhibit high texture consistency thereby reducing disparity error between the super-resolved images and the ground truth (GT) images. Additionally, a stereo omni attention control network (SOA ControlNet) is proposed to enhance the consistency of super-resolved images with GT images in the pixel, perceptual, and distribution space. Finally, DiffSteISR incorporates a stereo semantic extractor (SSE) to capture unique viewpoint soft semantic information and shared hard tag semantic information, thereby effectively improving the semantic accuracy and consistency of the generated left and right images. Extensive experimental results demonstrate that DiffSteISR accurately reconstructs natural and precise textures from low-resolution stereo images while maintaining a high consistency of semantic and texture between the left and right views.","sentences":["We introduce DiffSteISR, a pioneering framework for reconstructing real-world stereo images.","DiffSteISR utilizes the powerful prior knowledge embedded in pre-trained text-to-image model to efficiently recover the lost texture details in low-resolution stereo images.","Specifically, DiffSteISR implements a time-aware stereo cross attention with temperature adapter (TASCATA) to guide the diffusion process, ensuring that the generated left and right views exhibit high texture consistency thereby reducing disparity error between the super-resolved images and the ground truth (GT) images.","Additionally, a stereo omni attention control network (SOA ControlNet) is proposed to enhance the consistency of super-resolved images with GT images in the pixel, perceptual, and distribution space.","Finally, DiffSteISR incorporates a stereo semantic extractor (SSE) to capture unique viewpoint soft semantic information and shared hard tag semantic information, thereby effectively improving the semantic accuracy and consistency of the generated left and right images.","Extensive experimental results demonstrate that DiffSteISR accurately reconstructs natural and precise textures from low-resolution stereo images while maintaining a high consistency of semantic and texture between the left and right views."],"url":"http://arxiv.org/abs/2408.07516v1"}
{"created":"2024-08-14 12:48:37","title":"CNN-JEPA: Self-Supervised Pretraining Convolutional Neural Networks Using Joint Embedding Predictive Architecture","abstract":"Self-supervised learning (SSL) has become an important approach in pretraining large neural networks, enabling unprecedented scaling of model and dataset sizes. While recent advances like I-JEPA have shown promising results for Vision Transformers, adapting such methods to Convolutional Neural Networks (CNNs) presents unique challenges. In this paper, we introduce CNN-JEPA, a novel SSL method that successfully applies the joint embedding predictive architecture approach to CNNs. Our method incorporates a sparse CNN encoder to handle masked inputs, a fully convolutional predictor using depthwise separable convolutions, and an improved masking strategy. We demonstrate that CNN-JEPA outperforms I-JEPA with ViT architectures on ImageNet-100, achieving 73.3% linear top-1 accuracy with a standard ResNet-50 encoder. Compared to other CNN-based SSL methods, CNN-JEPA requires 17-35% less training time for the same number of epochs and approaches the linear and k-NN top-1 accuracies of BYOL, SimCLR, and VICReg. Our approach offers a simpler, more efficient alternative to existing SSL methods for CNNs, requiring minimal augmentations and no separate projector network.","sentences":["Self-supervised learning (SSL) has become an important approach in pretraining large neural networks, enabling unprecedented scaling of model and dataset sizes.","While recent advances like I-JEPA have shown promising results for Vision Transformers, adapting such methods to Convolutional Neural Networks (CNNs) presents unique challenges.","In this paper, we introduce CNN-JEPA, a novel SSL method that successfully applies the joint embedding predictive architecture approach to CNNs.","Our method incorporates a sparse CNN encoder to handle masked inputs, a fully convolutional predictor using depthwise separable convolutions, and an improved masking strategy.","We demonstrate that CNN-JEPA outperforms I-JEPA with ViT architectures on ImageNet-100, achieving 73.3% linear top-1 accuracy with a standard ResNet-50 encoder.","Compared to other CNN-based SSL methods, CNN-JEPA requires 17-35% less training time for the same number of epochs and approaches the linear and k-NN top-1 accuracies of BYOL, SimCLR, and VICReg.","Our approach offers a simpler, more efficient alternative to existing SSL methods for CNNs, requiring minimal augmentations and no separate projector network."],"url":"http://arxiv.org/abs/2408.07514v1"}
{"created":"2024-08-14 12:48:00","title":"Image Scaling Attack Simulation: A Measure of Stealth and Detectability","abstract":"Cybersecurity practices require effort to be maintained, and one weakness is a lack of awareness regarding potential attacks not only in the usage of machine learning models, but also in their development process. Previous studies have determined that preprocessing attacks, such as image scaling attacks, have been difficult to detect by humans (through visual response) and computers (through entropic algorithms). However, these studies fail to address the real-world performance and detectability of these attacks. The purpose of this work is to analyze the relationship between awareness of image scaling attacks with respect to demographic background and experience. We conduct a survey where we gather the subjects' demographics, analyze the subjects' experience in cybersecurity, record their responses to a poorly-performing convolutional neural network model that has been unknowingly hindered by an image scaling attack of a used dataset, and document their reactions after it is revealed that the images used within the broken models have been attacked. We find in this study that the overall detection rate of the attack is low enough to be viable in a workplace or academic setting, and even after discovery, subjects cannot conclusively determine benign images from attacked images.","sentences":["Cybersecurity practices require effort to be maintained, and one weakness is a lack of awareness regarding potential attacks not only in the usage of machine learning models, but also in their development process.","Previous studies have determined that preprocessing attacks, such as image scaling attacks, have been difficult to detect by humans (through visual response) and computers (through entropic algorithms).","However, these studies fail to address the real-world performance and detectability of these attacks.","The purpose of this work is to analyze the relationship between awareness of image scaling attacks with respect to demographic background and experience.","We conduct a survey where we gather the subjects' demographics, analyze the subjects' experience in cybersecurity, record their responses to a poorly-performing convolutional neural network model that has been unknowingly hindered by an image scaling attack of a used dataset, and document their reactions after it is revealed that the images used within the broken models have been attacked.","We find in this study that the overall detection rate of the attack is low enough to be viable in a workplace or academic setting, and even after discovery, subjects cannot conclusively determine benign images from attacked images."],"url":"http://arxiv.org/abs/2408.07513v1"}
{"created":"2024-08-14 12:40:57","title":"Protected Test-Time Adaptation via Online Entropy Matching: A Betting Approach","abstract":"We present a novel approach for test-time adaptation via online self-training, consisting of two components. First, we introduce a statistical framework that detects distribution shifts in the classifier's entropy values obtained on a stream of unlabeled samples. Second, we devise an online adaptation mechanism that utilizes the evidence of distribution shifts captured by the detection tool to dynamically update the classifier's parameters. The resulting adaptation process drives the distribution of test entropy values obtained from the self-trained classifier to match those of the source domain, building invariance to distribution shifts. This approach departs from the conventional self-training method, which focuses on minimizing the classifier's entropy. Our approach combines concepts in betting martingales and online learning to form a detection tool capable of quickly reacting to distribution shifts. We then reveal a tight relation between our adaptation scheme and optimal transport, which forms the basis of our novel self-supervised loss. Experimental results demonstrate that our approach improves test-time accuracy under distribution shifts while maintaining accuracy and calibration in their absence, outperforming leading entropy minimization methods across various scenarios.","sentences":["We present a novel approach for test-time adaptation via online self-training, consisting of two components.","First, we introduce a statistical framework that detects distribution shifts in the classifier's entropy values obtained on a stream of unlabeled samples.","Second, we devise an online adaptation mechanism that utilizes the evidence of distribution shifts captured by the detection tool to dynamically update the classifier's parameters.","The resulting adaptation process drives the distribution of test entropy values obtained from the self-trained classifier to match those of the source domain, building invariance to distribution shifts.","This approach departs from the conventional self-training method, which focuses on minimizing the classifier's entropy.","Our approach combines concepts in betting martingales and online learning to form a detection tool capable of quickly reacting to distribution shifts.","We then reveal a tight relation between our adaptation scheme and optimal transport, which forms the basis of our novel self-supervised loss.","Experimental results demonstrate that our approach improves test-time accuracy under distribution shifts while maintaining accuracy and calibration in their absence, outperforming leading entropy minimization methods across various scenarios."],"url":"http://arxiv.org/abs/2408.07511v1"}
{"created":"2024-08-14 12:38:12","title":"Dominating Set Reconfiguration with Answer Set Programming","abstract":"The dominating set reconfiguration problem is defined as determining, for a given dominating set problem and two among its feasible solutions, whether one is reachable from the other via a sequence of feasible solutions subject to a certain adjacency relation. This problem is PSPACE-complete in general. The concept of the dominating set is known to be quite useful for analyzing wireless networks, social networks, and sensor networks. We develop an approach to solve the dominating set reconfiguration problem based on Answer Set Programming (ASP). Our declarative approach relies on a high-level ASP encoding, and both the grounding and solving tasks are delegated to an ASP-based combinatorial reconfiguration solver. To evaluate the effectiveness of our approach, we conduct experiments on a newly created benchmark set.","sentences":["The dominating set reconfiguration problem is defined as determining, for a given dominating set problem and two among its feasible solutions, whether one is reachable from the other via a sequence of feasible solutions subject to a certain adjacency relation.","This problem is PSPACE-complete in general.","The concept of the dominating set is known to be quite useful for analyzing wireless networks, social networks, and sensor networks.","We develop an approach to solve the dominating set reconfiguration problem based on Answer Set Programming (ASP).","Our declarative approach relies on a high-level ASP encoding, and both the grounding and solving tasks are delegated to an ASP-based combinatorial reconfiguration solver.","To evaluate the effectiveness of our approach, we conduct experiments on a newly created benchmark set."],"url":"http://arxiv.org/abs/2408.07510v1"}
{"created":"2024-08-14 12:36:12","title":"Non-Gaited Legged Locomotion with Monte-Carlo Tree Search and Supervised Learning","abstract":"Legged robots are able to navigate complex terrains by continuously interacting with the environment through careful selection of contact sequences and timings. However, the combinatorial nature behind contact planning hinders the applicability of such optimization problems on hardware. In this work, we present a novel approach that optimizes gait sequences and respective timings for legged robots in the context of optimization-based controllers through the use of sampling-based methods and supervised learning techniques. We propose to bootstrap the search by learning an optimal value function in order to speed-up the gait planning procedure making it applicable in real-time. To validate our proposed method, we showcase its performance both in simulation and on hardware using a 22 kg electric quadruped robot. The method is assessed on different terrains, under external perturbations, and in comparison to a standard control approach where the gait sequence is fixed a priori.","sentences":["Legged robots are able to navigate complex terrains by continuously interacting with the environment through careful selection of contact sequences and timings.","However, the combinatorial nature behind contact planning hinders the applicability of such optimization problems on hardware.","In this work, we present a novel approach that optimizes gait sequences and respective timings for legged robots in the context of optimization-based controllers through the use of sampling-based methods and supervised learning techniques.","We propose to bootstrap the search by learning an optimal value function in order to speed-up the gait planning procedure making it applicable in real-time.","To validate our proposed method, we showcase its performance both in simulation and on hardware using a 22 kg electric quadruped robot.","The method is assessed on different terrains, under external perturbations, and in comparison to a standard control approach where the gait sequence is fixed a priori."],"url":"http://arxiv.org/abs/2408.07508v1"}
{"created":"2024-08-14 12:32:41","title":"Large Language Models Know What Makes Exemplary Contexts","abstract":"In-context learning (ICL) has proven to be a significant capability with the advancement of Large Language models (LLMs). By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without needing to update millions of parameters. This paper presents a unified framework for LLMs that allows them to self-select influential in-context examples to compose their contexts; self-rank candidates with different demonstration compositions; self-optimize the demonstration selection and ordering through reinforcement learning. Specifically, our method designs a parameter-efficient retrieval head that generates the optimized demonstration after training with rewards from LLM's own preference. Experimental results validate the proposed method's effectiveness in enhancing ICL performance. Additionally, our approach effectively identifies and selects the most representative examples for the current task, and includes more diversity in retrieval.","sentences":["In-context learning (ICL) has proven to be a significant capability with the advancement of Large Language models (LLMs).","By instructing LLMs using few-shot demonstrative examples, ICL enables them to perform a wide range of tasks without needing to update millions of parameters.","This paper presents a unified framework for LLMs that allows them to self-select influential in-context examples to compose their contexts; self-rank candidates with different demonstration compositions; self-optimize the demonstration selection and ordering through reinforcement learning.","Specifically, our method designs a parameter-efficient retrieval head that generates the optimized demonstration after training with rewards from LLM's own preference.","Experimental results validate the proposed method's effectiveness in enhancing ICL performance.","Additionally, our approach effectively identifies and selects the most representative examples for the current task, and includes more diversity in retrieval."],"url":"http://arxiv.org/abs/2408.07505v1"}
{"created":"2024-08-14 12:29:49","title":"Cross-Platform Video Person ReID: A New Benchmark Dataset and Adaptation Approach","abstract":"In this paper, we construct a large-scale benchmark dataset for Ground-to-Aerial Video-based person Re-Identification, named G2A-VReID, which comprises 185,907 images and 5,576 tracklets, featuring 2,788 distinct identities. To our knowledge, this is the first dataset for video ReID under Ground-to-Aerial scenarios. G2A-VReID dataset has the following characteristics: 1) Drastic view changes; 2) Large number of annotated identities; 3) Rich outdoor scenarios; 4) Huge difference in resolution. Additionally, we propose a new benchmark approach for cross-platform ReID by transforming the cross-platform visual alignment problem into visual-semantic alignment through vision-language model (i.e., CLIP) and applying a parameter-efficient Video Set-Level-Adapter module to adapt image-based foundation model to video ReID tasks, termed VSLA-CLIP. Besides, to further reduce the great discrepancy across the platforms, we also devise the platform-bridge prompts for efficient visual feature alignment. Extensive experiments demonstrate the superiority of the proposed method on all existing video ReID datasets and our proposed G2A-VReID dataset.","sentences":["In this paper, we construct a large-scale benchmark dataset for Ground-to-Aerial Video-based person Re-Identification, named G2A-VReID, which comprises 185,907 images and 5,576 tracklets, featuring 2,788 distinct identities.","To our knowledge, this is the first dataset for video ReID under Ground-to-Aerial scenarios.","G2A-VReID dataset has the following characteristics: 1) Drastic view changes; 2) Large number of annotated identities; 3) Rich outdoor scenarios; 4) Huge difference in resolution.","Additionally, we propose a new benchmark approach for cross-platform ReID by transforming the cross-platform visual alignment problem into visual-semantic alignment through vision-language model (i.e., CLIP) and applying a parameter-efficient Video Set-Level-Adapter module to adapt image-based foundation model to video ReID tasks, termed VSLA-CLIP.","Besides, to further reduce the great discrepancy across the platforms, we also devise the platform-bridge prompts for efficient visual feature alignment.","Extensive experiments demonstrate the superiority of the proposed method on all existing video ReID datasets and our proposed G2A-VReID dataset."],"url":"http://arxiv.org/abs/2408.07500v1"}
{"created":"2024-08-14 12:21:04","title":"A First Look at Related Website Sets","abstract":"We present the first measurement of the user-effect and privacy impact of \"Related Website Sets,\" a recent proposal to reduce browser privacy protections between two sites if those sites are related to each other. An assumption (both explicitly and implicitly) underpinning the Related Website Sets proposal is that users can accurately determine if two sites are related via the same entity. In this work, we probe this assumption via measurements and a user study of 30 participants, to assess the ability of Web users to determine if two sites are (according to the Related Website Sets feature) related to each other. We find that this is largely not the case. Our findings indicate that 42 (36.8%) of the user determinations in our study are incorrect in privacy-harming ways, where users think that sites are not related, but would be treated as related (and so due less privacy protections) by the Related Website Sets feature. Additionally, 22 (73.3%) of participants made at least one incorrect evaluation during the study. We also characterise the Related Website Sets list, its composition over time, and its governance.","sentences":["We present the first measurement of the user-effect and privacy impact of \"Related Website Sets,\" a recent proposal to reduce browser privacy protections between two sites if those sites are related to each other.","An assumption (both explicitly and implicitly) underpinning the Related Website Sets proposal is that users can accurately determine if two sites are related via the same entity.","In this work, we probe this assumption via measurements and a user study of 30 participants, to assess the ability of Web users to determine if two sites are (according to the Related Website Sets feature) related to each other.","We find that this is largely not the case.","Our findings indicate that 42 (36.8%) of the user determinations in our study are incorrect in privacy-harming ways, where users think that sites are not related, but would be treated as related (and so due less privacy protections) by the Related Website Sets feature.","Additionally, 22 (73.3%) of participants made at least one incorrect evaluation during the study.","We also characterise the Related Website Sets list, its composition over time, and its governance."],"url":"http://arxiv.org/abs/2408.07495v1"}
{"created":"2024-08-14 12:19:25","title":"QirK: Question Answering via Intermediate Representation on Knowledge Graphs","abstract":"We demonstrate QirK, a system for answering natural language questions on Knowledge Graphs (KG). QirK can answer structurally complex questions that are still beyond the reach of emerging Large Language Models (LLMs). It does so using a unique combination of database technology, LLMs, and semantic search over vector embeddings. The glue for these components is an intermediate representation (IR). The input question is mapped to IR using LLMs, which is then repaired into a valid relational database query with the aid of a semantic search on vector embeddings. This allows a practical synthesis of LLM capabilities and KG reliability.   A short video demonstrating QirK is available at https://youtu.be/6c81BLmOZ0U.","sentences":["We demonstrate QirK, a system for answering natural language questions on Knowledge Graphs (KG).","QirK can answer structurally complex questions that are still beyond the reach of emerging Large Language Models (LLMs).","It does so using a unique combination of database technology, LLMs, and semantic search over vector embeddings.","The glue for these components is an intermediate representation (IR).","The input question is mapped to IR using LLMs, which is then repaired into a valid relational database query with the aid of a semantic search on vector embeddings.","This allows a practical synthesis of LLM capabilities and KG reliability.   ","A short video demonstrating QirK is available at https://youtu.be/6c81BLmOZ0U."],"url":"http://arxiv.org/abs/2408.07494v1"}
