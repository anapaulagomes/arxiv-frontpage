{"created":"2024-11-05 18:59:51","title":"MME-Finance: A Multimodal Finance Benchmark for Expert-level Understanding and Reasoning","abstract":"In recent years, multimodal benchmarks for general domains have guided the rapid development of multimodal models on general tasks. However, the financial field has its peculiarities. It features unique graphical images (e.g., candlestick charts, technical indicator charts) and possesses a wealth of specialized financial knowledge (e.g., futures, turnover rate). Therefore, benchmarks from general fields often fail to measure the performance of multimodal models in the financial domain, and thus cannot effectively guide the rapid development of large financial models. To promote the development of large financial multimodal models, we propose MME-Finance, an bilingual open-ended and practical usage-oriented Visual Question Answering (VQA) benchmark. The characteristics of our benchmark are finance and expertise, which include constructing charts that reflect the actual usage needs of users (e.g., computer screenshots and mobile photography), creating questions according to the preferences in financial domain inquiries, and annotating questions by experts with 10+ years of experience in the financial industry. Additionally, we have developed a custom-designed financial evaluation system in which visual information is first introduced in the multi-modal evaluation process. Extensive experimental evaluations of 19 mainstream MLLMs are conducted to test their perception, reasoning, and cognition capabilities. The results indicate that models performing well on general benchmarks cannot do well on MME-Finance; for instance, the top-performing open-source and closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o), respectively. Their performance is particularly poor in categories most relevant to finance, such as candlestick charts and technical indicator charts. In addition, we propose a Chinese version, which helps compare performance of MLLMs under a Chinese context.","sentences":["In recent years, multimodal benchmarks for general domains have guided the rapid development of multimodal models on general tasks.","However, the financial field has its peculiarities.","It features unique graphical images (e.g., candlestick charts, technical indicator charts) and possesses a wealth of specialized financial knowledge (e.g., futures, turnover rate).","Therefore, benchmarks from general fields often fail to measure the performance of multimodal models in the financial domain, and thus cannot effectively guide the rapid development of large financial models.","To promote the development of large financial multimodal models, we propose MME-Finance, an bilingual open-ended and practical usage-oriented Visual Question Answering (VQA) benchmark.","The characteristics of our benchmark are finance and expertise, which include constructing charts that reflect the actual usage needs of users (e.g., computer screenshots and mobile photography), creating questions according to the preferences in financial domain inquiries, and annotating questions by experts with 10+ years of experience in the financial industry.","Additionally, we have developed a custom-designed financial evaluation system in which visual information is first introduced in the multi-modal evaluation process.","Extensive experimental evaluations of 19 mainstream MLLMs are conducted to test their perception, reasoning, and cognition capabilities.","The results indicate that models performing well on general benchmarks cannot do well on MME-Finance; for instance, the top-performing open-source and closed-source models obtain 65.69 (Qwen2VL-72B) and 63.18 (GPT-4o), respectively.","Their performance is particularly poor in categories most relevant to finance, such as candlestick charts and technical indicator charts.","In addition, we propose a Chinese version, which helps compare performance of MLLMs under a Chinese context."],"url":"http://arxiv.org/abs/2411.03314v1"}
{"created":"2024-11-05 18:58:15","title":"Classification Done Right for Vision-Language Pre-Training","abstract":"We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data. Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised classification labels, without the need for additional text filtering or selection. Due to the absence of the text encoding as contrastive target, SuperClass does not require a text encoder and does not need to maintain a large batch size as CLIP does. SuperClass demonstrated superior performance on various downstream tasks, including classic computer vision benchmarks and vision language downstream tasks. We further explored the scaling behavior of SuperClass on model size, training length, or data size, and reported encouraging results and comparisons to CLIP. https://github.com/x-cls/superclass","sentences":["We introduce SuperClass, a super simple classification method for vision-language pre-training on image-text data.","Unlike its contrastive counterpart CLIP who contrast with a text encoder, SuperClass directly utilizes tokenized raw text as supervised classification labels, without the need for additional text filtering or selection.","Due to the absence of the text encoding as contrastive target, SuperClass does not require a text encoder and does not need to maintain a large batch size as CLIP does.","SuperClass demonstrated superior performance on various downstream tasks, including classic computer vision benchmarks and vision language downstream tasks.","We further explored the scaling behavior of SuperClass on model size, training length, or data size, and reported encouraging results and comparisons to CLIP.","https://github.com/x-cls/superclass"],"url":"http://arxiv.org/abs/2411.03313v1"}
{"created":"2024-11-05 18:54:21","title":"Inference Optimal VLMs Need Only One Visual Token but Larger Models","abstract":"Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks. However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM. To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression. However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance. We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors. Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token. While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., $5-10\\times$), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios. Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings. Code is available at https://github.com/locuslab/llava-token-compression.","sentences":["Vision Language Models (VLMs) have demonstrated strong capabilities across various visual understanding and reasoning tasks.","However, their real-world deployment is often constrained by high latency during inference due to substantial compute required to process the large number of input tokens (predominantly from the image) by the LLM.","To reduce inference costs, one can either downsize the LLM or reduce the number of input image-tokens, the latter of which has been the focus of many recent works around token compression.","However, it is unclear what the optimal trade-off is, as both the factors directly affect the VLM performance.","We first characterize this optimal trade-off between the number of visual tokens and LLM parameters by establishing scaling laws that capture variations in performance with these two factors.","Our results reveal a surprising trend: for visual reasoning tasks, the inference-optimal behavior in VLMs, i.e., minimum downstream error at any given fixed inference compute, is achieved when using the largest LLM that fits within the inference budget while minimizing visual token count - often to a single token.","While the token reduction literature has mainly focused on maintaining base model performance by modestly reducing the token count (e.g., $5-10\\times$), our results indicate that the compute-optimal inference regime requires operating under even higher token compression ratios.","Based on these insights, we take some initial steps towards building approaches tailored for high token compression settings.","Code is available at https://github.com/locuslab/llava-token-compression."],"url":"http://arxiv.org/abs/2411.03312v1"}
{"created":"2024-11-05 18:01:12","title":"LLMs for Domain Generation Algorithm Detection","abstract":"This work analyzes the use of large language models (LLMs) for detecting domain generation algorithms (DGAs). We perform a detailed evaluation of two important techniques: In-Context Learning (ICL) and Supervised Fine-Tuning (SFT), showing how they can improve detection. SFT increases performance by using domain-specific data, whereas ICL helps the detection model to quickly adapt to new threats without requiring much retraining. We use Meta's Llama3 8B model, on a custom dataset with 68 malware families and normal domains, covering several hard-to-detect schemes, including recent word-based DGAs. Results proved that LLM-based methods can achieve competitive results in DGA detection. In particular, the SFT-based LLM DGA detector outperforms state-of-the-art models using attention layers, achieving 94% accuracy with a 4% false positive rate (FPR) and excelling at detecting word-based DGA domains.","sentences":["This work analyzes the use of large language models (LLMs) for detecting domain generation algorithms (DGAs).","We perform a detailed evaluation of two important techniques: In-Context Learning (ICL) and Supervised Fine-Tuning (SFT), showing how they can improve detection.","SFT increases performance by using domain-specific data, whereas ICL helps the detection model to quickly adapt to new threats without requiring much retraining.","We use Meta's Llama3 8B model, on a custom dataset with 68 malware families and normal domains, covering several hard-to-detect schemes, including recent word-based DGAs.","Results proved that LLM-based methods can achieve competitive results in DGA detection.","In particular, the SFT-based LLM DGA detector outperforms state-of-the-art models using attention layers, achieving 94% accuracy with a 4% false positive rate (FPR) and excelling at detecting word-based DGA domains."],"url":"http://arxiv.org/abs/2411.03307v1"}
{"created":"2024-11-05 17:56:27","title":"Monocular Event-Based Vision for Obstacle Avoidance with a Quadrotor","abstract":"We present the first static-obstacle avoidance method for quadrotors using just an onboard, monocular event camera. Quadrotors are capable of fast and agile flight in cluttered environments when piloted manually, but vision-based autonomous flight in unknown environments is difficult in part due to the sensor limitations of traditional onboard cameras. Event cameras, however, promise nearly zero motion blur and high dynamic range, but produce a very large volume of events under significant ego-motion and further lack a continuous-time sensor model in simulation, making direct sim-to-real transfer not possible. By leveraging depth prediction as a pretext task in our learning framework, we can pre-train a reactive obstacle avoidance events-to-control policy with approximated, simulated events and then fine-tune the perception component with limited events-and-depth real-world data to achieve obstacle avoidance in indoor and outdoor settings. We demonstrate this across two quadrotor-event camera platforms in multiple settings and find, contrary to traditional vision-based works, that low speeds (1m/s) make the task harder and more prone to collisions, while high speeds (5m/s) result in better event-based depth estimation and avoidance. We also find that success rates in outdoor scenes can be significantly higher than in certain indoor scenes.","sentences":["We present the first static-obstacle avoidance method for quadrotors using just an onboard, monocular event camera.","Quadrotors are capable of fast and agile flight in cluttered environments when piloted manually, but vision-based autonomous flight in unknown environments is difficult in part due to the sensor limitations of traditional onboard cameras.","Event cameras, however, promise nearly zero motion blur and high dynamic range, but produce a very large volume of events under significant ego-motion and further lack a continuous-time sensor model in simulation, making direct sim-to-real transfer not possible.","By leveraging depth prediction as a pretext task in our learning framework, we can pre-train a reactive obstacle avoidance events-to-control policy with approximated, simulated events and then fine-tune the perception component with limited events-and-depth real-world data to achieve obstacle avoidance in indoor and outdoor settings.","We demonstrate this across two quadrotor-event camera platforms in multiple settings and find, contrary to traditional vision-based works, that low speeds (1m/s) make the task harder and more prone to collisions, while high speeds (5m/s) result in better event-based depth estimation and avoidance.","We also find that success rates in outdoor scenes can be significantly higher than in certain indoor scenes."],"url":"http://arxiv.org/abs/2411.03303v1"}
{"created":"2024-11-05 17:53:25","title":"VERITAS: A Unified Approach to Reliability Evaluation","abstract":"Large language models (LLMs) often fail to synthesize information from their context to generate an accurate response. This renders them unreliable in knowledge intensive settings where reliability of the output is key. A critical component for reliable LLMs is the integration of a robust fact-checking system that can detect hallucinations across various formats. While several open-access fact-checking models are available, their functionality is often limited to specific tasks, such as grounded question-answering or entailment verification, and they perform less effectively in conversational settings. On the other hand, closed-access models like GPT-4 and Claude offer greater flexibility across different contexts, including grounded dialogue verification, but are hindered by high costs and latency. In this work, we introduce VERITAS, a family of hallucination detection models designed to operate flexibly across diverse contexts while minimizing latency and costs. VERITAS achieves state-of-the-art results considering average performance on all major hallucination detection benchmarks, with $10\\%$ increase in average performance when compared to similar-sized models and get close to the performance of GPT4 turbo with LLM-as-a-judge setting.","sentences":["Large language models (LLMs) often fail to synthesize information from their context to generate an accurate response.","This renders them unreliable in knowledge intensive settings where reliability of the output is key.","A critical component for reliable LLMs is the integration of a robust fact-checking system that can detect hallucinations across various formats.","While several open-access fact-checking models are available, their functionality is often limited to specific tasks, such as grounded question-answering or entailment verification, and they perform less effectively in conversational settings.","On the other hand, closed-access models like GPT-4 and Claude offer greater flexibility across different contexts, including grounded dialogue verification, but are hindered by high costs and latency.","In this work, we introduce VERITAS, a family of hallucination detection models designed to operate flexibly across diverse contexts while minimizing latency and costs.","VERITAS achieves state-of-the-art results considering average performance on all major hallucination detection benchmarks, with $10\\%$ increase in average performance when compared to similar-sized models and get close to the performance of GPT4 turbo with LLM-as-a-judge setting."],"url":"http://arxiv.org/abs/2411.03300v1"}
{"created":"2024-11-05 17:50:39","title":"Concurrent Composition for Continual Mechanisms","abstract":"A series of recent works by Lyu, Wang, Vadhan, and Zhang (TCC `21, NeurIPS `22, STOC `23) showed that composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of interactive differentially private mechanism, when differential privacy is measured using $f$-DP and the adversary is adaptive. We extend their work to the $\\textit{continual observation setting,}$ where the data is arriving online in a potentially adaptive manner. More specifically, we show that all composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of continual differentially private mechanism, where the adversary is adaptive. We show this result for $f$-DP, which also implies the result for pure DP and $(\\epsilon, \\delta)$-DP.","sentences":["A series of recent works by Lyu, Wang, Vadhan, and Zhang (TCC `21, NeurIPS `22, STOC `23) showed that composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of interactive differentially private mechanism, when differential privacy is measured using $f$-DP and the adversary is adaptive.","We extend their work to the $\\textit{continual observation setting,}$ where the data is arriving online in a potentially adaptive manner.","More specifically, we show that all composition theorems for non-interactive differentially private mechanisms extend to the concurrent composition of continual differentially private mechanism, where the adversary is adaptive.","We show this result for $f$-DP, which also implies the result for pure DP and $(\\epsilon, \\delta)$-DP."],"url":"http://arxiv.org/abs/2411.03299v1"}
{"created":"2024-11-05 17:42:43","title":"Examining Human-AI Collaboration for Co-Writing Constructive Comments Online","abstract":"This paper examines how large language models (LLMs) can help people write constructive comments in online debates on divisive social issues and whether the notions of constructiveness vary across cultures. Through controlled experiments with 600 participants from India and the US, who reviewed and wrote constructive comments on online threads on Islamophobia and homophobia, we found potential misalignment in how LLMs and humans perceive constructiveness in online comments. While the LLM was more likely to view dialectical comments as more constructive, participants favored comments that emphasized logic and facts more than the LLM did. Despite these differences, participants rated LLM-generated and human-AI co-written comments as significantly more constructive than those written independently by humans. Our analysis also revealed that LLM-generated and human-AI co-written comments exhibited more linguistic features associated with constructiveness compared to human-written comments on divisive topics. When participants used LLMs to refine their comments, the resulting comments were longer, more polite, positive, less toxic, and more readable, with added argumentative features that retained the original intent but occasionally lost nuances. Based on these findings, we discuss ethical and design considerations in using LLMs to facilitate constructive discourse online.","sentences":["This paper examines how large language models (LLMs) can help people write constructive comments in online debates on divisive social issues and whether the notions of constructiveness vary across cultures.","Through controlled experiments with 600 participants from India and the US, who reviewed and wrote constructive comments on online threads on Islamophobia and homophobia, we found potential misalignment in how LLMs and humans perceive constructiveness in online comments.","While the LLM was more likely to view dialectical comments as more constructive, participants favored comments that emphasized logic and facts more than the LLM did.","Despite these differences, participants rated LLM-generated and human-AI co-written comments as significantly more constructive than those written independently by humans.","Our analysis also revealed that LLM-generated and human-AI co-written comments exhibited more linguistic features associated with constructiveness compared to human-written comments on divisive topics.","When participants used LLMs to refine their comments, the resulting comments were longer, more polite, positive, less toxic, and more readable, with added argumentative features that retained the original intent but occasionally lost nuances.","Based on these findings, we discuss ethical and design considerations in using LLMs to facilitate constructive discourse online."],"url":"http://arxiv.org/abs/2411.03295v1"}
{"created":"2024-11-05 17:41:14","title":"Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy For Visuomotor Imitation Learning","abstract":"We propose an object-centric recovery policy framework to address the challenges of out-of-distribution (OOD) scenarios in visuomotor policy learning. Previous behavior cloning (BC) methods rely heavily on a large amount of labeled data coverage, failing in unfamiliar spatial states. Without relying on extra data collection, our approach learns a recovery policy constructed by an inverse policy inferred from object keypoint manifold gradient in the original training data. The recovery policy serves as a simple add-on to any base visuomotor BC policy, agnostic to a specific method, guiding the system back towards the training distribution to ensure task success even in OOD situations. We demonstrate the effectiveness of our object-centric framework in both simulation and real robot experiments, achieving an improvement of $\\textbf{77.7\\%}$ over the base policy in OOD. Project Website: https://sites.google.com/view/ocr-penn","sentences":["We propose an object-centric recovery policy framework to address the challenges of out-of-distribution (OOD) scenarios in visuomotor policy learning.","Previous behavior cloning (BC) methods rely heavily on a large amount of labeled data coverage, failing in unfamiliar spatial states.","Without relying on extra data collection, our approach learns a recovery policy constructed by an inverse policy inferred from object keypoint manifold gradient in the original training data.","The recovery policy serves as a simple add-on to any base visuomotor BC policy, agnostic to a specific method, guiding the system back towards the training distribution to ensure task success even in OOD situations.","We demonstrate the effectiveness of our object-centric framework in both simulation and real robot experiments, achieving an improvement of $\\textbf{77.7\\%}$ over the base policy in OOD.","Project Website: https://sites.google.com/view/ocr-penn"],"url":"http://arxiv.org/abs/2411.03294v1"}
{"created":"2024-11-05 17:40:03","title":"Interaction2Code: How Far Are We From Automatic Interactive Webpage Generation?","abstract":"Converting webpage design into functional UI code is a critical step for building websites, which can be labor-intensive and time-consuming. To automate this design-to-code transformation process, various automated methods using learning-based networks and multi-modal large language models (MLLMs) have been proposed. However, these studies were merely evaluated on a narrow range of static web pages and ignored dynamic interaction elements, making them less practical for real-world website deployment.   To fill in the blank, we present the first systematic investigation of MLLMs in generating interactive webpages. Specifically, we first formulate the Interaction-to-Code task and build the Interaction2Code benchmark that contains 97 unique web pages and 213 distinct interactions, spanning 15 webpage types and 30 interaction categories. We then conduct comprehensive experiments on three state-of-the-art (SOTA) MLLMs using both automatic metrics and human evaluations, thereby summarizing six findings accordingly. Our experimental results highlight the limitations of MLLMs in generating fine-grained interactive features and managing interactions with complex transformations and subtle visual modifications. We further analyze failure cases and their underlying causes, identifying 10 common failure types and assessing their severity. Additionally, our findings reveal three critical influencing factors, i.e., prompts, visual saliency, and textual descriptions, that can enhance the interaction generation performance of MLLMs. Based on these findings, we elicit implications for researchers and developers, providing a foundation for future advancements in this field. Datasets and source code are available at https://github.com/WebPAI/Interaction2Code.","sentences":["Converting webpage design into functional UI code is a critical step for building websites, which can be labor-intensive and time-consuming.","To automate this design-to-code transformation process, various automated methods using learning-based networks and multi-modal large language models (MLLMs) have been proposed.","However, these studies were merely evaluated on a narrow range of static web pages and ignored dynamic interaction elements, making them less practical for real-world website deployment.   ","To fill in the blank, we present the first systematic investigation of MLLMs in generating interactive webpages.","Specifically, we first formulate the Interaction-to-Code task and build the Interaction2Code benchmark that contains 97 unique web pages and 213 distinct interactions, spanning 15 webpage types and 30 interaction categories.","We then conduct comprehensive experiments on three state-of-the-art (SOTA) MLLMs using both automatic metrics and human evaluations, thereby summarizing six findings accordingly.","Our experimental results highlight the limitations of MLLMs in generating fine-grained interactive features and managing interactions with complex transformations and subtle visual modifications.","We further analyze failure cases and their underlying causes, identifying 10 common failure types and assessing their severity.","Additionally, our findings reveal three critical influencing factors, i.e., prompts, visual saliency, and textual descriptions, that can enhance the interaction generation performance of MLLMs.","Based on these findings, we elicit implications for researchers and developers, providing a foundation for future advancements in this field.","Datasets and source code are available at https://github.com/WebPAI/Interaction2Code."],"url":"http://arxiv.org/abs/2411.03292v1"}
{"created":"2024-11-05 17:39:32","title":"Using Assurance Cases to Guide Verification and Validation of Research Software","abstract":"Research software engineers can use Assurance Cases (ACs) to guide Verification and Validation (VnV) efforts. An AC is a structured argument that a property like correctness holds. We illustrate how ACs can guide VnV activities via a case study of software for automatically extracting the 3D segmentation of the aorta from medical images of the chest. The AC argument suggests that the following evidence is required: comparison to a pseudo-oracle; traceability between requirements, design, code and tests; review of all artifacts by a domain expert with proper credentials; documentation of input assumptions; and a warning that only qualified people should use the software. The case study highlights that code is not the only artifact of interest for building confidence and that making an explicit distinction between software and user responsibilities is useful.","sentences":["Research software engineers can use Assurance Cases (ACs) to guide Verification and Validation (VnV) efforts.","An AC is a structured argument that a property like correctness holds.","We illustrate how ACs can guide VnV activities via a case study of software for automatically extracting the 3D segmentation of the aorta from medical images of the chest.","The AC argument suggests that the following evidence is required: comparison to a pseudo-oracle; traceability between requirements, design, code and tests; review of all artifacts by a domain expert with proper credentials; documentation of input assumptions; and a warning that only qualified people should use the software.","The case study highlights that code is not the only artifact of interest for building confidence and that making an explicit distinction between software and user responsibilities is useful."],"url":"http://arxiv.org/abs/2411.03291v1"}
{"created":"2024-11-05 17:38:03","title":"Data-Driven Sampling Based Stochastic MPC for Skid-Steer Mobile Robot Navigation","abstract":"Traditional approaches to motion modeling for skid-steer robots struggle with capturing nonlinear tire-terrain dynamics, especially during high-speed maneuvers. In this paper, we tackle such nonlinearities by enhancing a dynamic unicycle model with Gaussian Process (GP) regression outputs. This enables us to develop an adaptive, uncertainty-informed navigation formulation. We solve the resultant stochastic optimal control problem using a chance-constrained Model Predictive Path Integral (MPPI) control method. This approach formulates both obstacle avoidance and path-following as chance constraints, accounting for residual uncertainties from the GP to ensure safety and reliability in control. Leveraging GPU acceleration, we efficiently manage the non-convex nature of the problem, ensuring real-time performance. Our approach unifies path-following and obstacle avoidance across different terrains, unlike prior works which typically focus on one or the other. We compare our GP-MPPI method against unicycle and data-driven kinematic models within the MPPI framework. In simulations, our approach shows superior tracking accuracy and obstacle avoidance. We further validate our approach through hardware experiments on a skid-steer robot platform, demonstrating its effectiveness in high-speed navigation. The GPU implementation of the proposed method and supplementary video footage are available at https: //stochasticmppi.github.io.","sentences":["Traditional approaches to motion modeling for skid-steer robots struggle with capturing nonlinear tire-terrain dynamics, especially during high-speed maneuvers.","In this paper, we tackle such nonlinearities by enhancing a dynamic unicycle model with Gaussian Process (GP) regression outputs.","This enables us to develop an adaptive, uncertainty-informed navigation formulation.","We solve the resultant stochastic optimal control problem using a chance-constrained Model Predictive Path Integral (MPPI) control method.","This approach formulates both obstacle avoidance and path-following as chance constraints, accounting for residual uncertainties from the GP to ensure safety and reliability in control.","Leveraging GPU acceleration, we efficiently manage the non-convex nature of the problem, ensuring real-time performance.","Our approach unifies path-following and obstacle avoidance across different terrains, unlike prior works which typically focus on one or the other.","We compare our GP-MPPI method against unicycle and data-driven kinematic models within the MPPI framework.","In simulations, our approach shows superior tracking accuracy and obstacle avoidance.","We further validate our approach through hardware experiments on a skid-steer robot platform, demonstrating its effectiveness in high-speed navigation.","The GPU implementation of the proposed method and supplementary video footage are available at https: //stochasticmppi.github.io."],"url":"http://arxiv.org/abs/2411.03289v1"}
{"created":"2024-11-05 17:36:32","title":"The Future of Intelligent Healthcare: A Systematic Analysis and Discussion on the Integration and Impact of Robots Using Large Language Models for Healthcare","abstract":"The potential use of large language models (LLMs) in healthcare robotics can help address the significant demand put on healthcare systems around the world with respect to an aging demographic and a shortage of healthcare professionals. Even though LLMs have already been integrated into medicine to assist both clinicians and patients, the integration of LLMs within healthcare robots has not yet been explored for clinical settings. In this perspective paper, we investigate the groundbreaking developments in robotics and LLMs to uniquely identify the needed system requirements for designing health specific LLM based robots in terms of multi modal communication through human robot interactions (HRIs), semantic reasoning, and task planning. Furthermore, we discuss the ethical issues, open challenges, and potential future research directions for this emerging innovative field.","sentences":["The potential use of large language models (LLMs) in healthcare robotics can help address the significant demand put on healthcare systems around the world with respect to an aging demographic and a shortage of healthcare professionals.","Even though LLMs have already been integrated into medicine to assist both clinicians and patients, the integration of LLMs within healthcare robots has not yet been explored for clinical settings.","In this perspective paper, we investigate the groundbreaking developments in robotics and LLMs to uniquely identify the needed system requirements for designing health specific LLM based robots in terms of multi modal communication through human robot interactions (HRIs), semantic reasoning, and task planning.","Furthermore, we discuss the ethical issues, open challenges, and potential future research directions for this emerging innovative field."],"url":"http://arxiv.org/abs/2411.03287v1"}
{"created":"2024-11-05 17:35:41","title":"DiT4Edit: Diffusion Transformer for Image Editing","abstract":"Despite recent advances in UNet-based image editing, methods for shape-aware object editing in high-resolution images are still lacking. Compared to UNet, Diffusion Transformers (DiT) demonstrate superior capabilities to effectively capture the long-range dependencies among patches, leading to higher-quality image generation. In this paper, we propose DiT4Edit, the first Diffusion Transformer-based image editing framework. Specifically, DiT4Edit uses the DPM-Solver inversion algorithm to obtain the inverted latents, reducing the number of steps compared to the DDIM inversion algorithm commonly used in UNet-based frameworks. Additionally, we design unified attention control and patches merging, tailored for transformer computation streams. This integration allows our framework to generate higher-quality edited images faster. Our design leverages the advantages of DiT, enabling it to surpass UNet structures in image editing, especially in high-resolution and arbitrary-size images. Extensive experiments demonstrate the strong performance of DiT4Edit across various editing scenarios, highlighting the potential of Diffusion Transformers in supporting image editing.","sentences":["Despite recent advances in UNet-based image editing, methods for shape-aware object editing in high-resolution images are still lacking.","Compared to UNet, Diffusion Transformers (DiT) demonstrate superior capabilities to effectively capture the long-range dependencies among patches, leading to higher-quality image generation.","In this paper, we propose DiT4Edit, the first Diffusion Transformer-based image editing framework.","Specifically, DiT4Edit uses the DPM-Solver inversion algorithm to obtain the inverted latents, reducing the number of steps compared to the DDIM inversion algorithm commonly used in UNet-based frameworks.","Additionally, we design unified attention control and patches merging, tailored for transformer computation streams.","This integration allows our framework to generate higher-quality edited images faster.","Our design leverages the advantages of DiT, enabling it to surpass UNet structures in image editing, especially in high-resolution and arbitrary-size images.","Extensive experiments demonstrate the strong performance of DiT4Edit across various editing scenarios, highlighting the potential of Diffusion Transformers in supporting image editing."],"url":"http://arxiv.org/abs/2411.03286v1"}
{"created":"2024-11-05 17:33:39","title":"SMoA: Improving Multi-agent Large Language Models with Sparse Mixture-of-Agents","abstract":"While multi-agent systems have been shown to significantly enhance the performance of Large Language Models (LLMs) across various tasks and applications, the dense interaction between scaling agents potentially hampers their efficiency and diversity. To address these challenges, we draw inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse mixture-of-agents (SMoA) framework to improve the efficiency and diversity of multi-agent LLMs. Unlike completely connected structures, SMoA introduces novel Response Selection and Early Stopping mechanisms to sparsify information flows among individual LLM agents, striking a balance between performance and efficiency. Additionally, inspired by the expert diversity principle in SMoE frameworks for workload balance between experts, we assign distinct role descriptions to each LLM agent, fostering diverse and divergent thinking. Extensive experiments on reasoning, alignment, and fairness benchmarks demonstrate that SMoA achieves performance comparable to traditional mixture-of-agents approaches but with significantly lower computational costs. Further analysis reveals that SMoA is more stable, has a greater capacity to scale, and offers considerable potential through hyper-parameter optimization. Code and data will be available at: https://github.com/David-Li0406/SMoA.","sentences":["While multi-agent systems have been shown to significantly enhance the performance of Large Language Models (LLMs) across various tasks and applications, the dense interaction between scaling agents potentially hampers their efficiency and diversity.","To address these challenges, we draw inspiration from the sparse mixture-of-agents (SMoE) and propose a sparse mixture-of-agents (SMoA) framework to improve the efficiency and diversity of multi-agent LLMs.","Unlike completely connected structures, SMoA introduces novel Response Selection and Early Stopping mechanisms to sparsify information flows among individual LLM agents, striking a balance between performance and efficiency.","Additionally, inspired by the expert diversity principle in SMoE frameworks for workload balance between experts, we assign distinct role descriptions to each LLM agent, fostering diverse and divergent thinking.","Extensive experiments on reasoning, alignment, and fairness benchmarks demonstrate that SMoA achieves performance comparable to traditional mixture-of-agents approaches but with significantly lower computational costs.","Further analysis reveals that SMoA is more stable, has a greater capacity to scale, and offers considerable potential through hyper-parameter optimization.","Code and data will be available at: https://github.com/David-Li0406/SMoA."],"url":"http://arxiv.org/abs/2411.03284v1"}
{"created":"2024-11-05 17:20:53","title":"Oblivious Defense in ML Models: Backdoor Removal without Detection","abstract":"As society grows more reliant on machine learning, ensuring the security of machine learning systems against sophisticated attacks becomes a pressing concern. A recent result of Goldwasser, Kim, Vaikuntanathan, and Zamir (2022) shows that an adversary can plant undetectable backdoors in machine learning models, allowing the adversary to covertly control the model's behavior. Backdoors can be planted in such a way that the backdoored machine learning model is computationally indistinguishable from an honest model without backdoors.   In this paper, we present strategies for defending against backdoors in ML models, even if they are undetectable. The key observation is that it is sometimes possible to provably mitigate or even remove backdoors without needing to detect them, using techniques inspired by the notion of random self-reducibility. This depends on properties of the ground-truth labels (chosen by nature), and not of the proposed ML model (which may be chosen by an attacker).   We give formal definitions for secure backdoor mitigation, and proceed to show two types of results. First, we show a \"global mitigation\" technique, which removes all backdoors from a machine learning model under the assumption that the ground-truth labels are close to a Fourier-heavy function. Second, we consider distributions where the ground-truth labels are close to a linear or polynomial function in $\\mathbb{R}^n$. Here, we show \"local mitigation\" techniques, which remove backdoors with high probability for every inputs of interest, and are computationally cheaper than global mitigation. All of our constructions are black-box, so our techniques work without needing access to the model's representation (i.e., its code or parameters). Along the way we prove a simple result for robust mean estimation.","sentences":["As society grows more reliant on machine learning, ensuring the security of machine learning systems against sophisticated attacks becomes a pressing concern.","A recent result of Goldwasser, Kim, Vaikuntanathan, and Zamir (2022) shows that an adversary can plant undetectable backdoors in machine learning models, allowing the adversary to covertly control the model's behavior.","Backdoors can be planted in such a way that the backdoored machine learning model is computationally indistinguishable from an honest model without backdoors.   ","In this paper, we present strategies for defending against backdoors in ML models, even if they are undetectable.","The key observation is that it is sometimes possible to provably mitigate or even remove backdoors without needing to detect them, using techniques inspired by the notion of random self-reducibility.","This depends on properties of the ground-truth labels (chosen by nature), and not of the proposed ML model (which may be chosen by an attacker).   ","We give formal definitions for secure backdoor mitigation, and proceed to show two types of results.","First, we show a \"global mitigation\" technique, which removes all backdoors from a machine learning model under the assumption that the ground-truth labels are close to a Fourier-heavy function.","Second, we consider distributions where the ground-truth labels are close to a linear or polynomial function in $\\mathbb{R}^n$. Here, we show \"local mitigation\" techniques, which remove backdoors with high probability for every inputs of interest, and are computationally cheaper than global mitigation.","All of our constructions are black-box, so our techniques work without needing access to the model's representation (i.e., its code or parameters).","Along the way we prove a simple result for robust mean estimation."],"url":"http://arxiv.org/abs/2411.03279v1"}
{"created":"2024-11-05 17:17:45","title":"Causal Responsibility Attribution for Human-AI Collaboration","abstract":"As Artificial Intelligence (AI) systems increasingly influence decision-making across various fields, the need to attribute responsibility for undesirable outcomes has become essential, though complicated by the complex interplay between humans and AI. Existing attribution methods based on actual causality and Shapley values tend to disproportionately blame agents who contribute more to an outcome and rely on real-world measures of blameworthiness that may misalign with responsible AI standards. This paper presents a causal framework using Structural Causal Models (SCMs) to systematically attribute responsibility in human-AI systems, measuring overall blameworthiness while employing counterfactual reasoning to account for agents' expected epistemic levels. Two case studies illustrate the framework's adaptability in diverse human-AI collaboration scenarios.","sentences":["As Artificial Intelligence (AI) systems increasingly influence decision-making across various fields, the need to attribute responsibility for undesirable outcomes has become essential, though complicated by the complex interplay between humans and AI.","Existing attribution methods based on actual causality and Shapley values tend to disproportionately blame agents who contribute more to an outcome and rely on real-world measures of blameworthiness that may misalign with responsible AI standards.","This paper presents a causal framework using Structural Causal Models (SCMs) to systematically attribute responsibility in human-AI systems, measuring overall blameworthiness while employing counterfactual reasoning to account for agents' expected epistemic levels.","Two case studies illustrate the framework's adaptability in diverse human-AI collaboration scenarios."],"url":"http://arxiv.org/abs/2411.03275v1"}
{"created":"2024-11-05 17:17:30","title":"Generalized Word-Representable Graphs","abstract":"The literature on word-representable graphs is quite rich, and a number of variations of the original definition have been proposed over the years. We are initiating a systematic study of such variations based on formal languages. In our framework, we can associate a graph class to each language over the binary alphabet \\{0,1\\}. All graph classes that are language-representable in this sense are hereditary and enjoy further common properties. Besides word-representable graphs and, more generally, 1^k- or k-11-representable graphs, we can identify many more graph classes in our framework, like (co)bipartite graphs, (co)comparability graphs, to name a few. It was already known that any graph is 111- or 2-11-representable. When such representations are considered for storing graphs, 111- or 2-11-representability bears the disadvantage of being significantly inferior to standard adjacency matrices or lists. We prove that quite famous languages like the palindromes, the copy language or the Lyndon words can match the efficiency of standard graph representations. The perspective of language theory allows us to prove general results that hold for all graph classes that can be defined in this way. This includes certain closure properties (e.g., all language-definable graph classes are hereditary) as well as certain limitations (e.g., all language-representable graph classes contain graphs of arbitrarily large treewidth and of arbitrarily large degeneracy, except a trivial case). As each language describes a graph class, we can also ask decidability questions concerning graph classes, given a concrete presentation of a formal language. We also present a systematic study of graph classes that can be represented by languages in which each letter occurs at most twice. Here, we find graph classes like interval, permutation, circle, bipartite chain, convex, and threshold graphs.","sentences":["The literature on word-representable graphs is quite rich, and a number of variations of the original definition have been proposed over the years.","We are initiating a systematic study of such variations based on formal languages.","In our framework, we can associate a graph class to each language over the binary alphabet \\{0,1\\}.","All graph classes that are language-representable in this sense are hereditary and enjoy further common properties.","Besides word-representable graphs and, more generally, 1^k- or k-11-representable graphs, we can identify many more graph classes in our framework, like (co)bipartite graphs, (co)comparability graphs, to name a few.","It was already known that any graph is 111- or 2-11-representable.","When such representations are considered for storing graphs, 111- or 2-11-representability bears the disadvantage of being significantly inferior to standard adjacency matrices or lists.","We prove that quite famous languages like the palindromes, the copy language or the Lyndon words can match the efficiency of standard graph representations.","The perspective of language theory allows us to prove general results that hold for all graph classes that can be defined in this way.","This includes certain closure properties (e.g., all language-definable graph classes are hereditary) as well as certain limitations (e.g., all language-representable graph classes contain graphs of arbitrarily large treewidth and of arbitrarily large degeneracy, except a trivial case).","As each language describes a graph class, we can also ask decidability questions concerning graph classes, given a concrete presentation of a formal language.","We also present a systematic study of graph classes that can be represented by languages in which each letter occurs at most twice.","Here, we find graph classes like interval, permutation, circle, bipartite chain, convex, and threshold graphs."],"url":"http://arxiv.org/abs/2411.03274v1"}
{"created":"2024-11-05 17:16:56","title":"Graph-Based Semi-Supervised Segregated Lipschitz Learning","abstract":"This paper presents an approach to semi-supervised learning for the classification of data using the Lipschitz Learning on graphs. We develop a graph-based semi-supervised learning framework that leverages the properties of the infinity Laplacian to propagate labels in a dataset where only a few samples are labeled. By extending the theory of spatial segregation from the Laplace operator to the infinity Laplace operator, both in continuum and discrete settings, our approach provides a robust method for dealing with class imbalance, a common challenge in machine learning. Experimental validation on several benchmark datasets demonstrates that our method not only improves classification accuracy compared to existing methods but also ensures efficient label propagation in scenarios with limited labeled data.","sentences":["This paper presents an approach to semi-supervised learning for the classification of data using the Lipschitz Learning on graphs.","We develop a graph-based semi-supervised learning framework that leverages the properties of the infinity Laplacian to propagate labels in a dataset where only a few samples are labeled.","By extending the theory of spatial segregation from the Laplace operator to the infinity Laplace operator, both in continuum and discrete settings, our approach provides a robust method for dealing with class imbalance, a common challenge in machine learning.","Experimental validation on several benchmark datasets demonstrates that our method not only improves classification accuracy compared to existing methods but also ensures efficient label propagation in scenarios with limited labeled data."],"url":"http://arxiv.org/abs/2411.03273v1"}
{"created":"2024-11-05 17:14:46","title":"Stable Matching with Ties: Approximation Ratios and Learning","abstract":"We study the problem of matching markets with ties, where one side of the market does not necessarily have strict preferences over members at its other side. For example, workers do not always have strict preferences over jobs, students can give the same ranking for different schools and more. In particular, assume w.l.o.g. that workers' preferences are determined by their utility from being matched to each job, which might admit ties. Notably, in contrast to classical two-sided markets with strict preferences, there is no longer a single stable matching that simultaneously maximizes the utility for all workers.   We aim to guarantee each worker the largest possible share from the utility in her best possible stable matching. We call the ratio between the worker's best possible stable utility and its assigned utility the \\emph{Optimal Stable Share} (OSS)-ratio. We first prove that distributions over stable matchings cannot guarantee an OSS-ratio that is sublinear in the number of workers. Instead, randomizing over possibly non-stable matchings, we show how to achieve a tight logarithmic OSS-ratio. Then, we analyze the case where the real utility is not necessarily known and can only be approximated. In particular, we provide an algorithm that guarantees a similar fraction of the utility compared to the best possible utility. Finally, we move to a bandit setting, where we select a matching at each round and only observe the utilities for matches we perform. We show how to utilize our results for approximate utilities to gracefully interpolate between problems without ties and problems with statistical ties (small suboptimality gaps).","sentences":["We study the problem of matching markets with ties, where one side of the market does not necessarily have strict preferences over members at its other side.","For example, workers do not always have strict preferences over jobs, students can give the same ranking for different schools and more.","In particular, assume w.l.o.g. that workers' preferences are determined by their utility from being matched to each job, which might admit ties.","Notably, in contrast to classical two-sided markets with strict preferences, there is no longer a single stable matching that simultaneously maximizes the utility for all workers.   ","We aim to guarantee each worker the largest possible share from the utility in her best possible stable matching.","We call the ratio between the worker's best possible stable utility and its assigned utility the \\emph{Optimal Stable Share} (OSS)-ratio.","We first prove that distributions over stable matchings cannot guarantee an OSS-ratio that is sublinear in the number of workers.","Instead, randomizing over possibly non-stable matchings, we show how to achieve a tight logarithmic OSS-ratio.","Then, we analyze the case where the real utility is not necessarily known and can only be approximated.","In particular, we provide an algorithm that guarantees a similar fraction of the utility compared to the best possible utility.","Finally, we move to a bandit setting, where we select a matching at each round and only observe the utilities for matches we perform.","We show how to utilize our results for approximate utilities to gracefully interpolate between problems without ties and problems with statistical ties (small suboptimality gaps)."],"url":"http://arxiv.org/abs/2411.03270v1"}
{"created":"2024-11-05 17:02:29","title":"Proxy-informed Bayesian transfer learning with unknown sources","abstract":"Generalization outside the scope of one's training data requires leveraging prior knowledge about the effects that transfer, and the effects that don't, between different data sources. Bayesian transfer learning is a principled paradigm for specifying this knowledge, and refining it on the basis of data from the source (training) and target (prediction) tasks. We address the challenging transfer learning setting where the learner (i) cannot fine-tune in the target task, and (ii) does not know which source data points correspond to the same task (i.e., the data sources are unknown). We propose a proxy-informed robust method for probabilistic transfer learning (PROMPT), which provides a posterior predictive estimate tailored to the structure of the target task, without requiring the learner have access to any outcome information from the target task. Instead, PROMPT relies on the availability of proxy information. PROMPT uses the same proxy information for two purposes: (i) estimation of effects specific to the target task, and (ii) construction of a robust reweighting of the source data for estimation of effects that transfer between tasks. We provide theoretical results on the effect of this reweighting on the risk of negative transfer, and demonstrate application of PROMPT in two synthetic settings.","sentences":["Generalization outside the scope of one's training data requires leveraging prior knowledge about the effects that transfer, and the effects that don't, between different data sources.","Bayesian transfer learning is a principled paradigm for specifying this knowledge, and refining it on the basis of data from the source (training) and target (prediction) tasks.","We address the challenging transfer learning setting where the learner (i) cannot fine-tune in the target task, and (ii) does not know which source data points correspond to the same task (i.e., the data sources are unknown).","We propose a proxy-informed robust method for probabilistic transfer learning (PROMPT), which provides a posterior predictive estimate tailored to the structure of the target task, without requiring the learner have access to any outcome information from the target task.","Instead, PROMPT relies on the availability of proxy information.","PROMPT uses the same proxy information for two purposes: (i) estimation of effects specific to the target task, and (ii) construction of a robust reweighting of the source data for estimation of effects that transfer between tasks.","We provide theoretical results on the effect of this reweighting on the risk of negative transfer, and demonstrate application of PROMPT in two synthetic settings."],"url":"http://arxiv.org/abs/2411.03263v1"}
{"created":"2024-11-05 16:59:06","title":"ShadowMamba: State-Space Model with Boundary-Region Selective Scan for Shadow Removal","abstract":"Image shadow removal is a typical low-level vision problem, where the presence of shadows leads to abrupt changes in brightness in certain regions, affecting the accuracy of upstream tasks. Current shadow removal methods still face challenges such as residual boundary artifacts, and capturing feature information at shadow boundaries is crucial for removing shadows and eliminating residual boundary artifacts. Recently, Mamba has achieved remarkable success in computer vision by globally modeling long-sequence information with linear complexity. However, when applied to image shadow removal, the original Mamba scanning method overlooks the semantic continuity of shadow boundaries as well as the continuity of semantics within the same region. Based on the unique characteristics of shadow images, this paper proposes a novel selective scanning method called boundary-region selective scanning. This method scans boundary regions, shadow regions, and non-shadow regions independently, bringing pixels of the same region type closer together in the long sequence, especially focusing on the local information at the boundaries, which is crucial for shadow removal. This method combines with global scanning and channel scanning to jointly accomplish the shadow removal. We name our model ShadowMamba, the first Mamba-based model for shadow removal. Extensive experimental results show that our method outperforms current state-of-the-art models across most metrics on multiple datasets. The code for ShadowMamba is available at (Code will be released upon acceptance).","sentences":["Image shadow removal is a typical low-level vision problem, where the presence of shadows leads to abrupt changes in brightness in certain regions, affecting the accuracy of upstream tasks.","Current shadow removal methods still face challenges such as residual boundary artifacts, and capturing feature information at shadow boundaries is crucial for removing shadows and eliminating residual boundary artifacts.","Recently, Mamba has achieved remarkable success in computer vision by globally modeling long-sequence information with linear complexity.","However, when applied to image shadow removal, the original Mamba scanning method overlooks the semantic continuity of shadow boundaries as well as the continuity of semantics within the same region.","Based on the unique characteristics of shadow images, this paper proposes a novel selective scanning method called boundary-region selective scanning.","This method scans boundary regions, shadow regions, and non-shadow regions independently, bringing pixels of the same region type closer together in the long sequence, especially focusing on the local information at the boundaries, which is crucial for shadow removal.","This method combines with global scanning and channel scanning to jointly accomplish the shadow removal.","We name our model ShadowMamba, the first Mamba-based model for shadow removal.","Extensive experimental results show that our method outperforms current state-of-the-art models across most metrics on multiple datasets.","The code for ShadowMamba is available at (Code will be released upon acceptance)."],"url":"http://arxiv.org/abs/2411.03260v1"}
{"created":"2024-11-05 16:50:54","title":"Discovering Data Structures: Nearest Neighbor Search and Beyond","abstract":"We propose a general framework for end-to-end learning of data structures. Our framework adapts to the underlying data distribution and provides fine-grained control over query and space complexity. Crucially, the data structure is learned from scratch, and does not require careful initialization or seeding with candidate data structures/algorithms. We first apply this framework to the problem of nearest neighbor search. In several settings, we are able to reverse-engineer the learned data structures and query algorithms. For 1D nearest neighbor search, the model discovers optimal distribution (in)dependent algorithms such as binary search and variants of interpolation search. In higher dimensions, the model learns solutions that resemble k-d trees in some regimes, while in others, they have elements of locality-sensitive hashing. The model can also learn useful representations of high-dimensional data and exploit them to design effective data structures. We also adapt our framework to the problem of estimating frequencies over a data stream, and believe it could also be a powerful discovery tool for new problems.","sentences":["We propose a general framework for end-to-end learning of data structures.","Our framework adapts to the underlying data distribution and provides fine-grained control over query and space complexity.","Crucially, the data structure is learned from scratch, and does not require careful initialization or seeding with candidate data structures/algorithms.","We first apply this framework to the problem of nearest neighbor search.","In several settings, we are able to reverse-engineer the learned data structures and query algorithms.","For 1D nearest neighbor search, the model discovers optimal distribution (in)dependent algorithms such as binary search and variants of interpolation search.","In higher dimensions, the model learns solutions that resemble k-d trees in some regimes, while in others, they have elements of locality-sensitive hashing.","The model can also learn useful representations of high-dimensional data and exploit them to design effective data structures.","We also adapt our framework to the problem of estimating frequencies over a data stream, and believe it could also be a powerful discovery tool for new problems."],"url":"http://arxiv.org/abs/2411.03253v1"}
{"created":"2024-11-05 16:49:33","title":"Spontaneous Emergence of Agent Individuality through Social Interactions in LLM-Based Communities","abstract":"We study the emergence of agency from scratch by using Large Language Model (LLM)-based agents. In previous studies of LLM-based agents, each agent's characteristics, including personality and memory, have traditionally been predefined. We focused on how individuality, such as behavior, personality, and memory, can be differentiated from an undifferentiated state. The present LLM agents engage in cooperative communication within a group simulation, exchanging context-based messages in natural language. By analyzing this multi-agent simulation, we report valuable new insights into how social norms, cooperation, and personality traits can emerge spontaneously. This paper demonstrates that autonomously interacting LLM-powered agents generate hallucinations and hashtags to sustain communication, which, in turn, increases the diversity of words within their interactions. Each agent's emotions shift through communication, and as they form communities, the personalities of the agents emerge and evolve accordingly. This computational modeling approach and its findings will provide a new method for analyzing collective artificial intelligence.","sentences":["We study the emergence of agency from scratch by using Large Language Model (LLM)-based agents.","In previous studies of LLM-based agents, each agent's characteristics, including personality and memory, have traditionally been predefined.","We focused on how individuality, such as behavior, personality, and memory, can be differentiated from an undifferentiated state.","The present LLM agents engage in cooperative communication within a group simulation, exchanging context-based messages in natural language.","By analyzing this multi-agent simulation, we report valuable new insights into how social norms, cooperation, and personality traits can emerge spontaneously.","This paper demonstrates that autonomously interacting LLM-powered agents generate hallucinations and hashtags to sustain communication, which, in turn, increases the diversity of words within their interactions.","Each agent's emotions shift through communication, and as they form communities, the personalities of the agents emerge and evolve accordingly.","This computational modeling approach and its findings will provide a new method for analyzing collective artificial intelligence."],"url":"http://arxiv.org/abs/2411.03252v1"}
{"created":"2024-11-05 16:47:53","title":"DiffLM: Controllable Synthetic Data Generation via Diffusion Language Models","abstract":"Recent advancements in large language models (LLMs) have significantly enhanced their knowledge and generative capabilities, leading to a surge of interest in leveraging LLMs for high-quality data synthesis. However, synthetic data generation via prompting LLMs remains challenging due to LLMs' limited understanding of target data distributions and the complexity of prompt engineering, especially for structured formatted data. To address these issues, we introduce DiffLM, a controllable data synthesis framework based on variational autoencoder (VAE), which further (1) leverages diffusion models to reserve more information of original distribution and format structure in the learned latent distribution and (2) decouples the learning of target distribution knowledge from the LLM's generative objectives via a plug-and-play latent feature injection module. As we observed significant discrepancies between the VAE's latent representations and the real data distribution, the latent diffusion module is introduced into our framework to learn a fully expressive latent distribution. Evaluations on seven real-world datasets with structured formatted data (i.e., Tabular, Code and Tool data) demonstrate that DiffLM generates high-quality data, with performance on downstream tasks surpassing that of real data by 2-7 percent in certain cases. The data and code will be publicly available upon completion of internal review.","sentences":["Recent advancements in large language models (LLMs) have significantly enhanced their knowledge and generative capabilities, leading to a surge of interest in leveraging LLMs for high-quality data synthesis.","However, synthetic data generation via prompting LLMs remains challenging due to LLMs' limited understanding of target data distributions and the complexity of prompt engineering, especially for structured formatted data.","To address these issues, we introduce DiffLM, a controllable data synthesis framework based on variational autoencoder (VAE), which further (1) leverages diffusion models to reserve more information of original distribution and format structure in the learned latent distribution and (2) decouples the learning of target distribution knowledge from the LLM's generative objectives via a plug-and-play latent feature injection module.","As we observed significant discrepancies between the VAE's latent representations and the real data distribution, the latent diffusion module is introduced into our framework to learn a fully expressive latent distribution.","Evaluations on seven real-world datasets with structured formatted data (i.e., Tabular, Code and Tool data) demonstrate that DiffLM generates high-quality data, with performance on downstream tasks surpassing that of real data by 2-7 percent in certain cases.","The data and code will be publicly available upon completion of internal review."],"url":"http://arxiv.org/abs/2411.03250v1"}
{"created":"2024-11-05 16:46:23","title":"On the Role of Constraints in the Complexity of Min-Max Optimization","abstract":"We investigate the role of constraints in the computational complexity of min-max optimization. First, we show that when the constraints are jointly convex (i.e., the min player and max player share the same constraints), computing a local min-max equilibrium with a nonconvex-concave objective is PPAD-hard. This improves the result of Daskalakis, Skoulakis, and Zampetakis [2021] along multiple directions: it applies to nonconvex-concave objectives (instead of nonconvex-nonconcave ones) that are degree-two polynomials, and it's essentially tight in the parameters. Second, we show that with general constraints (i.e., the min player and max player have different constraints), even convex-concave min-max optimization becomes PPAD-hard. Conversely, local min-max equilibria for nonconvex-concave and convex-concave objectives can be computed in polynomial time under simpler classes of constraints. Therefore, our results show that constraints are a key driver of the complexity of min-max optimization problems. Along the way, we also provide PPAD-membership of a general problem related to quasi-variational inequalities, which has applications beyond our problem.","sentences":["We investigate the role of constraints in the computational complexity of min-max optimization.","First, we show that when the constraints are jointly convex (i.e., the min player and max player share the same constraints), computing a local min-max equilibrium with a nonconvex-concave objective is PPAD-hard.","This improves the result of Daskalakis, Skoulakis, and Zampetakis","[2021] along multiple directions: it applies to nonconvex-concave objectives (instead of nonconvex-nonconcave ones) that are degree-two polynomials, and it's essentially tight in the parameters.","Second, we show that with general constraints (i.e., the min player and max player have different constraints), even convex-concave min-max optimization becomes PPAD-hard.","Conversely, local min-max equilibria for nonconvex-concave and convex-concave objectives can be computed in polynomial time under simpler classes of constraints.","Therefore, our results show that constraints are a key driver of the complexity of min-max optimization problems.","Along the way, we also provide PPAD-membership of a general problem related to quasi-variational inequalities, which has applications beyond our problem."],"url":"http://arxiv.org/abs/2411.03248v1"}
{"created":"2024-11-05 16:46:01","title":"Exploring Multi-Fidelity Aeroelastic Tailoring: Prospect and Model Assessment","abstract":"The design and optimisation of aircraft wings are critical tasks in aerospace engineering, requiring a balance between structural integrity, aerostructural performance, and manufacturability. This multifaceted challenge involves the interplay of various disciplines, each with distinct parameters and constraints. Traditional design approaches often fall short, necessitating advanced methodologies like Multidisciplinary Design Optimisation (MDO). MDO integrates aerodynamic, structural, and manufacturability analyses to explore a vast design space and identify optimal solutions that meet performance, safety, and cost criteria. The work highlights the challenge of optimising aircraft designs using multiple models of varying fidelity. Traditional sequential optimisation approaches, which progressively integrate disciplines, may miss potential superior designs due to limited initial information. Instead, concurrent optimisation schemes are explored, utilising both low-fidelity (beam-based) and high-fidelity (shell-based) models. This approach promises structural feasibility, reduces computational costs, and incorporates high-fidelity information early in the design process. The envisioned methodology bridges different design stages, enabling better overall aircraft performance. By aligning and comparing a beam-based and shell-based model, the study explores their use in multi-fidelity optimisation. The results demonstrate the feasibility and benefits of this approach, offering a robust framework for future aircraft design projects.","sentences":["The design and optimisation of aircraft wings are critical tasks in aerospace engineering, requiring a balance between structural integrity, aerostructural performance, and manufacturability.","This multifaceted challenge involves the interplay of various disciplines, each with distinct parameters and constraints.","Traditional design approaches often fall short, necessitating advanced methodologies like Multidisciplinary Design Optimisation (MDO).","MDO integrates aerodynamic, structural, and manufacturability analyses to explore a vast design space and identify optimal solutions that meet performance, safety, and cost criteria.","The work highlights the challenge of optimising aircraft designs using multiple models of varying fidelity.","Traditional sequential optimisation approaches, which progressively integrate disciplines, may miss potential superior designs due to limited initial information.","Instead, concurrent optimisation schemes are explored, utilising both low-fidelity (beam-based) and high-fidelity (shell-based) models.","This approach promises structural feasibility, reduces computational costs, and incorporates high-fidelity information early in the design process.","The envisioned methodology bridges different design stages, enabling better overall aircraft performance.","By aligning and comparing a beam-based and shell-based model, the study explores their use in multi-fidelity optimisation.","The results demonstrate the feasibility and benefits of this approach, offering a robust framework for future aircraft design projects."],"url":"http://arxiv.org/abs/2411.03247v1"}
{"created":"2024-11-05 16:39:46","title":"Guidelines para Desenvolvimento de Jogos Mobile Inclusivos","abstract":"Games represent a significant part of modern culture, which demonstrates the importance of ensuring that everyone can participate and play in order to feel included in our society. However, most digital games end up being inaccessible to people with disabilities. Part of the problem when thinking about inclusive game design is that there is no single solution for accessibility, and what works well for one group may not work for another. This work proposes a set of guidelines for the development of inclusive mobile games, considering the widespread use of smartphones by the population and the need to include people with disabilities in the gaming culture.","sentences":["Games represent a significant part of modern culture, which demonstrates the importance of ensuring that everyone can participate and play in order to feel included in our society.","However, most digital games end up being inaccessible to people with disabilities.","Part of the problem when thinking about inclusive game design is that there is no single solution for accessibility, and what works well for one group may not work for another.","This work proposes a set of guidelines for the development of inclusive mobile games, considering the widespread use of smartphones by the population and the need to include people with disabilities in the gaming culture."],"url":"http://arxiv.org/abs/2411.03243v1"}
{"created":"2024-11-05 16:38:49","title":"Distributed Quantum Advantage for Local Problems","abstract":"We present the first local problem that shows a super-constant separation between the classical randomized LOCAL model of distributed computing and its quantum counterpart. By prior work, such a separation was known only for an artificial graph problem with an inherently global definition [Le Gall et al. 2019]. We present a problem that we call iterated GHZ, which is defined using only local constraints. Formally, it is a family of locally checkable labeling problems [Naor and Stockmeyer 1995]; in particular, solutions can be verified with a constant-round distributed algorithm. We show that in graphs of maximum degree $\\Delta$, any classical (deterministic or randomized) LOCAL model algorithm will require $\\Omega(\\Delta)$ rounds to solve the iterated GHZ problem, while the problem can be solved in $1$ round in quantum-LOCAL. We use the round elimination technique to prove that the iterated GHZ problem requires $\\Omega(\\Delta)$ rounds for classical algorithms. This is the first work that shows that round elimination is indeed able to separate the two models, and this also demonstrates that round elimination cannot be used to prove lower bounds for quantum-LOCAL. To apply round elimination, we introduce a new technique that allows us to discover appropriate problem relaxations in a mechanical way; it turns out that this new technique extends beyond the scope of the iterated GHZ problem and can be used to e.g. reproduce prior results on maximal matchings [FOCS 2019, PODC 2020] in a systematic manner.","sentences":["We present the first local problem that shows a super-constant separation between the classical randomized LOCAL model of distributed computing and its quantum counterpart.","By prior work, such a separation was known only for an artificial graph problem with an inherently global definition [Le Gall et al. 2019].","We present a problem that we call iterated GHZ, which is defined using only local constraints.","Formally, it is a family of locally checkable labeling problems","[Naor and Stockmeyer 1995]; in particular, solutions can be verified with a constant-round distributed algorithm.","We show that in graphs of maximum degree $\\Delta$, any classical (deterministic or randomized) LOCAL model algorithm will require $\\Omega(\\Delta)$ rounds to solve the iterated GHZ problem, while the problem can be solved in $1$ round in quantum-LOCAL.","We use the round elimination technique to prove that the iterated GHZ problem requires $\\Omega(\\Delta)$ rounds for classical algorithms.","This is the first work that shows that round elimination is indeed able to separate the two models, and this also demonstrates that round elimination cannot be used to prove lower bounds for quantum-LOCAL.","To apply round elimination, we introduce a new technique that allows us to discover appropriate problem relaxations in a mechanical way; it turns out that this new technique extends beyond the scope of the iterated GHZ problem and can be used to e.g. reproduce prior results on maximal matchings","[FOCS 2019, PODC 2020] in a systematic manner."],"url":"http://arxiv.org/abs/2411.03240v1"}
{"created":"2024-11-05 16:37:30","title":"Decoupling Fine Detail and Global Geometry for Compressed Depth Map Super-Resolution","abstract":"Recovering high-quality depth maps from compressed sources has gained significant attention due to the limitations of consumer-grade depth cameras and the bandwidth restrictions during data transmission. However, current methods still suffer from two challenges. First, bit-depth compression produces a uniform depth representation in regions with subtle variations, hindering the recovery of detailed information. Second, densely distributed random noise reduces the accuracy of estimating the global geometric structure of the scene. To address these challenges, we propose a novel framework, termed geometry-decoupled network (GDNet), for compressed depth map super-resolution that decouples the high-quality depth map reconstruction process by handling global and detailed geometric features separately. To be specific, we propose the fine geometry detail encoder (FGDE), which is designed to aggregate fine geometry details in high-resolution low-level image features while simultaneously enriching them with complementary information from low-resolution context-level image features. In addition, we develop the global geometry encoder (GGE) that aims at suppressing noise and extracting global geometric information effectively via constructing compact feature representation in a low-rank space. We conduct experiments on multiple benchmark datasets, demonstrating that our GDNet significantly outperforms current methods in terms of geometric consistency and detail recovery. In the ECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st place award. Our codes will be available.","sentences":["Recovering high-quality depth maps from compressed sources has gained significant attention due to the limitations of consumer-grade depth cameras and the bandwidth restrictions during data transmission.","However, current methods still suffer from two challenges.","First, bit-depth compression produces a uniform depth representation in regions with subtle variations, hindering the recovery of detailed information.","Second, densely distributed random noise reduces the accuracy of estimating the global geometric structure of the scene.","To address these challenges, we propose a novel framework, termed geometry-decoupled network (GDNet), for compressed depth map super-resolution that decouples the high-quality depth map reconstruction process by handling global and detailed geometric features separately.","To be specific, we propose the fine geometry detail encoder (FGDE), which is designed to aggregate fine geometry details in high-resolution low-level image features while simultaneously enriching them with complementary information from low-resolution context-level image features.","In addition, we develop the global geometry encoder (GGE) that aims at suppressing noise and extracting global geometric information effectively via constructing compact feature representation in a low-rank space.","We conduct experiments on multiple benchmark datasets, demonstrating that our GDNet significantly outperforms current methods in terms of geometric consistency and detail recovery.","In the ECCV 2024 AIM Compressed Depth Upsampling Challenge, our solution won the 1st place award.","Our codes will be available."],"url":"http://arxiv.org/abs/2411.03239v1"}
{"created":"2024-11-05 16:36:51","title":"On the Detection of Non-Cooperative RISs: Scan B-Testing via Deep Support Vector Data Description","abstract":"In this paper, we study the problem of promptly detecting the presence of non-cooperative activity from one or more Reconfigurable Intelligent Surfaces (RISs) with unknown characteristics lying in the vicinity of a Multiple-Input Multiple-Output (MIMO) communication system using Orthogonal Frequency-Division Multiplexing (OFDM) transmissions. We first present a novel wideband channel model incorporating RISs as well as non-reconfigurable stationary surfaces, which captures both the effect of the RIS actuation time on the channel in the frequency domain as well as the difference between changing phase configurations during or among transmissions. Considering that RISs may operate under the coordination of a third-party system, and thus, may negatively impact the communication of the intended MIMO OFDM system, we present a novel RIS activity detection framework that is unaware of the distribution of the phase configuration of any of the non-cooperative RISs. In particular, capitalizing on the knowledge of the data distribution at the multi-antenna receiver, we design a novel online change point detection statistic that combines a deep support vector data description model with the scan $B$-test. The presented numerical investigations demonstrate the improved detection accuracy as well as decreased computational complexity of the proposed RIS detection approach over existing change point detection schemes.","sentences":["In this paper, we study the problem of promptly detecting the presence of non-cooperative activity from one or more Reconfigurable Intelligent Surfaces (RISs) with unknown characteristics lying in the vicinity of a Multiple-Input Multiple-Output (MIMO) communication system using Orthogonal Frequency-Division Multiplexing (OFDM) transmissions.","We first present a novel wideband channel model incorporating RISs as well as non-reconfigurable stationary surfaces, which captures both the effect of the RIS actuation time on the channel in the frequency domain as well as the difference between changing phase configurations during or among transmissions.","Considering that RISs may operate under the coordination of a third-party system, and thus, may negatively impact the communication of the intended MIMO OFDM system, we present a novel RIS activity detection framework that is unaware of the distribution of the phase configuration of any of the non-cooperative RISs.","In particular, capitalizing on the knowledge of the data distribution at the multi-antenna receiver, we design a novel online change point detection statistic that combines a deep support vector data description model with the scan $B$-test.","The presented numerical investigations demonstrate the improved detection accuracy as well as decreased computational complexity of the proposed RIS detection approach over existing change point detection schemes."],"url":"http://arxiv.org/abs/2411.03237v1"}
{"created":"2024-11-05 16:36:07","title":"Enhancing Transformer Training Efficiency with Dynamic Dropout","abstract":"We introduce Dynamic Dropout, a novel regularization technique designed to enhance the training efficiency of Transformer models by dynamically adjusting the dropout rate based on training epochs or validation loss improvements. This approach addresses the challenge of balancing regularization and model capacity, which is crucial for achieving fast convergence and high performance. Our method involves modifying the GPT model to accept a variable dropout rate and updating dropout layers during training using schedules such as linear decay, exponential decay, and validation loss-based adjustments. Extensive experiments on the Shakespeare\\_char dataset demonstrate that Dynamic Dropout significantly accelerates training and improves inference efficiency compared to a baseline model with a fixed dropout rate. The validation loss-based adjustment schedule provided the best overall performance, highlighting the potential of Dynamic Dropout as a valuable technique for training large-scale Transformer models.","sentences":["We introduce Dynamic Dropout, a novel regularization technique designed to enhance the training efficiency of Transformer models by dynamically adjusting the dropout rate based on training epochs or validation loss improvements.","This approach addresses the challenge of balancing regularization and model capacity, which is crucial for achieving fast convergence and high performance.","Our method involves modifying the GPT model to accept a variable dropout rate and updating dropout layers during training using schedules such as linear decay, exponential decay, and validation loss-based adjustments.","Extensive experiments on the Shakespeare\\_char dataset demonstrate that Dynamic Dropout significantly accelerates training and improves inference efficiency compared to a baseline model with a fixed dropout rate.","The validation loss-based adjustment schedule provided the best overall performance, highlighting the potential of Dynamic Dropout as a valuable technique for training large-scale Transformer models."],"url":"http://arxiv.org/abs/2411.03236v1"}
{"created":"2024-11-05 16:23:19","title":"Formal Logic-guided Robust Federated Learning against Poisoning Attacks","abstract":"Federated Learning (FL) offers a promising solution to the privacy concerns associated with centralized Machine Learning (ML) by enabling decentralized, collaborative learning. However, FL is vulnerable to various security threats, including poisoning attacks, where adversarial clients manipulate the training data or model updates to degrade overall model performance. Recognizing this threat, researchers have focused on developing defense mechanisms to counteract poisoning attacks in FL systems. However, existing robust FL methods predominantly focus on computer vision tasks, leaving a gap in addressing the unique challenges of FL with time series data. In this paper, we present FLORAL, a defense mechanism designed to mitigate poisoning attacks in federated learning for time-series tasks, even in scenarios with heterogeneous client data and a large number of adversarial participants. Unlike traditional model-centric defenses, FLORAL leverages logical reasoning to evaluate client trustworthiness by aligning their predictions with global time-series patterns, rather than relying solely on the similarity of client updates. Our approach extracts logical reasoning properties from clients, then hierarchically infers global properties, and uses these to verify client updates. Through formal logic verification, we assess the robustness of each client contribution, identifying deviations indicative of adversarial behavior. Experimental results on two datasets demonstrate the superior performance of our approach compared to existing baseline methods, highlighting its potential to enhance the robustness of FL to time series applications. Notably, FLORAL reduced the prediction error by 93.27\\% in the best-case scenario compared to the second-best baseline. Our code is available at \\url{https://anonymous.4open.science/r/FLORAL-Robust-FTS}.","sentences":["Federated Learning (FL) offers a promising solution to the privacy concerns associated with centralized Machine Learning (ML) by enabling decentralized, collaborative learning.","However, FL is vulnerable to various security threats, including poisoning attacks, where adversarial clients manipulate the training data or model updates to degrade overall model performance.","Recognizing this threat, researchers have focused on developing defense mechanisms to counteract poisoning attacks in FL systems.","However, existing robust FL methods predominantly focus on computer vision tasks, leaving a gap in addressing the unique challenges of FL with time series data.","In this paper, we present FLORAL, a defense mechanism designed to mitigate poisoning attacks in federated learning for time-series tasks, even in scenarios with heterogeneous client data and a large number of adversarial participants.","Unlike traditional model-centric defenses, FLORAL leverages logical reasoning to evaluate client trustworthiness by aligning their predictions with global time-series patterns, rather than relying solely on the similarity of client updates.","Our approach extracts logical reasoning properties from clients, then hierarchically infers global properties, and uses these to verify client updates.","Through formal logic verification, we assess the robustness of each client contribution, identifying deviations indicative of adversarial behavior.","Experimental results on two datasets demonstrate the superior performance of our approach compared to existing baseline methods, highlighting its potential to enhance the robustness of FL to time series applications.","Notably, FLORAL reduced the prediction error by 93.27\\% in the best-case scenario compared to the second-best baseline.","Our code is available at \\url{https://anonymous.4open.science/r/FLORAL-Robust-FTS}."],"url":"http://arxiv.org/abs/2411.03231v1"}
{"created":"2024-11-05 16:20:14","title":"Topograph: An efficient Graph-Based Framework for Strictly Topology Preserving Image Segmentation","abstract":"Topological correctness plays a critical role in many image segmentation tasks, yet most networks are trained using pixel-wise loss functions, such as Dice, neglecting topological accuracy. Existing topology-aware methods often lack robust topological guarantees, are limited to specific use cases, or impose high computational costs. In this work, we propose a novel, graph-based framework for topologically accurate image segmentation that is both computationally efficient and generally applicable. Our method constructs a component graph that fully encodes the topological information of both the prediction and ground truth, allowing us to efficiently identify topologically critical regions and aggregate a loss based on local neighborhood information. Furthermore, we introduce a strict topological metric capturing the homotopy equivalence between the union and intersection of prediction-label pairs. We formally prove the topological guarantees of our approach and empirically validate its effectiveness on binary and multi-class datasets. Our loss demonstrates state-of-the-art performance with up to fivefold faster loss computation compared to persistent homology methods.","sentences":["Topological correctness plays a critical role in many image segmentation tasks, yet most networks are trained using pixel-wise loss functions, such as Dice, neglecting topological accuracy.","Existing topology-aware methods often lack robust topological guarantees, are limited to specific use cases, or impose high computational costs.","In this work, we propose a novel, graph-based framework for topologically accurate image segmentation that is both computationally efficient and generally applicable.","Our method constructs a component graph that fully encodes the topological information of both the prediction and ground truth, allowing us to efficiently identify topologically critical regions and aggregate a loss based on local neighborhood information.","Furthermore, we introduce a strict topological metric capturing the homotopy equivalence between the union and intersection of prediction-label pairs.","We formally prove the topological guarantees of our approach and empirically validate its effectiveness on binary and multi-class datasets.","Our loss demonstrates state-of-the-art performance with up to fivefold faster loss computation compared to persistent homology methods."],"url":"http://arxiv.org/abs/2411.03228v1"}
{"created":"2024-11-05 16:19:43","title":"Tight Sampling Bounds for Eigenvalue Approximation","abstract":"We consider the problem of estimating the spectrum of a symmetric bounded entry (not necessarily PSD) matrix via entrywise sampling. This problem was introduced by [Bhattacharjee, Dexter, Drineas, Musco, Ray '22], where it was shown that one can obtain an $\\epsilon n$ additive approximation to all eigenvalues of $A$ by sampling a principal submatrix of dimension $\\frac{\\text{poly}(\\log n)}{\\epsilon^3}$. We improve their analysis by showing that it suffices to sample a principal submatrix of dimension $\\tilde{O}(\\frac{1}{\\epsilon^2})$ (with no dependence on $n$). This matches known lower bounds and therefore resolves the sample complexity of this problem up to $\\log\\frac{1}{\\epsilon}$ factors. Using similar techniques, we give a tight $\\tilde{O}(\\frac{1}{\\epsilon^2})$ bound for obtaining an additive $\\epsilon\\|A\\|_F$ approximation to the spectrum of $A$ via squared row-norm sampling, improving on the previous best $\\tilde{O}(\\frac{1}{\\epsilon^{8}})$ bound. We also address the problem of approximating the top eigenvector for a bounded entry, PSD matrix $A.$ In particular, we show that sampling $O(\\frac{1}{\\epsilon})$ columns of $A$ suffices to produce a unit vector $u$ with $u^T A u \\geq \\lambda_1(A) - \\epsilon n$. This matches what one could achieve via the sampling bound of [Musco, Musco'17] for the special case of approximating the top eigenvector, but does not require adaptivity.   As additional applications, we observe that our sampling results can be used to design a faster eigenvalue estimation sketch for dense matrices resolving a question of [Swartworth, Woodruff'23], and can also be combined with [Musco, Musco'17] to achieve $O(1/\\epsilon^3)$ (adaptive) sample complexity for approximating the spectrum of a bounded entry PSD matrix to $\\epsilon n$ additive error.","sentences":["We consider the problem of estimating the spectrum of a symmetric bounded entry (not necessarily PSD) matrix via entrywise sampling.","This problem was introduced by [Bhattacharjee, Dexter, Drineas, Musco, Ray '22], where it was shown that one can obtain an $\\epsilon n$ additive approximation to all eigenvalues of $A$ by sampling a principal submatrix of dimension $\\frac{\\text{poly}(\\log n)}{\\epsilon^3}$.","We improve their analysis by showing that it suffices to sample a principal submatrix of dimension $\\tilde{O}(\\frac{1}{\\epsilon^2})$ (with no dependence on $n$).","This matches known lower bounds and therefore resolves the sample complexity of this problem up to $\\log\\frac{1}{\\epsilon}$ factors.","Using similar techniques, we give a tight $\\tilde{O}(\\frac{1}{\\epsilon^2})$ bound for obtaining an additive $\\epsilon\\|A\\|_F$ approximation to the spectrum of $A$ via squared row-norm sampling, improving on the previous best $\\tilde{O}(\\frac{1}{\\epsilon^{8}})$ bound.","We also address the problem of approximating the top eigenvector for a bounded entry, PSD matrix $A.$","In particular, we show that sampling $O(\\frac{1}{\\epsilon})$ columns of $A$ suffices to produce a unit vector $u$ with $u^T","A u \\geq \\lambda_1(A) - \\epsilon n$. This matches what one could achieve via the sampling bound of [Musco, Musco'17] for the special case of approximating the top eigenvector, but does not require adaptivity.   ","As additional applications, we observe that our sampling results can be used to design a faster eigenvalue estimation sketch for dense matrices resolving a question of [Swartworth, Woodruff'23], and can also be combined with [Musco, Musco'17] to achieve $O(1/\\epsilon^3)$ (adaptive) sample complexity for approximating the spectrum of a bounded entry PSD matrix to $\\epsilon n$ additive error."],"url":"http://arxiv.org/abs/2411.03227v1"}
{"created":"2024-11-05 16:18:57","title":"Kernel Orthogonality does not necessarily imply a Decrease in Feature Map Redundancy in CNNs: Convolutional Similarity Minimization","abstract":"Convolutional Neural Networks (CNNs) have been heavily used in Deep Learning due to their success in various tasks. Nonetheless, it has been observed that CNNs suffer from redundancy in feature maps, leading to inefficient capacity utilization. Efforts to mitigate and solve this problem led to the emergence of multiple methods, amongst which is kernel orthogonality through variant means. In this work, we challenge the common belief that kernel orthogonality leads to a decrease in feature map redundancy, which is, supposedly, the ultimate objective behind kernel orthogonality. We prove, theoretically and empirically, that kernel orthogonality has an unpredictable effect on feature map similarity and does not necessarily decrease it. Based on our theoretical result, we propose an effective method to reduce feature map similarity independently of the input of the CNN. This is done by minimizing a novel loss function we call Convolutional Similarity. Empirical results show that minimizing the Convolutional Similarity increases the performance of classification models and can accelerate their convergence. Furthermore, using our proposed method pushes towards a more efficient use of the capacity of models, allowing the use of significantly smaller models to achieve the same levels of performance.","sentences":["Convolutional Neural Networks (CNNs) have been heavily used in Deep Learning due to their success in various tasks.","Nonetheless, it has been observed that CNNs suffer from redundancy in feature maps, leading to inefficient capacity utilization.","Efforts to mitigate and solve this problem led to the emergence of multiple methods, amongst which is kernel orthogonality through variant means.","In this work, we challenge the common belief that kernel orthogonality leads to a decrease in feature map redundancy, which is, supposedly, the ultimate objective behind kernel orthogonality.","We prove, theoretically and empirically, that kernel orthogonality has an unpredictable effect on feature map similarity and does not necessarily decrease it.","Based on our theoretical result, we propose an effective method to reduce feature map similarity independently of the input of the CNN.","This is done by minimizing a novel loss function we call Convolutional Similarity.","Empirical results show that minimizing the Convolutional Similarity increases the performance of classification models and can accelerate their convergence.","Furthermore, using our proposed method pushes towards a more efficient use of the capacity of models, allowing the use of significantly smaller models to achieve the same levels of performance."],"url":"http://arxiv.org/abs/2411.03226v1"}
{"created":"2024-11-05 16:15:33","title":"Knowledge Graphs of Driving Scenes to Empower the Emerging Capabilities of Neurosymbolic AI","abstract":"In the era of Generative AI, Neurosymbolic AI is emerging as a powerful approach for tasks spanning from perception to cognition. The use of Neurosymbolic AI has been shown to achieve enhanced capabilities, including improved grounding, alignment, explainability, and reliability. However, due to its nascent stage, there is a lack of widely available real-world benchmark datasets tailored to Neurosymbolic AI tasks. To address this gap and support the evaluation of current and future methods, we introduce DSceneKG -- a suite of knowledge graphs of driving scenes built from real-world, high-quality scenes from multiple open autonomous driving datasets. In this article, we detail the construction process of DSceneKG and highlight its application in seven different tasks. DSceneKG is publicly accessible at: https://github.com/ruwantw/DSceneKG","sentences":["In the era of Generative AI, Neurosymbolic AI is emerging as a powerful approach for tasks spanning from perception to cognition.","The use of Neurosymbolic AI has been shown to achieve enhanced capabilities, including improved grounding, alignment, explainability, and reliability.","However, due to its nascent stage, there is a lack of widely available real-world benchmark datasets tailored to Neurosymbolic AI tasks.","To address this gap and support the evaluation of current and future methods, we introduce DSceneKG -- a suite of knowledge graphs of driving scenes built from real-world, high-quality scenes from multiple open autonomous driving datasets.","In this article, we detail the construction process of DSceneKG and highlight its application in seven different tasks.","DSceneKG is publicly accessible at: https://github.com/ruwantw/DSceneKG"],"url":"http://arxiv.org/abs/2411.03225v1"}
{"created":"2024-11-05 16:15:25","title":"Interpretable Predictive Models for Healthcare via Rational Logistic Regression","abstract":"The healthcare sector has experienced a rapid accumulation of digital data recently, especially in the form of electronic health records (EHRs). EHRs constitute a precious resource that IS researchers could utilize for clinical applications (e.g., morbidity prediction). Deep learning seems like the obvious choice to exploit this surfeit of data. However, numerous studies have shown that deep learning does not enjoy the same kind of success on EHR data as it has in other domains; simple models like logistic regression are frequently as good as sophisticated deep learning ones. Inspired by this observation, we develop a novel model called rational logistic regression (RLR) that has standard logistic regression (LR) as its special case (and thus inherits LR's inductive bias that aligns with EHR data). RLR has rational series as its theoretical underpinnings, works on longitudinal time-series data, and learns interpretable patterns. Empirical comparisons on real-world clinical tasks demonstrate RLR's efficacy.","sentences":["The healthcare sector has experienced a rapid accumulation of digital data recently, especially in the form of electronic health records (EHRs).","EHRs constitute a precious resource that IS researchers could utilize for clinical applications (e.g., morbidity prediction).","Deep learning seems like the obvious choice to exploit this surfeit of data.","However, numerous studies have shown that deep learning does not enjoy the same kind of success on EHR data as it has in other domains; simple models like logistic regression are frequently as good as sophisticated deep learning ones.","Inspired by this observation, we develop a novel model called rational logistic regression (RLR) that has standard logistic regression (LR) as its special case (and thus inherits LR's inductive bias that aligns with EHR data).","RLR has rational series as its theoretical underpinnings, works on longitudinal time-series data, and learns interpretable patterns.","Empirical comparisons on real-world clinical tasks demonstrate RLR's efficacy."],"url":"http://arxiv.org/abs/2411.03224v1"}
{"created":"2024-11-05 16:12:12","title":"Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation","abstract":"Earth Observation (EO) data analysis has been significantly revolutionized by deep learning (DL), with applications typically limited to grid-like data structures. Graph Neural Networks (GNNs) emerge as an important innovation, propelling DL into the non-Euclidean domain. Naturally, GNNs can effectively tackle the challenges posed by diverse modalities, multiple sensors, and the heterogeneous nature of EO data. To introduce GNNs in the related domains, our review begins by offering fundamental knowledge on GNNs. Then, we summarize the generic problems in EO, to which GNNs can offer potential solutions. Following this, we explore a broad spectrum of GNNs' applications to scientific problems in Earth systems, covering areas such as weather and climate analysis, disaster management, air quality monitoring, agriculture, land cover classification, hydrological process modeling, and urban modeling. The rationale behind adopting GNNs in these fields is explained, alongside methodologies for organizing graphs and designing favorable architectures for various tasks. Furthermore, we highlight methodological challenges of implementing GNNs in these domains and possible solutions that could guide future research. While acknowledging that GNNs are not a universal solution, we conclude the paper by comparing them with other popular architectures like transformers and analyzing their potential synergies.","sentences":["Earth Observation (EO) data analysis has been significantly revolutionized by deep learning (DL), with applications typically limited to grid-like data structures.","Graph Neural Networks (GNNs) emerge as an important innovation, propelling DL into the non-Euclidean domain.","Naturally, GNNs can effectively tackle the challenges posed by diverse modalities, multiple sensors, and the heterogeneous nature of EO data.","To introduce GNNs in the related domains, our review begins by offering fundamental knowledge on GNNs.","Then, we summarize the generic problems in EO, to which GNNs can offer potential solutions.","Following this, we explore a broad spectrum of GNNs' applications to scientific problems in Earth systems, covering areas such as weather and climate analysis, disaster management, air quality monitoring, agriculture, land cover classification, hydrological process modeling, and urban modeling.","The rationale behind adopting GNNs in these fields is explained, alongside methodologies for organizing graphs and designing favorable architectures for various tasks.","Furthermore, we highlight methodological challenges of implementing GNNs in these domains and possible solutions that could guide future research.","While acknowledging that GNNs are not a universal solution, we conclude the paper by comparing them with other popular architectures like transformers and analyzing their potential synergies."],"url":"http://arxiv.org/abs/2411.03223v1"}
{"created":"2024-11-05 16:09:37","title":"Exploring the Cybersecurity-Resilience Gap: An Analysis of Student Attitudes and Behaviors in Higher Education","abstract":"Cyberattacks frequently target higher educational institutions, making cybersecurity awareness and resilience critical for students. However, limited research exists on cybersecurity awareness, attitudes, and resilience among students in higher education. This study addresses this gap using the Theory of Planned Behavior as a theoretical framework. A modified Human Aspects of Information Security Questionnaire was employed to gather 266 valid responses from undergraduate and postgraduate students at a South African higher education institution. Key dimensions of cybersecurity awareness and behavior, including password management, email usage, social media practices, and mobile device security, were assessed. A significant disparity in cybersecurity awareness and practices, with postgraduate students demonstrating superior performance across several dimensions was noted. This research postulates the existence of a Cybersecurity-Education Inflection Point during the transition to postgraduate studies, coined as the Cybersecurity-Resilience Gap. These concepts provide a foundation for developing targeted cybersecurity education initiatives in higher education, particularly highlighting the need for earlier intervention at the undergraduate level.","sentences":["Cyberattacks frequently target higher educational institutions, making cybersecurity awareness and resilience critical for students.","However, limited research exists on cybersecurity awareness, attitudes, and resilience among students in higher education.","This study addresses this gap using the Theory of Planned Behavior as a theoretical framework.","A modified Human Aspects of Information Security Questionnaire was employed to gather 266 valid responses from undergraduate and postgraduate students at a South African higher education institution.","Key dimensions of cybersecurity awareness and behavior, including password management, email usage, social media practices, and mobile device security, were assessed.","A significant disparity in cybersecurity awareness and practices, with postgraduate students demonstrating superior performance across several dimensions was noted.","This research postulates the existence of a Cybersecurity-Education Inflection Point during the transition to postgraduate studies, coined as the Cybersecurity-Resilience Gap.","These concepts provide a foundation for developing targeted cybersecurity education initiatives in higher education, particularly highlighting the need for earlier intervention at the undergraduate level."],"url":"http://arxiv.org/abs/2411.03219v1"}
{"created":"2024-11-05 16:02:54","title":"What Makes an Educational Robot Game Fun? Framework Analysis of Children's Design Ideas","abstract":"Fun acts as a catalyst for learning by enhancing motivation, active engagement and knowledge retention. As social robots gain traction as educational tools, understanding how their unique affordances can be leveraged to cultivate fun becomes crucial. This research investigates the concept of fun in educational games involving social robots to support the design of REMind:a robot-mediated role-play game aimed at encouraging bystander intervention against peer bullying among children. To incorporate fun elements into design of REMind, we conducted a user-centered Research through Design (RtD) study with focus groups of children to gain a deeper understanding of their perceptions of fun. We analyzed children's ideas by using Framework Analysis and leveraging LeBlanc's Taxonomy of Game Pleasures and identified 28 elements of fun that can be incorporated into robot-mediated games. We present our observations, discuss their impact on REMind's design, and offer recommendations for designing fun educational games using social robots.","sentences":["Fun acts as a catalyst for learning by enhancing motivation, active engagement and knowledge retention.","As social robots gain traction as educational tools, understanding how their unique affordances can be leveraged to cultivate fun becomes crucial.","This research investigates the concept of fun in educational games involving social robots to support the design of REMind:a robot-mediated role-play game aimed at encouraging bystander intervention against peer bullying among children.","To incorporate fun elements into design of REMind, we conducted a user-centered Research through Design (RtD) study with focus groups of children to gain a deeper understanding of their perceptions of fun.","We analyzed children's ideas by using Framework Analysis and leveraging LeBlanc's Taxonomy of Game Pleasures and identified 28 elements of fun that can be incorporated into robot-mediated games.","We present our observations, discuss their impact on REMind's design, and offer recommendations for designing fun educational games using social robots."],"url":"http://arxiv.org/abs/2411.03213v1"}
{"created":"2024-11-05 15:53:59","title":"GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis","abstract":"Recent advancements in Generative AI offer promising capabilities for spatial analysis. Despite their potential, the integration of generative AI with established GIS platforms remains underexplored. In this study, we propose a framework for integrating LLMs directly into existing GIS platforms, using QGIS as an example. Our approach leverages the reasoning and programming capabilities of LLMs to autonomously generate spatial analysis workflows and code through an informed agent that has comprehensive documentation of key GIS tools and parameters. The implementation of this framework resulted in the development of a \"GIS Copilot\" that allows GIS users to interact with QGIS using natural language commands for spatial analysis. The GIS Copilot was evaluated based on three complexity levels: basic tasks that require one GIS tool and typically involve one data layer to perform simple operations; intermediate tasks involving multi-step processes with multiple tools, guided by user instructions; and advanced tasks which involve multi-step processes that require multiple tools but not guided by user instructions, necessitating the agent to independently decide on and executes the necessary steps. The evaluation reveals that the GIS Copilot demonstrates strong potential in automating foundational GIS operations, with a high success rate in tool selection and code generation for basic and intermediate tasks, while challenges remain in achieving full autonomy for more complex tasks. This study contributes to the emerging vision of Autonomous GIS, providing a pathway for non-experts to engage with geospatial analysis with minimal prior expertise. While full autonomy is yet to be achieved, the GIS Copilot demonstrates significant potential for simplifying GIS workflows and enhancing decision-making processes.","sentences":["Recent advancements in Generative AI offer promising capabilities for spatial analysis.","Despite their potential, the integration of generative AI with established GIS platforms remains underexplored.","In this study, we propose a framework for integrating LLMs directly into existing GIS platforms, using QGIS as an example.","Our approach leverages the reasoning and programming capabilities of LLMs to autonomously generate spatial analysis workflows and code through an informed agent that has comprehensive documentation of key GIS tools and parameters.","The implementation of this framework resulted in the development of a \"GIS Copilot\" that allows GIS users to interact with QGIS using natural language commands for spatial analysis.","The GIS Copilot was evaluated based on three complexity levels: basic tasks that require one GIS tool and typically involve one data layer to perform simple operations; intermediate tasks involving multi-step processes with multiple tools, guided by user instructions; and advanced tasks which involve multi-step processes that require multiple tools but not guided by user instructions, necessitating the agent to independently decide on and executes the necessary steps.","The evaluation reveals that the GIS Copilot demonstrates strong potential in automating foundational GIS operations, with a high success rate in tool selection and code generation for basic and intermediate tasks, while challenges remain in achieving full autonomy for more complex tasks.","This study contributes to the emerging vision of Autonomous GIS, providing a pathway for non-experts to engage with geospatial analysis with minimal prior expertise.","While full autonomy is yet to be achieved, the GIS Copilot demonstrates significant potential for simplifying GIS workflows and enhancing decision-making processes."],"url":"http://arxiv.org/abs/2411.03205v1"}
{"created":"2024-11-05 15:50:26","title":"Statistical Analysis to Support CSI-Based Sensing Methods","abstract":"Building upon the foundational work of the Bachelor's Degree Thesis titled \"Analysis and Characterization of Wi-Fi Channel State Information'', this thesis significantly advances the research by conducting an in-depth analysis of CSIs, offering new insights that extend well beyond the original study. The goal of this work is to broaden the mathematical and statistical representation of a wireless channel through the study of CSI behavior and evolution over time and frequency.   CSI provides a high-level description of the behavior of a signal propagating from a transmitter to a receiver, thereby representing the structure of the environment where the signal propagates. This knowledge can be used to perform ambvient sensing, a technique that extracts relevant information about the surroundings of the receiver from the properties of the received signal, which are affected by interactions with the surfaces of the objects within the analyzed environment. Ambient sensing already plays an essential role in new wireless networks such as 5G and Beyond 5G; its use Joint Communication and Sensing applications and for the optimization of signal propagation through beamforming could also enable the implementation of efficient cooperative ambient sensing in vehicular networks, facilitating Cooperative Perception and, consequently, increasing road safety.   Due to the lack of research on CSI characterization, this study aims to begin analyzing the structure of CSI traces collected in a controlled experimental environment and to describe their statistical properties. The results of such characterization could provide mathematical support for environment classification and movement recognition tasks that are currently performed only through Machine Learning techniques, introducing instead efficient, dedicated algorithms.","sentences":["Building upon the foundational work of the Bachelor's Degree Thesis titled \"Analysis and Characterization of Wi-Fi Channel State Information'', this thesis significantly advances the research by conducting an in-depth analysis of CSIs, offering new insights that extend well beyond the original study.","The goal of this work is to broaden the mathematical and statistical representation of a wireless channel through the study of CSI behavior and evolution over time and frequency.   ","CSI provides a high-level description of the behavior of a signal propagating from a transmitter to a receiver, thereby representing the structure of the environment where the signal propagates.","This knowledge can be used to perform ambvient sensing, a technique that extracts relevant information about the surroundings of the receiver from the properties of the received signal, which are affected by interactions with the surfaces of the objects within the analyzed environment.","Ambient sensing already plays an essential role in new wireless networks such as 5G and Beyond 5G; its use Joint Communication and Sensing applications and for the optimization of signal propagation through beamforming could also enable the implementation of efficient cooperative ambient sensing in vehicular networks, facilitating Cooperative Perception and, consequently, increasing road safety.   ","Due to the lack of research on CSI characterization, this study aims to begin analyzing the structure of CSI traces collected in a controlled experimental environment and to describe their statistical properties.","The results of such characterization could provide mathematical support for environment classification and movement recognition tasks that are currently performed only through Machine Learning techniques, introducing instead efficient, dedicated algorithms."],"url":"http://arxiv.org/abs/2411.03203v1"}
{"created":"2024-11-05 15:39:36","title":"Energy Consumption in Robotics: A Simplified Modeling Approach","abstract":"The energy use of a robot is trajectory-dependent, and thus can be reduced by optimization of the trajectory. Current methods for robot trajectory optimization can reduce energy up to 15\\% for fixed start and end points, however their use in industrial robot planning is still restricted due to model complexity and lack of integration with planning tools which address other concerns (e.g. collision avoidance). We propose an approach that uses differentiable inertial and kinematic models from standard open-source tools, integrating with standard ROS planning methods. An inverse dynamics-based energy model is optionally extended with a single-parameter electrical model, simplifying the model identification process. We compare the inertial and electrical models on a collaborative robot, showing that simplified models provide competitive accuracy and are easier to deploy in practice.","sentences":["The energy use of a robot is trajectory-dependent, and thus can be reduced by optimization of the trajectory.","Current methods for robot trajectory optimization can reduce energy up to 15\\% for fixed start and end points, however their use in industrial robot planning is still restricted due to model complexity and lack of integration with planning tools which address other concerns (e.g. collision avoidance).","We propose an approach that uses differentiable inertial and kinematic models from standard open-source tools, integrating with standard ROS planning methods.","An inverse dynamics-based energy model is optionally extended with a single-parameter electrical model, simplifying the model identification process.","We compare the inertial and electrical models on a collaborative robot, showing that simplified models provide competitive accuracy and are easier to deploy in practice."],"url":"http://arxiv.org/abs/2411.03194v1"}
{"created":"2024-11-05 15:31:40","title":"Design-Reality Gap Analysis of Health Information Systems Failure","abstract":"This study investigates the factors contributing to the failure of Health Information Systems (HIS) in a public hospital in South Africa. While HIS have the potential to improve healthcare delivery by integrating services and enhancing effectiveness, failures can lead to service interruptions, revenue loss, data loss, administrative difficulties, and reputational damage. Using semi-structured interviews with key stakeholders, we employed a hybrid data analysis approach combining deductive analysis based on the Design- Reality Gap Model and inductive thematic analysis. Our findings highlight several factors contributing to HIS failures, including system capacity constraints, inadequate IT risk management, and critical skills gaps. Despite these challenges, end users perceive HIS positively and recommend its implementation for streamlining daily processes. This study underscores the importance of addressing design-reality gaps to improve HIS outcomes in public healthcare settings.","sentences":["This study investigates the factors contributing to the failure of Health Information Systems (HIS) in a public hospital in South Africa.","While HIS have the potential to improve healthcare delivery by integrating services and enhancing effectiveness, failures can lead to service interruptions, revenue loss, data loss, administrative difficulties, and reputational damage.","Using semi-structured interviews with key stakeholders, we employed a hybrid data analysis approach combining deductive analysis based on the Design- Reality Gap Model and inductive thematic analysis.","Our findings highlight several factors contributing to HIS failures, including system capacity constraints, inadequate IT risk management, and critical skills gaps.","Despite these challenges, end users perceive HIS positively and recommend its implementation for streamlining daily processes.","This study underscores the importance of addressing design-reality gaps to improve HIS outcomes in public healthcare settings."],"url":"http://arxiv.org/abs/2411.03187v1"}
{"created":"2024-11-05 15:22:26","title":"On Improved Conditioning Mechanisms and Pre-training Strategies for Diffusion Models","abstract":"Large-scale training of latent diffusion models (LDMs) has enabled unprecedented quality in image generation. However, the key components of the best performing LDM training recipes are oftentimes not available to the research community, preventing apple-to-apple comparisons and hindering the validation of progress in the field. In this work, we perform an in-depth study of LDM training recipes focusing on the performance of models and their training efficiency. To ensure apple-to-apple comparisons, we re-implement five previously published models with their corresponding recipes. Through our study, we explore the effects of (i)~the mechanisms used to condition the generative model on semantic information (e.g., text prompt) and control metadata (e.g., crop size, random flip flag, etc.) on the model performance, and (ii)~the transfer of the representations learned on smaller and lower-resolution datasets to larger ones on the training efficiency and model performance. We then propose a novel conditioning mechanism that disentangles semantic and control metadata conditionings and sets a new state-of-the-art in class-conditional generation on the ImageNet-1k dataset -- with FID improvements of 7% on 256 and 8% on 512 resolutions -- as well as text-to-image generation on the CC12M dataset -- with FID improvements of 8% on 256 and 23% on 512 resolution.","sentences":["Large-scale training of latent diffusion models (LDMs) has enabled unprecedented quality in image generation.","However, the key components of the best performing LDM training recipes are oftentimes not available to the research community, preventing apple-to-apple comparisons and hindering the validation of progress in the field.","In this work, we perform an in-depth study of LDM training recipes focusing on the performance of models and their training efficiency.","To ensure apple-to-apple comparisons, we re-implement five previously published models with their corresponding recipes.","Through our study, we explore the effects of (i)~the mechanisms used to condition the generative model on semantic information (e.g., text prompt) and control metadata (e.g., crop size, random flip flag, etc.) on the model performance, and (ii)~the transfer of the representations learned on smaller and lower-resolution datasets to larger ones on the training efficiency and model performance.","We then propose a novel conditioning mechanism that disentangles semantic and control metadata conditionings and sets a new state-of-the-art in class-conditional generation on the ImageNet-1k dataset -- with FID improvements of 7% on 256 and 8% on 512 resolutions -- as well as text-to-image generation on the CC12M dataset -- with FID improvements of 8% on 256 and 23% on 512 resolution."],"url":"http://arxiv.org/abs/2411.03177v1"}
{"created":"2024-11-05 15:22:21","title":"Developing Simulation Models for Soft Robotic Grippers in Webots","abstract":"Robotic simulators provide cost-effective and risk-free virtual environments for studying robotic designs, control algorithms, and sensor integrations. They typically host extensive libraries of sensors and actuators that facilitate rapid prototyping and design evaluations in simulation. The use of the most prominent existing robotic simulators is however limited to simulation of rigid-link robots. On the other hand, there exist dedicated specialized environments for simulating soft robots. This separation limits the study of soft robotic systems, particularly in hybrid scenarios where soft and rigid sub-systems co-exist. In this work, we develop a lightweight open-source digital twin of a commercially available soft gripper, directly integrated within the robotic simulator Webots. We use a Rigid-Link-Discretization (RLD) model to simulate the soft gripper. Using a Particle Swarm Optimization (PSO) approach, we identify the parameters of the RLD model based on the kinematics and dynamics of the physical system and show the efficacy of our modeling approach in validation experiments. All software and experimental details are available on github: https://github.com/anonymousgituser1/Robosoft2025","sentences":["Robotic simulators provide cost-effective and risk-free virtual environments for studying robotic designs, control algorithms, and sensor integrations.","They typically host extensive libraries of sensors and actuators that facilitate rapid prototyping and design evaluations in simulation.","The use of the most prominent existing robotic simulators is however limited to simulation of rigid-link robots.","On the other hand, there exist dedicated specialized environments for simulating soft robots.","This separation limits the study of soft robotic systems, particularly in hybrid scenarios where soft and rigid sub-systems co-exist.","In this work, we develop a lightweight open-source digital twin of a commercially available soft gripper, directly integrated within the robotic simulator Webots.","We use a Rigid-Link-Discretization (RLD) model to simulate the soft gripper.","Using a Particle Swarm Optimization (PSO) approach, we identify the parameters of the RLD model based on the kinematics and dynamics of the physical system and show the efficacy of our modeling approach in validation experiments.","All software and experimental details are available on github:","https://github.com/anonymousgituser1/Robosoft2025"],"url":"http://arxiv.org/abs/2411.03176v1"}
{"created":"2024-11-05 15:22:11","title":"ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression","abstract":"As a core component in modern data centers, key-value cache provides high-throughput and low-latency services for high-speed data processing. The effectiveness of a key-value cache relies on its ability of accommodating the needed data. However, expanding the cache capacity is often more difficult than commonly expected because of many practical constraints, such as server costs, cooling issues, rack space, and even human resource expenses. A potential solution is compression, which virtually extends the cache capacity by condensing data in cache. In practice, this seemingly simple idea has not gained much traction in key-value cache system design, due to several critical issues: the compression-unfriendly index structure, severe read/write amplification, wasteful decompression operations, and heavy computing cost. This paper presents a hybrid DRAM-SSD cache design to realize a systematic integration of data compression in key-value cache. By treating compression as an essential component, we have redesigned the indexing structure, data management, and leveraged the emerging computational SSD hardware for collaborative optimizations. We have developed a prototype, called ZipCache. Our experimental results show that ZipCache can achieve up to 72.4% higher throughput and 42.4% lower latency, while reducing the write amplification by up to 26.2 times.","sentences":["As a core component in modern data centers, key-value cache provides high-throughput and low-latency services for high-speed data processing.","The effectiveness of a key-value cache relies on its ability of accommodating the needed data.","However, expanding the cache capacity is often more difficult than commonly expected because of many practical constraints, such as server costs, cooling issues, rack space, and even human resource expenses.","A potential solution is compression, which virtually extends the cache capacity by condensing data in cache.","In practice, this seemingly simple idea has not gained much traction in key-value cache system design, due to several critical issues: the compression-unfriendly index structure, severe read/write amplification, wasteful decompression operations, and heavy computing cost.","This paper presents a hybrid DRAM-SSD cache design to realize a systematic integration of data compression in key-value cache.","By treating compression as an essential component, we have redesigned the indexing structure, data management, and leveraged the emerging computational SSD hardware for collaborative optimizations.","We have developed a prototype, called ZipCache.","Our experimental results show that ZipCache can achieve up to 72.4% higher throughput and 42.4% lower latency, while reducing the write amplification by up to 26.2 times."],"url":"http://arxiv.org/abs/2411.03174v1"}
{"created":"2024-11-05 15:19:29","title":"Navigating Extremes: Dynamic Sparsity in Large Output Space","abstract":"In recent years, Dynamic Sparse Training (DST) has emerged as an alternative to post-training pruning for generating efficient models. In principle, DST allows for a more memory efficient training process, as it maintains sparsity throughout the entire training run. However, current DST implementations fail to capitalize on this in practice. Because sparse matrix multiplication is much less efficient than dense matrix multiplication on GPUs, most implementations simulate sparsity by masking weights. In this paper, we leverage recent advances in semi-structured sparse training to apply DST in the domain of classification with large output spaces, where memory-efficiency is paramount. With a label space of possibly millions of candidates, the classification layer alone will consume several gigabytes of memory. Switching from a dense to a fixed fan-in sparse layer updated with sparse evolutionary training (SET); however, severely hampers training convergence, especially at the largest label spaces. We find that poor gradient flow from the sparse classifier to the dense text encoder make it difficult to learn good input representations. By employing an intermediate layer or adding an auxiliary training objective, we recover most of the generalisation performance of the dense model. Overall, we demonstrate the applicability and practical benefits of DST in a challenging domain -- characterized by a highly skewed label distribution that differs substantially from typical DST benchmark datasets -- which enables end-to-end training with millions of labels on commodity hardware.","sentences":["In recent years, Dynamic Sparse Training (DST) has emerged as an alternative to post-training pruning for generating efficient models.","In principle, DST allows for a more memory efficient training process, as it maintains sparsity throughout the entire training run.","However, current DST implementations fail to capitalize on this in practice.","Because sparse matrix multiplication is much less efficient than dense matrix multiplication on GPUs, most implementations simulate sparsity by masking weights.","In this paper, we leverage recent advances in semi-structured sparse training to apply DST in the domain of classification with large output spaces, where memory-efficiency is paramount.","With a label space of possibly millions of candidates, the classification layer alone will consume several gigabytes of memory.","Switching from a dense to a fixed fan-in sparse layer updated with sparse evolutionary training (SET); however, severely hampers training convergence, especially at the largest label spaces.","We find that poor gradient flow from the sparse classifier to the dense text encoder make it difficult to learn good input representations.","By employing an intermediate layer or adding an auxiliary training objective, we recover most of the generalisation performance of the dense model.","Overall, we demonstrate the applicability and practical benefits of DST in a challenging domain -- characterized by a highly skewed label distribution that differs substantially from typical DST benchmark datasets -- which enables end-to-end training with millions of labels on commodity hardware."],"url":"http://arxiv.org/abs/2411.03171v1"}
{"created":"2024-11-05 15:18:02","title":"Pre-trained Visual Dynamics Representations for Efficient Policy Learning","abstract":"Pre-training for Reinforcement Learning (RL) with purely video data is a valuable yet challenging problem. Although in-the-wild videos are readily available and inhere a vast amount of prior world knowledge, the absence of action annotations and the common domain gap with downstream tasks hinder utilizing videos for RL pre-training. To address the challenge of pre-training with videos, we propose Pre-trained Visual Dynamics Representations (PVDR) to bridge the domain gap between videos and downstream tasks for efficient policy learning. By adopting video prediction as a pre-training task, we use a Transformer-based Conditional Variational Autoencoder (CVAE) to learn visual dynamics representations. The pre-trained visual dynamics representations capture the visual dynamics prior knowledge in the videos. This abstract prior knowledge can be readily adapted to downstream tasks and aligned with executable actions through online adaptation. We conduct experiments on a series of robotics visual control tasks and verify that PVDR is an effective form for pre-training with videos to promote policy learning.","sentences":["Pre-training for Reinforcement Learning (RL) with purely video data is a valuable yet challenging problem.","Although in-the-wild videos are readily available and inhere a vast amount of prior world knowledge, the absence of action annotations and the common domain gap with downstream tasks hinder utilizing videos for RL pre-training.","To address the challenge of pre-training with videos, we propose Pre-trained Visual Dynamics Representations (PVDR) to bridge the domain gap between videos and downstream tasks for efficient policy learning.","By adopting video prediction as a pre-training task, we use a Transformer-based Conditional Variational Autoencoder (CVAE) to learn visual dynamics representations.","The pre-trained visual dynamics representations capture the visual dynamics prior knowledge in the videos.","This abstract prior knowledge can be readily adapted to downstream tasks and aligned with executable actions through online adaptation.","We conduct experiments on a series of robotics visual control tasks and verify that PVDR is an effective form for pre-training with videos to promote policy learning."],"url":"http://arxiv.org/abs/2411.03169v1"}
{"created":"2024-11-05 15:05:23","title":"A Machine Learning Approach for the Efficient Estimation of Ground-Level Air Temperature in Urban Areas","abstract":"The increasingly populated cities of the 21st Century face the challenge of being sustainable and resilient spaces for their inhabitants. However, climate change, among other problems, makes these objectives difficult to achieve. The Urban Heat Island (UHI) phenomenon that occurs in cities, increasing their thermal stress, is one of the stumbling blocks to achieve a more sustainable city. The ability to estimate temperatures with a high degree of accuracy allows for the identification of the highest priority areas in cities where urban improvements need to be made to reduce thermal discomfort. In this work we explore the usefulness of image-to-image deep neural networks (DNNs) for correlating spatial and meteorological variables of a urban area with street-level air temperature. The air temperature at street-level is estimated both spatially and temporally for a specific use case, and compared with existing, well-established numerical models. Based on the obtained results, deep neural networks are confirmed to be faster and less computationally expensive alternative for ground-level air temperature compared to numerical models.","sentences":["The increasingly populated cities of the 21st Century face the challenge of being sustainable and resilient spaces for their inhabitants.","However, climate change, among other problems, makes these objectives difficult to achieve.","The Urban Heat Island (UHI) phenomenon that occurs in cities, increasing their thermal stress, is one of the stumbling blocks to achieve a more sustainable city.","The ability to estimate temperatures with a high degree of accuracy allows for the identification of the highest priority areas in cities where urban improvements need to be made to reduce thermal discomfort.","In this work we explore the usefulness of image-to-image deep neural networks (DNNs) for correlating spatial and meteorological variables of a urban area with street-level air temperature.","The air temperature at street-level is estimated both spatially and temporally for a specific use case, and compared with existing, well-established numerical models.","Based on the obtained results, deep neural networks are confirmed to be faster and less computationally expensive alternative for ground-level air temperature compared to numerical models."],"url":"http://arxiv.org/abs/2411.03162v1"}
{"created":"2024-11-05 15:02:50","title":"Hybrid Rebeca Revisited","abstract":"Hybrid Rebeca is introduced for modeling asynchronous event-based Cyber-Physical Systems (CPSs). In this work, we extend Hybrid Rebeca to allow the modeling of non-deterministic time behavior. We provide a set of rules to define the semantic model of Hybrid Rebeca models in terms of Time Transition Systems which represents an over-approximation of the reachable states of a Hybrid Rebeca model. Then, we adapt the reachability analysis algorithm of Flow$^*$ for Hybrid Rebeca models leveraging our semantic rules. This improves the analysis significantly because the previous technique relied on the reachability analysis of hybrid automata by deriving a monolithic hybrid automaton from a given Hybrid Rebeca model, leading to a huge hybrid automaton. We illustrate the applicability of our approach through a case study.","sentences":["Hybrid Rebeca is introduced for modeling asynchronous event-based Cyber-Physical Systems (CPSs).","In this work, we extend Hybrid Rebeca to allow the modeling of non-deterministic time behavior.","We provide a set of rules to define the semantic model of Hybrid Rebeca models in terms of Time Transition Systems which represents an over-approximation of the reachable states of a Hybrid Rebeca model.","Then, we adapt the reachability analysis algorithm of Flow$^*$ for Hybrid Rebeca models leveraging our semantic rules.","This improves the analysis significantly because the previous technique relied on the reachability analysis of hybrid automata by deriving a monolithic hybrid automaton from a given Hybrid Rebeca model, leading to a huge hybrid automaton.","We illustrate the applicability of our approach through a case study."],"url":"http://arxiv.org/abs/2411.03160v1"}
{"created":"2024-11-05 14:48:16","title":"Hardware for converting floating-point to the microscaling (MX) format","abstract":"This paper proposes hardware converters for the microscaling format (MX-format), a reduced representation of floating-point numbers. We present an algorithm and a memory-free hardware model for converting 32 single-precision floating-point numbers to MX-format. The proposed model supports six different types of MX-format: E5M2, E4M3, E3M2, E2M3, E2M1, and INT8. The conversion process consists of three steps: calculating the maximum absolute value among 32 inputs, generating a shared scale, and producing 32 outputs in the selected MX-format type. The hardware converters were implemented in FPGA, and experimental results demonstrate.","sentences":["This paper proposes hardware converters for the microscaling format (MX-format), a reduced representation of floating-point numbers.","We present an algorithm and a memory-free hardware model for converting 32 single-precision floating-point numbers to MX-format.","The proposed model supports six different types of MX-format: E5M2, E4M3, E3M2, E2M3, E2M1, and INT8.","The conversion process consists of three steps: calculating the maximum absolute value among 32 inputs, generating a shared scale, and producing 32 outputs in the selected MX-format type.","The hardware converters were implemented in FPGA, and experimental results demonstrate."],"url":"http://arxiv.org/abs/2411.03149v1"}
{"created":"2024-11-05 14:44:28","title":"The Effect of Funding on Student Achievement: Evidence from District of Columbia, Virginia, and Maryland","abstract":"The question of how to best serve the student populations of our country is a complex topic. Since public funding is limited, we must explore the best ways to direct the money to improve student outcomes. Previous research has suggested that socio-economic status is the best predictor of student achievement, while other studies suggest that the amount of money spent on the student is a more significant factor. In this paper, we explore this question and its impacts on Maryland, Virginia, and the District of Columbia schools. We conclude that the graduation rate has a direct relationship with unemployment, suggesting that funding towards improving out-of-school opportunities and quality of life will significantly improve students chances of success. We do not find a significant relationship between per-pupil spending and student achievement.","sentences":["The question of how to best serve the student populations of our country is a complex topic.","Since public funding is limited, we must explore the best ways to direct the money to improve student outcomes.","Previous research has suggested that socio-economic status is the best predictor of student achievement, while other studies suggest that the amount of money spent on the student is a more significant factor.","In this paper, we explore this question and its impacts on Maryland, Virginia, and the District of Columbia schools.","We conclude that the graduation rate has a direct relationship with unemployment, suggesting that funding towards improving out-of-school opportunities and quality of life will significantly improve students chances of success.","We do not find a significant relationship between per-pupil spending and student achievement."],"url":"http://arxiv.org/abs/2411.03147v1"}
{"created":"2024-11-05 14:33:50","title":"Self-supervised Hierarchical Representation for Medication Recommendation","abstract":"Medication recommender is to suggest appropriate medication combinations based on a patient's health history, e.g., diagnoses and procedures. Existing works represent different diagnoses/procedures well separated by one-hot encodings. However, they ignore the latent hierarchical structures of these medical terms, undermining the generalization performance of the model. For example, \"Respiratory Diseases\", \"Chronic Respiratory Diseases\" and \"Chronic Bronchiti\" have a hierarchical relationship, progressing from general to specific. To address this issue, we propose a novel hierarchical encoder named HIER to hierarchically represent diagnoses and procedures, which is based on standard medical codes and compatible with any existing methods. Specifically, the proposed method learns relation embedding with a self-supervised objective for incorporating the neighbor hierarchical structure. Additionally, we develop the position encoding to explicitly introduce global hierarchical position. Extensive experiments demonstrate significant and consistent improvements in recommendation accuracy across four baselines and two real-world clinical datasets.","sentences":["Medication recommender is to suggest appropriate medication combinations based on a patient's health history, e.g., diagnoses and procedures.","Existing works represent different diagnoses/procedures well separated by one-hot encodings.","However, they ignore the latent hierarchical structures of these medical terms, undermining the generalization performance of the model.","For example, \"Respiratory Diseases\", \"Chronic Respiratory Diseases\" and \"Chronic Bronchiti\" have a hierarchical relationship, progressing from general to specific.","To address this issue, we propose a novel hierarchical encoder named HIER to hierarchically represent diagnoses and procedures, which is based on standard medical codes and compatible with any existing methods.","Specifically, the proposed method learns relation embedding with a self-supervised objective for incorporating the neighbor hierarchical structure.","Additionally, we develop the position encoding to explicitly introduce global hierarchical position.","Extensive experiments demonstrate significant and consistent improvements in recommendation accuracy across four baselines and two real-world clinical datasets."],"url":"http://arxiv.org/abs/2411.03143v1"}
{"created":"2024-11-05 14:31:20","title":"The Impact of Medicaid Expansion on Medicare Quality Measures","abstract":"The Affordable Care Act was signed into law in 2010, expanding Medicaid and improving access to care for millions of low-income Americans. Fewer uninsured individuals reduced the cost of uncompensated care, consequently improving the financial health of hospitals. We hypothesize that this amelioration in hospital finances resulted in a marked improvement of quality measures in states that chose to expand Medicaid. To our knowledge, the impact of Medicaid expansion on the Medicare population has not been investigated. Using a difference-in-difference analysis, we compare readmission rates for four measures from the Hospital Readmission Reduction Program: acute myocardial infarction, pneumonia, heart failure, and coronary artery bypass graft surgery. Our analysis provides evidence that between 2013 and 2021 expansion states improved hospital quality relative to non-expansion states as it relates to acute myocardial infarction readmissions (p = 0.015) and coronary artery bypass graft surgery readmissions (p = 0.039). Our analysis provides some evidence that expanding Medicaid improved hospital quality, as measured by a reduction in readmission rates. Using visualizations, we provide some evidence that hospital quality improved for the other two measures as well. We believe that a refinement of our estimation method and an improved dataset will increase our chances of finding significant results for these two other measures.","sentences":["The Affordable Care Act was signed into law in 2010, expanding Medicaid and improving access to care for millions of low-income Americans.","Fewer uninsured individuals reduced the cost of uncompensated care, consequently improving the financial health of hospitals.","We hypothesize that this amelioration in hospital finances resulted in a marked improvement of quality measures in states that chose to expand Medicaid.","To our knowledge, the impact of Medicaid expansion on the Medicare population has not been investigated.","Using a difference-in-difference analysis, we compare readmission rates for four measures from the Hospital Readmission Reduction Program: acute myocardial infarction, pneumonia, heart failure, and coronary artery bypass graft surgery.","Our analysis provides evidence that between 2013 and 2021 expansion states improved hospital quality relative to non-expansion states as it relates to acute myocardial infarction readmissions (p = 0.015) and coronary artery bypass graft surgery readmissions (p = 0.039).","Our analysis provides some evidence that expanding Medicaid improved hospital quality, as measured by a reduction in readmission rates.","Using visualizations, we provide some evidence that hospital quality improved for the other two measures as well.","We believe that a refinement of our estimation method and an improved dataset will increase our chances of finding significant results for these two other measures."],"url":"http://arxiv.org/abs/2411.03140v1"}
{"created":"2024-11-05 14:30:12","title":"From Pen to Prompt: How Creative Writers Integrate AI into their Writing Practice","abstract":"Creative writers have a love for their craft, yet AI systems using large language models (LLMs) offer the automation of significant parts of the writing process. So why do some creative writers choose to integrate AI into their workflows? To explore this, we interview and observe a writing session with 18 creative writers who already use AI regularly in their writing practice. Our findings reveal that creative writers are intentional about how they incorporate AI, making many deliberate decisions about when and how to engage AI based on the core values they hold about writing. These values, such as authenticity and craftsmanship, alongside writers' relationships with and use of AI influence the parts of writing over which they wish to maintain control. Through our analysis, we contribute a taxonomy of writer values, writer relationships with AI, and integration strategies, and discuss how these three elements interrelate.","sentences":["Creative writers have a love for their craft, yet AI systems using large language models (LLMs) offer the automation of significant parts of the writing process.","So why do some creative writers choose to integrate AI into their workflows?","To explore this, we interview and observe a writing session with 18 creative writers who already use AI regularly in their writing practice.","Our findings reveal that creative writers are intentional about how they incorporate AI, making many deliberate decisions about when and how to engage AI based on the core values they hold about writing.","These values, such as authenticity and craftsmanship, alongside writers' relationships with and use of AI influence the parts of writing over which they wish to maintain control.","Through our analysis, we contribute a taxonomy of writer values, writer relationships with AI, and integration strategies, and discuss how these three elements interrelate."],"url":"http://arxiv.org/abs/2411.03137v1"}
{"created":"2024-11-05 14:18:09","title":"User Centric Semantic Communications","abstract":"Current studies on semantic communications mainly focus on efficiently extracting semantic information to reduce bandwidth usage between a transmitter and a user. Although significant process has been made in the semantic communications, a fundamental design problem is that the semantic information is extracted based on certain criteria at the transmitter side along, without considering the user's actual requirements. As a result, critical information that is of primary concern to the user may be lost. In such cases, the semantic transmission becomes meaningless to the user, as all received information is irrelevant to the user's interests. To solve this problem, this paper presents a user centric semantic communication system, where the user sends its request for the desired semantic information to the transmitter at the start of each transmission. Then, the transmitter extracts the required semantic information accordingly. A key challenge is how the transmitter can understand the user's requests for semantic information and extract the required semantic information in a reasonable and robust manner. We solve this challenge by designing a well-structured framework and leveraging off-the-shelf products, such as GPT-4, along with several specialized tools for detection and estimation. Evaluation results demonstrate the feasibility and effectiveness of the proposed user centric semantic communication system.","sentences":["Current studies on semantic communications mainly focus on efficiently extracting semantic information to reduce bandwidth usage between a transmitter and a user.","Although significant process has been made in the semantic communications, a fundamental design problem is that the semantic information is extracted based on certain criteria at the transmitter side along, without considering the user's actual requirements.","As a result, critical information that is of primary concern to the user may be lost.","In such cases, the semantic transmission becomes meaningless to the user, as all received information is irrelevant to the user's interests.","To solve this problem, this paper presents a user centric semantic communication system, where the user sends its request for the desired semantic information to the transmitter at the start of each transmission.","Then, the transmitter extracts the required semantic information accordingly.","A key challenge is how the transmitter can understand the user's requests for semantic information and extract the required semantic information in a reasonable and robust manner.","We solve this challenge by designing a well-structured framework and leveraging off-the-shelf products, such as GPT-4, along with several specialized tools for detection and estimation.","Evaluation results demonstrate the feasibility and effectiveness of the proposed user centric semantic communication system."],"url":"http://arxiv.org/abs/2411.03127v1"}
{"created":"2024-11-05 14:08:43","title":"Fully Dynamic $k$-Median with Near-Optimal Update Time and Recourse","abstract":"In metric $k$-clustering, we are given as input a set of $n$ points in a general metric space, and we have to pick $k$ centers and cluster the input points around these chosen centers, so as to minimize an appropriate objective function. In recent years, significant effort has been devoted to the study of metric $k$-clustering problems in a dynamic setting, where the input keeps changing via updates (point insertions/deletions), and we have to maintain a good clustering throughout these updates. The performance of such a dynamic algorithm is measured in terms of three parameters: (i) Approximation ratio, which signifies the quality of the maintained solution, (ii) Recourse, which signifies how stable the maintained solution is, and (iii) Update time, which signifies the efficiency of the algorithm.   We consider the metric $k$-median problem, where the objective is the sum of the distances of the points to their nearest centers. We design the first dynamic algorithm for this problem with near-optimal guarantees across all three performance measures (up to a constant factor in approximation ratio, and polylogarithmic factors in recourse and update time). Specifically, we obtain a $O(1)$-approximation algorithm for dynamic metric $k$-median with $\\tilde{O}(1)$ recourse and $\\tilde{O}(k)$ update time. Prior to our work, the state-of-the-art here was the recent result of [Bhattacharya et al., FOCS'24], who obtained $O(\\epsilon^{-1})$-approximation ratio with $\\tilde{O}(k^{\\epsilon})$ recourse and $\\tilde{O}(k^{1+\\epsilon})$ update time.   We achieve our results by carefully synthesizing the concept of robust centers introduced in [Fichtenberger et al., SODA'21] along with the randomized local search subroutine from [Bhattacharya et al., FOCS'24], in addition to several key technical insights of our own.","sentences":["In metric $k$-clustering, we are given as input a set of $n$ points in a general metric space, and we have to pick $k$ centers and cluster the input points around these chosen centers, so as to minimize an appropriate objective function.","In recent years, significant effort has been devoted to the study of metric $k$-clustering problems in a dynamic setting, where the input keeps changing via updates (point insertions/deletions), and we have to maintain a good clustering throughout these updates.","The performance of such a dynamic algorithm is measured in terms of three parameters: (i) Approximation ratio, which signifies the quality of the maintained solution, (ii) Recourse, which signifies how stable the maintained solution is, and (iii) Update time, which signifies the efficiency of the algorithm.   ","We consider the metric $k$-median problem, where the objective is the sum of the distances of the points to their nearest centers.","We design the first dynamic algorithm for this problem with near-optimal guarantees across all three performance measures (up to a constant factor in approximation ratio, and polylogarithmic factors in recourse and update time).","Specifically, we obtain a $O(1)$-approximation algorithm for dynamic metric $k$-median with $\\tilde{O}(1)$ recourse and $\\tilde{O}(k)$ update time.","Prior to our work, the state-of-the-art here was the recent result of [Bhattacharya et al., FOCS'24], who obtained $O(\\epsilon^{-1})$-approximation ratio with $\\tilde{O}(k^{\\epsilon})$ recourse and $\\tilde{O}(k^{1+\\epsilon})$ update time.   ","We achieve our results by carefully synthesizing the concept of robust centers introduced in [Fichtenberger et al., SODA'21] along with the randomized local search subroutine from [Bhattacharya et al., FOCS'24], in addition to several key technical insights of our own."],"url":"http://arxiv.org/abs/2411.03121v1"}
