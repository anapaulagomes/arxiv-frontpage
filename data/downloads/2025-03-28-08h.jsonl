{"created":"2025-03-27 17:59:58","title":"Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation","abstract":"Open-vocabulary semantic segmentation models associate vision and text to label pixels from an undefined set of classes using textual queries, providing versatile performance on novel datasets. However, large shifts between training and test domains degrade their performance, requiring fine-tuning for effective real-world applications. We introduce Semantic Library Adaptation (SemLA), a novel framework for training-free, test-time domain adaptation. SemLA leverages a library of LoRA-based adapters indexed with CLIP embeddings, dynamically merging the most relevant adapters based on proximity to the target domain in the embedding space. This approach constructs an ad-hoc model tailored to each specific input without additional training. Our method scales efficiently, enhances explainability by tracking adapter contributions, and inherently protects data privacy, making it ideal for sensitive applications. Comprehensive experiments on a 20-domain benchmark built over 10 standard datasets demonstrate SemLA's superior adaptability and performance across diverse settings, establishing a new standard in domain adaptation for open-vocabulary semantic segmentation.","sentences":["Open-vocabulary semantic segmentation models associate vision and text to label pixels from an undefined set of classes using textual queries, providing versatile performance on novel datasets.","However, large shifts between training and test domains degrade their performance, requiring fine-tuning for effective real-world applications.","We introduce Semantic Library Adaptation (SemLA), a novel framework for training-free, test-time domain adaptation.","SemLA leverages a library of LoRA-based adapters indexed with CLIP embeddings, dynamically merging the most relevant adapters based on proximity to the target domain in the embedding space.","This approach constructs an ad-hoc model tailored to each specific input without additional training.","Our method scales efficiently, enhances explainability by tracking adapter contributions, and inherently protects data privacy, making it ideal for sensitive applications.","Comprehensive experiments on a 20-domain benchmark built over 10 standard datasets demonstrate SemLA's superior adaptability and performance across diverse settings, establishing a new standard in domain adaptation for open-vocabulary semantic segmentation."],"url":"http://arxiv.org/abs/2503.21780v1"}
{"created":"2025-03-27 17:59:58","title":"VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models","abstract":"Customized text-to-video generation aims to produce high-quality videos that incorporate user-specified subject identities or motion patterns. However, existing methods mainly focus on personalizing a single concept, either subject identity or motion pattern, limiting their effectiveness for multiple subjects with the desired motion patterns. To tackle this challenge, we propose a unified framework VideoMage for video customization over both multiple subjects and their interactive motions. VideoMage employs subject and motion LoRAs to capture personalized content from user-provided images and videos, along with an appearance-agnostic motion learning approach to disentangle motion patterns from visual appearance. Furthermore, we develop a spatial-temporal composition scheme to guide interactions among subjects within the desired motion patterns. Extensive experiments demonstrate that VideoMage outperforms existing methods, generating coherent, user-controlled videos with consistent subject identities and interactions.","sentences":["Customized text-to-video generation aims to produce high-quality videos that incorporate user-specified subject identities or motion patterns.","However, existing methods mainly focus on personalizing a single concept, either subject identity or motion pattern, limiting their effectiveness for multiple subjects with the desired motion patterns.","To tackle this challenge, we propose a unified framework VideoMage for video customization over both multiple subjects and their interactive motions.","VideoMage employs subject and motion LoRAs to capture personalized content from user-provided images and videos, along with an appearance-agnostic motion learning approach to disentangle motion patterns from visual appearance.","Furthermore, we develop a spatial-temporal composition scheme to guide interactions among subjects within the desired motion patterns.","Extensive experiments demonstrate that VideoMage outperforms existing methods, generating coherent, user-controlled videos with consistent subject identities and interactions."],"url":"http://arxiv.org/abs/2503.21781v1"}
{"created":"2025-03-27 17:59:58","title":"Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model","abstract":"Video understanding models often struggle with high computational requirements, extensive parameter counts, and slow inference speed, making them inefficient for practical use. To tackle these challenges, we propose Mobile-VideoGPT, an efficient multimodal framework designed to operate with fewer than a billion parameters. Unlike traditional video large multimodal models (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders, efficient projectors, and a small language model (SLM), enabling real-time throughput. To further improve efficiency, we present an Attention-Based Frame Scoring mechanism to select the key-frames, along with an efficient token projector that prunes redundant visual tokens and preserves essential contextual cues. We evaluate our model across well-established six video understanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest). Our results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens per second while outperforming existing state-of-the-art 0.5B-parameter models by 6 points on average with 40% fewer parameters and more than 2x higher throughput. Our code and models are publicly available at: https://github.com/Amshaker/Mobile-VideoGPT.","sentences":["Video understanding models often struggle with high computational requirements, extensive parameter counts, and slow inference speed, making them inefficient for practical use.","To tackle these challenges, we propose Mobile-VideoGPT, an efficient multimodal framework designed to operate with fewer than a billion parameters.","Unlike traditional video large multimodal models (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders, efficient projectors, and a small language model (SLM), enabling real-time throughput.","To further improve efficiency, we present an Attention-Based Frame Scoring mechanism to select the key-frames, along with an efficient token projector that prunes redundant visual tokens and preserves essential contextual cues.","We evaluate our model across well-established six video understanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest).","Our results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens per second while outperforming existing state-of-the-art 0.5B-parameter models by 6 points on average with 40% fewer parameters and more than 2x higher throughput.","Our code and models are publicly available at: https://github.com/Amshaker/Mobile-VideoGPT."],"url":"http://arxiv.org/abs/2503.21782v1"}
{"created":"2025-03-27 17:59:57","title":"X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction","abstract":"Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows. Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality. In this paper, We propose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning. Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization. To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization. Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques. By unifying continuous motion modeling with hardware-free period learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.","sentences":["Four-dimensional computed tomography (4D CT) reconstruction is crucial for capturing dynamic anatomical changes but faces inherent limitations from conventional phase-binning workflows.","Current methods discretize temporal resolution into fixed phases with respiratory gating devices, introducing motion misalignment and restricting clinical practicality.","In this paper, We propose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CT reconstruction by integrating dynamic radiative Gaussian splatting with self-supervised respiratory motion learning.","Our approach models anatomical dynamics through a spatiotemporal encoder-decoder architecture that predicts time-varying Gaussian deformations, eliminating phase discretization.","To remove dependency on external gating devices, we introduce a physiology-driven periodic consistency loss that learns patient-specific breathing cycles directly from projections via differentiable optimization.","Extensive experiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNR gain over traditional methods and 2.25 dB improvement against prior Gaussian splatting techniques.","By unifying continuous motion modeling with hardware-free period learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction for dynamic clinical imaging.","Project website at: https://x2-gaussian.github.io/."],"url":"http://arxiv.org/abs/2503.21779v1"}
{"created":"2025-03-27 17:59:54","title":"HS-SLAM: Hybrid Representation with Structural Supervision for Improved Dense SLAM","abstract":"NeRF-based SLAM has recently achieved promising results in tracking and reconstruction. However, existing methods face challenges in providing sufficient scene representation, capturing structural information, and maintaining global consistency in scenes emerging significant movement or being forgotten. To this end, we present HS-SLAM to tackle these problems. To enhance scene representation capacity, we propose a hybrid encoding network that combines the complementary strengths of hash-grid, tri-planes, and one-blob, improving the completeness and smoothness of reconstruction. Additionally, we introduce structural supervision by sampling patches of non-local pixels rather than individual rays to better capture the scene structure. To ensure global consistency, we implement an active global bundle adjustment (BA) to eliminate camera drifts and mitigate accumulative errors. Experimental results demonstrate that HS-SLAM outperforms the baselines in tracking and reconstruction accuracy while maintaining the efficiency required for robotics.","sentences":["NeRF-based SLAM has recently achieved promising results in tracking and reconstruction.","However, existing methods face challenges in providing sufficient scene representation, capturing structural information, and maintaining global consistency in scenes emerging significant movement or being forgotten.","To this end, we present HS-SLAM to tackle these problems.","To enhance scene representation capacity, we propose a hybrid encoding network that combines the complementary strengths of hash-grid, tri-planes, and one-blob, improving the completeness and smoothness of reconstruction.","Additionally, we introduce structural supervision by sampling patches of non-local pixels rather than individual rays to better capture the scene structure.","To ensure global consistency, we implement an active global bundle adjustment (BA) to eliminate camera drifts and mitigate accumulative errors.","Experimental results demonstrate that HS-SLAM outperforms the baselines in tracking and reconstruction accuracy while maintaining the efficiency required for robotics."],"url":"http://arxiv.org/abs/2503.21778v1"}
{"created":"2025-03-27 17:59:52","title":"Test-Time Visual In-Context Tuning","abstract":"Visual in-context learning (VICL), as a new paradigm in computer vision, allows the model to rapidly adapt to various tasks with only a handful of prompts and examples. While effective, the existing VICL paradigm exhibits poor generalizability under distribution shifts. In this work, we propose test-time Visual In-Context Tuning (VICT), a method that can adapt VICL models on the fly with a single test sample. Specifically, we flip the role between the task prompts and the test sample and use a cycle consistency loss to reconstruct the original task prompt output. Our key insight is that a model should be aware of a new test distribution if it can successfully recover the original task prompts. Extensive experiments on six representative vision tasks ranging from high-level visual understanding to low-level image processing, with 15 common corruptions, demonstrate that our VICT can improve the generalizability of VICL to unseen new domains. In addition, we show the potential of applying VICT for unseen tasks at test time. Code: https://github.com/Jiahao000/VICT.","sentences":["Visual in-context learning (VICL), as a new paradigm in computer vision, allows the model to rapidly adapt to various tasks with only a handful of prompts and examples.","While effective, the existing VICL paradigm exhibits poor generalizability under distribution shifts.","In this work, we propose test-time Visual In-Context Tuning (VICT), a method that can adapt VICL models on the fly with a single test sample.","Specifically, we flip the role between the task prompts and the test sample and use a cycle consistency loss to reconstruct the original task prompt output.","Our key insight is that a model should be aware of a new test distribution if it can successfully recover the original task prompts.","Extensive experiments on six representative vision tasks ranging from high-level visual understanding to low-level image processing, with 15 common corruptions, demonstrate that our VICT can improve the generalizability of VICL to unseen new domains.","In addition, we show the potential of applying VICT for unseen tasks at test time.","Code: https://github.com/Jiahao000/VICT."],"url":"http://arxiv.org/abs/2503.21777v1"}
{"created":"2025-03-27 17:59:51","title":"Video-R1: Reinforcing Video Reasoning in MLLMs","abstract":"Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs). However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data. To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning. Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process. We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data. Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc. Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. All codes, models, data are released.","sentences":["Inspired by DeepSeek-R1's success in eliciting reasoning abilities through rule-based reinforcement learning (RL), we introduce Video-R1 as the first attempt to systematically explore the R1 paradigm for eliciting video reasoning within multimodal large language models (MLLMs).","However, directly applying RL training with the GRPO algorithm to video reasoning presents two primary challenges: (i) a lack of temporal modeling for video reasoning, and (ii) the scarcity of high-quality video-reasoning data.","To address these issues, we first propose the T-GRPO algorithm, which encourages models to utilize temporal information in videos for reasoning.","Additionally, instead of relying solely on video data, we incorporate high-quality image-reasoning data into the training process.","We have constructed two datasets: Video-R1-COT-165k for SFT cold start and Video-R1-260k for RL training, both comprising image and video data.","Experimental results demonstrate that Video-R1 achieves significant improvements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, as well as on general video benchmarks including MVBench and TempCompass, etc.","Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoning benchmark VSI-bench, surpassing the commercial proprietary model GPT-4o.","All codes, models, data are released."],"url":"http://arxiv.org/abs/2503.21776v1"}
{"created":"2025-03-27 17:59:46","title":"Optimal Stepsize for Diffusion Sampling","abstract":"Diffusion models achieve remarkable generation quality but suffer from computational intensive sampling due to suboptimal step discretization. While existing works focus on optimizing denoising directions, we address the principled design of stepsize schedules. This paper proposes Optimal Stepsize Distillation, a dynamic programming framework that extracts theoretically optimal schedules by distilling knowledge from reference trajectories. By reformulating stepsize optimization as recursive error minimization, our method guarantees global discretization bounds through optimal substructure exploitation. Crucially, the distilled schedules demonstrate strong robustness across architectures, ODE solvers, and noise schedules. Experiments show 10x accelerated text-to-image generation while preserving 99.4% performance on GenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.","sentences":["Diffusion models achieve remarkable generation quality but suffer from computational intensive sampling due to suboptimal step discretization.","While existing works focus on optimizing denoising directions, we address the principled design of stepsize schedules.","This paper proposes Optimal Stepsize Distillation, a dynamic programming framework that extracts theoretically optimal schedules by distilling knowledge from reference trajectories.","By reformulating stepsize optimization as recursive error minimization, our method guarantees global discretization bounds through optimal substructure exploitation.","Crucially, the distilled schedules demonstrate strong robustness across architectures, ODE solvers, and noise schedules.","Experiments show 10x accelerated text-to-image generation while preserving 99.4% performance on GenEval.","Our code is available at https://github.com/bebebe666/OptimalSteps."],"url":"http://arxiv.org/abs/2503.21774v1"}
{"created":"2025-03-27 17:59:46","title":"StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion","abstract":"We present StyleMotif, a novel Stylized Motion Latent Diffusion model, generating motion conditioned on both content and style from multiple modalities. Unlike existing approaches that either focus on generating diverse motion content or transferring style from sequences, StyleMotif seamlessly synthesizes motion across a wide range of content while incorporating stylistic cues from multi-modal inputs, including motion, text, image, video, and audio. To achieve this, we introduce a style-content cross fusion mechanism and align a style encoder with a pre-trained multi-modal model, ensuring that the generated motion accurately captures the reference style while preserving realism. Extensive experiments demonstrate that our framework surpasses existing methods in stylized motion generation and exhibits emergent capabilities for multi-modal motion stylization, enabling more nuanced motion synthesis. Source code and pre-trained models will be released upon acceptance. Project Page: https://stylemotif.github.io","sentences":["We present StyleMotif, a novel Stylized Motion Latent Diffusion model, generating motion conditioned on both content and style from multiple modalities.","Unlike existing approaches that either focus on generating diverse motion content or transferring style from sequences, StyleMotif seamlessly synthesizes motion across a wide range of content while incorporating stylistic cues from multi-modal inputs, including motion, text, image, video, and audio.","To achieve this, we introduce a style-content cross fusion mechanism and align a style encoder with a pre-trained multi-modal model, ensuring that the generated motion accurately captures the reference style while preserving realism.","Extensive experiments demonstrate that our framework surpasses existing methods in stylized motion generation and exhibits emergent capabilities for multi-modal motion stylization, enabling more nuanced motion synthesis.","Source code and pre-trained models will be released upon acceptance.","Project Page: https://stylemotif.github.io"],"url":"http://arxiv.org/abs/2503.21775v1"}
{"created":"2025-03-27 17:59:44","title":"LOCORE: Image Re-ranking with Long-Context Sequence Modeling","abstract":"We introduce LOCORE, Long-Context Re-ranker, a model that takes as input local descriptors corresponding to an image query and a list of gallery images and outputs similarity scores between the query and each gallery image. This model is used for image retrieval, where typically a first ranking is performed with an efficient similarity measure, and then a shortlist of top-ranked images is re-ranked based on a more fine-grained similarity measure. Compared to existing methods that perform pair-wise similarity estimation with local descriptors or list-wise re-ranking with global descriptors, LOCORE is the first method to perform list-wise re-ranking with local descriptors. To achieve this, we leverage efficient long-context sequence models to effectively capture the dependencies between query and gallery images at the local-descriptor level. During testing, we process long shortlists with a sliding window strategy that is tailored to overcome the context size limitations of sequence models. Our approach achieves superior performance compared with other re-rankers on established image retrieval benchmarks of landmarks (ROxf and RPar), products (SOP), fashion items (In-Shop), and bird species (CUB-200) while having comparable latency to the pair-wise local descriptor re-rankers.","sentences":["We introduce LOCORE, Long-Context Re-ranker, a model that takes as input local descriptors corresponding to an image query and a list of gallery images and outputs similarity scores between the query and each gallery image.","This model is used for image retrieval, where typically a first ranking is performed with an efficient similarity measure, and then a shortlist of top-ranked images is re-ranked based on a more fine-grained similarity measure.","Compared to existing methods that perform pair-wise similarity estimation with local descriptors or list-wise re-ranking with global descriptors, LOCORE is the first method to perform list-wise re-ranking with local descriptors.","To achieve this, we leverage efficient long-context sequence models to effectively capture the dependencies between query and gallery images at the local-descriptor level.","During testing, we process long shortlists with a sliding window strategy that is tailored to overcome the context size limitations of sequence models.","Our approach achieves superior performance compared with other re-rankers on established image retrieval benchmarks of landmarks (ROxf and RPar), products (SOP), fashion items (In-Shop), and bird species (CUB-200) while having comparable latency to the pair-wise local descriptor re-rankers."],"url":"http://arxiv.org/abs/2503.21772v1"}
{"created":"2025-03-27 17:59:43","title":"A Unified Image-Dense Annotation Generation Model for Underwater Scenes","abstract":"Underwater dense prediction, especially depth estimation and semantic segmentation, is crucial for gaining a comprehensive understanding of underwater scenes. Nevertheless, high-quality and large-scale underwater datasets with dense annotations remain scarce because of the complex environment and the exorbitant data collection costs. This paper proposes a unified Text-to-Image and DEnse annotation generation method (TIDE) for underwater scenes. It relies solely on text as input to simultaneously generate realistic underwater images and multiple highly consistent dense annotations. Specifically, we unify the generation of text-to-image and text-to-dense annotations within a single model. The Implicit Layout Sharing mechanism (ILS) and cross-modal interaction method called Time Adaptive Normalization (TAN) are introduced to jointly optimize the consistency between image and dense annotations. We synthesize a large-scale underwater dataset using TIDE to validate the effectiveness of our method in underwater dense prediction tasks. The results demonstrate that our method effectively improves the performance of existing underwater dense prediction models and mitigates the scarcity of underwater data with dense annotations. We hope our method can offer new perspectives on alleviating data scarcity issues in other fields. The code is available at https: //github.com/HongkLin/TIDE.","sentences":["Underwater dense prediction, especially depth estimation and semantic segmentation, is crucial for gaining a comprehensive understanding of underwater scenes.","Nevertheless, high-quality and large-scale underwater datasets with dense annotations remain scarce because of the complex environment and the exorbitant data collection costs.","This paper proposes a unified Text-to-Image and DEnse annotation generation method (TIDE) for underwater scenes.","It relies solely on text as input to simultaneously generate realistic underwater images and multiple highly consistent dense annotations.","Specifically, we unify the generation of text-to-image and text-to-dense annotations within a single model.","The Implicit Layout Sharing mechanism (ILS) and cross-modal interaction method called Time Adaptive Normalization (TAN) are introduced to jointly optimize the consistency between image and dense annotations.","We synthesize a large-scale underwater dataset using TIDE to validate the effectiveness of our method in underwater dense prediction tasks.","The results demonstrate that our method effectively improves the performance of existing underwater dense prediction models and mitigates the scarcity of underwater data with dense annotations.","We hope our method can offer new perspectives on alleviating data scarcity issues in other fields.","The code is available at https: //github.com/HongkLin/TIDE."],"url":"http://arxiv.org/abs/2503.21771v1"}
{"created":"2025-03-27 17:59:33","title":"Visual Jenga: Discovering Object Dependencies via Counterfactual Inpainting","abstract":"This paper proposes a novel scene understanding task called Visual Jenga. Drawing inspiration from the game Jenga, the proposed task involves progressively removing objects from a single image until only the background remains. Just as Jenga players must understand structural dependencies to maintain tower stability, our task reveals the intrinsic relationships between scene elements by systematically exploring which objects can be removed while preserving scene coherence in both physical and geometric sense. As a starting point for tackling the Visual Jenga task, we propose a simple, data-driven, training-free approach that is surprisingly effective on a range of real-world images. The principle behind our approach is to utilize the asymmetry in the pairwise relationships between objects within a scene and employ a large inpainting model to generate a set of counterfactuals to quantify the asymmetry.","sentences":["This paper proposes a novel scene understanding task called Visual Jenga.","Drawing inspiration from the game Jenga, the proposed task involves progressively removing objects from a single image until only the background remains.","Just as Jenga players must understand structural dependencies to maintain tower stability, our task reveals the intrinsic relationships between scene elements by systematically exploring which objects can be removed while preserving scene coherence in both physical and geometric sense.","As a starting point for tackling the Visual Jenga task, we propose a simple, data-driven, training-free approach that is surprisingly effective on a range of real-world images.","The principle behind our approach is to utilize the asymmetry in the pairwise relationships between objects within a scene and employ a large inpainting model to generate a set of counterfactuals to quantify the asymmetry."],"url":"http://arxiv.org/abs/2503.21770v1"}
{"created":"2025-03-27 17:59:05","title":"Semantic Consistent Language Gaussian Splatting for Point-Level Open-vocabulary Querying","abstract":"Open-vocabulary querying in 3D Gaussian Splatting aims to identify semantically relevant regions within a 3D Gaussian representation based on a given text query. Prior work, such as LangSplat, addressed this task by retrieving these regions in the form of segmentation masks on 2D renderings. More recently, OpenGaussian introduced point-level querying, which directly selects a subset of 3D Gaussians. In this work, we propose a point-level querying method that builds upon LangSplat's framework. Our approach improves the framework in two key ways: (a) we leverage masklets from the Segment Anything Model 2 (SAM2) to establish semantic consistent ground-truth for distilling the language Gaussians; (b) we introduces a novel two-step querying approach that first retrieves the distilled ground-truth and subsequently uses the ground-truth to query the individual Gaussians. Experimental evaluations on three benchmark datasets demonstrate that the proposed method achieves better performance compared to state-of-the-art approaches. For instance, our method achieves an mIoU improvement of +20.42 on the 3D-OVS dataset.","sentences":["Open-vocabulary querying in 3D Gaussian Splatting aims to identify semantically relevant regions within a 3D Gaussian representation based on a given text query.","Prior work, such as LangSplat, addressed this task by retrieving these regions in the form of segmentation masks on 2D renderings.","More recently, OpenGaussian introduced point-level querying, which directly selects a subset of 3D Gaussians.","In this work, we propose a point-level querying method that builds upon LangSplat's framework.","Our approach improves the framework in two key ways: (a) we leverage masklets from the Segment Anything Model 2 (SAM2) to establish semantic consistent ground-truth for distilling the language Gaussians; (b) we introduces a novel two-step querying approach that first retrieves the distilled ground-truth and subsequently uses the ground-truth to query the individual Gaussians.","Experimental evaluations on three benchmark datasets demonstrate that the proposed method achieves better performance compared to state-of-the-art approaches.","For instance, our method achieves an mIoU improvement of +20.42 on the 3D-OVS dataset."],"url":"http://arxiv.org/abs/2503.21767v1"}
{"created":"2025-03-27 17:59:02","title":"Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence","abstract":"Establishing character shape correspondence is a critical and fundamental task in computer vision and graphics, with diverse applications including re-topology, attribute transfer, and shape interpolation. Current dominant functional map methods, while effective in controlled scenarios, struggle in real situations with more complex challenges such as non-isometric shape discrepancies. In response, we revisit registration-for-correspondence methods and tap their potential for more stable shape correspondence estimation. To overcome their common issues including unstable deformations and the necessity for careful pre-alignment or high-quality initial 3D correspondences, we introduce Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence. We first re-purpose a foundation model for 2D character correspondence that ensures reliable and stable 2D mappings. Crucially, we propose a novel Semantic Flow Guided Registration approach that leverages 2D correspondence to guide mesh deformations. Our framework significantly surpasses existing methods in challenging scenarios, and brings possibilities for a wide array of real applications, as demonstrated in our results.","sentences":["Establishing character shape correspondence is a critical and fundamental task in computer vision and graphics, with diverse applications including re-topology, attribute transfer, and shape interpolation.","Current dominant functional map methods, while effective in controlled scenarios, struggle in real situations with more complex challenges such as non-isometric shape discrepancies.","In response, we revisit registration-for-correspondence methods and tap their potential for more stable shape correspondence estimation.","To overcome their common issues including unstable deformations and the necessity for careful pre-alignment or high-quality initial 3D correspondences, we introduce Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence.","We first re-purpose a foundation model for 2D character correspondence that ensures reliable and stable 2D mappings.","Crucially, we propose a novel Semantic Flow Guided Registration approach that leverages 2D correspondence to guide mesh deformations.","Our framework significantly surpasses existing methods in challenging scenarios, and brings possibilities for a wide array of real applications, as demonstrated in our results."],"url":"http://arxiv.org/abs/2503.21766v1"}
{"created":"2025-03-27 17:58:33","title":"Exploring the Evolution of Physics Cognition in Video Generation: A Survey","abstract":"Recent advancements in video generation have witnessed significant progress, especially with the rapid advancement of diffusion models. Despite this, their deficiencies in physical cognition have gradually received widespread attention - generated content often violates the fundamental laws of physics, falling into the dilemma of ''visual realism but physical absurdity\". Researchers began to increasingly recognize the importance of physical fidelity in video generation and attempted to integrate heuristic physical cognition such as motion representations and physical knowledge into generative systems to simulate real-world dynamic scenarios. Considering the lack of a systematic overview in this field, this survey aims to provide a comprehensive summary of architecture designs and their applications to fill this gap. Specifically, we discuss and organize the evolutionary process of physical cognition in video generation from a cognitive science perspective, while proposing a three-tier taxonomy: 1) basic schema perception for generation, 2) passive cognition of physical knowledge for generation, and 3) active cognition for world simulation, encompassing state-of-the-art methods, classical paradigms, and benchmarks. Subsequently, we emphasize the inherent key challenges in this domain and delineate potential pathways for future research, contributing to advancing the frontiers of discussion in both academia and industry. Through structured review and interdisciplinary analysis, this survey aims to provide directional guidance for developing interpretable, controllable, and physically consistent video generation paradigms, thereby propelling generative models from the stage of ''visual mimicry'' towards a new phase of ''human-like physical comprehension''.","sentences":["Recent advancements in video generation have witnessed significant progress, especially with the rapid advancement of diffusion models.","Despite this, their deficiencies in physical cognition have gradually received widespread attention - generated content often violates the fundamental laws of physics, falling into the dilemma of ''visual realism but physical absurdity\".","Researchers began to increasingly recognize the importance of physical fidelity in video generation and attempted to integrate heuristic physical cognition such as motion representations and physical knowledge into generative systems to simulate real-world dynamic scenarios.","Considering the lack of a systematic overview in this field, this survey aims to provide a comprehensive summary of architecture designs and their applications to fill this gap.","Specifically, we discuss and organize the evolutionary process of physical cognition in video generation from a cognitive science perspective, while proposing a three-tier taxonomy: 1) basic schema perception for generation, 2) passive cognition of physical knowledge for generation, and 3) active cognition for world simulation, encompassing state-of-the-art methods, classical paradigms, and benchmarks.","Subsequently, we emphasize the inherent key challenges in this domain and delineate potential pathways for future research, contributing to advancing the frontiers of discussion in both academia and industry.","Through structured review and interdisciplinary analysis, this survey aims to provide directional guidance for developing interpretable, controllable, and physically consistent video generation paradigms, thereby propelling generative models from the stage of ''visual mimicry'' towards a new phase of ''human-like physical comprehension''."],"url":"http://arxiv.org/abs/2503.21765v1"}
{"created":"2025-03-27 17:57:32","title":"Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video","abstract":"This paper presents a unified approach to understanding dynamic scenes from casual videos. Large pretrained vision foundation models, such as vision-language, video depth prediction, motion tracking, and segmentation models, offer promising capabilities. However, training a single model for comprehensive 4D understanding remains challenging. We introduce Uni4D, a multi-stage optimization framework that harnesses multiple pretrained models to advance dynamic 3D modeling, including static/dynamic reconstruction, camera pose estimation, and dense 3D motion tracking. Our results show state-of-the-art performance in dynamic 4D modeling with superior visual quality. Notably, Uni4D requires no retraining or fine-tuning, highlighting the effectiveness of repurposing visual foundation models for 4D understanding.","sentences":["This paper presents a unified approach to understanding dynamic scenes from casual videos.","Large pretrained vision foundation models, such as vision-language, video depth prediction, motion tracking, and segmentation models, offer promising capabilities.","However, training a single model for comprehensive 4D understanding remains challenging.","We introduce Uni4D, a multi-stage optimization framework that harnesses multiple pretrained models to advance dynamic 3D modeling, including static/dynamic reconstruction, camera pose estimation, and dense 3D motion tracking.","Our results show state-of-the-art performance in dynamic 4D modeling with superior visual quality.","Notably, Uni4D requires no retraining or fine-tuning, highlighting the effectiveness of repurposing visual foundation models for 4D understanding."],"url":"http://arxiv.org/abs/2503.21761v1"}
{"created":"2025-03-27 17:57:28","title":"MemInsight: Autonomous Memory Augmentation for LLM Agents","abstract":"Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools. A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge. However, the growing memory size and need for semantic structuring pose significant challenges. In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms. By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses. We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization. On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%. Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval. Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks.","sentences":["Large language model (LLM) agents have evolved to intelligently process information, make decisions, and interact with users or tools.","A key capability is the integration of long-term memory capabilities, enabling these agents to draw upon historical interactions and knowledge.","However, the growing memory size and need for semantic structuring pose significant challenges.","In this work, we propose an autonomous memory augmentation approach, MemInsight, to enhance semantic data representation and retrieval mechanisms.","By leveraging autonomous augmentation to historical interactions, LLM agents are shown to deliver more accurate and contextualized responses.","We empirically validate the efficacy of our proposed approach in three task scenarios; conversational recommendation, question answering and event summarization.","On the LLM-REDIAL dataset, MemInsight boosts persuasiveness of recommendations by up to 14%.","Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.","Our empirical results show the potential of MemInsight to enhance the contextual performance of LLM agents across multiple tasks."],"url":"http://arxiv.org/abs/2503.21760v1"}
{"created":"2025-03-27 17:57:07","title":"Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck","abstract":"In this work, we aim to compress the vision tokens of a Large Vision Language Model (LVLM) into a representation that is simultaneously suitable for (a) generative and (b) discriminative tasks, (c) is nearly lossless, and (d) is storage-efficient. We propose a novel compression approach, called Fwd2Bot, that uses the LVLM itself to compress the visual information in a task-agnostic manner. At the core of Fwd2bot there exists a \"double-forward pass\" training strategy, whereby, during the first forward pass, the LLM (of the LVLM) creates a bottleneck by condensing the visual information into a small number of summary tokens. Then, using the same LLM, the second forward pass processes the language instruction(s) alongside the summary tokens, used as a direct replacement for the image ones. The training signal is provided by two losses: an autoregressive one applied after the second pass that provides a direct optimization objective for compression, and a contrastive loss, applied after the first pass, that further boosts the representation strength, especially for discriminative tasks. The training is further enhanced by stage-specific adapters. We accompany the proposed method by an in-depth ablation study. Overall, Fwd2Bot results in highly-informative compressed representations suitable for both generative and discriminative tasks. For generative tasks, we offer a 2x higher compression rate without compromising the generative capabilities, setting a new state-of-the-art result. For discriminative tasks, we set a new state-of-the-art on image retrieval and compositionality.","sentences":["In this work, we aim to compress the vision tokens of a Large Vision Language Model (LVLM) into a representation that is simultaneously suitable for (a) generative and (b) discriminative tasks, (c) is nearly lossless, and (d) is storage-efficient.","We propose a novel compression approach, called Fwd2Bot, that uses the LVLM itself to compress the visual information in a task-agnostic manner.","At the core of Fwd2bot there exists a \"double-forward pass\" training strategy, whereby, during the first forward pass, the LLM (of the LVLM) creates a bottleneck by condensing the visual information into a small number of summary tokens.","Then, using the same LLM, the second forward pass processes the language instruction(s) alongside the summary tokens, used as a direct replacement for the image ones.","The training signal is provided by two losses: an autoregressive one applied after the second pass that provides a direct optimization objective for compression, and a contrastive loss, applied after the first pass, that further boosts the representation strength, especially for discriminative tasks.","The training is further enhanced by stage-specific adapters.","We accompany the proposed method by an in-depth ablation study.","Overall, Fwd2Bot results in highly-informative compressed representations suitable for both generative and discriminative tasks.","For generative tasks, we offer a 2x higher compression rate without compromising the generative capabilities, setting a new state-of-the-art result.","For discriminative tasks, we set a new state-of-the-art on image retrieval and compositionality."],"url":"http://arxiv.org/abs/2503.21757v1"}
{"created":"2025-03-27 17:57:07","title":"Lumina-Image 2.0: A Unified and Efficient Image Generative Framework","abstract":"We introduce Lumina-Image 2.0, an advanced text-to-image generation framework that achieves significant progress compared to previous work, Lumina-Next. Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts a unified architecture (Unified Next-DiT) that treats text and image tokens as a joint sequence, enabling natural cross-modal interactions and allowing seamless task expansion. Besides, since high-quality captioners can provide semantically well-aligned text-image training pairs, we introduce a unified captioning system, Unified Captioner (UniCap), specifically designed for T2I generation tasks. UniCap excels at generating comprehensive and accurate captions, accelerating convergence and enhancing prompt adherence. (2) Efficiency - to improve the efficiency of our proposed model, we develop multi-stage progressive training strategies and introduce inference acceleration techniques without compromising image quality. Extensive evaluations on academic benchmarks and public text-to-image arenas show that Lumina-Image 2.0 delivers strong performances even with only 2.6B parameters, highlighting its scalability and design efficiency. We have released our training details, code, and models at https://github.com/Alpha-VLLM/Lumina-Image-2.0.","sentences":["We introduce Lumina-Image 2.0, an advanced text-to-image generation framework that achieves significant progress compared to previous work, Lumina-Next.","Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adopts a unified architecture (Unified Next-DiT) that treats text and image tokens as a joint sequence, enabling natural cross-modal interactions and allowing seamless task expansion.","Besides, since high-quality captioners can provide semantically well-aligned text-image training pairs, we introduce a unified captioning system, Unified Captioner (UniCap), specifically designed for T2I generation tasks.","UniCap excels at generating comprehensive and accurate captions, accelerating convergence and enhancing prompt adherence.","(2) Efficiency - to improve the efficiency of our proposed model, we develop multi-stage progressive training strategies and introduce inference acceleration techniques without compromising image quality.","Extensive evaluations on academic benchmarks and public text-to-image arenas show that Lumina-Image 2.0 delivers strong performances even with only 2.6B parameters, highlighting its scalability and design efficiency.","We have released our training details, code, and models at https://github.com/Alpha-VLLM/Lumina-Image-2.0."],"url":"http://arxiv.org/abs/2503.21758v1"}
{"created":"2025-03-27 17:57:03","title":"A Unified Framework for Diffusion Bridge Problems: Flow Matching and Schr\u00f6dinger Matching into One","abstract":"The bridge problem is to find an SDE (or sometimes an ODE) that bridges two given distributions. The application areas of the bridge problem are enormous, among which the recent generative modeling (e.g., conditional or unconditional image generation) is the most popular. Also the famous Schr\\\"{o}dinger bridge problem, a widely known problem for a century, is a special instance of the bridge problem. Two most popular algorithms to tackle the bridge problems in the deep learning era are: (conditional) flow matching and iterative fitting algorithms, where the former confined to ODE solutions, and the latter specifically for the Schr\\\"{o}dinger bridge problem. The main contribution of this article is in two folds: i) We provide concise reviews of these algorithms with technical details to some extent; ii) We propose a novel unified perspective and framework that subsumes these seemingly unrelated algorithms (and their variants) into one. In particular, we show that our unified framework can instantiate the Flow Matching (FM) algorithm, the (mini-batch) optimal transport FM algorithm, the (mini-batch) Schr\\\"{o}dinger bridge FM algorithm, and the deep Schr\\\"{o}dinger bridge matching (DSBM) algorithm as its special cases. We believe that this unified framework will be useful for viewing the bridge problems in a more general and flexible perspective, and in turn can help researchers and practitioners to develop new bridge algorithms in their fields.","sentences":["The bridge problem is to find an SDE (or sometimes an ODE) that bridges two given distributions.","The application areas of the bridge problem are enormous, among which the recent generative modeling (e.g., conditional or unconditional image generation) is the most popular.","Also the famous Schr\\\"{o}dinger bridge problem, a widely known problem for a century, is a special instance of the bridge problem.","Two most popular algorithms to tackle the bridge problems in the deep learning era are: (conditional) flow matching and iterative fitting algorithms, where the former confined to ODE solutions, and the latter specifically for the Schr\\\"{o}dinger bridge problem.","The main contribution of this article is in two folds: i) We provide concise reviews of these algorithms with technical details to some extent; ii) We propose a novel unified perspective and framework that subsumes these seemingly unrelated algorithms (and their variants) into one.","In particular, we show that our unified framework can instantiate the Flow Matching (FM) algorithm, the (mini-batch) optimal transport FM algorithm, the (mini-batch) Schr\\\"{o}dinger","bridge FM algorithm, and the deep Schr\\\"{o}dinger bridge matching (DSBM) algorithm as its special cases.","We believe that this unified framework will be useful for viewing the bridge problems in a more general and flexible perspective, and in turn can help researchers and practitioners to develop new bridge algorithms in their fields."],"url":"http://arxiv.org/abs/2503.21756v1"}
{"created":"2025-03-27 17:57:01","title":"VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness","abstract":"Video generation has advanced significantly, evolving from producing unrealistic outputs to generating videos that appear visually convincing and temporally coherent. To evaluate these video generative models, benchmarks such as VBench have been developed to assess their faithfulness, measuring factors like per-frame aesthetics, temporal consistency, and basic prompt adherence. However, these aspects mainly represent superficial faithfulness, which focus on whether the video appears visually convincing rather than whether it adheres to real-world principles. While recent models perform increasingly well on these metrics, they still struggle to generate videos that are not just visually plausible but fundamentally realistic. To achieve real \"world models\" through video generation, the next frontier lies in intrinsic faithfulness to ensure that generated videos adhere to physical laws, commonsense reasoning, anatomical correctness, and compositional integrity. Achieving this level of realism is essential for applications such as AI-assisted filmmaking and simulated world modeling. To bridge this gap, we introduce VBench-2.0, a next-generation benchmark designed to automatically evaluate video generative models for their intrinsic faithfulness. VBench-2.0 assesses five key dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense, each further broken down into fine-grained capabilities. Tailored for individual dimensions, our evaluation framework integrates generalists such as state-of-the-art VLMs and LLMs, and specialists, including anomaly detection methods proposed for video generation. We conduct extensive annotations to ensure alignment with human judgment. By pushing beyond superficial faithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new standard for the next generation of video generative models in pursuit of intrinsic faithfulness.","sentences":["Video generation has advanced significantly, evolving from producing unrealistic outputs to generating videos that appear visually convincing and temporally coherent.","To evaluate these video generative models, benchmarks such as VBench have been developed to assess their faithfulness, measuring factors like per-frame aesthetics, temporal consistency, and basic prompt adherence.","However, these aspects mainly represent superficial faithfulness, which focus on whether the video appears visually convincing rather than whether it adheres to real-world principles.","While recent models perform increasingly well on these metrics, they still struggle to generate videos that are not just visually plausible but fundamentally realistic.","To achieve real \"world models\" through video generation, the next frontier lies in intrinsic faithfulness to ensure that generated videos adhere to physical laws, commonsense reasoning, anatomical correctness, and compositional integrity.","Achieving this level of realism is essential for applications such as AI-assisted filmmaking and simulated world modeling.","To bridge this gap, we introduce VBench-2.0, a next-generation benchmark designed to automatically evaluate video generative models for their intrinsic faithfulness.","VBench-2.0 assesses five key dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense, each further broken down into fine-grained capabilities.","Tailored for individual dimensions, our evaluation framework integrates generalists such as state-of-the-art VLMs and LLMs, and specialists, including anomaly detection methods proposed for video generation.","We conduct extensive annotations to ensure alignment with human judgment.","By pushing beyond superficial faithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a new standard for the next generation of video generative models in pursuit of intrinsic faithfulness."],"url":"http://arxiv.org/abs/2503.21755v1"}
{"created":"2025-03-27 17:56:24","title":"Reconstructing Humans with a Biomechanically Accurate Skeleton","abstract":"In this paper, we introduce a method for reconstructing 3D humans from a single image using a biomechanically accurate skeleton model. To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model. Due to the lack of training data for this task, we build a pipeline to produce pseudo ground truth model parameters for single images and implement a training procedure that iteratively refines these pseudo labels. Compared to state-of-the-art methods for 3D human mesh recovery, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints. Additionally, we show that previous reconstruction methods frequently violate joint angle limits, leading to unnatural rotations. In contrast, our approach leverages the biomechanically plausible degrees of freedom making more realistic joint rotation estimates. We validate our approach across multiple human pose estimation benchmarks. We make the code, models and data available at: https://isshikihugh.github.io/HSMR/","sentences":["In this paper, we introduce a method for reconstructing 3D humans from a single image using a biomechanically accurate skeleton model.","To achieve this, we train a transformer that takes an image as input and estimates the parameters of the model.","Due to the lack of training data for this task, we build a pipeline to produce pseudo ground truth model parameters for single images and implement a training procedure that iteratively refines these pseudo labels.","Compared to state-of-the-art methods for 3D human mesh recovery, our model achieves competitive performance on standard benchmarks, while it significantly outperforms them in settings with extreme 3D poses and viewpoints.","Additionally, we show that previous reconstruction methods frequently violate joint angle limits, leading to unnatural rotations.","In contrast, our approach leverages the biomechanically plausible degrees of freedom making more realistic joint rotation estimates.","We validate our approach across multiple human pose estimation benchmarks.","We make the code, models and data available at: https://isshikihugh.github.io/HSMR/"],"url":"http://arxiv.org/abs/2503.21751v1"}
{"created":"2025-03-27 17:56:15","title":"LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis","abstract":"We introduce LeX-Art, a comprehensive suite for high-quality text-image synthesis that systematically bridges the gap between prompt expressiveness and text rendering fidelity. Our approach follows a data-centric paradigm, constructing a high-quality data synthesis pipeline based on Deepseek-R1 to curate LeX-10K, a dataset of 10K high-resolution, aesthetically refined 1024$\\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer, a robust prompt enrichment model, and train two text-to-image models, LeX-FLUX and LeX-Lumina, achieving state-of-the-art text rendering performance. To systematically evaluate visual text generation, we introduce LeX-Bench, a benchmark that assesses fidelity, aesthetics, and alignment, complemented by Pairwise Normalized Edit Distance (PNED), a novel metric for robust text accuracy evaluation. Experiments demonstrate significant improvements, with LeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX outperforming baselines in color (+3.18%), positional (+4.45%), and font accuracy (+3.81%). Our codes, models, datasets, and demo are publicly available.","sentences":["We introduce LeX-Art, a comprehensive suite for high-quality text-image synthesis that systematically bridges the gap between prompt expressiveness and text rendering fidelity.","Our approach follows a data-centric paradigm, constructing a high-quality data synthesis pipeline based on Deepseek-R1 to curate LeX-10K, a dataset of 10K high-resolution, aesthetically refined 1024$\\times$1024 images.","Beyond dataset construction, we develop LeX-Enhancer, a robust prompt enrichment model, and train two text-to-image models, LeX-FLUX and LeX-Lumina, achieving state-of-the-art text rendering performance.","To systematically evaluate visual text generation, we introduce LeX-Bench, a benchmark that assesses fidelity, aesthetics, and alignment, complemented by Pairwise Normalized Edit Distance (PNED), a novel metric for robust text accuracy evaluation.","Experiments demonstrate significant improvements, with LeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUX outperforming baselines in color (+3.18%), positional (+4.45%), and font accuracy (+3.81%).","Our codes, models, datasets, and demo are publicly available."],"url":"http://arxiv.org/abs/2503.21749v1"}
{"created":"2025-03-27 17:53:50","title":"CTRL-O: Language-Controllable Object-Centric Visual Representation Learning","abstract":"Object-centric representation learning aims to decompose visual scenes into fixed-size vectors called \"slots\" or \"object files\", where each slot captures a distinct object. Current state-of-the-art object-centric models have shown remarkable success in object discovery in diverse domains, including complex real-world scenes. However, these models suffer from a key limitation: they lack controllability. Specifically, current object-centric models learn representations based on their preconceived understanding of objects, without allowing user input to guide which objects are represented. Introducing controllability into object-centric models could unlock a range of useful capabilities, such as the ability to extract instance-specific representations from a scene. In this work, we propose a novel approach for user-directed control over slot representations by conditioning slots on language descriptions. The proposed ConTRoLlable Object-centric representation learning approach, which we term CTRL-O, achieves targeted object-language binding in complex real-world scenes without requiring mask supervision. Next, we apply these controllable slot representations on two downstream vision language tasks: text-to-image generation and visual question answering. The proposed approach enables instance-specific text-to-image generation and also achieves strong performance on visual question answering.","sentences":["Object-centric representation learning aims to decompose visual scenes into fixed-size vectors called \"slots\" or \"object files\", where each slot captures a distinct object.","Current state-of-the-art object-centric models have shown remarkable success in object discovery in diverse domains, including complex real-world scenes.","However, these models suffer from a key limitation: they lack controllability.","Specifically, current object-centric models learn representations based on their preconceived understanding of objects, without allowing user input to guide which objects are represented.","Introducing controllability into object-centric models could unlock a range of useful capabilities, such as the ability to extract instance-specific representations from a scene.","In this work, we propose a novel approach for user-directed control over slot representations by conditioning slots on language descriptions.","The proposed ConTRoLlable Object-centric representation learning approach, which we term CTRL-O, achieves targeted object-language binding in complex real-world scenes without requiring mask supervision.","Next, we apply these controllable slot representations on two downstream vision language tasks: text-to-image generation and visual question answering.","The proposed approach enables instance-specific text-to-image generation and also achieves strong performance on visual question answering."],"url":"http://arxiv.org/abs/2503.21747v1"}
{"created":"2025-03-27 17:53:00","title":"3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models","abstract":"3D generation is experiencing rapid advancements, while the development of 3D evaluation has not kept pace. How to keep automatic evaluation equitably aligned with human perception has become a well-recognized challenge. Recent advances in the field of language and image generation have explored human preferences and showcased respectable fitting ability. However, the 3D domain still lacks such a comprehensive preference dataset over generative models. To mitigate this absence, we develop 3DGen-Arena, an integrated platform in a battle manner. Then, we carefully design diverse text and image prompts and leverage the arena platform to gather human preferences from both public users and expert annotators, resulting in a large-scale multi-dimension human preference dataset 3DGen-Bench. Using this dataset, we further train a CLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator, 3DGen-Eval. These two models innovatively unify the quality evaluation of text-to-3D and image-to-3D generation, and jointly form our automated evaluation system with their respective strengths. Extensive experiments demonstrate the efficacy of our scoring model in predicting human preferences, exhibiting a superior correlation with human ranks compared to existing metrics. We believe that our 3DGen-Bench dataset and automated evaluation system will foster a more equitable evaluation in the field of 3D generation, further promoting the development of 3D generative models and their downstream applications.","sentences":["3D generation is experiencing rapid advancements, while the development of 3D evaluation has not kept pace.","How to keep automatic evaluation equitably aligned with human perception has become a well-recognized challenge.","Recent advances in the field of language and image generation have explored human preferences and showcased respectable fitting ability.","However, the 3D domain still lacks such a comprehensive preference dataset over generative models.","To mitigate this absence, we develop 3DGen-Arena, an integrated platform in a battle manner.","Then, we carefully design diverse text and image prompts and leverage the arena platform to gather human preferences from both public users and expert annotators, resulting in a large-scale multi-dimension human preference dataset 3DGen-Bench.","Using this dataset, we further train a CLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator, 3DGen-Eval.","These two models innovatively unify the quality evaluation of text-to-3D and image-to-3D generation, and jointly form our automated evaluation system with their respective strengths.","Extensive experiments demonstrate the efficacy of our scoring model in predicting human preferences, exhibiting a superior correlation with human ranks compared to existing metrics.","We believe that our 3DGen-Bench dataset and automated evaluation system will foster a more equitable evaluation in the field of 3D generation, further promoting the development of 3D generative models and their downstream applications."],"url":"http://arxiv.org/abs/2503.21745v1"}
{"created":"2025-03-27 17:48:32","title":"GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics","abstract":"Ensuring the reliability and effectiveness of software release decisions is critical, particularly in safety-critical domains like automotive systems. Precise analysis of release validation data, often presented in tabular form, plays a pivotal role in this process. However, traditional methods that rely on manual analysis of extensive test datasets and validation metrics are prone to delays and high costs. Large Language Models (LLMs) offer a promising alternative but face challenges in analytical reasoning, contextual understanding, handling out-of-scope queries, and processing structured test data consistently; limitations that hinder their direct application in safety-critical scenarios. This paper introduces GateLens, an LLM-based tool for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and then generates optimized Python code. It outperforms the baseline system on benchmarking datasets, achieving higher F1 scores and handling complex and ambiguous queries with greater robustness. Ablation studies confirm the critical role of the RA module, with performance dropping sharply when omitted. Industrial evaluations reveal that GateLens reduces analysis time by over 80% while maintaining high accuracy and reliability. As demonstrated by presented results, GateLens achieved high performance without relying on few-shot examples, showcasing strong generalization across various query types from diverse company roles. Insights from deploying GateLens with a partner automotive company offer practical guidance for integrating AI into critical workflows such as release validation. Results show that by automating test result analysis, GateLens enables faster, more informed, and dependable release decisions, and can thus advance software scalability and reliability in automotive systems.","sentences":["Ensuring the reliability and effectiveness of software release decisions is critical, particularly in safety-critical domains like automotive systems.","Precise analysis of release validation data, often presented in tabular form, plays a pivotal role in this process.","However, traditional methods that rely on manual analysis of extensive test datasets and validation metrics are prone to delays and high costs.","Large Language Models (LLMs) offer a promising alternative but face challenges in analytical reasoning, contextual understanding, handling out-of-scope queries, and processing structured test data consistently; limitations that hinder their direct application in safety-critical scenarios.","This paper introduces GateLens, an LLM-based tool for analyzing tabular data in the automotive domain.","GateLens translates natural language queries into Relational Algebra (RA) expressions and then generates optimized Python code.","It outperforms the baseline system on benchmarking datasets, achieving higher F1 scores and handling complex and ambiguous queries with greater robustness.","Ablation studies confirm the critical role of the RA module, with performance dropping sharply when omitted.","Industrial evaluations reveal that GateLens reduces analysis time by over 80% while maintaining high accuracy and reliability.","As demonstrated by presented results, GateLens achieved high performance without relying on few-shot examples, showcasing strong generalization across various query types from diverse company roles.","Insights from deploying GateLens with a partner automotive company offer practical guidance for integrating AI into critical workflows such as release validation.","Results show that by automating test result analysis, GateLens enables faster, more informed, and dependable release decisions, and can thus advance software scalability and reliability in automotive systems."],"url":"http://arxiv.org/abs/2503.21735v1"}
{"created":"2025-03-27 17:47:18","title":"Fully dynamic biconnectivity in $\\tilde{\\mathcal{O}}(\\log^2 n)$ time","abstract":"We present a deterministic fully-dynamic data structure for maintaining information about the cut-vertices in a graph; i.e. the vertices whose removal would disconnect the graph. Our data structure supports insertion and deletion of edges, as well as queries to whether a pair of connected vertices are either biconnected, or can be separated by a cutvertex, and in the latter case we support access to separating cutvertices. All update operations are supported in amortized $O(\\log^2 n \\log^2 \\log n)$ time, and queries take worst-case $O(\\log n \\log^2 \\log n)$ time. Note that these time bounds match the current best for deterministic dynamic connectivity up to $\\log \\log n$ factors.   We obtain our improved running time by a series of reductions from the original problem into well-defined data structure problems. While we do apply the well-known techniques for improving running time of two-edge connectivity [STOC'00, SODA'18], these techniques alone do not lead to an update time of $\\tilde{O}(\\log^3 n)$, let alone the $\\tilde{O}(\\log^2 n)$ we give as a final result.   Our contributions include a formally defined transient expose operation, which can be thought of as a cheaper read-only expose operation on a top tree. For each vertex in the graph, we maintain a data structure over its neighbors, and in this data structure we apply biasing (twice) to save two $\\tilde{O}(\\log n)$ factors. One of these biasing techniques is a new biased disjoint sets data structure, which may be of independent interest. Moreover, in this neighborhood data structure, we facilitate that the vertex can select two VIP neighbors that get special treatment, corresponding to its potentially two neighbors on an exposed path, improving a $\\log n$-time operation down to constant time. It is this combination of VIP neighbors with the transient expose that saves an $\\tilde{O}(\\log n)$-factor from another bottleneck.","sentences":["We present a deterministic fully-dynamic data structure for maintaining information about the cut-vertices in a graph; i.e. the vertices whose removal would disconnect the graph.","Our data structure supports insertion and deletion of edges, as well as queries to whether a pair of connected vertices are either biconnected, or can be separated by a cutvertex, and in the latter case we support access to separating cutvertices.","All update operations are supported in amortized $O(\\log^2 n \\log^2 \\log n)$ time, and queries take worst-case $O(\\log n \\log^2 \\log","n)$ time.","Note that these time bounds match the current best for deterministic dynamic connectivity up to $\\log \\log n$ factors.   ","We obtain our improved running time by a series of reductions from the original problem into well-defined data structure problems.","While we do apply the well-known techniques for improving running time of two-edge connectivity [STOC'00, SODA'18], these techniques alone do not lead to an update time of $\\tilde{O}(\\log^3 n)$, let alone the $\\tilde{O}(\\log^2 n)$ we give as a final result.   ","Our contributions include a formally defined transient expose operation, which can be thought of as a cheaper read-only expose operation on a top tree.","For each vertex in the graph, we maintain a data structure over its neighbors, and in this data structure we apply biasing (twice) to save two $\\tilde{O}(\\log n)$ factors.","One of these biasing techniques is a new biased disjoint sets data structure, which may be of independent interest.","Moreover, in this neighborhood data structure, we facilitate that the vertex can select two VIP neighbors that get special treatment, corresponding to its potentially two neighbors on an exposed path, improving a $\\log n$-time operation down to constant time.","It is this combination of VIP neighbors with the transient expose that saves an $\\tilde{O}(\\log n)$-factor from another bottleneck."],"url":"http://arxiv.org/abs/2503.21733v1"}
{"created":"2025-03-27 17:46:42","title":"SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling","abstract":"Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge. Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions. This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to $1024^3$ directly from rendering losses. SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces. Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training. This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision. Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation. Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology. By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling.","sentences":["Creating high-fidelity 3D meshes with arbitrary topology, including open surfaces and complex interiors, remains a significant challenge.","Existing implicit field methods often require costly and detail-degrading watertight conversion, while other approaches struggle with high resolutions.","This paper introduces SparseFlex, a novel sparse-structured isosurface representation that enables differentiable mesh reconstruction at resolutions up to $1024^3$ directly from rendering losses.","SparseFlex combines the accuracy of Flexicubes with a sparse voxel structure, focusing computation on surface-adjacent regions and efficiently handling open surfaces.","Crucially, we introduce a frustum-aware sectional voxel training strategy that activates only relevant voxels during rendering, dramatically reducing memory consumption and enabling high-resolution training.","This also allows, for the first time, the reconstruction of mesh interiors using only rendering supervision.","Building upon this, we demonstrate a complete shape modeling pipeline by training a variational autoencoder (VAE) and a rectified flow transformer for high-quality 3D shape generation.","Our experiments show state-of-the-art reconstruction accuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase in F-score compared to previous methods, and demonstrate the generation of high-resolution, detailed 3D shapes with arbitrary topology.","By enabling high-resolution, differentiable mesh reconstruction and generation with rendering losses, SparseFlex significantly advances the state-of-the-art in 3D shape representation and modeling."],"url":"http://arxiv.org/abs/2503.21732v1"}
{"created":"2025-03-27 17:46:24","title":"Cylindrical Algebraic Decomposition in \\textit{Macaulay2}","abstract":"\\texttt{CylindricalAlgebraicDecomposition.m2} is the first implementation of Cylindrical Algebraic Decomposition (CAD) in \\textit{Macaulay2}. CAD decomposes space into `cells' where input polynomials are sign-invariant. This package computes an Open CAD (full-dimensional cells only) for sets of real polynomials with rational coefficients, enabling users to solve existential problems involving strict inequalities. With the construction of a full CAD (cells of all dimensions), this tool could be extended to solve any real quantifier elimination problem. The current implementation employs the Lazard projection and introduces a new heuristic for choosing the variable ordering.","sentences":["\\texttt{CylindricalAlgebraicDecomposition.m2} is the first implementation of Cylindrical Algebraic Decomposition (CAD) in \\textit{Macaulay2}.","CAD decomposes space into `cells' where input polynomials are sign-invariant.","This package computes an Open CAD (full-dimensional cells only) for sets of real polynomials with rational coefficients, enabling users to solve existential problems involving strict inequalities.","With the construction of a full CAD (cells of all dimensions), this tool could be extended to solve any real quantifier elimination problem.","The current implementation employs the Lazard projection and introduces a new heuristic for choosing the variable ordering."],"url":"http://arxiv.org/abs/2503.21731v1"}
{"created":"2025-03-27 17:45:06","title":"Effective Skill Unlearning through Intervention and Abstention","abstract":"Large language Models (LLMs) have demonstrated remarkable skills across various domains. Understanding the mechanisms behind their abilities and implementing controls over them is becoming increasingly important for developing better models. In this paper, we focus on skill unlearning in LLMs, specifically unlearning a particular skill while retaining their overall capabilities. We introduce two lightweight, training-free machine skill unlearning techniques for LLMs. First, we observe that the pre-activation distribution of neurons in each Feed-Forward Layer (FFL) differs when the model demonstrates different skills. Additionally, we find that queries triggering the same skill cluster within the FFL key space and can be separated from other queries using a hypercube. Based on these observations, we propose two lightweight, training-free skill unlearning methods via \\textit{intervention} and \\textit{abstention} respectively: \\texttt{Neuron Adjust} and \\texttt{Key Space Detection}. We evaluate our methods on unlearning math-solving, Python-coding, and comprehension skills across seven different languages. The results demonstrate their strong unlearning capabilities for the designated skills. Specifically, \\texttt{Key Space Detection} achieves over 80\\% relative performance drop on the forgetting skill and less than 10\\% relative performance drop on other skills and the model's general knowledge (MMLU) for most unlearning tasks. Our code is available at https://github.com/Trustworthy-ML-Lab/effective_skill_unlearning","sentences":["Large language Models (LLMs) have demonstrated remarkable skills across various domains.","Understanding the mechanisms behind their abilities and implementing controls over them is becoming increasingly important for developing better models.","In this paper, we focus on skill unlearning in LLMs, specifically unlearning a particular skill while retaining their overall capabilities.","We introduce two lightweight, training-free machine skill unlearning techniques for LLMs.","First, we observe that the pre-activation distribution of neurons in each Feed-Forward Layer (FFL) differs when the model demonstrates different skills.","Additionally, we find that queries triggering the same skill cluster within the FFL key space and can be separated from other queries using a hypercube.","Based on these observations, we propose two lightweight, training-free skill unlearning methods via \\textit{intervention} and \\textit{abstention} respectively: \\texttt{Neuron Adjust} and \\texttt{Key Space Detection}.","We evaluate our methods on unlearning math-solving, Python-coding, and comprehension skills across seven different languages.","The results demonstrate their strong unlearning capabilities for the designated skills.","Specifically, \\texttt{Key Space Detection} achieves over 80\\% relative performance drop on the forgetting skill and less than 10\\% relative performance drop on other skills and the model's general knowledge (MMLU) for most unlearning tasks.","Our code is available at https://github.com/Trustworthy-ML-Lab/effective_skill_unlearning"],"url":"http://arxiv.org/abs/2503.21730v1"}
{"created":"2025-03-27 17:44:18","title":"ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation","abstract":"Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG).","sentences":["Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy.","While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks.","To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations.","Our solution includes a novel data construction framework with an upper bound on the reasoning chain length.","Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish).","For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later.","This process iterates until a Finish action is chosen.","Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA.","Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory.","Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG)."],"url":"http://arxiv.org/abs/2503.21729v1"}
{"created":"2025-03-27 17:38:43","title":"Enhancing Underwater Navigation through Cross-Correlation-Aware Deep INS/DVL Fusion","abstract":"The accurate navigation of autonomous underwater vehicles critically depends on the precision of Doppler velocity log (DVL) velocity measurements. Recent advancements in deep learning have demonstrated significant potential in improving DVL outputs by leveraging spatiotemporal dependencies across multiple sensor modalities. However, integrating these estimates into model-based filters, such as the extended Kalman filter, introduces statistical inconsistencies, most notably, cross-correlations between process and measurement noise. This paper addresses this challenge by proposing a cross-correlation-aware deep INS/DVL fusion framework. Building upon BeamsNet, a convolutional neural network designed to estimate AUV velocity using DVL and inertial data, we integrate its output into a navigation filter that explicitly accounts for the cross-correlation induced between the noise sources. This approach improves filter consistency and better reflects the underlying sensor error structure. Evaluated on two real-world underwater trajectories, the proposed method outperforms both least squares and cross-correlation-neglecting approaches in terms of state uncertainty. Notably, improvements exceed 10% in velocity and misalignment angle confidence metrics. Beyond demonstrating empirical performance, this framework provides a theoretically principled mechanism for embedding deep learning outputs within stochastic filters.","sentences":["The accurate navigation of autonomous underwater vehicles critically depends on the precision of Doppler velocity log (DVL) velocity measurements.","Recent advancements in deep learning have demonstrated significant potential in improving DVL outputs by leveraging spatiotemporal dependencies across multiple sensor modalities.","However, integrating these estimates into model-based filters, such as the extended Kalman filter, introduces statistical inconsistencies, most notably, cross-correlations between process and measurement noise.","This paper addresses this challenge by proposing a cross-correlation-aware deep INS/DVL fusion framework.","Building upon BeamsNet, a convolutional neural network designed to estimate AUV velocity using DVL and inertial data, we integrate its output into a navigation filter that explicitly accounts for the cross-correlation induced between the noise sources.","This approach improves filter consistency and better reflects the underlying sensor error structure.","Evaluated on two real-world underwater trajectories, the proposed method outperforms both least squares and cross-correlation-neglecting approaches in terms of state uncertainty.","Notably, improvements exceed 10% in velocity and misalignment angle confidence metrics.","Beyond demonstrating empirical performance, this framework provides a theoretically principled mechanism for embedding deep learning outputs within stochastic filters."],"url":"http://arxiv.org/abs/2503.21727v1"}
{"created":"2025-03-27 17:36:55","title":"OccRobNet : Occlusion Robust Network for Accurate 3D Interacting Hand-Object Pose Estimation","abstract":"Occlusion is one of the challenging issues when estimating 3D hand pose. This problem becomes more prominent when hand interacts with an object or two hands are involved. In the past works, much attention has not been given to these occluded regions. But these regions contain important and beneficial information that is vital for 3D hand pose estimation. Thus, in this paper, we propose an occlusion robust and accurate method for the estimation of 3D hand-object pose from the input RGB image. Our method includes first localising the hand joints using a CNN based model and then refining them by extracting contextual information. The self attention transformer then identifies the specific joints along with the hand identity. This helps the model to identify the hand belongingness of a particular joint which helps to detect the joint even in the occluded region. Further, these joints with hand identity are then used to estimate the pose using cross attention mechanism. Thus, by identifying the joints in the occluded region, the obtained network becomes robust to occlusion. Hence, this network achieves state-of-the-art results when evaluated on the InterHand2.6M, HO3D and H$_2$O3D datasets.","sentences":["Occlusion is one of the challenging issues when estimating 3D hand pose.","This problem becomes more prominent when hand interacts with an object or two hands are involved.","In the past works, much attention has not been given to these occluded regions.","But these regions contain important and beneficial information that is vital for 3D hand pose estimation.","Thus, in this paper, we propose an occlusion robust and accurate method for the estimation of 3D hand-object pose from the input RGB image.","Our method includes first localising the hand joints using a CNN based model and then refining them by extracting contextual information.","The self attention transformer then identifies the specific joints along with the hand identity.","This helps the model to identify the hand belongingness of a particular joint which helps to detect the joint even in the occluded region.","Further, these joints with hand identity are then used to estimate the pose using cross attention mechanism.","Thus, by identifying the joints in the occluded region, the obtained network becomes robust to occlusion.","Hence, this network achieves state-of-the-art results when evaluated on the InterHand2.6M, HO3D and H$_2$O3D datasets."],"url":"http://arxiv.org/abs/2503.21723v1"}
{"created":"2025-03-27 17:35:38","title":"Energy Minimization for Participatory Federated Learning in IoT Analyzed via Game Theory","abstract":"The Internet of Things requires intelligent decision making in many scenarios. To this end, resources available at the individual nodes for sensing or computing, or both, can be leveraged. This results in approaches known as participatory sensing and federated learning, respectively. We investigate the simultaneous implementation of both, through a distributed approach based on empowering local nodes with game theoretic decision making. A global objective of energy minimization is combined with the individual node's optimization of local expenditure for sensing and transmitting data over multiple learning rounds. We present extensive evaluations of this technique, based on both a theoretical framework and experiments in a simulated network scenario with real data. Such a distributed approach can reach a desired level of accuracy for federated learning without a centralized supervision of the data collector. However, depending on the weight attributed to the local costs of the single node, it may also result in a significantly high Price of Anarchy (from 1.28 onwards). Thus, we argue for the need of incentive mechanisms, possibly based on Age of Information of the single nodes.","sentences":["The Internet of Things requires intelligent decision making in many scenarios.","To this end, resources available at the individual nodes for sensing or computing, or both, can be leveraged.","This results in approaches known as participatory sensing and federated learning, respectively.","We investigate the simultaneous implementation of both, through a distributed approach based on empowering local nodes with game theoretic decision making.","A global objective of energy minimization is combined with the individual node's optimization of local expenditure for sensing and transmitting data over multiple learning rounds.","We present extensive evaluations of this technique, based on both a theoretical framework and experiments in a simulated network scenario with real data.","Such a distributed approach can reach a desired level of accuracy for federated learning without a centralized supervision of the data collector.","However, depending on the weight attributed to the local costs of the single node, it may also result in a significantly high Price of Anarchy (from 1.28 onwards).","Thus, we argue for the need of incentive mechanisms, possibly based on Age of Information of the single nodes."],"url":"http://arxiv.org/abs/2503.21722v1"}
{"created":"2025-03-27 17:35:14","title":"Evaluating Text-to-Image Synthesis with a Conditional Fr\u00e9chet Distance","abstract":"Evaluating text-to-image synthesis is challenging due to misalignment between established metrics and human preferences. We propose cFreD, a metric based on the notion of Conditional Fr\\'echet Distance that explicitly accounts for both visual fidelity and text-prompt alignment. Existing metrics such as Inception Score (IS), Fr\\'echet Inception Distance (FID) and CLIPScore assess either image quality or image-text alignment but not both which limits their correlation with human preferences. Scoring models explicitly trained to replicate human preferences require constant updates and may not generalize to novel generation techniques or out-of-domain inputs. Through extensive experiments across multiple recently proposed text-to-image models and diverse prompt datasets, we demonstrate that cFreD exhibits a higher correlation with human judgments compared to statistical metrics, including metrics trained with human preferences. Our findings validate cFreD as a robust, future-proof metric for the systematic evaluation of text-to-image models, standardizing benchmarking in this rapidly evolving field. We release our evaluation toolkit and benchmark in the appendix.","sentences":["Evaluating text-to-image synthesis is challenging due to misalignment between established metrics and human preferences.","We propose cFreD, a metric based on the notion of Conditional Fr\\'echet Distance that explicitly accounts for both visual fidelity and text-prompt alignment.","Existing metrics such as Inception Score (IS), Fr\\'echet Inception Distance (FID) and CLIPScore assess either image quality or image-text alignment but not both which limits their correlation with human preferences.","Scoring models explicitly trained to replicate human preferences require constant updates and may not generalize to novel generation techniques or out-of-domain inputs.","Through extensive experiments across multiple recently proposed text-to-image models and diverse prompt datasets, we demonstrate that cFreD exhibits a higher correlation with human judgments compared to statistical metrics, including metrics trained with human preferences.","Our findings validate cFreD as a robust, future-proof metric for the systematic evaluation of text-to-image models, standardizing benchmarking in this rapidly evolving field.","We release our evaluation toolkit and benchmark in the appendix."],"url":"http://arxiv.org/abs/2503.21721v1"}
{"created":"2025-03-27 17:34:25","title":"Collab: Controlled Decoding using Mixture of Agents for LLM Alignment","abstract":"Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications. Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences and broader utilities, but it requires updating billions of model parameters, which is computationally expensive. Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining. However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks. To strengthen the test-time performance w.r.t the target task, we propose a mixture of agent-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies. Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents. For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric. This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding. Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward for the given off-the-shelf models. We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines. Notably, Collab surpasses the current SoTA decoding strategy, achieving an improvement of up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.","sentences":["Alignment of Large Language models (LLMs) is crucial for safe and trustworthy deployment in applications.","Reinforcement learning from human feedback (RLHF) has emerged as an effective technique to align LLMs to human preferences and broader utilities, but it requires updating billions of model parameters, which is computationally expensive.","Controlled Decoding, by contrast, provides a mechanism for aligning a model at inference time without retraining.","However, single-agent decoding approaches often struggle to adapt to diverse tasks due to the complexity and variability inherent in these tasks.","To strengthen the test-time performance w.r.t the target task, we propose a mixture of agent-based decoding strategies leveraging the existing off-the-shelf aligned LLM policies.","Treating each prior policy as an agent in the spirit of mixture of agent collaboration, we develop a decoding method that allows for inference-time alignment through a token-level selection strategy among multiple agents.","For each token, the most suitable LLM is dynamically chosen from a pool of models based on a long-term utility metric.","This policy-switching mechanism ensures optimal model selection at each step, enabling efficient collaboration and alignment among LLMs during decoding.","Theoretical analysis of our proposed algorithm establishes optimal performance with respect to the target task represented via a target reward for the given off-the-shelf models.","We conduct comprehensive empirical evaluations with open-source aligned models on diverse tasks and preferences, which demonstrates the merits of this approach over single-agent decoding baselines.","Notably, Collab surpasses the current SoTA decoding strategy, achieving an improvement of up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate."],"url":"http://arxiv.org/abs/2503.21720v1"}
{"created":"2025-03-27 17:30:50","title":"Outlier dimensions favor frequent tokens in language model","abstract":"We study last-layer outlier dimensions, i.e.dimensions that display extreme activations for the majority of inputs. We show that outlier dimensions arise in many different modern language models, and trace their function back to the heuristic of constantly predicting frequent words. We further show how a model can block this heuristic when it is not contextually appropriate, by assigning a counterbalancing weight mass to the remaining dimensions, and we investigate which model parameters boost outlier dimensions and when they arise during training. We conclude that outlier dimensions are a specialized mechanism discovered by many distinct models to implement a useful token prediction heuristic.","sentences":["We study last-layer outlier dimensions, i.e.dimensions that display extreme activations for the majority of inputs.","We show that outlier dimensions arise in many different modern language models, and trace their function back to the heuristic of constantly predicting frequent words.","We further show how a model can block this heuristic when it is not contextually appropriate, by assigning a counterbalancing weight mass to the remaining dimensions, and we investigate which model parameters boost outlier dimensions and when they arise during training.","We conclude that outlier dimensions are a specialized mechanism discovered by many distinct models to implement a useful token prediction heuristic."],"url":"http://arxiv.org/abs/2503.21718v1"}
{"created":"2025-03-27 17:29:45","title":"CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?","abstract":"A core part of scientific peer review involves providing expert critiques that directly assess the scientific claims a paper makes. While it is now possible to automatically generate plausible (if generic) reviews, ensuring that these reviews are sound and grounded in the papers' claims remains challenging. To facilitate LLM benchmarking on these challenges, we introduce CLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions and reviews mined from OpenReview. CLAIMCHECK is richly annotated by ML experts for weakness statements in the reviews and the paper claims that they dispute, as well as fine-grained labels of the validity, objectivity, and type of the identified weaknesses. We benchmark several LLMs on three claim-centric tasks supported by CLAIMCHECK, requiring models to (1) associate weaknesses with the claims they dispute, (2) predict fine-grained labels for weaknesses and rewrite the weaknesses to enhance their specificity, and (3) verify a paper's claims with grounded reasoning. Our experiments reveal that cutting-edge LLMs, while capable of predicting weakness labels in (2), continue to underperform relative to human experts on all other tasks.","sentences":["A core part of scientific peer review involves providing expert critiques that directly assess the scientific claims a paper makes.","While it is now possible to automatically generate plausible (if generic) reviews, ensuring that these reviews are sound and grounded in the papers' claims remains challenging.","To facilitate LLM benchmarking on these challenges, we introduce CLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions and reviews mined from OpenReview.","CLAIMCHECK is richly annotated by ML experts for weakness statements in the reviews and the paper claims that they dispute, as well as fine-grained labels of the validity, objectivity, and type of the identified weaknesses.","We benchmark several LLMs on three claim-centric tasks supported by CLAIMCHECK, requiring models to (1) associate weaknesses with the claims they dispute, (2) predict fine-grained labels for weaknesses and rewrite the weaknesses to enhance their specificity, and (3) verify a paper's claims with grounded reasoning.","Our experiments reveal that cutting-edge LLMs, while capable of predicting weakness labels in (2), continue to underperform relative to human experts on all other tasks."],"url":"http://arxiv.org/abs/2503.21717v1"}
{"created":"2025-03-27 17:26:32","title":"As easy as PIE: understanding when pruning causes language models to disagree","abstract":"Language Model (LM) pruning compresses the model by removing weights, nodes, or other parts of its architecture. Typically, pruning focuses on the resulting efficiency gains at the cost of effectiveness. However, when looking at how individual data points are affected by pruning, it turns out that a particular subset of data points always bears most of the brunt (in terms of reduced accuracy) when pruning, but this effect goes unnoticed when reporting the mean accuracy of all data points. These data points are called PIEs and have been studied in image processing, but not in NLP. In a study of various NLP datasets, pruning methods, and levels of compression, we find that PIEs impact inference quality considerably, regardless of class frequency, and that BERT is more prone to this than BiLSTM. We also find that PIEs contain a high amount of data points that have the largest influence on how well the model generalises to unseen data. This means that when pruning, with seemingly moderate loss to accuracy across all data points, we in fact hurt tremendously those data points that matter the most. We trace what makes PIEs both hard and impactful to inference to their overall longer and more semantically complex text. These findings are novel and contribute to understanding how LMs are affected by pruning. The code is available at: https://github.com/pietrotrope/AsEasyAsPIE","sentences":["Language Model (LM) pruning compresses the model by removing weights, nodes, or other parts of its architecture.","Typically, pruning focuses on the resulting efficiency gains at the cost of effectiveness.","However, when looking at how individual data points are affected by pruning, it turns out that a particular subset of data points always bears most of the brunt (in terms of reduced accuracy) when pruning, but this effect goes unnoticed when reporting the mean accuracy of all data points.","These data points are called PIEs and have been studied in image processing, but not in NLP.","In a study of various NLP datasets, pruning methods, and levels of compression, we find that PIEs impact inference quality considerably, regardless of class frequency, and that BERT is more prone to this than BiLSTM.","We also find that PIEs contain a high amount of data points that have the largest influence on how well the model generalises to unseen data.","This means that when pruning, with seemingly moderate loss to accuracy across all data points, we in fact hurt tremendously those data points that matter the most.","We trace what makes PIEs both hard and impactful to inference to their overall longer and more semantically complex text.","These findings are novel and contribute to understanding how LMs are affected by pruning.","The code is available at: https://github.com/pietrotrope/AsEasyAsPIE"],"url":"http://arxiv.org/abs/2503.21714v1"}
{"created":"2025-03-27 17:22:14","title":"Efficient Computation of the Directional Extremal Boundary of a Union of Equal-Radius Circles","abstract":"This paper focuses on computing the directional extremal boundary of a union of equal-radius circles. We introduce an efficient algorithm that accurately determines this boundary by analyzing the intersections and dominant relationships among the circles. The algorithm has time complexity of O(n log n).","sentences":["This paper focuses on computing the directional extremal boundary of a union of equal-radius circles.","We introduce an efficient algorithm that accurately determines this boundary by analyzing the intersections and dominant relationships among the circles.","The algorithm has time complexity of O(n log n)."],"url":"http://arxiv.org/abs/2503.21711v1"}
{"created":"2025-03-27 17:21:47","title":"Enhancing Repository-Level Software Repair via Repository-Aware Knowledge Graphs","abstract":"Repository-level software repair faces challenges in bridging semantic gaps between issue descriptions and code patches. Existing approaches, which mostly depend on large language models (LLMs), suffer from semantic ambiguities, limited structural context understanding, and insufficient reasoning capability. To address these limitations, we propose KGCompass with two innovations: (1) a novel repository-aware knowledge graph (KG) that accurately links repository artifacts (issues and pull requests) and codebase entities (files, classes, and functions), allowing us to effectively narrow down the vast search space to only 20 most relevant functions with accurate candidate bug locations and contextual information, and (2) a path-guided repair mechanism that leverages KG-mined entity path, tracing through which allows us to augment LLMs with relevant contextual information to generate precise patches along with their explanations. Experimental results in the SWE-Bench-Lite demonstrate that KGCompass achieves state-of-the-art repair performance (45.67%) and function-level localization accuracy (51.33%) across open-source approaches, costing only $0.20 per repair. Our analysis reveals that among successfully localized bugs, 69.7% require multi-hop traversals through the knowledge graph, without which LLM-based approaches struggle to accurately locate bugs. The knowledge graph built in KGCompass is language agnostic and can be incrementally updated, making it a practical solution for real-world development environments.","sentences":["Repository-level software repair faces challenges in bridging semantic gaps between issue descriptions and code patches.","Existing approaches, which mostly depend on large language models (LLMs), suffer from semantic ambiguities, limited structural context understanding, and insufficient reasoning capability.","To address these limitations, we propose KGCompass with two innovations: (1) a novel repository-aware knowledge graph (KG) that accurately links repository artifacts (issues and pull requests) and codebase entities (files, classes, and functions), allowing us to effectively narrow down the vast search space to only 20 most relevant functions with accurate candidate bug locations and contextual information, and (2) a path-guided repair mechanism that leverages KG-mined entity path, tracing through which allows us to augment LLMs with relevant contextual information to generate precise patches along with their explanations.","Experimental results in the SWE-Bench-Lite demonstrate that KGCompass achieves state-of-the-art repair performance (45.67%) and function-level localization accuracy (51.33%) across open-source approaches, costing only $0.20 per repair.","Our analysis reveals that among successfully localized bugs, 69.7% require multi-hop traversals through the knowledge graph, without which LLM-based approaches struggle to accurately locate bugs.","The knowledge graph built in KGCompass is language agnostic and can be incrementally updated, making it a practical solution for real-world development environments."],"url":"http://arxiv.org/abs/2503.21710v1"}
{"created":"2025-03-27 17:21:46","title":"Redefining Network Topology in Complex Systems: Merging Centrality Metrics, Spectral Theory, and Diffusion Dynamics","abstract":"This paper introduces a novel framework that combines traditional centrality measures with eigenvalue spectra and diffusion processes for a more comprehensive analysis of complex networks. While centrality measures such as degree, closeness, and betweenness have been commonly used to assess nodal importance, they provide limited insight into dynamic network behaviors. By incorporating eigenvalue analysis, which evaluates network robustness and connectivity through spectral properties, and diffusion processes that model information flow, this framework offers a deeper understanding of how networks function under dynamic conditions. Applied to synthetic networks, the approach identifies key nodes not only by centrality but also by their role in diffusion dynamics and vulnerability points, offering a multi-dimensional view that traditional methods alone cannot. This integrated analysis enables a more precise identification of critical nodes and potential weaknesses, with implications for improving network resilience in fields ranging from epidemiology to cybersecurity. Keywords: Centrality measures, eigenvalue spectra, diffusion processes, network analysis, network robustness, information flow, synthetic networks.","sentences":["This paper introduces a novel framework that combines traditional centrality measures with eigenvalue spectra and diffusion processes for a more comprehensive analysis of complex networks.","While centrality measures such as degree, closeness, and betweenness have been commonly used to assess nodal importance, they provide limited insight into dynamic network behaviors.","By incorporating eigenvalue analysis, which evaluates network robustness and connectivity through spectral properties, and diffusion processes that model information flow, this framework offers a deeper understanding of how networks function under dynamic conditions.","Applied to synthetic networks, the approach identifies key nodes not only by centrality but also by their role in diffusion dynamics and vulnerability points, offering a multi-dimensional view that traditional methods alone cannot.","This integrated analysis enables a more precise identification of critical nodes and potential weaknesses, with implications for improving network resilience in fields ranging from epidemiology to cybersecurity.","Keywords: Centrality measures, eigenvalue spectra, diffusion processes, network analysis, network robustness, information flow, synthetic networks."],"url":"http://arxiv.org/abs/2503.21709v1"}
{"created":"2025-03-27 17:20:44","title":"Elementwise Layer Normalization","abstract":"A recent paper proposed Dynamic Tanh (DyT) as a drop-in replacement for Layer Normalization. Although the method is empirically well-motivated and appealing from a practical point of view, it lacks a theoretical foundation. In this work, we derive DyT mathematically and show that a well-defined approximation is needed to do so. By dropping said approximation, an alternative element-wise transformation is obtained, which we call Elementwise Layer Normalization (ELN). We demonstrate that ELN resembles Layer Normalization more accurately than DyT does.","sentences":["A recent paper proposed Dynamic Tanh (DyT) as a drop-in replacement for Layer Normalization.","Although the method is empirically well-motivated and appealing from a practical point of view, it lacks a theoretical foundation.","In this work, we derive DyT mathematically and show that a well-defined approximation is needed to do so.","By dropping said approximation, an alternative element-wise transformation is obtained, which we call Elementwise Layer Normalization (ELN).","We demonstrate that ELN resembles Layer Normalization more accurately than DyT does."],"url":"http://arxiv.org/abs/2503.21708v1"}
{"created":"2025-03-27 17:10:38","title":"SoK: Towards Reproducibility for Software Packages in Scripting Language Ecosystems","abstract":"The disconnect between distributed software artifacts and their supposed source code enables attackers to leverage the build process for inserting malicious functionality. Past research in this field focuses on compiled language ecosystems, mostly analysing Linux distribution packages. However, the popular scripting language ecosystems potentially face unique issues given the systematic difference in distributed artifacts. This SoK provides an overview of existing research, aiming to highlight future directions, as well as chances to transfer existing knowledge from compiled language ecosystems. To that end, we work out key aspects in current research, systematize identified challenges for software reproducibility, and map them between the ecosystems. We find that the literature is sparse, focusing on few individual problems and ecosystems. This allows us to effectively identify next steps to improve reproducibility in this field.","sentences":["The disconnect between distributed software artifacts and their supposed source code enables attackers to leverage the build process for inserting malicious functionality.","Past research in this field focuses on compiled language ecosystems, mostly analysing Linux distribution packages.","However, the popular scripting language ecosystems potentially face unique issues given the systematic difference in distributed artifacts.","This SoK provides an overview of existing research, aiming to highlight future directions, as well as chances to transfer existing knowledge from compiled language ecosystems.","To that end, we work out key aspects in current research, systematize identified challenges for software reproducibility, and map them between the ecosystems.","We find that the literature is sparse, focusing on few individual problems and ecosystems.","This allows us to effectively identify next steps to improve reproducibility in this field."],"url":"http://arxiv.org/abs/2503.21705v1"}
{"created":"2025-03-27 17:10:05","title":"Learning to Represent Individual Differences for Choice Decision Making","abstract":"Human decision making can be challenging to predict because decisions are affected by a number of complex factors. Adding to this complexity, decision-making processes can differ considerably between individuals, and methods aimed at predicting human decisions need to take individual differences into account. Behavioral science offers methods by which to measure individual differences (e.g., questionnaires, behavioral models), but these are often narrowed down to low dimensions and not tailored to specific prediction tasks. This paper investigates the use of representation learning to measure individual differences from behavioral experiment data. Representation learning offers a flexible approach to create individual embeddings from data that are both structured (e.g., demographic information) and unstructured (e.g., free text), where the flexibility provides more options for individual difference measures for personalization, e.g., free text responses may allow for open-ended questions that are less privacy-sensitive. In the current paper we use representation learning to characterize individual differences in human performance on an economic decision-making task. We demonstrate that models using representation learning to capture individual differences consistently improve decision predictions over models without representation learning, and even outperform well-known theory-based behavioral models used in these environments. Our results propose that representation learning offers a useful and flexible tool to capture individual differences.","sentences":["Human decision making can be challenging to predict because decisions are affected by a number of complex factors.","Adding to this complexity, decision-making processes can differ considerably between individuals, and methods aimed at predicting human decisions need to take individual differences into account.","Behavioral science offers methods by which to measure individual differences (e.g., questionnaires, behavioral models), but these are often narrowed down to low dimensions and not tailored to specific prediction tasks.","This paper investigates the use of representation learning to measure individual differences from behavioral experiment data.","Representation learning offers a flexible approach to create individual embeddings from data that are both structured (e.g., demographic information) and unstructured (e.g., free text), where the flexibility provides more options for individual difference measures for personalization, e.g., free text responses may allow for open-ended questions that are less privacy-sensitive.","In the current paper we use representation learning to characterize individual differences in human performance on an economic decision-making task.","We demonstrate that models using representation learning to capture individual differences consistently improve decision predictions over models without representation learning, and even outperform well-known theory-based behavioral models used in these environments.","Our results propose that representation learning offers a useful and flexible tool to capture individual differences."],"url":"http://arxiv.org/abs/2503.21704v1"}
{"created":"2025-03-27 17:04:33","title":"MAVERIX: Multimodal Audio-Visual Evaluation Reasoning IndeX","abstract":"Frontier models have either been language-only or have primarily focused on vision and language modalities. Although recent advancements in models with vision and audio understanding capabilities have shown substantial progress, the field lacks a standardized evaluation framework for thoroughly assessing their cross-modality perception performance. We introduce MAVERIX~(Multimodal Audio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and 2,556 questions explicitly designed to evaluate multimodal models through tasks that necessitate close integration of video and audio information. MAVERIX uniquely provides models with audiovisual tasks, closely mimicking the multimodal perceptual experiences available to humans during inference and decision-making processes. To our knowledge, MAVERIX is the first benchmark aimed explicitly at assessing comprehensive audiovisual integration. Experiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show performance approaching human levels (around 70% accuracy), while human experts reach near-ceiling performance (95.1%). With standardized evaluation protocols, a rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a challenging testbed for advancing audiovisual multimodal intelligence.","sentences":["Frontier models have either been language-only or have primarily focused on vision and language modalities.","Although recent advancements in models with vision and audio understanding capabilities have shown substantial progress, the field lacks a standardized evaluation framework for thoroughly assessing their cross-modality perception performance.","We introduce MAVERIX~(Multimodal Audio-Visual Evaluation Reasoning IndeX), a novel benchmark with 700 videos and 2,556 questions explicitly designed to evaluate multimodal models through tasks that necessitate close integration of video and audio information.","MAVERIX uniquely provides models with audiovisual tasks, closely mimicking the multimodal perceptual experiences available to humans during inference and decision-making processes.","To our knowledge, MAVERIX is the first benchmark aimed explicitly at assessing comprehensive audiovisual integration.","Experiments with state-of-the-art models, including Gemini 1.5 Pro and o1, show performance approaching human levels (around 70% accuracy), while human experts reach near-ceiling performance (95.1%).","With standardized evaluation protocols, a rigorously annotated pipeline, and a public toolkit, MAVERIX establishes a challenging testbed for advancing audiovisual multimodal intelligence."],"url":"http://arxiv.org/abs/2503.21699v1"}
{"created":"2025-03-27 17:01:19","title":"The commutativity problem for effective varieties of formal series, and applications","abstract":"A formal series in noncommuting variables $\\Sigma$ over the rationals is a mapping $\\Sigma^* \\to \\mathbb Q$. We say that a series is commutative if the value in the output does not depend on the order of the symbols in the input. The commutativity problem for a class of series takes as input a (finite presentation of) a series from the class and amounts to establishing whether it is commutative. This is a very natural, albeit nontrivial problem, which has not been considered before from an algorithmic perspective.   We show that commutativity is decidable for all classes of series that constitute a so-called effective prevariety, a notion generalising Reutenauer's varieties of formal series. For example, the class of rational series, introduced by Sch\\\"utzenberger in the 1960's, is well-known to be an effective (pre)variety, and thus commutativity is decidable for it.   In order to showcase the applicability of our result, we consider classes of formal series generalising the rational ones. We consider polynomial automata, shuffle automata, and infiltration automata, and we show that each of these models recognises an effective prevariety of formal series. Consequently, their commutativity problem is decidable, which is a novel result. We find it remarkable that commutativity can be decided in a uniform way for such disparate computation models.   Finally, we present applications of commutativity outside the theory of formal series. We show that we can decide solvability in sequences and in power series for restricted classes of algebraic difference and differential equations, for which such problems are undecidable in full generality. Thanks to this, we can prove that the syntaxes of multivariate polynomial recursive sequences and of constructible differentially algebraic power series are effective, which are new results which were left open in previous work.","sentences":["A formal series in noncommuting variables $\\Sigma$ over the rationals is a mapping $\\Sigma^* \\to \\mathbb Q$.","We say that a series is commutative if the value in the output does not depend on the order of the symbols in the input.","The commutativity problem for a class of series takes as input a (finite presentation of) a series from the class and amounts to establishing whether it is commutative.","This is a very natural, albeit nontrivial problem, which has not been considered before from an algorithmic perspective.   ","We show that commutativity is decidable for all classes of series that constitute a so-called effective prevariety, a notion generalising Reutenauer's varieties of formal series.","For example, the class of rational series, introduced by Sch\\\"utzenberger in the 1960's, is well-known to be an effective (pre)variety, and thus commutativity is decidable for it.   ","In order to showcase the applicability of our result, we consider classes of formal series generalising the rational ones.","We consider polynomial automata, shuffle automata, and infiltration automata, and we show that each of these models recognises an effective prevariety of formal series.","Consequently, their commutativity problem is decidable, which is a novel result.","We find it remarkable that commutativity can be decided in a uniform way for such disparate computation models.   ","Finally, we present applications of commutativity outside the theory of formal series.","We show that we can decide solvability in sequences and in power series for restricted classes of algebraic difference and differential equations, for which such problems are undecidable in full generality.","Thanks to this, we can prove that the syntaxes of multivariate polynomial recursive sequences and of constructible differentially algebraic power series are effective, which are new results which were left open in previous work."],"url":"http://arxiv.org/abs/2503.21697v1"}
{"created":"2025-03-27 17:00:51","title":"Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks","abstract":"Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks. However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored. We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks. Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history. To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification). We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning. The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\\%, 24\\%, and +13\\%. Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks. Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases.","sentences":["Recent advances in deep thinking models have demonstrated remarkable reasoning capabilities on mathematical and coding tasks.","However, their effectiveness in embodied domains which require continuous interaction with environments through image action interleaved trajectories remains largely -unexplored.","We present Embodied Reasoner, a model that extends o1 style reasoning to interactive embodied search tasks.","Unlike mathematical reasoning that relies primarily on logical deduction, embodied scenarios demand spatial understanding, temporal reasoning, and ongoing self-reflection based on interaction history.","To address these challenges, we synthesize 9.3k coherent Observation-Thought-Action trajectories containing 64k interactive images and 90k diverse thinking processes (analysis, spatial reasoning, reflection, planning, and verification).","We develop a three-stage training pipeline that progressively enhances the model's capabilities through imitation learning, self-exploration via rejection sampling, and self-correction through reflection tuning.","The evaluation shows that our model significantly outperforms those advanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, and Claude-3.7 by +9\\%, 24\\%, and +13\\%.","Analysis reveals our model exhibits fewer repeated searches and logical inconsistencies, with particular advantages in complex long-horizon tasks.","Real-world environments also show our superiority while exhibiting fewer repeated searches and logical inconsistency cases."],"url":"http://arxiv.org/abs/2503.21696v1"}
{"created":"2025-03-27 16:59:39","title":"AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation","abstract":"Accurate segmentation of cell nuclei in histopathology images is essential for numerous biomedical research and clinical applications. However, existing cell nucleus segmentation methods only consider a single dataset (i.e., primary domain), while neglecting to leverage supplementary data from diverse sources (i.e., auxiliary domains) to reduce overfitting and enhance the performance. Although incorporating multiple datasets could alleviate overfitting, it often exacerbates performance drops caused by domain shifts. In this work, we introduce Adversarial Multi-domain Alignment of Segment Anything Model (AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these obstacles through two key innovations. First, we propose a Conditional Gradient Reversal Layer (CGRL), a multi-domain alignment module that harmonizes features from diverse domains to promote domain-invariant representation learning while preserving crucial discriminative features for the primary dataset. Second, we address SAM's inherent low-resolution output by designing a High-Resolution Decoder (HR-Decoder), which directly produces fine-grained segmentation maps in order to capture intricate nuclei boundaries in high-resolution histology images. To the best of our knowledge, this is the first attempt to adapt SAM for multi-dataset learning with application to histology nuclei segmentation. We validate our method on several publicly available datasets, demonstrating consistent and significant improvements over state-of-the-art approaches.","sentences":["Accurate segmentation of cell nuclei in histopathology images is essential for numerous biomedical research and clinical applications.","However, existing cell nucleus segmentation methods only consider a single dataset (i.e., primary domain), while neglecting to leverage supplementary data from diverse sources (i.e., auxiliary domains) to reduce overfitting and enhance the performance.","Although incorporating multiple datasets could alleviate overfitting, it often exacerbates performance drops caused by domain shifts.","In this work, we introduce Adversarial Multi-domain Alignment of Segment Anything Model (AMA-SAM) that extends the Segment Anything Model (SAM) to overcome these obstacles through two key innovations.","First, we propose a Conditional Gradient Reversal Layer (CGRL), a multi-domain alignment module that harmonizes features from diverse domains to promote domain-invariant representation learning while preserving crucial discriminative features for the primary dataset.","Second, we address SAM's inherent low-resolution output by designing a High-Resolution Decoder (HR-Decoder), which directly produces fine-grained segmentation maps in order to capture intricate nuclei boundaries in high-resolution histology images.","To the best of our knowledge, this is the first attempt to adapt SAM for multi-dataset learning with application to histology nuclei segmentation.","We validate our method on several publicly available datasets, demonstrating consistent and significant improvements over state-of-the-art approaches."],"url":"http://arxiv.org/abs/2503.21695v1"}
{"created":"2025-03-27 16:59:15","title":"Progressive Rendering Distillation: Adapting Stable Diffusion for Instant Text-to-Mesh Generation without 3D Data","abstract":"It is highly desirable to obtain a model that can generate high-quality 3D meshes from text prompts in just seconds. While recent attempts have adapted pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into generators of 3D representations (e.g., Triplane), they often suffer from poor quality due to the lack of sufficient high-quality 3D training data. Aiming at overcoming the data shortage, we propose a novel training scheme, termed as Progressive Rendering Distillation (PRD), eliminating the need for 3D ground-truths by distilling multi-view diffusion models and adapting SD into a native 3D generator. In each iteration of training, PRD uses the U-Net to progressively denoise the latent from random noise for a few steps, and in each step it decodes the denoised latent into 3D output. Multi-view diffusion models, including MVDream and RichDreamer, are used in joint with SD to distill text-consistent textures and geometries into the 3D outputs through score distillation. Since PRD supports training without 3D ground-truths, we can easily scale up the training data and improve generation quality for challenging text prompts with creative concepts. Meanwhile, PRD can accelerate the inference speed of the generation model in just a few steps. With PRD, we train a Triplane generator, namely TriplaneTurbo, which adds only $2.5\\%$ trainable parameters to adapt SD for Triplane generation. TriplaneTurbo outperforms previous text-to-3D generators in both efficiency and quality. Specifically, it can produce high-quality 3D meshes in 1.2 seconds and generalize well for challenging text input. The code is available at https://github.com/theEricMa/TriplaneTurbo.","sentences":["It is highly desirable to obtain a model that can generate high-quality 3D meshes from text prompts in just seconds.","While recent attempts have adapted pre-trained text-to-image diffusion models, such as Stable Diffusion (SD), into generators of 3D representations (e.g., Triplane), they often suffer from poor quality due to the lack of sufficient high-quality 3D training data.","Aiming at overcoming the data shortage, we propose a novel training scheme, termed as Progressive Rendering Distillation (PRD), eliminating the need for 3D ground-truths by distilling multi-view diffusion models and adapting SD into a native 3D generator.","In each iteration of training, PRD uses the U-Net to progressively denoise the latent from random noise for a few steps, and in each step it decodes the denoised latent into 3D output.","Multi-view diffusion models, including MVDream and RichDreamer, are used in joint with SD to distill text-consistent textures and geometries into the 3D outputs through score distillation.","Since PRD supports training without 3D ground-truths, we can easily scale up the training data and improve generation quality for challenging text prompts with creative concepts.","Meanwhile, PRD can accelerate the inference speed of the generation model in just a few steps.","With PRD, we train a Triplane generator, namely TriplaneTurbo, which adds only $2.5\\%$ trainable parameters to adapt SD for Triplane generation.","TriplaneTurbo outperforms previous text-to-3D generators in both efficiency and quality.","Specifically, it can produce high-quality 3D meshes in 1.2 seconds and generalize well for challenging text input.","The code is available at https://github.com/theEricMa/TriplaneTurbo."],"url":"http://arxiv.org/abs/2503.21694v1"}
{"created":"2025-03-27 16:57:33","title":"RapidPoseTriangulation: Multi-view Multi-person Whole-body Human Pose Triangulation in a Millisecond","abstract":"The integration of multi-view imaging and pose estimation represents a significant advance in computer vision applications, offering new possibilities for understanding human movement and interactions. This work presents a new algorithm that improves multi-view multi-person pose estimation, focusing on fast triangulation speeds and good generalization capabilities. The approach extends to whole-body pose estimation, capturing details from facial expressions to finger movements across multiple individuals and viewpoints. Adaptability to different settings is demonstrated through strong performance across unseen datasets and configurations. To support further progress in this field, all of this work is publicly accessible.","sentences":["The integration of multi-view imaging and pose estimation represents a significant advance in computer vision applications, offering new possibilities for understanding human movement and interactions.","This work presents a new algorithm that improves multi-view multi-person pose estimation, focusing on fast triangulation speeds and good generalization capabilities.","The approach extends to whole-body pose estimation, capturing details from facial expressions to finger movements across multiple individuals and viewpoints.","Adaptability to different settings is demonstrated through strong performance across unseen datasets and configurations.","To support further progress in this field, all of this work is publicly accessible."],"url":"http://arxiv.org/abs/2503.21692v1"}
{"created":"2025-03-27 16:55:41","title":"Place Capability Graphs: A General-Purpose Model of Rust's Ownership and Borrowing Guarantees","abstract":"Rust's novel type system has proved an attractive target for verification and program analysis tools, due to the rich guarantees it provides for controlling aliasing and mutability. However, fully understanding, extracting and exploiting these guarantees is subtle and challenging: existing models for Rust's type checking either support a smaller idealised language disconnected from real-world Rust code, or come with severe limitations in terms of precise modelling of Rust borrows, composite types storing them, function signatures and loops.   In this paper, we present a novel model of Rust's type-checking called Place Capability Graphs, which lifts these limitations, and which can be directly calculated from the Rust compiler's own programmatic representations and analyses. We demonstrate that our model supports over 98% of Rust functions in the most popular public crates, and show its suitability as a general-purpose basis for verification and program analysis tools by developing promising new prototype versions of the existing Flowistry and Prusti tools.","sentences":["Rust's novel type system has proved an attractive target for verification and program analysis tools, due to the rich guarantees it provides for controlling aliasing and mutability.","However, fully understanding, extracting and exploiting these guarantees is subtle and challenging: existing models for Rust's type checking either support a smaller idealised language disconnected from real-world Rust code, or come with severe limitations in terms of precise modelling of Rust borrows, composite types storing them, function signatures and loops.   ","In this paper, we present a novel model of Rust's type-checking called Place Capability Graphs, which lifts these limitations, and which can be directly calculated from the Rust compiler's own programmatic representations and analyses.","We demonstrate that our model supports over 98% of Rust functions in the most popular public crates, and show its suitability as a general-purpose basis for verification and program analysis tools by developing promising new prototype versions of the existing Flowistry and Prusti tools."],"url":"http://arxiv.org/abs/2503.21691v1"}
{"created":"2025-03-27 16:55:32","title":"CMED: A Child Micro-Expression Dataset","abstract":"Micro-expressions are short bursts of emotion that are difficult to hide. Their detection in children is an important cue to assist psychotherapists in conducting better therapy. However, existing research on the detection of micro-expressions has focused on adults, whose expressions differ in their characteristics from those of children. The lack of research is a direct consequence of the lack of a child-based micro-expressions dataset as it is much more challenging to capture children's facial expressions due to the lack of predictability and controllability. This study compiles a dataset of spontaneous child micro-expression videos, the first of its kind, to the best of the authors knowledge. The dataset is captured in the wild using video conferencing software. This dataset enables us to then explore key features and differences between adult and child micro-expressions. This study also establishes a baseline for the automated spotting and recognition of micro-expressions in children using three approaches comprising of hand-created and learning-based approaches.","sentences":["Micro-expressions are short bursts of emotion that are difficult to hide.","Their detection in children is an important cue to assist psychotherapists in conducting better therapy.","However, existing research on the detection of micro-expressions has focused on adults, whose expressions differ in their characteristics from those of children.","The lack of research is a direct consequence of the lack of a child-based micro-expressions dataset as it is much more challenging to capture children's facial expressions due to the lack of predictability and controllability.","This study compiles a dataset of spontaneous child micro-expression videos, the first of its kind, to the best of the authors knowledge.","The dataset is captured in the wild using video conferencing software.","This dataset enables us to then explore key features and differences between adult and child micro-expressions.","This study also establishes a baseline for the automated spotting and recognition of micro-expressions in children using three approaches comprising of hand-created and learning-based approaches."],"url":"http://arxiv.org/abs/2503.21690v1"}
{"created":"2025-03-27 16:52:25","title":"LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning","abstract":"In recent years, large language models (LLMs) have shown significant advancements in natural language processing (NLP), with strong capa-bilities in generation, comprehension, and rea-soning. These models have found applications in education, intelligent decision-making, and gaming. However, effectively utilizing LLMs for strategic planning and decision-making in the game of Gomoku remains a challenge. This study aims to develop a Gomoku AI system based on LLMs, simulating the human learning process of playing chess. The system is de-signed to understand and apply Gomoku strat-egies and logic to make rational decisions. The research methods include enabling the model to \"read the board,\" \"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while en-hancing its abilities through self-play and rein-forcement learning. The results demonstrate that this approach significantly improves the se-lection of move positions, resolves the issue of generating illegal positions, and reduces pro-cess time through parallel position evaluation. After extensive self-play training, the model's Gomoku-playing capabilities have been notably enhanced.","sentences":["In recent years, large language models (LLMs) have shown significant advancements in natural language processing (NLP), with strong capa-bilities in generation, comprehension, and rea-soning.","These models have found applications in education, intelligent decision-making, and gaming.","However, effectively utilizing LLMs for strategic planning and decision-making in the game of Gomoku remains a challenge.","This study aims to develop a Gomoku AI system based on LLMs, simulating the human learning process of playing chess.","The system is de-signed to understand and apply Gomoku strat-egies and logic to make rational decisions.","The research methods include enabling the model to \"read the board,\" \"understand the rules,\" \"select strategies,\" and \"evaluate positions,\" while en-hancing its abilities through self-play and rein-forcement learning.","The results demonstrate that this approach significantly improves the se-lection of move positions, resolves the issue of generating illegal positions, and reduces pro-cess time through parallel position evaluation.","After extensive self-play training, the model's Gomoku-playing capabilities have been notably enhanced."],"url":"http://arxiv.org/abs/2503.21683v1"}
{"created":"2025-03-27 16:48:58","title":"JiraiBench: A Bilingual Benchmark for Evaluating Large Language Models' Detection of Human Self-Destructive Behavior Content in Jirai Community","abstract":"This paper introduces JiraiBench, the first bilingual benchmark for evaluating large language models' effectiveness in detecting self-destructive content across Chinese and Japanese social media communities. Focusing on the transnational \"Jirai\" (landmine) online subculture that encompasses multiple forms of self-destructive behaviors including drug overdose, eating disorders, and self-harm, we present a comprehensive evaluation framework incorporating both linguistic and cultural dimensions. Our dataset comprises 10,419 Chinese posts and 5,000 Japanese posts with multidimensional annotation along three behavioral categories, achieving substantial inter-annotator agreement. Experimental evaluations across four state-of-the-art models reveal significant performance variations based on instructional language, with Japanese prompts unexpectedly outperforming Chinese prompts when processing Chinese content. This emergent cross-cultural transfer suggests that cultural proximity can sometimes outweigh linguistic similarity in detection tasks. Cross-lingual transfer experiments with fine-tuned models further demonstrate the potential for knowledge transfer between these language systems without explicit target language training. These findings highlight the need for culturally-informed approaches to multilingual content moderation and provide empirical evidence for the importance of cultural context in developing more effective detection systems for vulnerable online communities.","sentences":["This paper introduces JiraiBench, the first bilingual benchmark for evaluating large language models' effectiveness in detecting self-destructive content across Chinese and Japanese social media communities.","Focusing on the transnational \"Jirai\" (landmine) online subculture that encompasses multiple forms of self-destructive behaviors including drug overdose, eating disorders, and self-harm, we present a comprehensive evaluation framework incorporating both linguistic and cultural dimensions.","Our dataset comprises 10,419 Chinese posts and 5,000 Japanese posts with multidimensional annotation along three behavioral categories, achieving substantial inter-annotator agreement.","Experimental evaluations across four state-of-the-art models reveal significant performance variations based on instructional language, with Japanese prompts unexpectedly outperforming Chinese prompts when processing Chinese content.","This emergent cross-cultural transfer suggests that cultural proximity can sometimes outweigh linguistic similarity in detection tasks.","Cross-lingual transfer experiments with fine-tuned models further demonstrate the potential for knowledge transfer between these language systems without explicit target language training.","These findings highlight the need for culturally-informed approaches to multilingual content moderation and provide empirical evidence for the importance of cultural context in developing more effective detection systems for vulnerable online communities."],"url":"http://arxiv.org/abs/2503.21679v1"}
{"created":"2025-03-27 16:47:46","title":"A tale of two goals: leveraging sequentiality in multi-goal scenarios","abstract":"Several hierarchical reinforcement learning methods leverage planning to create a graph or sequences of intermediate goals, guiding a lower-level goal-conditioned (GC) policy to reach some final goals. The low-level policy is typically conditioned on the current goal, with the aim of reaching it as quickly as possible. However, this approach can fail when an intermediate goal can be reached in multiple ways, some of which may make it impossible to continue toward subsequent goals. To address this issue, we introduce two instances of Markov Decision Process (MDP) where the optimization objective favors policies that not only reach the current goal but also subsequent ones. In the first, the agent is conditioned on both the current and final goals, while in the second, it is conditioned on the next two goals in the sequence. We conduct a series of experiments on navigation and pole-balancing tasks in which sequences of intermediate goals are given. By evaluating policies trained with TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that, in most cases, conditioning on the next two goals improves stability and sample efficiency over other approaches.","sentences":["Several hierarchical reinforcement learning methods leverage planning to create a graph or sequences of intermediate goals, guiding a lower-level goal-conditioned (GC) policy to reach some final goals.","The low-level policy is typically conditioned on the current goal, with the aim of reaching it as quickly as possible.","However, this approach can fail when an intermediate goal can be reached in multiple ways, some of which may make it impossible to continue toward subsequent goals.","To address this issue, we introduce two instances of Markov Decision Process (MDP) where the optimization objective favors policies that not only reach the current goal but also subsequent ones.","In the first, the agent is conditioned on both the current and final goals, while in the second, it is conditioned on the next two goals in the sequence.","We conduct a series of experiments on navigation and pole-balancing tasks in which sequences of intermediate goals are given.","By evaluating policies trained with TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that, in most cases, conditioning on the next two goals improves stability and sample efficiency over other approaches."],"url":"http://arxiv.org/abs/2503.21677v1"}
{"created":"2025-03-27 16:43:45","title":"How do language models learn facts? Dynamics, curricula and hallucinations","abstract":"Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories. Our results emphasize the importance of data distribution in knowledge acquisition and suggest novel data scheduling strategies to accelerate neural network training.","sentences":["Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood.","This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings:","First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge.","Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall.","Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus.","Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories.","Our results emphasize the importance of data distribution in knowledge acquisition and suggest novel data scheduling strategies to accelerate neural network training."],"url":"http://arxiv.org/abs/2503.21676v1"}
{"created":"2025-03-27 16:41:57","title":"Intelligent IoT Attack Detection Design via ODLLM with Feature Ranking-based Knowledge Base","abstract":"The widespread adoption of Internet of Things (IoT) devices has introduced significant cybersecurity challenges, particularly with the increasing frequency and sophistication of Distributed Denial of Service (DDoS) attacks. Traditional machine learning (ML) techniques often fall short in detecting such attacks due to the complexity of blended and evolving patterns. To address this, we propose a novel framework leveraging On-Device Large Language Models (ODLLMs) augmented with fine-tuning and knowledge base (KB) integration for intelligent IoT network attack detection. By implementing feature ranking techniques and constructing both long and short KBs tailored to model capacities, the proposed framework ensures efficient and accurate detection of DDoS attacks while overcoming computational and privacy limitations. Simulation results demonstrate that the optimized framework achieves superior accuracy across diverse attack types, especially when using compact models in edge computing environments. This work provides a scalable and secure solution for real-time IoT security, advancing the applicability of edge intelligence in cybersecurity.","sentences":["The widespread adoption of Internet of Things (IoT) devices has introduced significant cybersecurity challenges, particularly with the increasing frequency and sophistication of Distributed Denial of Service (DDoS) attacks.","Traditional machine learning (ML) techniques often fall short in detecting such attacks due to the complexity of blended and evolving patterns.","To address this, we propose a novel framework leveraging On-Device Large Language Models (ODLLMs) augmented with fine-tuning and knowledge base (KB) integration for intelligent IoT network attack detection.","By implementing feature ranking techniques and constructing both long and short KBs tailored to model capacities, the proposed framework ensures efficient and accurate detection of DDoS attacks while overcoming computational and privacy limitations.","Simulation results demonstrate that the optimized framework achieves superior accuracy across diverse attack types, especially when using compact models in edge computing environments.","This work provides a scalable and secure solution for real-time IoT security, advancing the applicability of edge intelligence in cybersecurity."],"url":"http://arxiv.org/abs/2503.21674v1"}
{"created":"2025-03-27 16:37:18","title":"A Bespoke Design Approach to Low-Power Printed Microprocessors for Machine Learning Applications","abstract":"Printed electronics have gained significant traction in recent years, presenting a viable path to integrating computing into everyday items, from disposable products to low-cost healthcare. However, the adoption of computing in these domains is hindered by strict area and power constraints, limiting the effectiveness of general-purpose microprocessors. This paper proposes a bespoke microprocessor design approach to address these challenges, by tailoring the design to specific applications and eliminating unnecessary logic. Targeting machine learning applications, we further optimize core operations by integrating a SIMD MAC unit supporting 4 precision configurations that boost the efficiency of microprocessors. Our evaluation across 6 ML models and the large-scale Zero-Riscy core, shows that our methodology can achieve improvements of 22.2%, 23.6%, and 33.79% in area, power, and speed, respectively, without compromising accuracy. Against state-of-the-art printed processors, our approach can still offer significant speedups, but along with some accuracy degradation. This work explores how such trade-offs can enable low-power printed microprocessors for diverse ML applications.","sentences":["Printed electronics have gained significant traction in recent years, presenting a viable path to integrating computing into everyday items, from disposable products to low-cost healthcare.","However, the adoption of computing in these domains is hindered by strict area and power constraints, limiting the effectiveness of general-purpose microprocessors.","This paper proposes a bespoke microprocessor design approach to address these challenges, by tailoring the design to specific applications and eliminating unnecessary logic.","Targeting machine learning applications, we further optimize core operations by integrating a SIMD MAC unit supporting 4 precision configurations that boost the efficiency of microprocessors.","Our evaluation across 6 ML models and the large-scale Zero-Riscy core, shows that our methodology can achieve improvements of 22.2%, 23.6%, and 33.79% in area, power, and speed, respectively, without compromising accuracy.","Against state-of-the-art printed processors, our approach can still offer significant speedups, but along with some accuracy degradation.","This work explores how such trade-offs can enable low-power printed microprocessors for diverse ML applications."],"url":"http://arxiv.org/abs/2503.21671v1"}
{"created":"2025-03-27 16:36:39","title":"COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing","abstract":"The rapid growth of digital communication has driven the widespread use of code-mixing, particularly Hindi-English, in multilingual communities. Existing datasets often focus on romanized text, have limited scope, or rely on synthetic data, which fails to capture realworld language nuances. Human annotations are crucial for assessing the naturalness and acceptability of code-mixed text. To address these challenges, We introduce COMI-LINGUA, the largest manually annotated dataset for code-mixed text, comprising 100,970 instances evaluated by three expert annotators in both Devanagari and Roman scripts. The dataset supports five fundamental NLP tasks: Language Identification, Matrix Language Identification, Part-of-Speech Tagging, Named Entity Recognition, and Translation. We evaluate LLMs on these tasks using COMILINGUA, revealing limitations in current multilingual modeling strategies and emphasizing the need for improved code-mixed text processing capabilities. COMI-LINGUA is publically availabe at: https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.","sentences":["The rapid growth of digital communication has driven the widespread use of code-mixing, particularly Hindi-English, in multilingual communities.","Existing datasets often focus on romanized text, have limited scope, or rely on synthetic data, which fails to capture realworld language nuances.","Human annotations are crucial for assessing the naturalness and acceptability of code-mixed text.","To address these challenges, We introduce COMI-LINGUA, the largest manually annotated dataset for code-mixed text, comprising 100,970 instances evaluated by three expert annotators in both Devanagari and Roman scripts.","The dataset supports five fundamental NLP tasks: Language Identification, Matrix Language Identification, Part-of-Speech Tagging, Named Entity Recognition, and Translation.","We evaluate LLMs on these tasks using COMILINGUA, revealing limitations in current multilingual modeling strategies and emphasizing the need for improved code-mixed text processing capabilities.","COMI-LINGUA is publically availabe at: https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA."],"url":"http://arxiv.org/abs/2503.21670v1"}
{"created":"2025-03-27 16:35:02","title":"Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI","abstract":"One of the core components of our world models is 'intuitive physics' - an understanding of objects, space, and causality. This capability enables us to predict events, plan action and navigate environments, all of which rely on a composite sense of objecthood. Despite its importance, there is no single, unified account of objecthood, though multiple theoretical frameworks provide insights. In the first part of this paper, we present a comprehensive overview of the main theoretical frameworks in objecthood research - Gestalt psychology, enactive cognition, and developmental psychology - and identify the core capabilities each framework attributes to object understanding, as well as what functional roles they play in shaping world models in biological agents. Given the foundational role of objecthood in world modelling, understanding objecthood is also essential in AI. In the second part of the paper, we evaluate how current AI paradigms approach and test objecthood capabilities compared to those in cognitive science. We define an AI paradigm as a combination of how objecthood is conceptualised, the methods used for studying objecthood, the data utilised, and the evaluation techniques. We find that, whilst benchmarks can detect that AI systems model isolated aspects of objecthood, the benchmarks cannot detect when AI systems lack functional integration across these capabilities, not solving the objecthood challenge fully. Finally, we explore novel evaluation approaches that align with the integrated vision of objecthood outlined in this paper. These methods are promising candidates for advancing from isolated object capabilities toward general-purpose AI with genuine object understanding in real-world contexts.","sentences":["One of the core components of our world models is 'intuitive physics' - an understanding of objects, space, and causality.","This capability enables us to predict events, plan action and navigate environments, all of which rely on a composite sense of objecthood.","Despite its importance, there is no single, unified account of objecthood, though multiple theoretical frameworks provide insights.","In the first part of this paper, we present a comprehensive overview of the main theoretical frameworks in objecthood research - Gestalt psychology, enactive cognition, and developmental psychology - and identify the core capabilities each framework attributes to object understanding, as well as what functional roles they play in shaping world models in biological agents.","Given the foundational role of objecthood in world modelling, understanding objecthood is also essential in AI.","In the second part of the paper, we evaluate how current AI paradigms approach and test objecthood capabilities compared to those in cognitive science.","We define an AI paradigm as a combination of how objecthood is conceptualised, the methods used for studying objecthood, the data utilised, and the evaluation techniques.","We find that, whilst benchmarks can detect that AI systems model isolated aspects of objecthood, the benchmarks cannot detect when AI systems lack functional integration across these capabilities, not solving the objecthood challenge fully.","Finally, we explore novel evaluation approaches that align with the integrated vision of objecthood outlined in this paper.","These methods are promising candidates for advancing from isolated object capabilities toward general-purpose AI with genuine object understanding in real-world contexts."],"url":"http://arxiv.org/abs/2503.21668v1"}
{"created":"2025-03-27 16:24:12","title":"From conceptualization to operationalized meaning via ontological components","abstract":"Ontologies enable knowledge sharing and interdisciplinary collaboration by providing standardized, structured vocabularies for diverse communities. While logical axioms are a cornerstone of ontology design, natural language elements such as annotations are equally critical for conveying intended meaning and ensuring consistent term usage. This paper explores how meaning is represented in ontologies and how it can be effectively represented and communicated, addressing challenges such as indeterminacy of reference and meaning holism. To this end, it proposes an approach founded on the use of a new structure, named 'ontological component' and defined by: a term-centered design; enhanced characterization of both formal and natural language statements; an operationalizable definition of communicated meaning based on general assertions; and the integration of natural language elements into the logical theory. By formalizing the meaning of ontological components, this work seeks to enhance the semantic robustness of terms, improving their clarity and accessibility across domains. Furthermore, it aims to address practical challenges in applied ontologies, such as facilitating reuse and managing versioning, thereby strengthening their role in diverse applications.","sentences":["Ontologies enable knowledge sharing and interdisciplinary collaboration by providing standardized, structured vocabularies for diverse communities.","While logical axioms are a cornerstone of ontology design, natural language elements such as annotations are equally critical for conveying intended meaning and ensuring consistent term usage.","This paper explores how meaning is represented in ontologies and how it can be effectively represented and communicated, addressing challenges such as indeterminacy of reference and meaning holism.","To this end, it proposes an approach founded on the use of a new structure, named 'ontological component' and defined by: a term-centered design; enhanced characterization of both formal and natural language statements; an operationalizable definition of communicated meaning based on general assertions; and the integration of natural language elements into the logical theory.","By formalizing the meaning of ontological components, this work seeks to enhance the semantic robustness of terms, improving their clarity and accessibility across domains.","Furthermore, it aims to address practical challenges in applied ontologies, such as facilitating reuse and managing versioning, thereby strengthening their role in diverse applications."],"url":"http://arxiv.org/abs/2503.21661v1"}
{"created":"2025-03-27 16:23:15","title":"InteractionMap: Improving Online Vectorized HDMap Construction with Interaction","abstract":"Vectorized high-definition (HD) maps are essential for an autonomous driving system. Recently, state-of-the-art map vectorization methods are mainly based on DETR-like framework to generate HD maps in an end-to-end manner. In this paper, we propose InteractionMap, which improves previous map vectorization methods by fully leveraging local-to-global information interaction in both time and space. Firstly, we explore enhancing DETR-like detectors by explicit position relation prior from point-level to instance-level, since map elements contain strong shape priors. Secondly, we propose a key-frame-based hierarchical temporal fusion module, which interacts temporal information from local to global. Lastly, the separate classification branch and regression branch lead to the problem of misalignment in the output distribution. We interact semantic information with geometric information by introducing a novel geometric-aware classification loss in optimization and a geometric-aware matching cost in label assignment. InteractionMap achieves state-of-the-art performance on both nuScenes and Argoverse2 benchmarks.","sentences":["Vectorized high-definition (HD) maps are essential for an autonomous driving system.","Recently, state-of-the-art map vectorization methods are mainly based on DETR-like framework to generate HD maps in an end-to-end manner.","In this paper, we propose InteractionMap, which improves previous map vectorization methods by fully leveraging local-to-global information interaction in both time and space.","Firstly, we explore enhancing DETR-like detectors by explicit position relation prior from point-level to instance-level, since map elements contain strong shape priors.","Secondly, we propose a key-frame-based hierarchical temporal fusion module, which interacts temporal information from local to global.","Lastly, the separate classification branch and regression branch lead to the problem of misalignment in the output distribution.","We interact semantic information with geometric information by introducing a novel geometric-aware classification loss in optimization and a geometric-aware matching cost in label assignment.","InteractionMap achieves state-of-the-art performance on both nuScenes and Argoverse2 benchmarks."],"url":"http://arxiv.org/abs/2503.21659v1"}
{"created":"2025-03-27 16:21:53","title":"Model Assembly Learning with Heterogeneous Layer Weight Merging","abstract":"Model merging acquires general capabilities without extra data or training by combining multiple models' parameters. Previous approaches achieve linear mode connectivity by aligning parameters into the same loss basin using permutation invariance. In this paper, we introduce Model Assembly Learning (MAL), a novel paradigm for model merging that iteratively integrates parameters from diverse models in an open-ended model zoo to enhance the base model's capabilities. Unlike previous works that require identical architectures, MAL allows the merging of heterogeneous architectures and selective parameters across layers. Specifically, the base model can incorporate parameters from different layers of multiple pre-trained models. We systematically investigate the conditions and fundamental settings of heterogeneous parameter merging, addressing all possible mismatches in layer widths between the base and target models. Furthermore, we establish key laws and provide practical guidelines for effectively implementing MAL.","sentences":["Model merging acquires general capabilities without extra data or training by combining multiple models' parameters.","Previous approaches achieve linear mode connectivity by aligning parameters into the same loss basin using permutation invariance.","In this paper, we introduce Model Assembly Learning (MAL), a novel paradigm for model merging that iteratively integrates parameters from diverse models in an open-ended model zoo to enhance the base model's capabilities.","Unlike previous works that require identical architectures, MAL allows the merging of heterogeneous architectures and selective parameters across layers.","Specifically, the base model can incorporate parameters from different layers of multiple pre-trained models.","We systematically investigate the conditions and fundamental settings of heterogeneous parameter merging, addressing all possible mismatches in layer widths between the base and target models.","Furthermore, we establish key laws and provide practical guidelines for effectively implementing MAL."],"url":"http://arxiv.org/abs/2503.21657v1"}
{"created":"2025-03-27 16:20:15","title":"Output-sensitive approximate counting via a measure-bounded hyperedge oracle, or: How asymmetry helps estimate $k$-clique counts faster","abstract":"Dell, Lapinskas and Meeks [DLM SICOMP 2022] presented a general reduction from approximate counting to decision for a class of fine-grained problems that can be viewed as hyperedge counting or detection problems in an implicit hypergraph, thus obtaining tight equivalences between approximate counting and decision for many key problems such as $k$-clique, $k$-sum and more. Their result is a reduction from approximately counting the number of hyperedges in an implicit $k$-partite hypergraph to a polylogarithmic number of calls to a hyperedge oracle that returns whether a given subhypergraph contains an edge.   The main result of this paper is a generalization of the DLM result for {\\em output-sensitive} approximate counting, where the running time of the desired counting algorithm is inversely proportional to the number of witnesses. Our theorem is a reduction from approximately counting the (unknown) number of hyperedges in an implicit $k$-partite hypergraph to a polylogarithmic number of calls to a hyperedge oracle called only on subhypergraphs with a small ``measure''. If a subhypergraph has $u_i$ nodes in the $i$th node partition of the $k$-partite hypergraph, then its measure is $\\prod_i u_i$.   Using the new general reduction and by efficiently implementing measure-bounded colorful independence oracles, we obtain new improved output-sensitive approximate counting algorithms for $k$-clique, $k$-dominating set and $k$-sum. In graphs with $n^t$ $k$-cliques, for instance, our algorithm $(1\\pm \\epsilon)$-approximates the $k$-clique count in time   $$\\tilde{O}_\\epsilon(n^{\\omega(\\frac{k-t-1}{3},\\frac{k-t}{3},\\frac{k-t+2}{3}) }+n^2),$$ where $\\omega(a,b,c)$ is the exponent of $n^a\\times n^b$ by $n^b\\times n^c$ matrix multiplication. For large $k$ and $t>2$, this is a substantial improvement over prior work, even if $\\omega=2$.","sentences":["Dell, Lapinskas and Meeks","[DLM SICOMP 2022] presented a general reduction from approximate counting to decision for a class of fine-grained problems that can be viewed as hyperedge counting or detection problems in an implicit hypergraph, thus obtaining tight equivalences between approximate counting and decision for many key problems such as $k$-clique, $k$-sum and more.","Their result is a reduction from approximately counting the number of hyperedges in an implicit $k$-partite hypergraph to a polylogarithmic number of calls to a hyperedge oracle that returns whether a given subhypergraph contains an edge.   ","The main result of this paper is a generalization of the DLM result for {\\em output-sensitive} approximate counting, where the running time of the desired counting algorithm is inversely proportional to the number of witnesses.","Our theorem is a reduction from approximately counting the (unknown) number of hyperedges in an implicit $k$-partite hypergraph to a polylogarithmic number of calls to a hyperedge oracle called only on subhypergraphs with a small ``measure''.","If a subhypergraph has $u_i$ nodes in the $i$th node partition of the $k$-partite hypergraph, then its measure is $\\prod_i u_i$.   Using the new general reduction and by efficiently implementing measure-bounded colorful independence oracles, we obtain new improved output-sensitive approximate counting algorithms for $k$-clique, $k$-dominating set and $k$-sum.","In graphs with $n^t$ $k$-cliques, for instance, our algorithm $(1\\pm \\epsilon)$-approximates the $k$-clique count in time   $$\\tilde{O}_\\epsilon(n^{\\omega(\\frac{k-t-1}{3},\\frac{k-t}{3},\\frac{k-t+2}{3}) }+n^2),$$ where $\\omega(a,b,c)$ is the exponent of $n^a\\times n^b$ by $n^b\\times n^c$ matrix multiplication.","For large $k$ and $t>2$, this is a substantial improvement over prior work, even if $\\omega=2$."],"url":"http://arxiv.org/abs/2503.21655v1"}
{"created":"2025-03-27 16:10:02","title":"Unlocking the Potential of Past Research: Using Generative AI to Reconstruct Healthcare Simulation Models","abstract":"Discrete-event simulation (DES) is widely used in healthcare Operations Research, but the models themselves are rarely shared. This limits their potential for reuse and long-term impact in the modelling and healthcare communities. This study explores the feasibility of using generative artificial intelligence (AI) to recreate published models using Free and Open Source Software (FOSS), based on the descriptions provided in an academic journal. Using a structured methodology, we successfully generated, tested and internally reproduced two DES models, including user interfaces. The reported results were replicated for one model, but not the other, likely due to missing information on distributions. These models are substantially more complex than AI-generated DES models published to date. Given the challenges we faced in prompt engineering, code generation, and model testing, we conclude that our iterative approach to model development, systematic comparison and testing, and the expertise of our team were necessary to the success of our recreated simulation models.","sentences":["Discrete-event simulation (DES) is widely used in healthcare Operations Research, but the models themselves are rarely shared.","This limits their potential for reuse and long-term impact in the modelling and healthcare communities.","This study explores the feasibility of using generative artificial intelligence (AI) to recreate published models using Free and Open Source Software (FOSS), based on the descriptions provided in an academic journal.","Using a structured methodology, we successfully generated, tested and internally reproduced two DES models, including user interfaces.","The reported results were replicated for one model, but not the other, likely due to missing information on distributions.","These models are substantially more complex than AI-generated DES models published to date.","Given the challenges we faced in prompt engineering, code generation, and model testing, we conclude that our iterative approach to model development, systematic comparison and testing, and the expertise of our team were necessary to the success of our recreated simulation models."],"url":"http://arxiv.org/abs/2503.21646v1"}
{"created":"2025-03-27 16:08:15","title":"Mapping the Digital Diplomatic Infrastructure: A Comparative Evaluation of Global Online Directories for Diplomatic Missions","abstract":"This study provides a comparative evaluation of global diplomatic mission directories. DiplomaticMonitor.org, EmbassyPages.com, and WikiData.org are strategically selected among the top ten global services. After analyzing nearly all available online global diplomatic directory services, these three platforms are selected as they represent fundamentally different approaches to creating worldwide diplomatic mission databases. Using official diplomatic lists from over 150 countries as benchmarks, we assessed data coverage, accuracy, and update frequency across these platforms. DiplomaticMonitor consistently outperforms its counterparts in structure, completeness, and timeliness, accurately reflecting ambassadorial appointment cycles and maintaining high precision across contact and personnel records. EmbassyPages, despite strong search engine visibility and widespread usage, exhibits significant data currency issues, with markedly diminished ambassadorial accuracy attributable to delayed refresh cycles. WikiData offers valuable historical documentation and open-source accessibility but lacks the consistency and verification protocols necessary for reliable real-time diplomatic information. Our findings highlight the critical challenge posed by the absence of a standardized global diplomatic mission registry. In this fragmented landscape, methodologically rigorous third-party platforms can occasionally surpass government-published records in quality and utility. The research demonstrates that in contemporary digital diplomacy, data reliability correlates less with institutional provenance than with disciplined, transparent, and consistent data stewardship practices.","sentences":["This study provides a comparative evaluation of global diplomatic mission directories.","DiplomaticMonitor.org, EmbassyPages.com, and WikiData.org are strategically selected among the top ten global services.","After analyzing nearly all available online global diplomatic directory services, these three platforms are selected as they represent fundamentally different approaches to creating worldwide diplomatic mission databases.","Using official diplomatic lists from over 150 countries as benchmarks, we assessed data coverage, accuracy, and update frequency across these platforms.","DiplomaticMonitor consistently outperforms its counterparts in structure, completeness, and timeliness, accurately reflecting ambassadorial appointment cycles and maintaining high precision across contact and personnel records.","EmbassyPages, despite strong search engine visibility and widespread usage, exhibits significant data currency issues, with markedly diminished ambassadorial accuracy attributable to delayed refresh cycles.","WikiData offers valuable historical documentation and open-source accessibility but lacks the consistency and verification protocols necessary for reliable real-time diplomatic information.","Our findings highlight the critical challenge posed by the absence of a standardized global diplomatic mission registry.","In this fragmented landscape, methodologically rigorous third-party platforms can occasionally surpass government-published records in quality and utility.","The research demonstrates that in contemporary digital diplomacy, data reliability correlates less with institutional provenance than with disciplined, transparent, and consistent data stewardship practices."],"url":"http://arxiv.org/abs/2503.21645v1"}
{"created":"2025-03-27 16:06:59","title":"Towards Fully Automated Decision-Making Systems for Greenhouse Control: Challenges and Opportunities","abstract":"Machine learning has been successful in building control policies to drive a complex system to desired states in various applications (e.g. games, robotics, etc.). To be specific, a number of parameters of policy can be automatically optimized from the observations of environment to be able to generate a sequence of decisions leading to the best performance. In this survey paper, we particularly explore such policy-learning techniques for another unique, practical use-case scenario--farming, in which critical decisions (e.g., water supply, heating, etc.) must be made in a timely manner to minimize risks (e.g., damage to plants) while maximizing the revenue (e.g., healthy crops) in the end. We first provide a broad overview of latest studies on it to identify not only domain-specific challenges but opportunities with potential solutions, some of which are suggested as promising directions for future research. Also, we then introduce our successful approach to being ranked second among 46 teams at the ''3rd Autonomous Greenhouse Challenge'' to use this specific example to discuss the lessons learned about important considerations for design to create autonomous farm-management systems.","sentences":["Machine learning has been successful in building control policies to drive a complex system to desired states in various applications (e.g. games, robotics, etc.).","To be specific, a number of parameters of policy can be automatically optimized from the observations of environment to be able to generate a sequence of decisions leading to the best performance.","In this survey paper, we particularly explore such policy-learning techniques for another unique, practical use-case scenario--farming, in which critical decisions (e.g., water supply, heating, etc.) must be made in a timely manner to minimize risks (e.g., damage to plants) while maximizing the revenue (e.g., healthy crops) in the end.","We first provide a broad overview of latest studies on it to identify not only domain-specific challenges but opportunities with potential solutions, some of which are suggested as promising directions for future research.","Also, we then introduce our successful approach to being ranked second among 46 teams at the ''3rd Autonomous Greenhouse Challenge'' to use this specific example to discuss the lessons learned about important considerations for design to create autonomous farm-management systems."],"url":"http://arxiv.org/abs/2503.21640v1"}
{"created":"2025-03-27 16:03:46","title":"Data-Driven Extreme Response Estimation","abstract":"A method to rapidly estimate extreme ship response events is developed in this paper. The method involves training by a Long Short-Term Memory (LSTM) neural network to correct a lower-fidelity hydrodynamic model to the level of a higher-fidelity simulation. More focus is placed on larger responses by isolating the time-series near peak events identified in the lower-fidelity simulations and training on only the shorter time-series around the large event. The method is tested on the estimation of pitch time-series maxima in Sea State 5 (significant wave height of 4.0 meters and modal period of 15.0 seconds,) generated by a lower-fidelity hydrodynamic solver known as SimpleCode and a higher-fidelity tool known as the Large Amplitude Motion Program (LAMP). The results are also compared with an LSTM trained without special considerations for large events.","sentences":["A method to rapidly estimate extreme ship response events is developed in this paper.","The method involves training by a Long Short-Term Memory (LSTM) neural network to correct a lower-fidelity hydrodynamic model to the level of a higher-fidelity simulation.","More focus is placed on larger responses by isolating the time-series near peak events identified in the lower-fidelity simulations and training on only the shorter time-series around the large event.","The method is tested on the estimation of pitch time-series maxima in Sea State 5 (significant wave height of 4.0 meters and modal period of 15.0 seconds,) generated by a lower-fidelity hydrodynamic solver known as SimpleCode and a higher-fidelity tool known as the Large Amplitude Motion Program (LAMP).","The results are also compared with an LSTM trained without special considerations for large events."],"url":"http://arxiv.org/abs/2503.21638v1"}
{"created":"2025-03-27 15:59:57","title":"KRAFT -- A Knowledge-Graph-Based Resource Allocation Framework","abstract":"Resource allocation in business process management involves assigning resources to open tasks while considering factors such as individual roles, aptitudes, case-specific characteristics, and regulatory constraints. Current information systems for resource allocation often require extensive manual effort to specify and maintain allocation rules, making them rigid and challenging to adapt. In contrast, fully automated approaches provide limited explainability, making it difficult to understand and justify allocation decisions. Knowledge graphs, which represent real-world entities and their relationships, offer a promising solution by capturing complex dependencies and enabling dynamic, context-aware resource allocation. This paper introduces KRAFT, a novel approach that leverages knowledge graphs and reasoning techniques to support resource allocation decisions. We demonstrate that integrating knowledge graphs into resource allocation software allows for adaptable and transparent decision-making based on an evolving knowledge base.","sentences":["Resource allocation in business process management involves assigning resources to open tasks while considering factors such as individual roles, aptitudes, case-specific characteristics, and regulatory constraints.","Current information systems for resource allocation often require extensive manual effort to specify and maintain allocation rules, making them rigid and challenging to adapt.","In contrast, fully automated approaches provide limited explainability, making it difficult to understand and justify allocation decisions.","Knowledge graphs, which represent real-world entities and their relationships, offer a promising solution by capturing complex dependencies and enabling dynamic, context-aware resource allocation.","This paper introduces KRAFT, a novel approach that leverages knowledge graphs and reasoning techniques to support resource allocation decisions.","We demonstrate that integrating knowledge graphs into resource allocation software allows for adaptable and transparent decision-making based on an evolving knowledge base."],"url":"http://arxiv.org/abs/2503.21636v1"}
{"created":"2025-03-27 15:56:55","title":"When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in Morocco","abstract":"The accurate determination of the beginning of each Hijri month is essential for religious, cultural, and administrative purposes. Manazel (The code and datasets are available at https://github.com/lairgiyassir/manazel) addresses this challenge in Morocco by leveraging 13 years of crescent visibility data to refine the ODEH criterion, a widely used standard for lunar crescent visibility prediction. The study integrates two key features, the Arc of Vision (ARCV) and the total width of the crescent (W), to enhance the accuracy of lunar visibility assessments. A machine learning approach utilizing the Logistic Regression algorithm is employed to classify crescent visibility conditions, achieving a predictive accuracy of 98.83%. This data-driven methodology offers a robust and reliable framework for determining the start of the Hijri month, comparing different data classification tools, and improving the consistency of lunar calendar calculations in Morocco. The findings demonstrate the effectiveness of machine learning in astronomical applications and highlight the potential for further enhancements in the modeling of crescent visibility.","sentences":["The accurate determination of the beginning of each Hijri month is essential for religious, cultural, and administrative purposes.","Manazel (The code and datasets are available at https://github.com/lairgiyassir/manazel) addresses this challenge in Morocco by leveraging 13 years of crescent visibility data to refine the ODEH criterion, a widely used standard for lunar crescent visibility prediction.","The study integrates two key features, the Arc of Vision (ARCV) and the total width of the crescent (W), to enhance the accuracy of lunar visibility assessments.","A machine learning approach utilizing the Logistic Regression algorithm is employed to classify crescent visibility conditions, achieving a predictive accuracy of 98.83%.","This data-driven methodology offers a robust and reliable framework for determining the start of the Hijri month, comparing different data classification tools, and improving the consistency of lunar calendar calculations in Morocco.","The findings demonstrate the effectiveness of machine learning in astronomical applications and highlight the potential for further enhancements in the modeling of crescent visibility."],"url":"http://arxiv.org/abs/2503.21634v1"}
{"created":"2025-03-27 15:55:52","title":"Static and Repeated Cooperative Games for the Optimization of the AoI in IoT Networks","abstract":"Wireless sensing and the internet of things (IoT) are nowadays pervasive in 5G and beyond networks, and they are expected to play a crucial role in 6G. However, a centralized optimization of a distributed system is not always possible and cost-efficient. In this paper, we analyze a setting in which two sensors collaboratively update a common server seeking to minimize the age of information (AoI) of the latest sample of a common physical process. We consider a distributed and uncoordinated setting where each sensor lacks information about whether the other decides to update the server. This strategic setting is modeled through game theory (GT) and two games are defined: i) a static game of complete information with an incentive mechanism for cooperation, and ii) a repeated game over a finite horizon where the static game is played at each stage. We perform a mathematical analysis of the static game finding three Nash Equilibria (NEs) in pure strategies and one in mixed strategies. A numerical simulation of the repeated game is also presented and novel and valuable insight into the setting is given thanks to the definition of a new metric, the price of delayed updates (PoDU), which shows that the decentralized solution provides results close to the centralized optimum.","sentences":["Wireless sensing and the internet of things (IoT) are nowadays pervasive in 5G and beyond networks, and they are expected to play a crucial role in 6G. However, a centralized optimization of a distributed system is not always possible and cost-efficient.","In this paper, we analyze a setting in which two sensors collaboratively update a common server seeking to minimize the age of information (AoI) of the latest sample of a common physical process.","We consider a distributed and uncoordinated setting where each sensor lacks information about whether the other decides to update the server.","This strategic setting is modeled through game theory (GT) and two games are defined: i) a static game of complete information with an incentive mechanism for cooperation, and ii) a repeated game over a finite horizon where the static game is played at each stage.","We perform a mathematical analysis of the static game finding three Nash Equilibria (NEs) in pure strategies and one in mixed strategies.","A numerical simulation of the repeated game is also presented and novel and valuable insight into the setting is given thanks to the definition of a new metric, the price of delayed updates (PoDU), which shows that the decentralized solution provides results close to the centralized optimum."],"url":"http://arxiv.org/abs/2503.21633v1"}
{"created":"2025-03-27 15:50:32","title":"ClusterSC: Advancing Synthetic Control with Donor Selection","abstract":"In causal inference with observational studies, synthetic control (SC) has emerged as a prominent tool. SC has traditionally been applied to aggregate-level datasets, but more recent work has extended its use to individual-level data. As they contain a greater number of observed units, this shift introduces the curse of dimensionality to SC. To address this, we propose Cluster Synthetic Control (ClusterSC), based on the idea that groups of individuals may exist where behavior aligns internally but diverges between groups. ClusterSC incorporates a clustering step to select only the relevant donors for the target. We provide theoretical guarantees on the improvements induced by ClusterSC, supported by empirical demonstrations on synthetic and real-world datasets. The results indicate that ClusterSC consistently outperforms classical SC approaches.","sentences":["In causal inference with observational studies, synthetic control (SC) has emerged as a prominent tool.","SC has traditionally been applied to aggregate-level datasets, but more recent work has extended its use to individual-level data.","As they contain a greater number of observed units, this shift introduces the curse of dimensionality to SC.","To address this, we propose Cluster Synthetic Control (ClusterSC), based on the idea that groups of individuals may exist where behavior aligns internally but diverges between groups.","ClusterSC incorporates a clustering step to select only the relevant donors for the target.","We provide theoretical guarantees on the improvements induced by ClusterSC, supported by empirical demonstrations on synthetic and real-world datasets.","The results indicate that ClusterSC consistently outperforms classical SC approaches."],"url":"http://arxiv.org/abs/2503.21629v1"}
{"created":"2025-03-27 15:48:34","title":"Provable Reduction in Communication Rounds for Non-Smooth Convex Federated Learning","abstract":"Multiple local steps are key to communication-efficient federated learning. However, theoretical guarantees for such algorithms, without data heterogeneity-bounding assumptions, have been lacking in general non-smooth convex problems. Leveraging projection-efficient optimization methods, we propose FedMLS, a federated learning algorithm with provable improvements from multiple local steps. FedMLS attains an $\\epsilon$-suboptimal solution in $\\mathcal{O}(1/\\epsilon)$ communication rounds, requiring a total of $\\mathcal{O}(1/\\epsilon^2)$ stochastic subgradient oracle calls.","sentences":["Multiple local steps are key to communication-efficient federated learning.","However, theoretical guarantees for such algorithms, without data heterogeneity-bounding assumptions, have been lacking in general non-smooth convex problems.","Leveraging projection-efficient optimization methods, we propose FedMLS, a federated learning algorithm with provable improvements from multiple local steps.","FedMLS attains an $\\epsilon$-suboptimal solution in $\\mathcal{O}(1/\\epsilon)$ communication rounds, requiring a total of $\\mathcal{O}(1/\\epsilon^2)$ stochastic subgradient oracle calls."],"url":"http://arxiv.org/abs/2503.21627v1"}
{"created":"2025-03-27 15:42:17","title":"RIS-Measurements for Codebook Design","abstract":"Reconfigurable Intelligent Surfaces (RIS) have gained significant attention for some time. Thanks to the possibility of individual steering of each reflecting element of the boards, they are envisaged to impact the propagation environment significantly. In this work, we concentrate on the practical verification of this concept. We present the results of detailed measurements of the reflection characteristics of the RIS boards, which have been conducted intentionally in the real environment. Various potential impacting factors have been considered (impact of azimuth and elevation angle, polarization, number of RIS boards, and distance). Achieved measurement results constituted the basis for conceptual analysis on the practical possibility of creating a codebook (consisting of RIS patterns - codewords) for some applications.","sentences":["Reconfigurable Intelligent Surfaces (RIS) have gained significant attention for some time.","Thanks to the possibility of individual steering of each reflecting element of the boards, they are envisaged to impact the propagation environment significantly.","In this work, we concentrate on the practical verification of this concept.","We present the results of detailed measurements of the reflection characteristics of the RIS boards, which have been conducted intentionally in the real environment.","Various potential impacting factors have been considered (impact of azimuth and elevation angle, polarization, number of RIS boards, and distance).","Achieved measurement results constituted the basis for conceptual analysis on the practical possibility of creating a codebook (consisting of RIS patterns - codewords) for some applications."],"url":"http://arxiv.org/abs/2503.21623v1"}
{"created":"2025-03-27 15:41:46","title":"The MVTec AD 2 Dataset: Advanced Scenarios for Unsupervised Anomaly Detection","abstract":"In recent years, performance on existing anomaly detection benchmarks like MVTec AD and VisA has started to saturate in terms of segmentation AU-PRO, with state-of-the-art models often competing in the range of less than one percentage point. This lack of discriminatory power prevents a meaningful comparison of models and thus hinders progress of the field, especially when considering the inherent stochastic nature of machine learning results. We present MVTec AD 2, a collection of eight anomaly detection scenarios with more than 8000 high-resolution images. It comprises challenging and highly relevant industrial inspection use cases that have not been considered in previous datasets, including transparent and overlapping objects, dark-field and back light illumination, objects with high variance in the normal data, and extremely small defects. We provide comprehensive evaluations of state-of-the-art methods and show that their performance remains below 60% average AU-PRO. Additionally, our dataset provides test scenarios with lighting condition changes to assess the robustness of methods under real-world distribution shifts. We host a publicly accessible evaluation server that holds the pixel-precise ground truth of the test set (https://benchmark.mvtec.com/). All image data is available at https://www.mvtec.com/company/research/datasets/mvtec-ad-2.","sentences":["In recent years, performance on existing anomaly detection benchmarks like MVTec AD and VisA has started to saturate in terms of segmentation AU-PRO, with state-of-the-art models often competing in the range of less than one percentage point.","This lack of discriminatory power prevents a meaningful comparison of models and thus hinders progress of the field, especially when considering the inherent stochastic nature of machine learning results.","We present MVTec AD 2, a collection of eight anomaly detection scenarios with more than 8000 high-resolution images.","It comprises challenging and highly relevant industrial inspection use cases that have not been considered in previous datasets, including transparent and overlapping objects, dark-field and back light illumination, objects with high variance in the normal data, and extremely small defects.","We provide comprehensive evaluations of state-of-the-art methods and show that their performance remains below 60% average AU-PRO.","Additionally, our dataset provides test scenarios with lighting condition changes to assess the robustness of methods under real-world distribution shifts.","We host a publicly accessible evaluation server that holds the pixel-precise ground truth of the test set (https://benchmark.mvtec.com/).","All image data is available at https://www.mvtec.com/company/research/datasets/mvtec-ad-2."],"url":"http://arxiv.org/abs/2503.21622v1"}
{"created":"2025-03-27 15:39:30","title":"UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning","abstract":"The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards. Building on this idea, we are the first to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for graphic user interface (GUI) action prediction tasks. To this end, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices. We also introduce a unified rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO). Experimental results demonstrate that our proposed data-efficient model, UI-R1-3B, achieves substantial improvements on both in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the ID benchmark AndroidControl, the action type accuracy improves by 15%, while grounding accuracy increases by 10.3%, compared with the base model (i.e. Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our model surpasses the base model by 6.0% and achieves competitive performance with larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning (SFT) on 76K data. These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain.","sentences":["The recent DeepSeek-R1 has showcased the emergence of reasoning capabilities in LLMs through reinforcement learning (RL) with rule-based rewards.","Building on this idea, we are the first to explore how rule-based RL can enhance the reasoning capabilities of multimodal large language models (MLLMs) for graphic user interface (GUI) action prediction tasks.","To this end, we curate a small yet high-quality dataset of 136 challenging tasks, encompassing five common action types on mobile devices.","We also introduce a unified rule-based action reward, enabling model optimization via policy-based algorithms such as Group Relative Policy Optimization (GRPO).","Experimental results demonstrate that our proposed data-efficient model, UI-R1-3B, achieves substantial improvements on both in-domain (ID) and out-of-domain (OOD) tasks.","Specifically, on the ID benchmark AndroidControl, the action type accuracy improves by 15%, while grounding accuracy increases by 10.3%, compared with the base model (i.e. Qwen2.5-VL-3B).","On the OOD GUI grounding benchmark ScreenSpot-Pro, our model surpasses the base model by 6.0% and achieves competitive performance with larger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning (SFT) on 76K data.","These results underscore the potential of rule-based reinforcement learning to advance GUI understanding and control, paving the way for future research in this domain."],"url":"http://arxiv.org/abs/2503.21620v1"}
{"created":"2025-03-27 15:37:23","title":"Leveraging Language Models for Analyzing Longitudinal Experiential Data in Education","abstract":"We propose a novel approach to leveraging pre-trained language models (LMs) for early forecasting of academic trajectories in STEM students using high-dimensional longitudinal experiential data. This data, which captures students' study-related activities, behaviors, and psychological states, offers valuable insights for forecasting-based interventions. Key challenges in handling such data include high rates of missing values, limited dataset size due to costly data collection, and complex temporal variability across modalities. Our approach addresses these issues through a comprehensive data enrichment process, integrating strategies for managing missing values, augmenting data, and embedding task-specific instructions and contextual cues to enhance the models' capacity for learning temporal patterns. Through extensive experiments on a curated student learning dataset, we evaluate both encoder-decoder and decoder-only LMs. While our findings show that LMs effectively integrate data across modalities and exhibit resilience to missing data, they primarily rely on high-level statistical patterns rather than demonstrating a deeper understanding of temporal dynamics. Furthermore, their ability to interpret explicit temporal information remains limited. This work advances educational data science by highlighting both the potential and limitations of LMs in modeling student trajectories for early intervention based on longitudinal experiential data.","sentences":["We propose a novel approach to leveraging pre-trained language models (LMs) for early forecasting of academic trajectories in STEM students using high-dimensional longitudinal experiential data.","This data, which captures students' study-related activities, behaviors, and psychological states, offers valuable insights for forecasting-based interventions.","Key challenges in handling such data include high rates of missing values, limited dataset size due to costly data collection, and complex temporal variability across modalities.","Our approach addresses these issues through a comprehensive data enrichment process, integrating strategies for managing missing values, augmenting data, and embedding task-specific instructions and contextual cues to enhance the models' capacity for learning temporal patterns.","Through extensive experiments on a curated student learning dataset, we evaluate both encoder-decoder and decoder-only LMs.","While our findings show that LMs effectively integrate data across modalities and exhibit resilience to missing data, they primarily rely on high-level statistical patterns rather than demonstrating a deeper understanding of temporal dynamics.","Furthermore, their ability to interpret explicit temporal information remains limited.","This work advances educational data science by highlighting both the potential and limitations of LMs in modeling student trajectories for early intervention based on longitudinal experiential data."],"url":"http://arxiv.org/abs/2503.21617v1"}
{"created":"2025-03-27 15:37:16","title":"Audio-driven Gesture Generation via Deviation Feature in the Latent Space","abstract":"Gestures are essential for enhancing co-speech communication, offering visual emphasis and complementing verbal interactions. While prior work has concentrated on point-level motion or fully supervised data-driven methods, we focus on co-speech gestures, advocating for weakly supervised learning and pixel-level motion deviations. We introduce a weakly supervised framework that learns latent representation deviations, tailored for co-speech gesture video generation. Our approach employs a diffusion model to integrate latent motion features, enabling more precise and nuanced gesture representation. By leveraging weakly supervised deviations in latent space, we effectively generate hand gestures and mouth movements, crucial for realistic video production. Experiments show our method significantly improves video quality, surpassing current state-of-the-art techniques.","sentences":["Gestures are essential for enhancing co-speech communication, offering visual emphasis and complementing verbal interactions.","While prior work has concentrated on point-level motion or fully supervised data-driven methods, we focus on co-speech gestures, advocating for weakly supervised learning and pixel-level motion deviations.","We introduce a weakly supervised framework that learns latent representation deviations, tailored for co-speech gesture video generation.","Our approach employs a diffusion model to integrate latent motion features, enabling more precise and nuanced gesture representation.","By leveraging weakly supervised deviations in latent space, we effectively generate hand gestures and mouth movements, crucial for realistic video production.","Experiments show our method significantly improves video quality, surpassing current state-of-the-art techniques."],"url":"http://arxiv.org/abs/2503.21616v1"}
{"created":"2025-03-27 15:36:49","title":"A Measure Based Generalizable Approach to Understandability","abstract":"Successful agent-human partnerships require that any agent generated information is understandable to the human, and that the human can easily steer the agent towards a goal. Such effective communication requires the agent to develop a finer-level notion of what is understandable to the human. State-of-the-art agents, including LLMs, lack this detailed notion of understandability because they only capture average human sensibilities from the training data, and therefore afford limited steerability (e.g., requiring non-trivial prompt engineering).   In this paper, instead of only relying on data, we argue for developing generalizable, domain-agnostic measures of understandability that can be used as directives for these agents. Existing research on understandability measures is fragmented, we survey various such efforts across domains, and lay a cognitive-science-rooted groundwork for more coherent and domain-agnostic research investigations in future.","sentences":["Successful agent-human partnerships require that any agent generated information is understandable to the human, and that the human can easily steer the agent towards a goal.","Such effective communication requires the agent to develop a finer-level notion of what is understandable to the human.","State-of-the-art agents, including LLMs, lack this detailed notion of understandability because they only capture average human sensibilities from the training data, and therefore afford limited steerability (e.g., requiring non-trivial prompt engineering).   ","In this paper, instead of only relying on data, we argue for developing generalizable, domain-agnostic measures of understandability that can be used as directives for these agents.","Existing research on understandability measures is fragmented, we survey various such efforts across domains, and lay a cognitive-science-rooted groundwork for more coherent and domain-agnostic research investigations in future."],"url":"http://arxiv.org/abs/2503.21615v1"}
{"created":"2025-03-27 15:36:30","title":"A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond","abstract":"Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.","sentences":["Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference.","However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks.","This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical.","In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm.","We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research.","To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field.","We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area."],"url":"http://arxiv.org/abs/2503.21614v1"}
{"created":"2025-03-27 15:36:24","title":"Evaluating book summaries from internal knowledge in Large Language Models: a cross-model and semantic consistency approach","abstract":"We study the ability of large language models (LLMs) to generate comprehensive and accurate book summaries solely from their internal knowledge, without recourse to the original text. Employing a diverse set of books and multiple LLM architectures, we examine whether these models can synthesize meaningful narratives that align with established human interpretations. Evaluation is performed with a LLM-as-a-judge paradigm: each AI-generated summary is compared against a high-quality, human-written summary via a cross-model assessment, where all participating LLMs evaluate not only their own outputs but also those produced by others. This methodology enables the identification of potential biases, such as the proclivity for models to favor their own summarization style over others. In addition, alignment between the human-crafted and LLM-generated summaries is quantified using ROUGE and BERTScore metrics, assessing the depth of grammatical and semantic correspondence. The results reveal nuanced variations in content representation and stylistic preferences among the models, highlighting both strengths and limitations inherent in relying on internal knowledge for summarization tasks. These findings contribute to a deeper understanding of LLM internal encodings of factual information and the dynamics of cross-model evaluation, with implications for the development of more robust natural language generative systems.","sentences":["We study the ability of large language models (LLMs) to generate comprehensive and accurate book summaries solely from their internal knowledge, without recourse to the original text.","Employing a diverse set of books and multiple LLM architectures, we examine whether these models can synthesize meaningful narratives that align with established human interpretations.","Evaluation is performed with a LLM-as-a-judge paradigm: each AI-generated summary is compared against a high-quality, human-written summary via a cross-model assessment, where all participating LLMs evaluate not only their own outputs but also those produced by others.","This methodology enables the identification of potential biases, such as the proclivity for models to favor their own summarization style over others.","In addition, alignment between the human-crafted and LLM-generated summaries is quantified using ROUGE and BERTScore metrics, assessing the depth of grammatical and semantic correspondence.","The results reveal nuanced variations in content representation and stylistic preferences among the models, highlighting both strengths and limitations inherent in relying on internal knowledge for summarization tasks.","These findings contribute to a deeper understanding of LLM internal encodings of factual information and the dynamics of cross-model evaluation, with implications for the development of more robust natural language generative systems."],"url":"http://arxiv.org/abs/2503.21613v1"}
{"created":"2025-03-27 15:22:02","title":"GenEdit: Compounding Operators and Continuous Improvement to Tackle Text-to-SQL in the Enterprise","abstract":"Recent advancements in Text-to-SQL, driven by large language models, are democratizing data access. Despite these advancements, enterprise deployments remain challenging due to the need to capture business-specific knowledge, handle complex queries, and meet expectations of continuous improvements. To address these issues, we designed and implemented GenEdit: our Text-to-SQL generation system that improves with user feedback. GenEdit builds and maintains a company-specific knowledge set, employs a pipeline of operators decomposing SQL generation, and uses feedback to update its knowledge set to improve future SQL generations.   We describe GenEdit's architecture made of two core modules: (i) decomposed SQL generation; and (ii) knowledge set edits based on user feedback. For generation, GenEdit leverages compounding operators to improve knowledge retrieval and to create a plan as chain-of-thought steps that guides generation. GenEdit first retrieves relevant examples in an initial retrieval stage where original SQL queries are decomposed into sub-statements, clauses or sub-queries. It then also retrieves instructions and schema elements. Using the retrieved contextual information, GenEdit then generates step-by-step plan in natural language on how to produce the query. Finally, GenEdit uses the plan to generate SQL, minimizing the need for model reasoning, which enhances complex SQL generation. If necessary, GenEdit regenerates the query based on syntactic and semantic errors. The knowledge set edits are recommended through an interactive copilot, allowing users to iterate on their feedback and to regenerate SQL queries as needed. Each generation uses staged edits which update the generation prompt. Once the feedback is submitted, it gets merged after passing regression testing and obtaining an approval, improving future generations.","sentences":["Recent advancements in Text-to-SQL, driven by large language models, are democratizing data access.","Despite these advancements, enterprise deployments remain challenging due to the need to capture business-specific knowledge, handle complex queries, and meet expectations of continuous improvements.","To address these issues, we designed and implemented GenEdit: our Text-to-SQL generation system that improves with user feedback.","GenEdit builds and maintains a company-specific knowledge set, employs a pipeline of operators decomposing SQL generation, and uses feedback to update its knowledge set to improve future SQL generations.   ","We describe GenEdit's architecture made of two core modules: (i) decomposed SQL generation; and (ii) knowledge set edits based on user feedback.","For generation, GenEdit leverages compounding operators to improve knowledge retrieval and to create a plan as chain-of-thought steps that guides generation.","GenEdit first retrieves relevant examples in an initial retrieval stage where original SQL queries are decomposed into sub-statements, clauses or sub-queries.","It then also retrieves instructions and schema elements.","Using the retrieved contextual information, GenEdit then generates step-by-step plan in natural language on how to produce the query.","Finally, GenEdit uses the plan to generate SQL, minimizing the need for model reasoning, which enhances complex SQL generation.","If necessary, GenEdit regenerates the query based on syntactic and semantic errors.","The knowledge set edits are recommended through an interactive copilot, allowing users to iterate on their feedback and to regenerate SQL queries as needed.","Each generation uses staged edits which update the generation prompt.","Once the feedback is submitted, it gets merged after passing regression testing and obtaining an approval, improving future generations."],"url":"http://arxiv.org/abs/2503.21602v1"}
{"created":"2025-03-27 15:20:59","title":"A Deep Reinforcement Learning-based Approach for Adaptive Handover Protocols","abstract":"The use of higher frequencies in mobile communication systems leads to smaller cell sizes, resulting in the deployment of more base stations and an increase in handovers to support user mobility. This can lead to frequent radio link failures and reduced data rates. In this work, we propose a handover optimization method using proximal policy optimization (PPO) to develop an adaptive handover protocol. Our PPO-based agent, implemented in the base stations, is highly adaptive to varying user equipment speeds and outperforms the 3GPP-standardized 5G NR handover procedure in terms of average data rate and radio link failure rate. Additionally, our simulation environment is carefully designed to ensure high accuracy, realistic user movements, and fair benchmarking against the 3GPP handover method.","sentences":["The use of higher frequencies in mobile communication systems leads to smaller cell sizes, resulting in the deployment of more base stations and an increase in handovers to support user mobility.","This can lead to frequent radio link failures and reduced data rates.","In this work, we propose a handover optimization method using proximal policy optimization (PPO) to develop an adaptive handover protocol.","Our PPO-based agent, implemented in the base stations, is highly adaptive to varying user equipment speeds and outperforms the 3GPP-standardized 5G NR handover procedure in terms of average data rate and radio link failure rate.","Additionally, our simulation environment is carefully designed to ensure high accuracy, realistic user movements, and fair benchmarking against the 3GPP handover method."],"url":"http://arxiv.org/abs/2503.21601v1"}
{"created":"2025-03-27 15:19:55","title":"Prompt, Divide, and Conquer: Bypassing Large Language Model Safety Filters via Segmented and Distributed Prompt Processing","abstract":"Large Language Models (LLMs) have transformed task automation and content generation across various domains while incorporating safety filters to prevent misuse. We introduce a novel jailbreaking framework that employs distributed prompt processing combined with iterative refinements to bypass these safety measures, particularly in generating malicious code. Our architecture consists of four key modules: prompt segmentation, parallel processing, response aggregation, and LLM-based jury evaluation. Tested on 500 malicious prompts across 10 cybersecurity categories, the framework achieves a 73.2% Success Rate (SR) in generating malicious code. Notably, our comparative analysis reveals that traditional single-LLM judge evaluation overestimates SRs (93.8%) compared to our LLM jury system (73.2%), with manual verification confirming that single-judge assessments often accept incomplete implementations. Moreover, we demonstrate that our distributed architecture improves SRs by 12% over the non-distributed approach in an ablation study, highlighting both the effectiveness of distributed prompt processing and the importance of robust evaluation methodologies in assessing jailbreak attempts.","sentences":["Large Language Models (LLMs) have transformed task automation and content generation across various domains while incorporating safety filters to prevent misuse.","We introduce a novel jailbreaking framework that employs distributed prompt processing combined with iterative refinements to bypass these safety measures, particularly in generating malicious code.","Our architecture consists of four key modules: prompt segmentation, parallel processing, response aggregation, and LLM-based jury evaluation.","Tested on 500 malicious prompts across 10 cybersecurity categories, the framework achieves a 73.2% Success Rate (SR) in generating malicious code.","Notably, our comparative analysis reveals that traditional single-LLM judge evaluation overestimates SRs (93.8%) compared to our LLM jury system (73.2%), with manual verification confirming that single-judge assessments often accept incomplete implementations.","Moreover, we demonstrate that our distributed architecture improves SRs by 12% over the non-distributed approach in an ablation study, highlighting both the effectiveness of distributed prompt processing and the importance of robust evaluation methodologies in assessing jailbreak attempts."],"url":"http://arxiv.org/abs/2503.21598v1"}
{"created":"2025-03-27 15:14:03","title":"FusionSegReID: Advancing Person Re-Identification with Multimodal Retrieval and Precise Segmentation","abstract":"Person re-identification (ReID) plays a critical role in applications like security surveillance and criminal investigations by matching individuals across large image galleries captured by non-overlapping cameras. Traditional ReID methods rely on unimodal inputs, typically images, but face limitations due to challenges like occlusions, lighting changes, and pose variations. While advancements in image-based and text-based ReID systems have been made, the integration of both modalities has remained under-explored. This paper presents FusionSegReID, a multimodal model that combines both image and text inputs for enhanced ReID performance. By leveraging the complementary strengths of these modalities, our model improves matching accuracy and robustness, particularly in complex, real-world scenarios where one modality may struggle. Our experiments show significant improvements in Top-1 accuracy and mean Average Precision (mAP) for ReID, as well as better segmentation results in challenging scenarios like occlusion and low-quality images. Ablation studies further confirm that multimodal fusion and segmentation modules contribute to enhanced re-identification and mask accuracy. The results show that FusionSegReID outperforms traditional unimodal models, offering a more robust and flexible solution for real-world person ReID tasks.","sentences":["Person re-identification (ReID) plays a critical role in applications like security surveillance and criminal investigations by matching individuals across large image galleries captured by non-overlapping cameras.","Traditional ReID methods rely on unimodal inputs, typically images, but face limitations due to challenges like occlusions, lighting changes, and pose variations.","While advancements in image-based and text-based ReID systems have been made, the integration of both modalities has remained under-explored.","This paper presents FusionSegReID, a multimodal model that combines both image and text inputs for enhanced ReID performance.","By leveraging the complementary strengths of these modalities, our model improves matching accuracy and robustness, particularly in complex, real-world scenarios where one modality may struggle.","Our experiments show significant improvements in Top-1 accuracy and mean Average Precision (mAP) for ReID, as well as better segmentation results in challenging scenarios like occlusion and low-quality images.","Ablation studies further confirm that multimodal fusion and segmentation modules contribute to enhanced re-identification and mask accuracy.","The results show that FusionSegReID outperforms traditional unimodal models, offering a more robust and flexible solution for real-world person ReID tasks."],"url":"http://arxiv.org/abs/2503.21595v1"}
{"created":"2025-03-27 15:08:58","title":"Critical Iterative Denoising: A Discrete Generative Model Applied to Graphs","abstract":"Discrete Diffusion and Flow Matching models have significantly advanced generative modeling for discrete structures, including graphs. However, the time dependencies in the noising process of these models lead to error accumulation and propagation during the backward process. This issue, particularly pronounced in mask diffusion, is a known limitation in sequence modeling and, as we demonstrate, also impacts discrete diffusion models for graphs.   To address this problem, we propose a novel framework called Iterative Denoising, which simplifies discrete diffusion and circumvents the issue by assuming conditional independence across time. Additionally, we enhance our model by incorporating a Critic, which during generation selectively retains or corrupts elements in an instance based on their likelihood under the data distribution. Our empirical evaluations demonstrate that the proposed method significantly outperforms existing discrete diffusion baselines in graph generation tasks.","sentences":["Discrete Diffusion and Flow Matching models have significantly advanced generative modeling for discrete structures, including graphs.","However, the time dependencies in the noising process of these models lead to error accumulation and propagation during the backward process.","This issue, particularly pronounced in mask diffusion, is a known limitation in sequence modeling and, as we demonstrate, also impacts discrete diffusion models for graphs.   ","To address this problem, we propose a novel framework called Iterative Denoising, which simplifies discrete diffusion and circumvents the issue by assuming conditional independence across time.","Additionally, we enhance our model by incorporating a Critic, which during generation selectively retains or corrupts elements in an instance based on their likelihood under the data distribution.","Our empirical evaluations demonstrate that the proposed method significantly outperforms existing discrete diffusion baselines in graph generation tasks."],"url":"http://arxiv.org/abs/2503.21592v1"}
{"created":"2025-03-27 15:08:03","title":"Dataset and Analysis of Long-Term Skill Acquisition in Robot-Assisted Minimally Invasive Surgery","abstract":"Objective: We aim to investigate long-term robotic surgical skill acquisition among surgical residents and the effects of training intervals and fatigue on performance. Methods: For six months, surgical residents participated in three training sessions once a month, surrounding a single 26-hour hospital shift. In each shift, they participated in training sessions scheduled before, during, and after the shift. In each training session, they performed three dry-lab training tasks: Ring Tower Transfer, Knot-Tying, and Suturing. We collected a comprehensive dataset, including videos synchronized with kinematic data, activity tracking, and scans of the suturing pads. Results: We collected a dataset of 972 trials performed by 18 residents of different surgical specializations. Participants demonstrated consistent performance improvement across all tasks. In addition, we found variations in between-shift learning and forgetting across metrics and tasks, and hints for possible effects of fatigue. Conclusion: The findings from our first analysis shed light on the long-term learning processes of robotic surgical skills with extended intervals and varying levels of fatigue. Significance: This study lays the groundwork for future research aimed at optimizing training protocols and enhancing AI applications in surgery, ultimately contributing to improved patient outcomes. The dataset will be made available upon acceptance of our journal submission.","sentences":["Objective: We aim to investigate long-term robotic surgical skill acquisition among surgical residents and the effects of training intervals and fatigue on performance.","Methods: For six months, surgical residents participated in three training sessions once a month, surrounding a single 26-hour hospital shift.","In each shift, they participated in training sessions scheduled before, during, and after the shift.","In each training session, they performed three dry-lab training tasks: Ring Tower Transfer, Knot-Tying, and Suturing.","We collected a comprehensive dataset, including videos synchronized with kinematic data, activity tracking, and scans of the suturing pads.","Results:","We collected a dataset of 972 trials performed by 18 residents of different surgical specializations.","Participants demonstrated consistent performance improvement across all tasks.","In addition, we found variations in between-shift learning and forgetting across metrics and tasks, and hints for possible effects of fatigue.","Conclusion: The findings from our first analysis shed light on the long-term learning processes of robotic surgical skills with extended intervals and varying levels of fatigue.","Significance: This study lays the groundwork for future research aimed at optimizing training protocols and enhancing AI applications in surgery, ultimately contributing to improved patient outcomes.","The dataset will be made available upon acceptance of our journal submission."],"url":"http://arxiv.org/abs/2503.21591v1"}
{"created":"2025-03-27 15:04:52","title":"Generalizable Implicit Neural Representations via Parameterized Latent Dynamics for Baroclinic Ocean Forecasting","abstract":"Mesoscale ocean dynamics play a critical role in climate systems, governing heat transport, hurricane genesis, and drought patterns. However, simulating these processes at high resolution remains computationally prohibitive due to their nonlinear, multiscale nature and vast spatiotemporal domains. Implicit neural representations (INRs) reduce the computational costs as resolution-independent surrogates but fail in many-query scenarios (inverse modeling) requiring rapid evaluations across diverse parameters. We present PINROD, a novel framework combining dynamics-aware implicit neural representations with parameterized neural ordinary differential equations to address these limitations. By integrating parametric dependencies into latent dynamics, our method efficiently captures nonlinear oceanic behavior across varying boundary conditions and physical parameters. Experiments on ocean mesoscale activity data show superior accuracy over existing baselines and improved computational efficiency compared to standard numerical simulations.","sentences":["Mesoscale ocean dynamics play a critical role in climate systems, governing heat transport, hurricane genesis, and drought patterns.","However, simulating these processes at high resolution remains computationally prohibitive due to their nonlinear, multiscale nature and vast spatiotemporal domains.","Implicit neural representations (INRs) reduce the computational costs as resolution-independent surrogates but fail in many-query scenarios (inverse modeling) requiring rapid evaluations across diverse parameters.","We present PINROD, a novel framework combining dynamics-aware implicit neural representations with parameterized neural ordinary differential equations to address these limitations.","By integrating parametric dependencies into latent dynamics, our method efficiently captures nonlinear oceanic behavior across varying boundary conditions and physical parameters.","Experiments on ocean mesoscale activity data show superior accuracy over existing baselines and improved computational efficiency compared to standard numerical simulations."],"url":"http://arxiv.org/abs/2503.21588v1"}
{"created":"2025-03-27 14:59:59","title":"AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion","abstract":"Accurate camera calibration is a fundamental task for 3D perception, especially when dealing with real-world, in-the-wild environments where complex optical distortions are common. Existing methods often rely on pre-rectified images or calibration patterns, which limits their applicability and flexibility. In this work, we introduce a novel framework that addresses these challenges by jointly modeling camera intrinsic and extrinsic parameters using a generic ray camera model. Unlike previous approaches, AlignDiff shifts focus from semantic to geometric features, enabling more accurate modeling of local distortions. We propose AlignDiff, a diffusion model conditioned on geometric priors, enabling the simultaneous estimation of camera distortions and scene geometry. To enhance distortion prediction, we incorporate edge-aware attention, focusing the model on geometric features around image edges, rather than semantic content. Furthermore, to enhance generalizability to real-world captures, we incorporate a large database of ray-traced lenses containing over three thousand samples. This database characterizes the distortion inherent in a diverse variety of lens forms. Our experiments demonstrate that the proposed method significantly reduces the angular error of estimated ray bundles by ~8.2 degrees and overall calibration accuracy, outperforming existing approaches on challenging, real-world datasets.","sentences":["Accurate camera calibration is a fundamental task for 3D perception, especially when dealing with real-world, in-the-wild environments where complex optical distortions are common.","Existing methods often rely on pre-rectified images or calibration patterns, which limits their applicability and flexibility.","In this work, we introduce a novel framework that addresses these challenges by jointly modeling camera intrinsic and extrinsic parameters using a generic ray camera model.","Unlike previous approaches, AlignDiff shifts focus from semantic to geometric features, enabling more accurate modeling of local distortions.","We propose AlignDiff, a diffusion model conditioned on geometric priors, enabling the simultaneous estimation of camera distortions and scene geometry.","To enhance distortion prediction, we incorporate edge-aware attention, focusing the model on geometric features around image edges, rather than semantic content.","Furthermore, to enhance generalizability to real-world captures, we incorporate a large database of ray-traced lenses containing over three thousand samples.","This database characterizes the distortion inherent in a diverse variety of lens forms.","Our experiments demonstrate that the proposed method significantly reduces the angular error of estimated ray bundles by ~8.2 degrees and overall calibration accuracy, outperforming existing approaches on challenging, real-world datasets."],"url":"http://arxiv.org/abs/2503.21581v1"}
{"created":"2025-03-27 14:59:54","title":"Fusion of Graph Neural Networks via Optimal Transport","abstract":"In this paper, we explore the idea of combining GCNs into one model. To that end, we align the weights of different models layer-wise using optimal transport (OT). We present and evaluate three types of transportation costs and show that the studied fusion method consistently outperforms the performance of vanilla averaging. Finally, we present results suggesting that model fusion using OT is harder in the case of GCNs than MLPs and that incorporating the graph structure into the process does not improve the performance of the method.","sentences":["In this paper, we explore the idea of combining GCNs into one model.","To that end, we align the weights of different models layer-wise using optimal transport (OT).","We present and evaluate three types of transportation costs and show that the studied fusion method consistently outperforms the performance of vanilla averaging.","Finally, we present results suggesting that model fusion using OT is harder in the case of GCNs than MLPs and that incorporating the graph structure into the process does not improve the performance of the method."],"url":"http://arxiv.org/abs/2503.21579v1"}
