{"created":"2025-01-16 18:59:53","title":"Distilling Multi-modal Large Language Models for Autonomous Driving","abstract":"Autonomous driving demands safe motion planning, especially in critical \"long-tail\" scenarios. Recent end-to-end autonomous driving systems leverage large language models (LLMs) as planners to improve generalizability to rare events. However, using LLMs at test time introduces high computational costs. To address this, we propose DiMA, an end-to-end autonomous driving system that maintains the efficiency of an LLM-free (or vision-based) planner while leveraging the world knowledge of an LLM. DiMA distills the information from a multi-modal LLM to a vision-based end-to-end planner through a set of specially designed surrogate tasks. Under a joint training strategy, a scene encoder common to both networks produces structured representations that are semantically grounded as well as aligned to the final planning objective. Notably, the LLM is optional at inference, enabling robust planning without compromising on efficiency. Training with DiMA results in a 37% reduction in the L2 trajectory error and an 80% reduction in the collision rate of the vision-based planner, as well as a 44% trajectory error reduction in longtail scenarios. DiMA also achieves state-of-the-art performance on the nuScenes planning benchmark.","sentences":["Autonomous driving demands safe motion planning, especially in critical \"long-tail\" scenarios.","Recent end-to-end autonomous driving systems leverage large language models (LLMs) as planners to improve generalizability to rare events.","However, using LLMs at test time introduces high computational costs.","To address this, we propose DiMA, an end-to-end autonomous driving system that maintains the efficiency of an LLM-free (or vision-based) planner while leveraging the world knowledge of an LLM.","DiMA distills the information from a multi-modal LLM to a vision-based end-to-end planner through a set of specially designed surrogate tasks.","Under a joint training strategy, a scene encoder common to both networks produces structured representations that are semantically grounded as well as aligned to the final planning objective.","Notably, the LLM is optional at inference, enabling robust planning without compromising on efficiency.","Training with DiMA results in a 37% reduction in the L2 trajectory error and an 80% reduction in the collision rate of the vision-based planner, as well as a 44% trajectory error reduction in longtail scenarios.","DiMA also achieves state-of-the-art performance on the nuScenes planning benchmark."],"url":"http://arxiv.org/abs/2501.09757v1"}
{"created":"2025-01-16 18:59:48","title":"SynthLight: Portrait Relighting with Diffusion Model by Learning to Re-render Synthetic Faces","abstract":"We introduce SynthLight, a diffusion model for portrait relighting. Our approach frames image relighting as a re-rendering problem, where pixels are transformed in response to changes in environmental lighting conditions. Using a physically-based rendering engine, we synthesize a dataset to simulate this lighting-conditioned transformation with 3D head assets under varying lighting. We propose two training and inference strategies to bridge the gap between the synthetic and real image domains: (1) multi-task training that takes advantage of real human portraits without lighting labels; (2) an inference time diffusion sampling procedure based on classifier-free guidance that leverages the input portrait to better preserve details. Our method generalizes to diverse real photographs and produces realistic illumination effects, including specular highlights and cast shadows, while preserving the subject's identity. Our quantitative experiments on Light Stage data demonstrate results comparable to state-of-the-art relighting methods. Our qualitative results on in-the-wild images showcase rich and unprecedented illumination effects. Project Page: \\url{https://vrroom.github.io/synthlight/}","sentences":["We introduce SynthLight, a diffusion model for portrait relighting.","Our approach frames image relighting as a re-rendering problem, where pixels are transformed in response to changes in environmental lighting conditions.","Using a physically-based rendering engine, we synthesize a dataset to simulate this lighting-conditioned transformation with 3D head assets under varying lighting.","We propose two training and inference strategies to bridge the gap between the synthetic and real image domains: (1) multi-task training that takes advantage of real human portraits without lighting labels; (2) an inference time diffusion sampling procedure based on classifier-free guidance that leverages the input portrait to better preserve details.","Our method generalizes to diverse real photographs and produces realistic illumination effects, including specular highlights and cast shadows, while preserving the subject's identity.","Our quantitative experiments on Light Stage data demonstrate results comparable to state-of-the-art relighting methods.","Our qualitative results on in-the-wild images showcase rich and unprecedented illumination effects.","Project Page: \\url{https://vrroom.github.io/synthlight/}"],"url":"http://arxiv.org/abs/2501.09756v1"}
{"created":"2025-01-16 18:59:04","title":"Learnings from Scaling Visual Tokenizers for Reconstruction and Generation","abstract":"Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space. Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance. Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank. To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok). We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling. We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation -- and find that while it is highly correlated with reconstruction, its relationship with generation is more complex. We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance. Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed. Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs. When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101.","sentences":["Visual tokenization via auto-encoding empowers state-of-the-art image and video generative models by compressing pixels into a latent space.","Although scaling Transformer-based generators has been central to recent advances, the tokenizer component itself is rarely scaled, leaving open questions about how auto-encoder design choices influence both its objective of reconstruction and downstream generative performance.","Our work aims to conduct an exploration of scaling in auto-encoders to fill in this blank.","To facilitate this exploration, we replace the typical convolutional backbone with an enhanced Vision Transformer architecture for Tokenization (ViTok).","We train ViTok on large-scale image and video datasets far exceeding ImageNet-1K, removing data constraints on tokenizer scaling.","We first study how scaling the auto-encoder bottleneck affects both reconstruction and generation -- and find that while it is highly correlated with reconstruction, its relationship with generation is more complex.","We next explored the effect of separately scaling the auto-encoders' encoder and decoder on reconstruction and generation performance.","Crucially, we find that scaling the encoder yields minimal gains for either reconstruction or generation, while scaling the decoder boosts reconstruction but the benefits for generation are mixed.","Building on our exploration, we design ViTok as a lightweight auto-encoder that achieves competitive performance with state-of-the-art auto-encoders on ImageNet-1K and COCO reconstruction tasks (256p and 512p) while outperforming existing auto-encoders on 16-frame 128p video reconstruction for UCF-101, all with 2-5x fewer FLOPs.","When integrated with Diffusion Transformers, ViTok demonstrates competitive performance on image generation for ImageNet-1K and sets new state-of-the-art benchmarks for class-conditional video generation on UCF-101."],"url":"http://arxiv.org/abs/2501.09755v1"}
{"created":"2025-01-16 18:59:03","title":"Lost in Translation, Found in Context: Sign Language Translation with Contextual Cues","abstract":"Our objective is to translate continuous sign language into spoken language text. Inspired by the way human interpreters rely on context for accurate translation, we incorporate additional contextual cues together with the signing video, into a new translation framework. Specifically, besides visual sign recognition features that encode the input video, we integrate complementary textual information from (i) captions describing the background show, (ii) translation of previous sentences, as well as (iii) pseudo-glosses transcribing the signing. These are automatically extracted and inputted along with the visual features to a pre-trained large language model (LLM), which we fine-tune to generate spoken language translations in text form. Through extensive ablation studies, we show the positive contribution of each input cue to the translation performance. We train and evaluate our approach on BOBSL -- the largest British Sign Language dataset currently available. We show that our contextual approach significantly enhances the quality of the translations compared to previously reported results on BOBSL, and also to state-of-the-art methods that we implement as baselines. Furthermore, we demonstrate the generality of our approach by applying it also to How2Sign, an American Sign Language dataset, and achieve competitive results.","sentences":["Our objective is to translate continuous sign language into spoken language text.","Inspired by the way human interpreters rely on context for accurate translation, we incorporate additional contextual cues together with the signing video, into a new translation framework.","Specifically, besides visual sign recognition features that encode the input video, we integrate complementary textual information from (i) captions describing the background show, (ii) translation of previous sentences, as well as (iii) pseudo-glosses transcribing the signing.","These are automatically extracted and inputted along with the visual features to a pre-trained large language model (LLM), which we fine-tune to generate spoken language translations in text form.","Through extensive ablation studies, we show the positive contribution of each input cue to the translation performance.","We train and evaluate our approach on BOBSL -- the largest British Sign Language dataset currently available.","We show that our contextual approach significantly enhances the quality of the translations compared to previously reported results on BOBSL, and also to state-of-the-art methods that we implement as baselines.","Furthermore, we demonstrate the generality of our approach by applying it also to How2Sign, an American Sign Language dataset, and achieve competitive results."],"url":"http://arxiv.org/abs/2501.09754v1"}
{"created":"2025-01-16 18:59:02","title":"SRE-Conv: Symmetric Rotation Equivariant Convolution for Biomedical Image Classification","abstract":"Convolutional neural networks (CNNs) are essential tools for computer vision tasks, but they lack traditionally desired properties of extracted features that could further improve model performance, e.g., rotational equivariance. Such properties are ubiquitous in biomedical images, which often lack explicit orientation. While current work largely relies on data augmentation or explicit modules to capture orientation information, this comes at the expense of increased training costs or ineffective approximations of the desired equivariance. To overcome these challenges, we propose a novel and efficient implementation of the Symmetric Rotation-Equivariant (SRE) Convolution (SRE-Conv) kernel, designed to learn rotation-invariant features while simultaneously compressing the model size. The SRE-Conv kernel can easily be incorporated into any CNN backbone. We validate the ability of a deep SRE-CNN to capture equivariance to rotation using the public MedMNISTv2 dataset (16 total tasks). SRE-Conv-CNN demonstrated improved rotated image classification performance accuracy on all 16 test datasets in both 2D and 3D images, all while increasing efficiency with fewer parameters and reduced memory footprint. The code is available at https://github.com/XYPB/SRE-Conv.","sentences":["Convolutional neural networks (CNNs) are essential tools for computer vision tasks, but they lack traditionally desired properties of extracted features that could further improve model performance, e.g., rotational equivariance.","Such properties are ubiquitous in biomedical images, which often lack explicit orientation.","While current work largely relies on data augmentation or explicit modules to capture orientation information, this comes at the expense of increased training costs or ineffective approximations of the desired equivariance.","To overcome these challenges, we propose a novel and efficient implementation of the Symmetric Rotation-Equivariant (SRE) Convolution (SRE-Conv) kernel, designed to learn rotation-invariant features while simultaneously compressing the model size.","The SRE-Conv kernel can easily be incorporated into any CNN backbone.","We validate the ability of a deep SRE-CNN to capture equivariance to rotation using the public MedMNISTv2 dataset (16 total tasks).","SRE-Conv-CNN demonstrated improved rotated image classification performance accuracy on all 16 test datasets in both 2D and 3D images, all while increasing efficiency with fewer parameters and reduced memory footprint.","The code is available at https://github.com/XYPB/SRE-Conv."],"url":"http://arxiv.org/abs/2501.09753v1"}
{"created":"2025-01-16 18:58:06","title":"OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking","abstract":"Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, utility, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, repetitive, and unoriginal outputs. To address these issues, we propose OmniThink, a machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they progressively deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles.","sentences":["Machine writing with large language models often relies on retrieval-augmented generation.","However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information.","Specifically, vanilla-retrieved information tends to lack depth, utility, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, repetitive, and unoriginal outputs.","To address these issues, we propose OmniThink, a machine writing framework that emulates the human-like process of iterative expansion and reflection.","The core idea behind OmniThink is to simulate the cognitive behavior of learners as they progressively deepen their knowledge of the topics.","Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth.","Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles."],"url":"http://arxiv.org/abs/2501.09751v1"}
{"created":"2025-01-16 18:57:20","title":"Enhancing Lexicon-Based Text Embeddings with Large Language Models","abstract":"Recent large language models (LLMs) have demonstrated exceptional performance on general-purpose text embedding tasks. While dense embeddings have dominated related research, we introduce the first Lexicon-based EmbeddiNgS (LENS) leveraging LLMs that achieve competitive performance on these tasks. Regarding the inherent tokenization redundancy issue and unidirectional attention limitations in traditional causal LLMs, LENS consolidates the vocabulary space through token embedding clustering, and investigates bidirectional attention and various pooling strategies. Specifically, LENS simplifies lexicon matching by assigning each dimension to a specific token cluster, where semantically similar tokens are grouped together, and unlocking the full potential of LLMs through bidirectional attention. Extensive experiments demonstrate that LENS outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB), delivering compact feature representations that match the sizes of dense counterparts. Notably, combining LENSE with dense embeddings achieves state-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR).","sentences":["Recent large language models (LLMs) have demonstrated exceptional performance on general-purpose text embedding tasks.","While dense embeddings have dominated related research, we introduce the first Lexicon-based EmbeddiNgS (LENS) leveraging LLMs that achieve competitive performance on these tasks.","Regarding the inherent tokenization redundancy issue and unidirectional attention limitations in traditional causal LLMs, LENS consolidates the vocabulary space through token embedding clustering, and investigates bidirectional attention and various pooling strategies.","Specifically, LENS simplifies lexicon matching by assigning each dimension to a specific token cluster, where semantically similar tokens are grouped together, and unlocking the full potential of LLMs through bidirectional attention.","Extensive experiments demonstrate that LENS outperforms dense embeddings on the Massive Text Embedding Benchmark (MTEB), delivering compact feature representations that match the sizes of dense counterparts.","Notably, combining LENSE with dense embeddings achieves state-of-the-art performance on the retrieval subset of MTEB (i.e. BEIR)."],"url":"http://arxiv.org/abs/2501.09749v1"}
{"created":"2025-01-16 18:57:04","title":"FAST: Efficient Action Tokenization for Vision-Language-Action Models","abstract":"Autoregressive sequence models, such as Transformer-based vision-language action (VLA) policies, can be tremendously effective for capturing complex and generalizable robotic behaviors. However, such models require us to choose a tokenization of our continuous action signals, which determines how the discrete symbols predicted by the model map to continuous robot actions. We find that current approaches for robot action tokenization, based on simple per-dimension, per-timestep binning schemes, typically perform poorly when learning dexterous skills from high-frequency robot data. To address this challenge, we propose a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform. Our tokenization approach, Frequency-space Action Sequence Tokenization (FAST), enables us to train autoregressive VLAs for highly dexterous and high-frequency tasks where standard discretization methods fail completely. Based on FAST, we release FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories. It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies. Finally, we show that, when combined with the pi0 VLA, our method can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x.","sentences":["Autoregressive sequence models, such as Transformer-based vision-language action (VLA) policies, can be tremendously effective for capturing complex and generalizable robotic behaviors.","However, such models require us to choose a tokenization of our continuous action signals, which determines how the discrete symbols predicted by the model map to continuous robot actions.","We find that current approaches for robot action tokenization, based on simple per-dimension, per-timestep binning schemes, typically perform poorly when learning dexterous skills from high-frequency robot data.","To address this challenge, we propose a new compression-based tokenization scheme for robot actions, based on the discrete cosine transform.","Our tokenization approach, Frequency-space Action Sequence Tokenization (FAST), enables us to train autoregressive VLAs for highly dexterous and high-frequency tasks where standard discretization methods fail completely.","Based on FAST, we release FAST+, a universal robot action tokenizer, trained on 1M real robot action trajectories.","It can be used as a black-box tokenizer for a wide range of robot action sequences, with diverse action spaces and control frequencies.","Finally, we show that, when combined with the pi0 VLA, our method can scale to training on 10k hours of robot data and match the performance of diffusion VLAs, while reducing training time by up to 5x."],"url":"http://arxiv.org/abs/2501.09747v1"}
{"created":"2025-01-16 18:55:38","title":"Suggesting Code Edits in Interactive Machine Learning Notebooks Using Large Language Models","abstract":"Machine learning developers frequently use interactive computational notebooks, such as Jupyter notebooks, to host code for data processing and model training. Jupyter notebooks provide a convenient tool for writing machine learning pipelines and interactively observing outputs, however, maintaining Jupyter notebooks, e.g., to add new features or fix bugs, can be challenging due to the length and complexity of the notebooks. Moreover, there is no existing benchmark related to developer edits on Jupyter notebooks. To address this, we present the first dataset of 48,398 Jupyter notebook edits derived from 20,095 revisions of 792 machine learning repositories on GitHub, and perform the first study of the using LLMs to predict code edits in Jupyter notebooks. Our dataset captures granular details of cell-level and line-level modifications, offering a foundation for understanding real-world maintenance patterns in machine learning workflows. We observed that the edits on Jupyter notebooks are highly localized, with changes averaging only 166 lines of code in repositories. While larger models outperform smaller counterparts in code editing, all models have low accuracy on our dataset even after finetuning, demonstrating the complexity of real-world machine learning maintenance tasks. Our findings emphasize the critical role of contextual information in improving model performance and point toward promising avenues for advancing large language models' capabilities in engineering machine learning code.","sentences":["Machine learning developers frequently use interactive computational notebooks, such as Jupyter notebooks, to host code for data processing and model training.","Jupyter notebooks provide a convenient tool for writing machine learning pipelines and interactively observing outputs, however, maintaining Jupyter notebooks, e.g., to add new features or fix bugs, can be challenging due to the length and complexity of the notebooks.","Moreover, there is no existing benchmark related to developer edits on Jupyter notebooks.","To address this, we present the first dataset of 48,398 Jupyter notebook edits derived from 20,095 revisions of 792 machine learning repositories on GitHub, and perform the first study of the using LLMs to predict code edits in Jupyter notebooks.","Our dataset captures granular details of cell-level and line-level modifications, offering a foundation for understanding real-world maintenance patterns in machine learning workflows.","We observed that the edits on Jupyter notebooks are highly localized, with changes averaging only 166 lines of code in repositories.","While larger models outperform smaller counterparts in code editing, all models have low accuracy on our dataset even after finetuning, demonstrating the complexity of real-world machine learning maintenance tasks.","Our findings emphasize the critical role of contextual information in improving model performance and point toward promising avenues for advancing large language models' capabilities in engineering machine learning code."],"url":"http://arxiv.org/abs/2501.09745v1"}
{"created":"2025-01-16 18:53:32","title":"KU AIGEN ICL EDI@BC8 Track 3: Advancing Phenotype Named Entity Recognition and Normalization for Dysmorphology Physical Examination Reports","abstract":"The objective of BioCreative8 Track 3 is to extract phenotypic key medical findings embedded within EHR texts and subsequently normalize these findings to their Human Phenotype Ontology (HPO) terms. However, the presence of diverse surface forms in phenotypic findings makes it challenging to accurately normalize them to the correct HPO terms. To address this challenge, we explored various models for named entity recognition and implemented data augmentation techniques such as synonym marginalization to enhance the normalization step. Our pipeline resulted in an exact extraction and normalization F1 score 2.6\\% higher than the mean score of all submissions received in response to the challenge. Furthermore, in terms of the normalization F1 score, our approach surpassed the average performance by 1.9\\%. These findings contribute to the advancement of automated medical data extraction and normalization techniques, showcasing potential pathways for future research and application in the biomedical domain.","sentences":["The objective of BioCreative8 Track 3 is to extract phenotypic key medical findings embedded within EHR texts and subsequently normalize these findings to their Human Phenotype Ontology (HPO) terms.","However, the presence of diverse surface forms in phenotypic findings makes it challenging to accurately normalize them to the correct HPO terms.","To address this challenge, we explored various models for named entity recognition and implemented data augmentation techniques such as synonym marginalization to enhance the normalization step.","Our pipeline resulted in an exact extraction and normalization F1 score 2.6\\% higher than the mean score of all submissions received in response to the challenge.","Furthermore, in terms of the normalization F1 score, our approach surpassed the average performance by 1.9\\%.","These findings contribute to the advancement of automated medical data extraction and normalization techniques, showcasing potential pathways for future research and application in the biomedical domain."],"url":"http://arxiv.org/abs/2501.09744v1"}
{"created":"2025-01-16 18:49:12","title":"Regulation of Algorithmic Collusion, Refined: Testing Pessimistic Calibrated Regret","abstract":"We study the regulation of algorithmic (non-)collusion amongst sellers in dynamic imperfect price competition by auditing their data as introduced by Hartline et al. [2024].   We develop an auditing method that tests whether a seller's pessimistic calibrated regret is low. The pessimistic calibrated regret is the highest calibrated regret of outcomes compatible with the observed data. This method relaxes the previous requirement that a pricing algorithm must use fully-supported price distributions to be auditable. This method is at least as permissive as any auditing method that has a high probability of failing algorithmic outcomes with non-vanishing calibrated regret. Additionally, we strengthen the justification for using vanishing calibrated regret, versus vanishing best-in-hindsight regret, as the non-collusion definition, by showing that even without any side information, the pricing algorithms that only satisfy weaker vanishing best-in-hindsight regret allow an opponent to manipulate them into posting supra-competitive prices. This manipulation cannot be excluded with a non-collusion definition of vanishing best-in-hindsight regret.   We motivate and interpret the approach of auditing algorithms from their data as suggesting a per se rule. However, we demonstrate that it is possible for algorithms to pass the audit by pretending to have higher costs than they actually do. For such scenarios, the rule of reason can be applied to bound the range of costs to those that are reasonable for the domain.","sentences":["We study the regulation of algorithmic (non-)collusion amongst sellers in dynamic imperfect price competition by auditing their data as introduced by Hartline et al.","[2024].   ","We develop an auditing method that tests whether a seller's pessimistic calibrated regret is low.","The pessimistic calibrated regret is the highest calibrated regret of outcomes compatible with the observed data.","This method relaxes the previous requirement that a pricing algorithm must use fully-supported price distributions to be auditable.","This method is at least as permissive as any auditing method that has a high probability of failing algorithmic outcomes with non-vanishing calibrated regret.","Additionally, we strengthen the justification for using vanishing calibrated regret, versus vanishing best-in-hindsight regret, as the non-collusion definition, by showing that even without any side information, the pricing algorithms that only satisfy weaker vanishing best-in-hindsight regret allow an opponent to manipulate them into posting supra-competitive prices.","This manipulation cannot be excluded with a non-collusion definition of vanishing best-in-hindsight regret.   ","We motivate and interpret the approach of auditing algorithms from their data as suggesting a per se rule.","However, we demonstrate that it is possible for algorithms to pass the audit by pretending to have higher costs than they actually do.","For such scenarios, the rule of reason can be applied to bound the range of costs to those that are reasonable for the domain."],"url":"http://arxiv.org/abs/2501.09740v1"}
{"created":"2025-01-16 18:39:36","title":"MultiGraphMatch: a subgraph matching algorithm for multigraphs","abstract":"Subgraph matching is the problem of finding all the occurrences of a small graph, called the query, in a larger graph, called the target. Although the problem has been widely studied in simple graphs, few solutions have been proposed for multigraphs, in which two nodes can be connected by multiple edges, each denoting a possibly different type of relationship. In our new algorithm MultiGraphMatch, nodes and edges can be associated with labels and multiple properties. MultiGraphMatch introduces a novel data structure called bit matrix to efficiently index both the query and the target and filter the set of target edges that are matchable with each query edge. In addition, the algorithm proposes a new technique for ordering the processing of query edges based on the cardinalities of the sets of matchable edges. Using the CYPHER query definition language, MultiGraphMatch can perform queries with logical conditions on node and edge labels. We compare MultiGraphMatch with SuMGra and graph database systems Memgraph and Neo4J, showing comparable or better performance in all queries on a wide variety of synthetic and real-world graphs.","sentences":["Subgraph matching is the problem of finding all the occurrences of a small graph, called the query, in a larger graph, called the target.","Although the problem has been widely studied in simple graphs, few solutions have been proposed for multigraphs, in which two nodes can be connected by multiple edges, each denoting a possibly different type of relationship.","In our new algorithm MultiGraphMatch, nodes and edges can be associated with labels and multiple properties.","MultiGraphMatch introduces a novel data structure called bit matrix to efficiently index both the query and the target and filter the set of target edges that are matchable with each query edge.","In addition, the algorithm proposes a new technique for ordering the processing of query edges based on the cardinalities of the sets of matchable edges.","Using the CYPHER query definition language, MultiGraphMatch can perform queries with logical conditions on node and edge labels.","We compare MultiGraphMatch with SuMGra and graph database systems Memgraph and Neo4J, showing comparable or better performance in all queries on a wide variety of synthetic and real-world graphs."],"url":"http://arxiv.org/abs/2501.09736v1"}
{"created":"2025-01-16 18:35:45","title":"ComplexVAD: Detecting Interaction Anomalies in Video","abstract":"Existing video anomaly detection datasets are inadequate for representing complex anomalies that occur due to the interactions between objects. The absence of complex anomalies in previous video anomaly detection datasets affects research by shifting the focus onto simple anomalies. To address this problem, we introduce a new large-scale dataset: ComplexVAD. In addition, we propose a novel method to detect complex anomalies via modeling the interactions between objects using a scene graph with spatio-temporal attributes. With our proposed method and two other state-of-the-art video anomaly detection methods, we obtain baseline scores on ComplexVAD and demonstrate that our new method outperforms existing works.","sentences":["Existing video anomaly detection datasets are inadequate for representing complex anomalies that occur due to the interactions between objects.","The absence of complex anomalies in previous video anomaly detection datasets affects research by shifting the focus onto simple anomalies.","To address this problem, we introduce a new large-scale dataset: ComplexVAD.","In addition, we propose a novel method to detect complex anomalies via modeling the interactions between objects using a scene graph with spatio-temporal attributes.","With our proposed method and two other state-of-the-art video anomaly detection methods, we obtain baseline scores on ComplexVAD and demonstrate that our new method outperforms existing works."],"url":"http://arxiv.org/abs/2501.09733v1"}
{"created":"2025-01-16 18:30:37","title":"Inference-Time Scaling for Diffusion Models beyond Scaling Denoising Steps","abstract":"Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws. Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference. Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen. In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation. Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process. We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates. Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario.","sentences":["Generative models have made significant impacts across various domains, largely due to their ability to scale during training by increasing data, computational resources, and model size, a phenomenon characterized by the scaling laws.","Recent research has begun to explore inference-time scaling behavior in Large Language Models (LLMs), revealing how performance can further improve with additional computation during inference.","Unlike LLMs, diffusion models inherently possess the flexibility to adjust inference-time computation via the number of denoising steps, although the performance gains typically flatten after a few dozen.","In this work, we explore the inference-time scaling behavior of diffusion models beyond increasing denoising steps and investigate how the generation performance can further improve with increased computation.","Specifically, we consider a search problem aimed at identifying better noises for the diffusion sampling process.","We structure the design space along two axes: the verifiers used to provide feedback, and the algorithms used to find better noise candidates.","Through extensive experiments on class-conditioned and text-conditioned image generation benchmarks, our findings reveal that increasing inference-time compute leads to substantial improvements in the quality of samples generated by diffusion models, and with the complicated nature of images, combinations of the components in the framework can be specifically chosen to conform with different application scenario."],"url":"http://arxiv.org/abs/2501.09732v1"}
{"created":"2025-01-16 18:25:50","title":"Generating particle physics Lagrangians with transformers","abstract":"In physics, Lagrangians provide a systematic way to describe laws governing physical systems. In the context of particle physics, they encode the interactions and behavior of the fundamental building blocks of our universe. By treating Lagrangians as complex, rule-based constructs similar to linguistic expressions, we trained a transformer model -- proven to be effective in natural language tasks -- to predict the Lagrangian corresponding to a given list of particles. We report on the transformer's performance in constructing Lagrangians respecting the Standard Model $\\mathrm{SU}(3)\\times \\mathrm{SU}(2)\\times \\mathrm{U}(1)$ gauge symmetries. The resulting model is shown to achieve high accuracies (over 90\\%) with Lagrangians up to six matter fields, with the capacity to generalize beyond the training distribution, albeit within architectural constraints. We show through an analysis of input embeddings that the model has internalized concepts such as group representations and conjugation operations as it learned to generate Lagrangians. We make the model and training datasets available to the community. An interactive demonstration can be found at: \\url{https://huggingface.co/spaces/JoseEliel/generate-lagrangians}.","sentences":["In physics, Lagrangians provide a systematic way to describe laws governing physical systems.","In the context of particle physics, they encode the interactions and behavior of the fundamental building blocks of our universe.","By treating Lagrangians as complex, rule-based constructs similar to linguistic expressions, we trained a transformer model -- proven to be effective in natural language tasks -- to predict the Lagrangian corresponding to a given list of particles.","We report on the transformer's performance in constructing Lagrangians respecting the Standard Model $\\mathrm{SU}(3)\\times \\mathrm{SU}(2)\\times \\mathrm{U}(1)$ gauge symmetries.","The resulting model is shown to achieve high accuracies (over 90\\%) with Lagrangians up to six matter fields, with the capacity to generalize beyond the training distribution, albeit within architectural constraints.","We show through an analysis of input embeddings that the model has internalized concepts such as group representations and conjugation operations as it learned to generate Lagrangians.","We make the model and training datasets available to the community.","An interactive demonstration can be found at: \\url{https://huggingface.co/spaces/JoseEliel/generate-lagrangians}."],"url":"http://arxiv.org/abs/2501.09729v1"}
{"created":"2025-01-16 18:16:34","title":"Parallel multi-objective metaheuristics for smart communications in vehicular networks","abstract":"This article analyzes the use of two parallel multi-objective soft computing algorithms to automatically search for high-quality settings of the Ad hoc On Demand Vector routing protocol for vehicular networks. These methods are based on an evolutionary algorithm and on a swarm intelligence approach. The experimental analysis demonstrates that the configurations computed by our optimization algorithms outperform other state-of-the-art optimized ones. In turn, the computational efficiency achieved by all the parallel versions is greater than 87 %. Therefore, the line of work presented in this article represents an efficient framework to improve vehicular communications.","sentences":["This article analyzes the use of two parallel multi-objective soft computing algorithms to automatically search for high-quality settings of the Ad hoc On Demand Vector routing protocol for vehicular networks.","These methods are based on an evolutionary algorithm and on a swarm intelligence approach.","The experimental analysis demonstrates that the configurations computed by our optimization algorithms outperform other state-of-the-art optimized ones.","In turn, the computational efficiency achieved by all the parallel versions is greater than 87 %.","Therefore, the line of work presented in this article represents an efficient framework to improve vehicular communications."],"url":"http://arxiv.org/abs/2501.09725v1"}
{"created":"2025-01-16 18:10:37","title":"Attention based Bidirectional GRU hybrid model for inappropriate content detection in Urdu language","abstract":"With the increased use of the internet and social networks for online discussions, the spread of toxic and inappropriate content on social networking sites has also increased. Several studies have been conducted in different languages. However, there is less work done for South Asian languages for inappropriate content identification using deep learning techniques. In Urdu language, the spellings are not unique, and people write different common spellings for the same word, while mixing it other languages, like English in the text makes it more challenging, and limited research work is available to process such language with the finest algorithms. The use of attention layer with a deep learning model can help handling the long-term dependencies and increase its efficiency . To explore the effects of the attention layer, this study proposes attention-based Bidirectional GRU hybrid model for identifying inappropriate content in Urdu Unicode text language. Four different baseline deep learning models; LSTM, Bi-LSTM, GRU, and TCN, are used to compare the performance of the proposed model. The results of these models were compared based on evaluation metrics, dataset size, and impact of the word embedding layer. The pre-trained Urdu word2Vec embeddings were utilized for our case. Our proposed model BiGRU-A outperformed all other baseline models by yielding 84\\% accuracy without using pre-trained word2Vec layer. From our experiments, we have established that the attention layer improves the model's efficiency, and pre-trained word2Vec embedding does not work well with an inappropriate content dataset.","sentences":["With the increased use of the internet and social networks for online discussions, the spread of toxic and inappropriate content on social networking sites has also increased.","Several studies have been conducted in different languages.","However, there is less work done for South Asian languages for inappropriate content identification using deep learning techniques.","In Urdu language, the spellings are not unique, and people write different common spellings for the same word, while mixing it other languages, like English in the text makes it more challenging, and limited research work is available to process such language with the finest algorithms.","The use of attention layer with a deep learning model can help handling the long-term dependencies and increase its efficiency .","To explore the effects of the attention layer, this study proposes attention-based Bidirectional GRU hybrid model for identifying inappropriate content in Urdu Unicode text language.","Four different baseline deep learning models; LSTM, Bi-LSTM, GRU, and TCN, are used to compare the performance of the proposed model.","The results of these models were compared based on evaluation metrics, dataset size, and impact of the word embedding layer.","The pre-trained Urdu word2Vec embeddings were utilized for our case.","Our proposed model BiGRU-A outperformed all other baseline models by yielding 84\\% accuracy without using pre-trained word2Vec layer.","From our experiments, we have established that the attention layer improves the model's efficiency, and pre-trained word2Vec embedding does not work well with an inappropriate content dataset."],"url":"http://arxiv.org/abs/2501.09722v1"}
{"created":"2025-01-16 18:09:22","title":"A Simple Aerial Detection Baseline of Multimodal Language Models","abstract":"The multimodal language models (MLMs) based on generative pre-trained Transformer are considered powerful candidates for unifying various domains and tasks. MLMs developed for remote sensing (RS) have demonstrated outstanding performance in multiple tasks, such as visual question answering and visual grounding. In addition to visual grounding that detects specific objects corresponded to given instruction, aerial detection, which detects all objects of multiple categories, is also a valuable and challenging task for RS foundation models. However, aerial detection has not been explored by existing RS MLMs because the autoregressive prediction mechanism of MLMs differs significantly from the detection outputs. In this paper, we present a simple baseline for applying MLMs to aerial detection for the first time, named LMMRotate. Specifically, we first introduce a normalization method to transform detection outputs into textual outputs to be compatible with the MLM framework. Then, we propose a evaluation method, which ensures a fair comparison between MLMs and conventional object detection models. We construct the baseline by fine-tuning open-source general-purpose MLMs and achieve impressive detection performance comparable to conventional detector. We hope that this baseline will serve as a reference for future MLM development, enabling more comprehensive capabilities for understanding RS images. Code is available at https://github.com/Li-Qingyun/mllm-mmrotate.","sentences":["The multimodal language models (MLMs) based on generative pre-trained Transformer are considered powerful candidates for unifying various domains and tasks.","MLMs developed for remote sensing (RS) have demonstrated outstanding performance in multiple tasks, such as visual question answering and visual grounding.","In addition to visual grounding that detects specific objects corresponded to given instruction, aerial detection, which detects all objects of multiple categories, is also a valuable and challenging task for RS foundation models.","However, aerial detection has not been explored by existing RS MLMs because the autoregressive prediction mechanism of MLMs differs significantly from the detection outputs.","In this paper, we present a simple baseline for applying MLMs to aerial detection for the first time, named LMMRotate.","Specifically, we first introduce a normalization method to transform detection outputs into textual outputs to be compatible with the MLM framework.","Then, we propose a evaluation method, which ensures a fair comparison between MLMs and conventional object detection models.","We construct the baseline by fine-tuning open-source general-purpose MLMs and achieve impressive detection performance comparable to conventional detector.","We hope that this baseline will serve as a reference for future MLM development, enabling more comprehensive capabilities for understanding RS images.","Code is available at https://github.com/Li-Qingyun/mllm-mmrotate."],"url":"http://arxiv.org/abs/2501.09720v1"}
{"created":"2025-01-16 18:06:22","title":"Comparative Insights from 12 Machine Learning Models in Extracting Economic Ideology from Political Text","abstract":"This study conducts a systematic assessment of the capabilities of 12 machine learning models and model variations in detecting economic ideology. As an evaluation benchmark, I use manifesto data spanning six elections in the United Kingdom and pre-annotated by expert and crowd coders. The analysis assesses the performance of several generative, fine-tuned, and zero-shot models at the granular and aggregate levels. The results show that generative models such as GPT-4o and Gemini 1.5 Flash consistently outperform other models against all benchmarks. However, they pose issues of accessibility and resource availability. Fine-tuning yielded competitive performance and offers a reliable alternative through domain-specific optimization. But its dependency on training data severely limits scalability. Zero-shot models consistently face difficulties with identifying signals of economic ideology, often resulting in negative associations with human coding. Using general knowledge for the domain-specific task of ideology scaling proved to be unreliable. Other key findings include considerable within-party variation, fine-tuning benefiting from larger training data, and zero-shot's sensitivity to prompt content. The assessments include the strengths and limitations of each model and derive best-practices for automated analyses of political content.","sentences":["This study conducts a systematic assessment of the capabilities of 12 machine learning models and model variations in detecting economic ideology.","As an evaluation benchmark, I use manifesto data spanning six elections in the United Kingdom and pre-annotated by expert and crowd coders.","The analysis assesses the performance of several generative, fine-tuned, and zero-shot models at the granular and aggregate levels.","The results show that generative models such as GPT-4o and Gemini 1.5 Flash consistently outperform other models against all benchmarks.","However, they pose issues of accessibility and resource availability.","Fine-tuning yielded competitive performance and offers a reliable alternative through domain-specific optimization.","But its dependency on training data severely limits scalability.","Zero-shot models consistently face difficulties with identifying signals of economic ideology, often resulting in negative associations with human coding.","Using general knowledge for the domain-specific task of ideology scaling proved to be unreliable.","Other key findings include considerable within-party variation, fine-tuning benefiting from larger training data, and zero-shot's sensitivity to prompt content.","The assessments include the strengths and limitations of each model and derive best-practices for automated analyses of political content."],"url":"http://arxiv.org/abs/2501.09719v1"}
{"created":"2025-01-16 18:06:09","title":"FLOL: Fast Baselines for Real-World Low-Light Enhancement","abstract":"Low-Light Image Enhancement (LLIE) is a key task in computational photography and imaging. The problem of enhancing images captured during night or in dark environments has been well-studied in the image signal processing literature. However, current deep learning-based solutions struggle with efficiency and robustness in real-world scenarios (e.g. scenes with noise, saturated pixels, bad illumination). We propose a lightweight neural network that combines image processing in the frequency and spatial domains. Our method, FLOL+, is one of the fastest models for this task, achieving state-of-the-art results on popular real scenes datasets such as LOL and LSRW. Moreover, we are able to process 1080p images under 12ms. Code and models at https://github.com/cidautai/FLOL","sentences":["Low-Light Image Enhancement (LLIE) is a key task in computational photography and imaging.","The problem of enhancing images captured during night or in dark environments has been well-studied in the image signal processing literature.","However, current deep learning-based solutions struggle with efficiency and robustness in real-world scenarios (e.g. scenes with noise, saturated pixels, bad illumination).","We propose a lightweight neural network that combines image processing in the frequency and spatial domains.","Our method, FLOL+, is one of the fastest models for this task, achieving state-of-the-art results on popular real scenes datasets such as LOL and LSRW.","Moreover, we are able to process 1080p images under 12ms.","Code and models at https://github.com/cidautai/FLOL"],"url":"http://arxiv.org/abs/2501.09718v1"}
{"created":"2025-01-16 18:05:28","title":"Intelligent OLSR Routing Protocol Optimization for VANETs","abstract":"Recent advances in wireless technologies have given rise to the emergence of vehicular ad hoc networks (VANETs). In such networks, the limited coverage of WiFi and the high mobility of the nodes generate frequent topology changes and network fragmentations. For these reasons, and taking into account that there is no central manager entity, routing packets through the network is a challenging task. Therefore, offering an efficient routing strategy is crucial to the deployment of VANETs. This paper deals with the optimal parameter setting of the optimized link state routing (OLSR), which is a well-known mobile ad hoc network routing protocol, by defining an optimization problem. This way, a series of representative metaheuristic algorithms (particle swarm optimization, differential evolution, genetic algorithm, and simulated annealing) are studied in this paper to find automatically optimal configurations of this routing protocol. In addition, a set of realistic VANET scenarios (based in the city of M\\'alaga) have been defined to accurately evaluate the performance of the network under our automatic OLSR. In the experiments, our tuned OLSR configurations result in better quality of service (QoS) than the standard request for comments (RFC 3626), as well as several human experts, making it amenable for utilization in VANET configurations.","sentences":["Recent advances in wireless technologies have given rise to the emergence of vehicular ad hoc networks (VANETs).","In such networks, the limited coverage of WiFi and the high mobility of the nodes generate frequent topology changes and network fragmentations.","For these reasons, and taking into account that there is no central manager entity, routing packets through the network is a challenging task.","Therefore, offering an efficient routing strategy is crucial to the deployment of VANETs.","This paper deals with the optimal parameter setting of the optimized link state routing (OLSR), which is a well-known mobile ad hoc network routing protocol, by defining an optimization problem.","This way, a series of representative metaheuristic algorithms (particle swarm optimization, differential evolution, genetic algorithm, and simulated annealing) are studied in this paper to find automatically optimal configurations of this routing protocol.","In addition, a set of realistic VANET scenarios (based in the city of M\\'alaga) have been defined to accurately evaluate the performance of the network under our automatic OLSR.","In the experiments, our tuned OLSR configurations result in better quality of service (QoS) than the standard request for comments (RFC 3626), as well as several human experts, making it amenable for utilization in VANET configurations."],"url":"http://arxiv.org/abs/2501.09716v1"}
{"created":"2025-01-16 18:00:37","title":"On equidistant single-orbit cyclic and quasi-cyclic subspace codes","abstract":"A code is said to be equidistant if the distance between any two distinct codewords of the code is the same. In this paper, we have studied equidistant single-orbit cyclic and quasi-cyclic subspace codes. The orbit code generated by a subspace $U$ in $\\mathbb{F}_{q^n}$ such that the dimension of $U$ over $\\mathbb{F}_q$ is $t$ or $n-t$, $\\mbox{where}~t=\\dim_{\\mathbb{F}_q}(\\mbox{Stab}(U)\\cup\\{0\\})$, is equidistant and is termed a trivial equidistant orbit code. Using the concept of cyclic difference sets, we have proved that only the trivial equidistant single-orbit cyclic subspace codes exist. Further, we have explored equidistant single-orbit quasi-cyclic subspace codes, focusing specifically on those which are sunflowers.","sentences":["A code is said to be equidistant if the distance between any two distinct codewords of the code is the same.","In this paper, we have studied equidistant single-orbit cyclic and quasi-cyclic subspace codes.","The orbit code generated by a subspace $U$ in $\\mathbb{F}_{q^n}$ such that the dimension of $U$ over $\\mathbb{F}_q$ is $t$ or $n-t$, $\\mbox{where}~t=\\dim_{\\mathbb{F}_q}(\\mbox{Stab}(U)\\cup\\{0\\})$, is equidistant and is termed a trivial equidistant orbit code.","Using the concept of cyclic difference sets, we have proved that only the trivial equidistant single-orbit cyclic subspace codes exist.","Further, we have explored equidistant single-orbit quasi-cyclic subspace codes, focusing specifically on those which are sunflowers."],"url":"http://arxiv.org/abs/2501.09710v1"}
{"created":"2025-01-16 18:00:06","title":"CyberMentor: AI Powered Learning Tool Platform to Address Diverse Student Needs in Cybersecurity Education","abstract":"Many non-traditional students in cybersecurity programs often lack access to advice from peers, family members and professors, which can hinder their educational experiences. Additionally, these students may not fully benefit from various LLM-powered AI assistants due to issues like content relevance, locality of advice, minimum expertise, and timing. This paper addresses these challenges by introducing an application designed to provide comprehensive support by answering questions related to knowledge, skills, and career preparation advice tailored to the needs of these students. We developed a learning tool platform, CyberMentor, to address the diverse needs and pain points of students majoring in cybersecurity. Powered by agentic workflow and Generative Large Language Models (LLMs), the platform leverages Retrieval-Augmented Generation (RAG) for accurate and contextually relevant information retrieval to achieve accessibility and personalization. We demonstrated its value in addressing knowledge requirements for cybersecurity education and for career marketability, in tackling skill requirements for analytical and programming assignments, and in delivering real time on demand learning support. Using three use scenarios, we showcased CyberMentor in facilitating knowledge acquisition and career preparation and providing seamless skill-based guidance and support. We also employed the LangChain prompt-based evaluation methodology to evaluate the platform's impact, confirming its strong performance in helpfulness, correctness, and completeness. These results underscore the system's ability to support students in developing practical cybersecurity skills while improving equity and sustainability within higher education. Furthermore, CyberMentor's open-source design allows for adaptation across other disciplines, fostering educational innovation and broadening its potential impact.","sentences":["Many non-traditional students in cybersecurity programs often lack access to advice from peers, family members and professors, which can hinder their educational experiences.","Additionally, these students may not fully benefit from various LLM-powered AI assistants due to issues like content relevance, locality of advice, minimum expertise, and timing.","This paper addresses these challenges by introducing an application designed to provide comprehensive support by answering questions related to knowledge, skills, and career preparation advice tailored to the needs of these students.","We developed a learning tool platform, CyberMentor, to address the diverse needs and pain points of students majoring in cybersecurity.","Powered by agentic workflow and Generative Large Language Models (LLMs), the platform leverages Retrieval-Augmented Generation (RAG) for accurate and contextually relevant information retrieval to achieve accessibility and personalization.","We demonstrated its value in addressing knowledge requirements for cybersecurity education and for career marketability, in tackling skill requirements for analytical and programming assignments, and in delivering real time on demand learning support.","Using three use scenarios, we showcased CyberMentor in facilitating knowledge acquisition and career preparation and providing seamless skill-based guidance and support.","We also employed the LangChain prompt-based evaluation methodology to evaluate the platform's impact, confirming its strong performance in helpfulness, correctness, and completeness.","These results underscore the system's ability to support students in developing practical cybersecurity skills while improving equity and sustainability within higher education.","Furthermore, CyberMentor's open-source design allows for adaptation across other disciplines, fostering educational innovation and broadening its potential impact."],"url":"http://arxiv.org/abs/2501.09709v1"}
{"created":"2025-01-16 17:58:58","title":"The Goofus & Gallant Story Corpus for Practical Value Alignment","abstract":"Values or principles are key elements of human society that influence people to behave and function according to an accepted standard set of social rules to maintain social order. As AI systems are becoming ubiquitous in human society, it is a major concern that they could violate these norms or values and potentially cause harm. Thus, to prevent intentional or unintentional harm, AI systems are expected to take actions that align with these principles. Training systems to exhibit this type of behavior is difficult and often requires a specialized dataset. This work presents a multi-modal dataset illustrating normative and non-normative behavior in real-life situations described through natural language and artistic images. This training set contains curated sets of images that are designed to teach young children about social principles. We argue that this is an ideal dataset to use for training socially normative agents given this fact.","sentences":["Values or principles are key elements of human society that influence people to behave and function according to an accepted standard set of social rules to maintain social order.","As AI systems are becoming ubiquitous in human society, it is a major concern that they could violate these norms or values and potentially cause harm.","Thus, to prevent intentional or unintentional harm, AI systems are expected to take actions that align with these principles.","Training systems to exhibit this type of behavior is difficult and often requires a specialized dataset.","This work presents a multi-modal dataset illustrating normative and non-normative behavior in real-life situations described through natural language and artistic images.","This training set contains curated sets of images that are designed to teach young children about social principles.","We argue that this is an ideal dataset to use for training socially normative agents given this fact."],"url":"http://arxiv.org/abs/2501.09707v1"}
{"created":"2025-01-16 17:58:32","title":"Domain Adaptation of Foundation LLMs for e-Commerce","abstract":"We present the e-Llama models: 8 billion and 70 billion parameter large language models that are adapted towards the e-commerce domain. These models are meant as foundation models with deep knowledge about e-commerce, that form a base for instruction- and fine-tuning. The e-Llama models are obtained by continuously pretraining the Llama 3.1 base models on 1 trillion tokens of domain-specific data.   We discuss our approach and motivate our choice of hyperparameters with a series of ablation studies. To quantify how well the models have been adapted to the e-commerce domain, we define and implement a set of multilingual, e-commerce specific evaluation tasks.   We show that, when carefully choosing the training setup, the Llama 3.1 models can be adapted towards the new domain without sacrificing significant performance on general domain tasks. We also explore the possibility of merging the adapted model and the base model for a better control of the performance trade-off between domains.","sentences":["We present the e-Llama models: 8 billion and 70 billion parameter large language models that are adapted towards the e-commerce domain.","These models are meant as foundation models with deep knowledge about e-commerce, that form a base for instruction- and fine-tuning.","The e-Llama models are obtained by continuously pretraining the Llama 3.1 base models on 1 trillion tokens of domain-specific data.   ","We discuss our approach and motivate our choice of hyperparameters with a series of ablation studies.","To quantify how well the models have been adapted to the e-commerce domain, we define and implement a set of multilingual, e-commerce specific evaluation tasks.   ","We show that, when carefully choosing the training setup, the Llama 3.1 models can be adapted towards the new domain without sacrificing significant performance on general domain tasks.","We also explore the possibility of merging the adapted model and the base model for a better control of the performance trade-off between domains."],"url":"http://arxiv.org/abs/2501.09706v1"}
{"created":"2025-01-16 17:57:53","title":"Practical Continual Forgetting for Pre-trained Vision Models","abstract":"For privacy and security concerns, the need to erase unwanted information from pre-trained vision models is becoming evident nowadays. In real-world scenarios, erasure requests originate at any time from both users and model owners, and these requests usually form a sequence. Therefore, under such a setting, selective information is expected to be continuously removed from a pre-trained model while maintaining the rest. We define this problem as continual forgetting and identify three key challenges. (i) For unwanted knowledge, efficient and effective deleting is crucial. (ii) For remaining knowledge, the impact brought by the forgetting procedure should be minimal. (iii) In real-world scenarios, the training samples may be scarce or partially missing during the process of forgetting. To address them, we first propose Group Sparse LoRA (GS-LoRA). Specifically, towards (i), we introduce LoRA modules to fine-tune the FFN layers in Transformer blocks for each forgetting task independently, and towards (ii), a simple group sparse regularization is adopted, enabling automatic selection of specific LoRA groups and zeroing out the others. To further extend GS-LoRA to more practical scenarios, we incorporate prototype information as additional supervision and introduce a more practical approach, GS-LoRA++. For each forgotten class, we move the logits away from its original prototype. For the remaining classes, we pull the logits closer to their respective prototypes. We conduct extensive experiments on face recognition, object detection and image classification and demonstrate that our method manages to forget specific classes with minimal impact on other classes. Codes have been released on https://github.com/bjzhb666/GS-LoRA.","sentences":["For privacy and security concerns, the need to erase unwanted information from pre-trained vision models is becoming evident nowadays.","In real-world scenarios, erasure requests originate at any time from both users and model owners, and these requests usually form a sequence.","Therefore, under such a setting, selective information is expected to be continuously removed from a pre-trained model while maintaining the rest.","We define this problem as continual forgetting and identify three key challenges.","(i) For unwanted knowledge, efficient and effective deleting is crucial.","(ii) For remaining knowledge, the impact brought by the forgetting procedure should be minimal.","(iii)","In real-world scenarios, the training samples may be scarce or partially missing during the process of forgetting.","To address them, we first propose Group Sparse LoRA (GS-LoRA).","Specifically, towards (i), we introduce LoRA modules to fine-tune the FFN layers in Transformer blocks for each forgetting task independently, and towards (ii), a simple group sparse regularization is adopted, enabling automatic selection of specific LoRA groups and zeroing out the others.","To further extend GS-LoRA to more practical scenarios, we incorporate prototype information as additional supervision and introduce a more practical approach, GS-LoRA++.","For each forgotten class, we move the logits away from its original prototype.","For the remaining classes, we pull the logits closer to their respective prototypes.","We conduct extensive experiments on face recognition, object detection and image classification and demonstrate that our method manages to forget specific classes with minimal impact on other classes.","Codes have been released on https://github.com/bjzhb666/GS-LoRA."],"url":"http://arxiv.org/abs/2501.09705v1"}
{"created":"2025-01-16 17:54:56","title":"Cueless EEG imagined speech for subject identification: dataset and benchmarks","abstract":"Electroencephalogram (EEG) signals have emerged as a promising modality for biometric identification. While previous studies have explored the use of imagined speech with semantically meaningful words for subject identification, most have relied on additional visual or auditory cues. In this study, we introduce a cueless EEG-based imagined speech paradigm, where subjects imagine the pronunciation of semantically meaningful words without any external cues. This innovative approach addresses the limitations of prior methods by requiring subjects to select and imagine words from a predefined list naturally. The dataset comprises over 4,350 trials from 11 subjects across five sessions. We assess a variety of classification methods, including traditional machine learning techniques such as Support Vector Machines (SVM) and XGBoost, as well as time-series foundation models and deep learning architectures specifically designed for EEG classification, such as EEG Conformer and Shallow ConvNet. A session-based hold-out validation strategy was employed to ensure reliable evaluation and prevent data leakage. Our results demonstrate outstanding classification accuracy, reaching 97.93%. These findings highlight the potential of cueless EEG paradigms for secure and reliable subject identification in real-world applications, such as brain-computer interfaces (BCIs).","sentences":["Electroencephalogram (EEG) signals have emerged as a promising modality for biometric identification.","While previous studies have explored the use of imagined speech with semantically meaningful words for subject identification, most have relied on additional visual or auditory cues.","In this study, we introduce a cueless EEG-based imagined speech paradigm, where subjects imagine the pronunciation of semantically meaningful words without any external cues.","This innovative approach addresses the limitations of prior methods by requiring subjects to select and imagine words from a predefined list naturally.","The dataset comprises over 4,350 trials from 11 subjects across five sessions.","We assess a variety of classification methods, including traditional machine learning techniques such as Support Vector Machines (SVM) and XGBoost, as well as time-series foundation models and deep learning architectures specifically designed for EEG classification, such as EEG Conformer and Shallow ConvNet.","A session-based hold-out validation strategy was employed to ensure reliable evaluation and prevent data leakage.","Our results demonstrate outstanding classification accuracy, reaching 97.93%.","These findings highlight the potential of cueless EEG paradigms for secure and reliable subject identification in real-world applications, such as brain-computer interfaces (BCIs)."],"url":"http://arxiv.org/abs/2501.09700v1"}
{"created":"2025-01-16 17:48:03","title":"Mitigating Hallucinations in Large Vision-Language Models via DPO: On-Policy Data Hold the Key","abstract":"Hallucination remains a major challenge for Large Vision-Language Models (LVLMs). Direct Preference Optimization (DPO) has gained increasing attention as a simple solution to hallucination issues. It directly learns from constructed preference pairs that reflect the severity of hallucinations in responses to the same prompt and image. Nonetheless, different data construction methods in existing works bring notable performance variations. We identify a crucial factor here: outcomes are largely contingent on whether the constructed data aligns on-policy w.r.t the initial (reference) policy of DPO. Theoretical analysis suggests that learning from off-policy data is impeded by the presence of KL-divergence between the updated policy and the reference policy. From the perspective of dataset distribution, we systematically summarize the inherent flaws in existing algorithms that employ DPO to address hallucination issues. To alleviate the problems, we propose On-Policy Alignment (OPA)-DPO framework, which uniquely leverages expert feedback to correct hallucinated responses and aligns both the original and expert-revised responses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO achieves an additional reduction in the hallucination rate of LLaVA-1.5-7B: 13.26% on the AMBER benchmark and 5.39% on the Object-Hal benchmark, compared to the previous SOTA algorithm trained with 16k samples.","sentences":["Hallucination remains a major challenge for Large Vision-Language Models (LVLMs).","Direct Preference Optimization (DPO) has gained increasing attention as a simple solution to hallucination issues.","It directly learns from constructed preference pairs that reflect the severity of hallucinations in responses to the same prompt and image.","Nonetheless, different data construction methods in existing works bring notable performance variations.","We identify a crucial factor here: outcomes are largely contingent on whether the constructed data aligns on-policy w.r.t the initial (reference) policy of DPO.","Theoretical analysis suggests that learning from off-policy data is impeded by the presence of KL-divergence between the updated policy and the reference policy.","From the perspective of dataset distribution, we systematically summarize the inherent flaws in existing algorithms that employ DPO to address hallucination issues.","To alleviate the problems, we propose On-Policy Alignment (OPA)-DPO framework, which uniquely leverages expert feedback to correct hallucinated responses and aligns both the original and expert-revised responses in an on-policy manner.","Notably, with only 4.8k data, OPA-DPO achieves an additional reduction in the hallucination rate of LLaVA-1.5-7B: 13.26% on the AMBER benchmark and 5.39% on the Object-Hal benchmark, compared to the previous SOTA algorithm trained with 16k samples."],"url":"http://arxiv.org/abs/2501.09695v1"}
{"created":"2025-01-16 17:47:18","title":"Simulated Interactive Debugging","abstract":"Debugging software, i.e., the localization of faults and their repair, is a main activity in software engineering. Therefore, effective and efficient debugging is one of the core skills a software engineer must develop. However, the teaching of debugging techniques is usually very limited or only taught in indirect ways, e.g., during software projects. As a result, most Computer Science (CS) students learn debugging only in an ad-hoc and unstructured way. In this work, we present our approach called Simulated Interactive Debugging that interactively guides students along the debugging process. The guidance aims to empower the students to repair their solutions and have a proper \"learning\" experience. We envision that such guided debugging techniques can be integrated into programming courses early in the CS education curriculum. To perform an initial evaluation, we developed a prototypical implementation using traditional fault localization techniques and large language models. Students can use features like the automated setting of breakpoints or an interactive chatbot. We designed and executed a controlled experiment that included this IDE-integrated tooling with eight undergraduate CS students. Based on the responses, we conclude that the participants liked the systematic guidance by the assisted debugger. In particular, they rated the automated setting of breakpoints as the most effective, followed by the interactive debugging and chatting, and the explanations for how breakpoints were set. In our future work, we will improve our concept and implementation, add new features, and perform more intensive user studies.","sentences":["Debugging software, i.e., the localization of faults and their repair, is a main activity in software engineering.","Therefore, effective and efficient debugging is one of the core skills a software engineer must develop.","However, the teaching of debugging techniques is usually very limited or only taught in indirect ways, e.g., during software projects.","As a result, most Computer Science (CS) students learn debugging only in an ad-hoc and unstructured way.","In this work, we present our approach called Simulated Interactive Debugging that interactively guides students along the debugging process.","The guidance aims to empower the students to repair their solutions and have a proper \"learning\" experience.","We envision that such guided debugging techniques can be integrated into programming courses early in the CS education curriculum.","To perform an initial evaluation, we developed a prototypical implementation using traditional fault localization techniques and large language models.","Students can use features like the automated setting of breakpoints or an interactive chatbot.","We designed and executed a controlled experiment that included this IDE-integrated tooling with eight undergraduate CS students.","Based on the responses, we conclude that the participants liked the systematic guidance by the assisted debugger.","In particular, they rated the automated setting of breakpoints as the most effective, followed by the interactive debugging and chatting, and the explanations for how breakpoints were set.","In our future work, we will improve our concept and implementation, add new features, and perform more intensive user studies."],"url":"http://arxiv.org/abs/2501.09694v1"}
{"created":"2025-01-16 17:44:18","title":"A Near-optimal Algorithm for Learning Margin Halfspaces with Massart Noise","abstract":"We study the problem of PAC learning $\\gamma$-margin halfspaces in the presence of Massart noise. Without computational considerations, the sample complexity of this learning problem is known to be $\\widetilde{\\Theta}(1/(\\gamma^2 \\epsilon))$. Prior computationally efficient algorithms for the problem incur sample complexity $\\tilde{O}(1/(\\gamma^4 \\epsilon^3))$ and achieve 0-1 error of $\\eta+\\epsilon$, where $\\eta<1/2$ is the upper bound on the noise rate. Recent work gave evidence of an information-computation tradeoff, suggesting that a quadratic dependence on $1/\\epsilon$ is required for computationally efficient algorithms. Our main result is a computationally efficient learner with sample complexity $\\widetilde{\\Theta}(1/(\\gamma^2 \\epsilon^2))$, nearly matching this lower bound. In addition, our algorithm is simple and practical, relying on online SGD on a carefully selected sequence of convex losses.","sentences":["We study the problem of PAC learning $\\gamma$-margin halfspaces in the presence of Massart noise.","Without computational considerations, the sample complexity of this learning problem is known to be $\\widetilde{\\Theta}(1/(\\gamma^2 \\epsilon))$. Prior computationally efficient algorithms for the problem incur sample complexity $\\tilde{O}(1/(\\gamma^4 \\epsilon^3))$ and achieve 0-1 error of $\\eta+\\epsilon$, where $\\eta<1/2$ is the upper bound on the noise rate.","Recent work gave evidence of an information-computation tradeoff, suggesting that a quadratic dependence on $1/\\epsilon$ is required for computationally efficient algorithms.","Our main result is a computationally efficient learner with sample complexity $\\widetilde{\\Theta}(1/(\\gamma^2 \\epsilon^2))$, nearly matching this lower bound.","In addition, our algorithm is simple and practical, relying on online SGD on a carefully selected sequence of convex losses."],"url":"http://arxiv.org/abs/2501.09691v1"}
{"created":"2025-01-16 17:40:19","title":"Fine-Grained Image-Text Correspondence with Cost Aggregation for Open-Vocabulary Part Segmentation","abstract":"Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing fine-grained parts in unseen categories. We identify two primary challenges in OVPS: (1) the difficulty in aligning part-level image-text correspondence, and (2) the lack of structural understanding in segmenting object parts. To address these issues, we propose PartCATSeg, a novel framework that integrates object-aware part-level cost aggregation, compositional loss, and structural guidance from DINO. Our approach employs a disentangled cost aggregation strategy that handles object and part-level costs separately, enhancing the precision of part-level segmentation. We also introduce a compositional loss to better capture part-object relationships, compensating for the limited part annotations. Additionally, structural guidance from DINO features improves boundary delineation and inter-part understanding. Extensive experiments on Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that our method significantly outperforms state-of-the-art approaches, setting a new baseline for robust generalization to unseen part categories.","sentences":["Open-Vocabulary Part Segmentation (OVPS) is an emerging field for recognizing fine-grained parts in unseen categories.","We identify two primary challenges in OVPS: (1) the difficulty in aligning part-level image-text correspondence, and (2) the lack of structural understanding in segmenting object parts.","To address these issues, we propose PartCATSeg, a novel framework that integrates object-aware part-level cost aggregation, compositional loss, and structural guidance from DINO.","Our approach employs a disentangled cost aggregation strategy that handles object and part-level costs separately, enhancing the precision of part-level segmentation.","We also introduce a compositional loss to better capture part-object relationships, compensating for the limited part annotations.","Additionally, structural guidance from DINO features improves boundary delineation and inter-part understanding.","Extensive experiments on Pascal-Part-116, ADE20K-Part-234, and PartImageNet datasets demonstrate that our method significantly outperforms state-of-the-art approaches, setting a new baseline for robust generalization to unseen part categories."],"url":"http://arxiv.org/abs/2501.09688v1"}
{"created":"2025-01-16 17:39:25","title":"U-Fair: Uncertainty-based Multimodal Multitask Learning for Fairer Depression Detection","abstract":"Machine learning bias in mental health is becoming an increasingly pertinent challenge. Despite promising efforts indicating that multitask approaches often work better than unitask approaches, there is minimal work investigating the impact of multitask learning on performance and fairness in depression detection nor leveraged it to achieve fairer prediction outcomes. In this work, we undertake a systematic investigation of using a multitask approach to improve performance and fairness for depression detection. We propose a novel gender-based task-reweighting method using uncertainty grounded in how the PHQ-8 questionnaire is structured. Our results indicate that, although a multitask approach improves performance and fairness compared to a unitask approach, the results are not always consistent and we see evidence of negative transfer and a reduction in the Pareto frontier, which is concerning given the high-stake healthcare setting. Our proposed approach of gender-based reweighting with uncertainty improves performance and fairness and alleviates both challenges to a certain extent. Our findings on each PHQ-8 subitem task difficulty are also in agreement with the largest study conducted on the PHQ-8 subitem discrimination capacity, thus providing the very first tangible evidence linking ML findings with large-scale empirical population studies conducted on the PHQ-8.","sentences":["Machine learning bias in mental health is becoming an increasingly pertinent challenge.","Despite promising efforts indicating that multitask approaches often work better than unitask approaches, there is minimal work investigating the impact of multitask learning on performance and fairness in depression detection nor leveraged it to achieve fairer prediction outcomes.","In this work, we undertake a systematic investigation of using a multitask approach to improve performance and fairness for depression detection.","We propose a novel gender-based task-reweighting method using uncertainty grounded in how the PHQ-8 questionnaire is structured.","Our results indicate that, although a multitask approach improves performance and fairness compared to a unitask approach, the results are not always consistent and we see evidence of negative transfer and a reduction in the Pareto frontier, which is concerning given the high-stake healthcare setting.","Our proposed approach of gender-based reweighting with uncertainty improves performance and fairness and alleviates both challenges to a certain extent.","Our findings on each PHQ-8 subitem task difficulty are also in agreement with the largest study conducted on the PHQ-8 subitem discrimination capacity, thus providing the very first tangible evidence linking ML findings with large-scale empirical population studies conducted on the PHQ-8."],"url":"http://arxiv.org/abs/2501.09687v1"}
{"created":"2025-01-16 17:37:58","title":"Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models","abstract":"Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of \"thought\" -- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\" with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI's o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.","sentences":["Language has long been conceived as an essential tool for human reasoning.","The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks.","Researchers have moved beyond simple autoregressive token generation by introducing the concept of \"thought\" -- a sequence of tokens representing intermediate steps in the reasoning process.","This innovative paradigm enables LLMs' to mimic complex human reasoning processes, such as tree search and reflective thinking.","Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes.","This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs' reasoning capacity by providing substantially more training data.","Furthermore, recent studies demonstrate that encouraging LLMs to \"think\" with more tokens during test-time inference can further significantly boost reasoning accuracy.","Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model.","The introduction of OpenAI's o1 series marks a significant milestone in this research direction.","In this survey, we present a comprehensive review of recent progress in LLM reasoning.","We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling.","We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions."],"url":"http://arxiv.org/abs/2501.09686v1"}
{"created":"2025-01-16 17:37:35","title":"Reward-Guided Controlled Generation for Inference-Time Alignment in Diffusion Models: Tutorial and Review","abstract":"This tutorial provides an in-depth guide on inference-time guidance and alignment methods for optimizing downstream reward functions in diffusion models. While diffusion models are renowned for their generative modeling capabilities, practical applications in fields such as biology often require sample generation that maximizes specific metrics (e.g., stability, affinity in proteins, closeness to target structures). In these scenarios, diffusion models can be adapted not only to generate realistic samples but also to explicitly maximize desired measures at inference time without fine-tuning. This tutorial explores the foundational aspects of such inference-time algorithms. We review these methods from a unified perspective, demonstrating that current techniques -- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling, and classifier guidance -- aim to approximate soft optimal denoising processes (a.k.a. policies in RL) that combine pre-trained denoising processes with value functions serving as look-ahead functions that predict from intermediate states to terminal rewards. Within this framework, we present several novel algorithms not yet covered in the literature. Furthermore, we discuss (1) fine-tuning methods combined with inference-time techniques, (2) inference-time algorithms based on search algorithms such as Monte Carlo tree search, which have received limited attention in current research, and (3) connections between inference-time algorithms in language models and diffusion models. The code of this tutorial on protein design is available at https://github.com/masa-ue/AlignInversePro","sentences":["This tutorial provides an in-depth guide on inference-time guidance and alignment methods for optimizing downstream reward functions in diffusion models.","While diffusion models are renowned for their generative modeling capabilities, practical applications in fields such as biology often require sample generation that maximizes specific metrics (e.g., stability, affinity in proteins, closeness to target structures).","In these scenarios, diffusion models can be adapted not only to generate realistic samples but also to explicitly maximize desired measures at inference time without fine-tuning.","This tutorial explores the foundational aspects of such inference-time algorithms.","We review these methods from a unified perspective, demonstrating that current techniques -- such as Sequential Monte Carlo (SMC)-based guidance, value-based sampling, and classifier guidance -- aim to approximate soft optimal denoising processes (a.k.a. policies in RL) that combine pre-trained denoising processes with value functions serving as look-ahead functions that predict from intermediate states to terminal rewards.","Within this framework, we present several novel algorithms not yet covered in the literature.","Furthermore, we discuss (1) fine-tuning methods combined with inference-time techniques, (2) inference-time algorithms based on search algorithms such as Monte Carlo tree search, which have received limited attention in current research, and (3) connections between inference-time algorithms in language models and diffusion models.","The code of this tutorial on protein design is available at https://github.com/masa-ue/AlignInversePro"],"url":"http://arxiv.org/abs/2501.09685v1"}
{"created":"2025-01-16 17:31:27","title":"CoNav Chair: Design of a ROS-based Smart Wheelchair for Shared Control Navigation in the Built Environment","abstract":"With the number of people with disabilities (PWD) increasing worldwide each year, the demand for mobility support to enable independent living and social integration is also growing. Wheelchairs commonly support the mobility of PWD in both indoor and outdoor environments. However, current powered wheelchairs (PWC) often fail to meet the needs of PWD, who may find it difficult to operate them. Furthermore, existing research on robotic wheelchairs typically focuses either on full autonomy or enhanced manual control, which can lead to reduced efficiency and user trust. To address these issues, this paper proposes a Robot Operating System (ROS)-based smart wheelchair, called CoNav Chair, that incorporates a shared control navigation algorithm and obstacle avoidance to support PWD while fostering efficiency and trust between the robot and the user. Our design consists of hardware and software components. Experimental results conducted in a typical indoor social environment demonstrate the performance and effectiveness of the smart wheelchair hardware and software design. This integrated design promotes trust and autonomy, which are crucial for the acceptance of assistive mobility technologies in the built environment.","sentences":["With the number of people with disabilities (PWD) increasing worldwide each year, the demand for mobility support to enable independent living and social integration is also growing.","Wheelchairs commonly support the mobility of PWD in both indoor and outdoor environments.","However, current powered wheelchairs (PWC) often fail to meet the needs of PWD, who may find it difficult to operate them.","Furthermore, existing research on robotic wheelchairs typically focuses either on full autonomy or enhanced manual control, which can lead to reduced efficiency and user trust.","To address these issues, this paper proposes a Robot Operating System (ROS)-based smart wheelchair, called CoNav Chair, that incorporates a shared control navigation algorithm and obstacle avoidance to support PWD while fostering efficiency and trust between the robot and the user.","Our design consists of hardware and software components.","Experimental results conducted in a typical indoor social environment demonstrate the performance and effectiveness of the smart wheelchair hardware and software design.","This integrated design promotes trust and autonomy, which are crucial for the acceptance of assistive mobility technologies in the built environment."],"url":"http://arxiv.org/abs/2501.09680v1"}
{"created":"2025-01-16 17:11:21","title":"Authenticated Delegation and Authorized AI Agents","abstract":"The rapid deployment of autonomous AI agents creates urgent challenges around authorization, accountability, and access control in digital spaces. New standards are needed to know whom AI agents act on behalf of and guide their use appropriately, protecting online spaces while unlocking the value of task delegation to autonomous agents. We introduce a novel framework for authenticated, authorized, and auditable delegation of authority to AI agents, where human users can securely delegate and restrict the permissions and scope of agents while maintaining clear chains of accountability. This framework builds on existing identification and access management protocols, extending OAuth 2.0 and OpenID Connect with agent-specific credentials and metadata, maintaining compatibility with established authentication and web infrastructure. Further, we propose a framework for translating flexible, natural language permissions into auditable access control configurations, enabling robust scoping of AI agent capabilities across diverse interaction modalities. Taken together, this practical approach facilitates immediate deployment of AI agents while addressing key security and accountability concerns, working toward ensuring agentic AI systems perform only appropriate actions and providing a tool for digital service providers to enable AI agent interactions without risking harm from scalable interaction.","sentences":["The rapid deployment of autonomous AI agents creates urgent challenges around authorization, accountability, and access control in digital spaces.","New standards are needed to know whom AI agents act on behalf of and guide their use appropriately, protecting online spaces while unlocking the value of task delegation to autonomous agents.","We introduce a novel framework for authenticated, authorized, and auditable delegation of authority to AI agents, where human users can securely delegate and restrict the permissions and scope of agents while maintaining clear chains of accountability.","This framework builds on existing identification and access management protocols, extending OAuth 2.0 and OpenID Connect with agent-specific credentials and metadata, maintaining compatibility with established authentication and web infrastructure.","Further, we propose a framework for translating flexible, natural language permissions into auditable access control configurations, enabling robust scoping of AI agent capabilities across diverse interaction modalities.","Taken together, this practical approach facilitates immediate deployment of AI agents while addressing key security and accountability concerns, working toward ensuring agentic AI systems perform only appropriate actions and providing a tool for digital service providers to enable AI agent interactions without risking harm from scalable interaction."],"url":"http://arxiv.org/abs/2501.09674v1"}
{"created":"2025-01-16 17:08:12","title":"Robin: a Suite of Multi-Scale Vision-Language Models and the CHIRP Evaluation Benchmark","abstract":"The proliferation of Vision-Language Models (VLMs) in the past several years calls for rigorous and comprehensive evaluation methods and benchmarks. This work analyzes existing VLM evaluation techniques, including automated metrics, AI-based assessments, and human evaluations across diverse tasks. We first introduce Robin - a novel suite of VLMs that we built by combining Large Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use Robin to identify shortcomings of current evaluation approaches across scales. Next, to overcome the identified limitations, we introduce CHIRP - a new long form response benchmark we developed for more robust and complete VLM evaluation. We provide open access to the Robin training code, model suite, and CHIRP benchmark to promote reproducibility and advance VLM research.","sentences":["The proliferation of Vision-Language Models (VLMs) in the past several years calls for rigorous and comprehensive evaluation methods and benchmarks.","This work analyzes existing VLM evaluation techniques, including automated metrics, AI-based assessments, and human evaluations across diverse tasks.","We first introduce Robin - a novel suite of VLMs that we built by combining Large Language Models (LLMs) and Vision Encoders (VEs) at multiple scales, and use Robin to identify shortcomings of current evaluation approaches across scales.","Next, to overcome the identified limitations, we introduce CHIRP - a new long form response benchmark we developed for more robust and complete VLM evaluation.","We provide open access to the Robin training code, model suite, and CHIRP benchmark to promote reproducibility and advance VLM research."],"url":"http://arxiv.org/abs/2501.09672v1"}
{"created":"2025-01-16 17:05:54","title":"Model Predictive Path Integral Docking of Fully Actuated Surface Vessel","abstract":"Autonomous docking remains one of the most challenging maneuvers in marine robotics, requiring precise control and robust perception in confined spaces. This paper presents a novel approach integrating Model Predictive Path Integral(MPPI) control with real-time LiDAR-based dock detection for autonomous surface vessel docking. Our framework uniquely combines probabilistic trajectory optimization with a multiobjective cost function that simultaneously considers docking precision, safety constraints, and motion efficiency. The MPPI controller generates optimal trajectories by intelligently sampling control sequences and evaluating their costs based on dynamic clearance requirements, orientation alignment, and target position objectives. We introduce an adaptive dock detection pipeline that processes LiDAR point clouds to extract critical geometric features, enabling real-time updates of docking parameters. The proposed method is extensively validated in a physics-based simulation environment that incorporates realistic sensor noise, vessel dynamics, and environmental constraints. Results demonstrate successful docking from various initial positions while maintaining safe clearances and smooth motion characteristics.","sentences":["Autonomous docking remains one of the most challenging maneuvers in marine robotics, requiring precise control and robust perception in confined spaces.","This paper presents a novel approach integrating Model Predictive Path Integral(MPPI) control with real-time LiDAR-based dock detection for autonomous surface vessel docking.","Our framework uniquely combines probabilistic trajectory optimization with a multiobjective cost function that simultaneously considers docking precision, safety constraints, and motion efficiency.","The MPPI controller generates optimal trajectories by intelligently sampling control sequences and evaluating their costs based on dynamic clearance requirements, orientation alignment, and target position objectives.","We introduce an adaptive dock detection pipeline that processes LiDAR point clouds to extract critical geometric features, enabling real-time updates of docking parameters.","The proposed method is extensively validated in a physics-based simulation environment that incorporates realistic sensor noise, vessel dynamics, and environmental constraints.","Results demonstrate successful docking from various initial positions while maintaining safe clearances and smooth motion characteristics."],"url":"http://arxiv.org/abs/2501.09668v1"}
{"created":"2025-01-16 17:05:41","title":"Unitary Expressions: A Necessary Abstraction for Extensible Quantum Programming Languages and Systems","abstract":"Quantum gates are the fundamental instructions of digital quantum computers. Current programming languages, systems, and software development toolkits identify these operational gates by their titles, which requires a shared understanding of their meanings. However, in the continuously developing software ecosystem surrounding quantum computing -- spanning high-level programming systems to low-level control stacks -- this identification process is often error-prone, challenging to debug, maintenance-heavy, and resistant to change. In this paper, we propose replacing this nominal gate representation with a functional one. We introduce the OpenQudit system for describing, parsing, optimizing, analyzing, and utilizing programs comprising gates described as symbolic unitary expressions. As part of this effort, we design the Qudit Gate Language (QGL), a unitary-specific expression language, and implement a differentiating just-in-time compiler in OpenQudit towards embedding this language in quantum programming languages and systems. Additionally, we have precisely designed and implemented the Qudit Virtual Machine (QVM) to evaluate quantum programs and their gradients efficiently. This evaluation is performed millions of times during the compilation of quantum programs. Our QVM can compute gradients approximately ten times faster than current leading numerical quantum compilation frameworks in the most common use cases. Altogether, the OpenQudit system is envisioned to (1) support many-level or qudit-based quantum systems, (2) enable the safe composition of program transformation tools, (3) accelerate circuit optimizers and transpilers, (4) enable compiler extensibility, and (5) provide a productive, simple-to-use interface to quantum practitioners.","sentences":["Quantum gates are the fundamental instructions of digital quantum computers.","Current programming languages, systems, and software development toolkits identify these operational gates by their titles, which requires a shared understanding of their meanings.","However, in the continuously developing software ecosystem surrounding quantum computing -- spanning high-level programming systems to low-level control stacks -- this identification process is often error-prone, challenging to debug, maintenance-heavy, and resistant to change.","In this paper, we propose replacing this nominal gate representation with a functional one.","We introduce the OpenQudit system for describing, parsing, optimizing, analyzing, and utilizing programs comprising gates described as symbolic unitary expressions.","As part of this effort, we design the Qudit Gate Language (QGL), a unitary-specific expression language, and implement a differentiating just-in-time compiler in OpenQudit towards embedding this language in quantum programming languages and systems.","Additionally, we have precisely designed and implemented the Qudit Virtual Machine (QVM) to evaluate quantum programs and their gradients efficiently.","This evaluation is performed millions of times during the compilation of quantum programs.","Our QVM can compute gradients approximately ten times faster than current leading numerical quantum compilation frameworks in the most common use cases.","Altogether, the OpenQudit system is envisioned to (1) support many-level or qudit-based quantum systems, (2) enable the safe composition of program transformation tools, (3) accelerate circuit optimizers and transpilers, (4) enable compiler extensibility, and (5) provide a productive, simple-to-use interface to quantum practitioners."],"url":"http://arxiv.org/abs/2501.09667v1"}
{"created":"2025-01-16 17:04:16","title":"Evaluating the diversity of scientific discourse on twenty-one multilingual Wikipedias using citation analysis","abstract":"INTRODUCTION: Wikipedia is a major source of information, particularly for medical and health content, citing over 4 million scholarly publications. However, the representation of research-based knowledge across different languages on Wikipedia has been under explored. This study analyses the largest database of Wikipedia citations collected to date, examining the uniqueness of content and research representation across languages. METHOD: The study included nearly 3.5 million unique research articles and their Wikipedia mentions from 21 languages. These were categorized into three groups: Group A (publications uniquely cited by a single non-English Wikipedia), Group B (co-cited by English and non-English Wikipedias), and Group C (co-cited by multiple non-English Wikipedias). Descriptive and comparative statistics were conducted by Wikipedia language, group, and discipline. RESULTS: Significant differences were found between twenty non-English languages and English Wikipedia (p<0.001). While English Wikipedia is the largest, non-English Wikipedias cite an additional 1.5 million publications. CONCLUSION: English Wikipedia should not be seen as a comprehensive body of information. Non-English Wikipedias cover unique subjects and disciplines, offering a more complete representation of research collectively. The uniqueness of voice in non-English Wikipedias correlates with their size, though other factors may also influence these differences.","sentences":["INTRODUCTION:","Wikipedia is a major source of information, particularly for medical and health content, citing over 4 million scholarly publications.","However, the representation of research-based knowledge across different languages on Wikipedia has been under explored.","This study analyses the largest database of Wikipedia citations collected to date, examining the uniqueness of content and research representation across languages.","METHOD:","The study included nearly 3.5 million unique research articles and their Wikipedia mentions from 21 languages.","These were categorized into three groups: Group A (publications uniquely cited by a single non-English Wikipedia), Group B (co-cited by English and non-English Wikipedias), and Group C (co-cited by multiple non-English Wikipedias).","Descriptive and comparative statistics were conducted by Wikipedia language, group, and discipline.","RESULTS:","Significant differences were found between twenty non-English languages and English Wikipedia (p<0.001).","While English Wikipedia is the largest, non-English Wikipedias cite an additional 1.5 million publications.","CONCLUSION:","English Wikipedia should not be seen as a comprehensive body of information.","Non-English Wikipedias cover unique subjects and disciplines, offering a more complete representation of research collectively.","The uniqueness of voice in non-English Wikipedias correlates with their size, though other factors may also influence these differences."],"url":"http://arxiv.org/abs/2501.09666v1"}
{"created":"2025-01-16 16:54:40","title":"Fokker-Planck to Callan-Symanzik: evolution of weight matrices under training","abstract":"The dynamical evolution of a neural network during training has been an incredibly fascinating subject of study. First principal derivation of generic evolution of variables in statistical physics systems has proved useful when used to describe training dynamics conceptually, which in practice means numerically solving equations such as Fokker-Planck equation. Simulating entire networks inevitably runs into the curse of dimensionality. In this paper, we utilize Fokker-Planck to simulate the probability density evolution of individual weight matrices in the bottleneck layers of a simple 2-bottleneck-layered auto-encoder and compare the theoretical evolutions against the empirical ones by examining the output data distributions. We also derive physically relevant partial differential equations such as Callan-Symanzik and Kardar-Parisi-Zhang equations from the dynamical equation we have.","sentences":["The dynamical evolution of a neural network during training has been an incredibly fascinating subject of study.","First principal derivation of generic evolution of variables in statistical physics systems has proved useful when used to describe training dynamics conceptually, which in practice means numerically solving equations such as Fokker-Planck equation.","Simulating entire networks inevitably runs into the curse of dimensionality.","In this paper, we utilize Fokker-Planck to simulate the probability density evolution of individual weight matrices in the bottleneck layers of a simple 2-bottleneck-layered auto-encoder and compare the theoretical evolutions against the empirical ones by examining the output data distributions.","We also derive physically relevant partial differential equations such as Callan-Symanzik and Kardar-Parisi-Zhang equations from the dynamical equation we have."],"url":"http://arxiv.org/abs/2501.09659v1"}
{"created":"2025-01-16 16:52:55","title":"On the Energy Consumption of Test Generation","abstract":"Research in the area of automated test generation has seen remarkable progress in recent years, resulting in several approaches and tools for effective and efficient generation of test cases. In particular, the EvoSuite tool has been at the forefront of this progress embodying various algorithms for automated test generation of Java programs. EvoSuite has been used to generate test cases for a wide variety of programs as well. While there are a number of empirical studies that report results on the effectiveness, in terms of code coverage and other related metrics, of the various test generation strategies and algorithms implemented in EvoSuite, there are no studies, to the best of our knowledge, on the energy consumption associated to the automated test generation. In this paper, we set out to investigate this aspect by measuring the energy consumed by EvoSuite when generating tests. We also measure the energy consumed in the execution of the test cases generated, comparing them with those manually written by developers. The results show that the different test generation algorithms consumed different amounts of energy, in particular on classes with high cyclomatic complexity. Furthermore, we also observe that manual tests tend to consume more energy as compared to automatically generated tests, without necessarily achieving higher code coverage. Our results also give insight into the methods that consume significantly higher levels of energy, indicating potential points of improvement both for EvoSuite as well as the different programs under test.","sentences":["Research in the area of automated test generation has seen remarkable progress in recent years, resulting in several approaches and tools for effective and efficient generation of test cases.","In particular, the EvoSuite tool has been at the forefront of this progress embodying various algorithms for automated test generation of Java programs.","EvoSuite has been used to generate test cases for a wide variety of programs as well.","While there are a number of empirical studies that report results on the effectiveness, in terms of code coverage and other related metrics, of the various test generation strategies and algorithms implemented in EvoSuite, there are no studies, to the best of our knowledge, on the energy consumption associated to the automated test generation.","In this paper, we set out to investigate this aspect by measuring the energy consumed by EvoSuite when generating tests.","We also measure the energy consumed in the execution of the test cases generated, comparing them with those manually written by developers.","The results show that the different test generation algorithms consumed different amounts of energy, in particular on classes with high cyclomatic complexity.","Furthermore, we also observe that manual tests tend to consume more energy as compared to automatically generated tests, without necessarily achieving higher code coverage.","Our results also give insight into the methods that consume significantly higher levels of energy, indicating potential points of improvement both for EvoSuite as well as the different programs under test."],"url":"http://arxiv.org/abs/2501.09657v1"}
{"created":"2025-01-16 16:51:59","title":"A Survey of Research in Large Language Models for Electronic Design Automation","abstract":"Within the rapidly evolving domain of Electronic Design Automation (EDA), Large Language Models (LLMs) have emerged as transformative technologies, offering unprecedented capabilities for optimizing and automating various aspects of electronic design. This survey provides a comprehensive exploration of LLM applications in EDA, focusing on advancements in model architectures, the implications of varying model sizes, and innovative customization techniques that enable tailored analytical insights. By examining the intersection of LLM capabilities and EDA requirements, the paper highlights the significant impact these models have on extracting nuanced understandings from complex datasets. Furthermore, it addresses the challenges and opportunities in integrating LLMs into EDA workflows, paving the way for future research and application in this dynamic field. Through this detailed analysis, the survey aims to offer valuable insights to professionals in the EDA industry, AI researchers, and anyone interested in the convergence of advanced AI technologies and electronic design.","sentences":["Within the rapidly evolving domain of Electronic Design Automation (EDA), Large Language Models (LLMs) have emerged as transformative technologies, offering unprecedented capabilities for optimizing and automating various aspects of electronic design.","This survey provides a comprehensive exploration of LLM applications in EDA, focusing on advancements in model architectures, the implications of varying model sizes, and innovative customization techniques that enable tailored analytical insights.","By examining the intersection of LLM capabilities and EDA requirements, the paper highlights the significant impact these models have on extracting nuanced understandings from complex datasets.","Furthermore, it addresses the challenges and opportunities in integrating LLMs into EDA workflows, paving the way for future research and application in this dynamic field.","Through this detailed analysis, the survey aims to offer valuable insights to professionals in the EDA industry, AI researchers, and anyone interested in the convergence of advanced AI technologies and electronic design."],"url":"http://arxiv.org/abs/2501.09655v1"}
{"created":"2025-01-16 16:48:41","title":"The Heap: A Contamination-Free Multilingual Code Dataset for Evaluating Large Language Models","abstract":"The recent rise in the popularity of large language models has spurred the development of extensive code datasets needed to train them. This has left limited code available for collection and use in the downstream investigation of specific behaviors, or evaluation of large language models without suffering from data contamination. To address this problem, we release The Heap, a large multilingual dataset covering 57 programming languages that has been deduplicated with respect to other open datasets of code, enabling researchers to conduct fair evaluations of large language models without significant data cleaning overhead.","sentences":["The recent rise in the popularity of large language models has spurred the development of extensive code datasets needed to train them.","This has left limited code available for collection and use in the downstream investigation of specific behaviors, or evaluation of large language models without suffering from data contamination.","To address this problem, we release The Heap, a large multilingual dataset covering 57 programming languages that has been deduplicated with respect to other open datasets of code, enabling researchers to conduct fair evaluations of large language models without significant data cleaning overhead."],"url":"http://arxiv.org/abs/2501.09653v1"}
{"created":"2025-01-16 16:45:08","title":"Monte Carlo Tree Search with Velocity Obstacles for safe and efficient motion planning in dynamic environments","abstract":"Online motion planning is a challenging problem for intelligent robots moving in dense environments with dynamic obstacles, e.g., crowds. In this work, we propose a novel approach for optimal and safe online motion planning with minimal information about dynamic obstacles. Specifically, our approach requires only the current position of the obstacles and their maximum speed, but it does not need any information about their exact trajectories or dynamic model. The proposed methodology combines Monte Carlo Tree Search (MCTS), for online optimal planning via model simulations, with Velocity Obstacles (VO), for obstacle avoidance. We perform experiments in a cluttered simulated environment with walls, and up to 40 dynamic obstacles moving with random velocities and directions. With an ablation study, we show the key contribution of VO in scaling up the efficiency of MCTS, selecting the safest and most rewarding actions in the tree of simulations. Moreover, we show the superiority of our methodology with respect to state-of-the-art planners, including Non-linear Model Predictive Control (NMPC), in terms of improved collision rate, computational and task performance.","sentences":["Online motion planning is a challenging problem for intelligent robots moving in dense environments with dynamic obstacles, e.g., crowds.","In this work, we propose a novel approach for optimal and safe online motion planning with minimal information about dynamic obstacles.","Specifically, our approach requires only the current position of the obstacles and their maximum speed, but it does not need any information about their exact trajectories or dynamic model.","The proposed methodology combines Monte Carlo Tree Search (MCTS), for online optimal planning via model simulations, with Velocity Obstacles (VO), for obstacle avoidance.","We perform experiments in a cluttered simulated environment with walls, and up to 40 dynamic obstacles moving with random velocities and directions.","With an ablation study, we show the key contribution of VO in scaling up the efficiency of MCTS, selecting the safest and most rewarding actions in the tree of simulations.","Moreover, we show the superiority of our methodology with respect to state-of-the-art planners, including Non-linear Model Predictive Control (NMPC), in terms of improved collision rate, computational and task performance."],"url":"http://arxiv.org/abs/2501.09649v1"}
{"created":"2025-01-16 16:38:33","title":"NS-Gym: Open-Source Simulation Environments and Benchmarks for Non-Stationary Markov Decision Processes","abstract":"In many real-world applications, agents must make sequential decisions in environments where conditions are subject to change due to various exogenous factors. These non-stationary environments pose significant challenges to traditional decision-making models, which typically assume stationary dynamics. Non-stationary Markov decision processes (NS-MDPs) offer a framework to model and solve decision problems under such changing conditions. However, the lack of standardized benchmarks and simulation tools has hindered systematic evaluation and advance in this field. We present NS-Gym, the first simulation toolkit designed explicitly for NS-MDPs, integrated within the popular Gymnasium framework. In NS-Gym, we segregate the evolution of the environmental parameters that characterize non-stationarity from the agent's decision-making module, allowing for modular and flexible adaptations to dynamic environments. We review prior work in this domain and present a toolkit encapsulating key problem characteristics and types in NS-MDPs. This toolkit is the first effort to develop a set of standardized interfaces and benchmark problems to enable consistent and reproducible evaluation of algorithms under non-stationary conditions. We also benchmark six algorithmic approaches from prior work on NS-MDPs using NS-Gym. Our vision is that NS-Gym will enable researchers to assess the adaptability and robustness of their decision-making algorithms to non-stationary conditions.","sentences":["In many real-world applications, agents must make sequential decisions in environments where conditions are subject to change due to various exogenous factors.","These non-stationary environments pose significant challenges to traditional decision-making models, which typically assume stationary dynamics.","Non-stationary Markov decision processes (NS-MDPs) offer a framework to model and solve decision problems under such changing conditions.","However, the lack of standardized benchmarks and simulation tools has hindered systematic evaluation and advance in this field.","We present NS-Gym, the first simulation toolkit designed explicitly for NS-MDPs, integrated within the popular Gymnasium framework.","In NS-Gym, we segregate the evolution of the environmental parameters that characterize non-stationarity from the agent's decision-making module, allowing for modular and flexible adaptations to dynamic environments.","We review prior work in this domain and present a toolkit encapsulating key problem characteristics and types in NS-MDPs.","This toolkit is the first effort to develop a set of standardized interfaces and benchmark problems to enable consistent and reproducible evaluation of algorithms under non-stationary conditions.","We also benchmark six algorithmic approaches from prior work on NS-MDPs using NS-Gym.","Our vision is that NS-Gym will enable researchers to assess the adaptability and robustness of their decision-making algorithms to non-stationary conditions."],"url":"http://arxiv.org/abs/2501.09646v1"}
{"created":"2025-01-16 16:37:33","title":"CarMem: Enhancing Long-Term Memory in LLM Voice Assistants through Category-Bounding","abstract":"In today's assistant landscape, personalisation enhances interactions, fosters long-term relationships, and deepens engagement. However, many systems struggle with retaining user preferences, leading to repetitive user requests and disengagement. Furthermore, the unregulated and opaque extraction of user preferences in industry applications raises significant concerns about privacy and trust, especially in regions with stringent regulations like Europe. In response to these challenges, we propose a long-term memory system for voice assistants, structured around predefined categories. This approach leverages Large Language Models to efficiently extract, store, and retrieve preferences within these categories, ensuring both personalisation and transparency. We also introduce a synthetic multi-turn, multi-session conversation dataset (CarMem), grounded in real industry data, tailored to an in-car voice assistant setting. Benchmarked on the dataset, our system achieves an F1-score of .78 to .95 in preference extraction, depending on category granularity. Our maintenance strategy reduces redundant preferences by 95% and contradictory ones by 92%, while the accuracy of optimal retrieval is at .87. Collectively, the results demonstrate the system's suitability for industrial applications.","sentences":["In today's assistant landscape, personalisation enhances interactions, fosters long-term relationships, and deepens engagement.","However, many systems struggle with retaining user preferences, leading to repetitive user requests and disengagement.","Furthermore, the unregulated and opaque extraction of user preferences in industry applications raises significant concerns about privacy and trust, especially in regions with stringent regulations like Europe.","In response to these challenges, we propose a long-term memory system for voice assistants, structured around predefined categories.","This approach leverages Large Language Models to efficiently extract, store, and retrieve preferences within these categories, ensuring both personalisation and transparency.","We also introduce a synthetic multi-turn, multi-session conversation dataset (CarMem), grounded in real industry data, tailored to an in-car voice assistant setting.","Benchmarked on the dataset, our system achieves an F1-score of .78 to .95 in preference extraction, depending on category granularity.","Our maintenance strategy reduces redundant preferences by 95% and contradictory ones by 92%, while the accuracy of optimal retrieval is at .87.","Collectively, the results demonstrate the system's suitability for industrial applications."],"url":"http://arxiv.org/abs/2501.09645v1"}
{"created":"2025-01-16 16:30:02","title":"Electronic Health Records: Towards Digital Twins in Healthcare","abstract":"The pivotal shift from traditional paper-based records to sophisticated Electronic Health Records (EHR), enabled systematic collection and analysis of patient data through descriptive statistics, providing insight into patterns and trends across patient populations. This evolution continued toward predictive analytics, allowing healthcare providers to anticipate patient outcomes and potential complications before they occur. This progression from basic digital record-keeping to sophisticated predictive modelling and digital twins reflects healthcare's broader evolution toward more integrated, patient-centred approaches that combine data-driven insights with personalized care delivery. This chapter explores the evolution and significance of healthcare information systems, beginning with an examination of the implementation of EHR in the UK and the USA. It provides a comprehensive overview of the International Classification of Diseases (ICD) system, tracing its development from ICD-9 to ICD-10. Central to this discussion is the MIMIC-III database, a landmark achievement in healthcare data sharing and arguably the most comprehensive critical care database freely available to researchers worldwide. MIMIC-III has democratized access to high-quality healthcare data, enabling unprecedented opportunities for research and analysis. The chapter examines its structure, clinical outcome analysis capabilities, and practical applications through case studies, with a particular focus on mortality and length of stay metrics, vital signs extraction, and ICD coding. Through detailed entity-relationship diagrams and practical examples, the text illustrates MIMIC's complex data structure and demonstrates how different querying approaches can lead to subtly different results, emphasizing the critical importance of understanding the database's architecture for accurate data extraction.","sentences":["The pivotal shift from traditional paper-based records to sophisticated Electronic Health Records (EHR), enabled systematic collection and analysis of patient data through descriptive statistics, providing insight into patterns and trends across patient populations.","This evolution continued toward predictive analytics, allowing healthcare providers to anticipate patient outcomes and potential complications before they occur.","This progression from basic digital record-keeping to sophisticated predictive modelling and digital twins reflects healthcare's broader evolution toward more integrated, patient-centred approaches that combine data-driven insights with personalized care delivery.","This chapter explores the evolution and significance of healthcare information systems, beginning with an examination of the implementation of EHR in the UK and the USA.","It provides a comprehensive overview of the International Classification of Diseases (ICD) system, tracing its development from ICD-9 to ICD-10.","Central to this discussion is the MIMIC-III database, a landmark achievement in healthcare data sharing and arguably the most comprehensive critical care database freely available to researchers worldwide.","MIMIC-III has democratized access to high-quality healthcare data, enabling unprecedented opportunities for research and analysis.","The chapter examines its structure, clinical outcome analysis capabilities, and practical applications through case studies, with a particular focus on mortality and length of stay metrics, vital signs extraction, and ICD coding.","Through detailed entity-relationship diagrams and practical examples, the text illustrates MIMIC's complex data structure and demonstrates how different querying approaches can lead to subtly different results, emphasizing the critical importance of understanding the database's architecture for accurate data extraction."],"url":"http://arxiv.org/abs/2501.09640v1"}
{"created":"2025-01-16 16:25:30","title":"LLM-Based Routing in Mixture of Experts: A Novel Framework for Trading","abstract":"Recent advances in deep learning and large language models (LLMs) have facilitated the deployment of the mixture-of-experts (MoE) mechanism in the stock investment domain. While these models have demonstrated promising trading performance, they are often unimodal, neglecting the wealth of information available in other modalities, such as textual data. Moreover, the traditional neural network-based router selection mechanism fails to consider contextual and real-world nuances, resulting in suboptimal expert selection. To address these limitations, we propose LLMoE, a novel framework that employs LLMs as the router within the MoE architecture. Specifically, we replace the conventional neural network-based router with LLMs, leveraging their extensive world knowledge and reasoning capabilities to select experts based on historical price data and stock news. This approach provides a more effective and interpretable selection mechanism. Our experiments on multimodal real-world stock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models and other deep neural network approaches. Additionally, the flexible architecture of LLMoE allows for easy adaptation to various downstream tasks.","sentences":["Recent advances in deep learning and large language models (LLMs) have facilitated the deployment of the mixture-of-experts (MoE) mechanism in the stock investment domain.","While these models have demonstrated promising trading performance, they are often unimodal, neglecting the wealth of information available in other modalities, such as textual data.","Moreover, the traditional neural network-based router selection mechanism fails to consider contextual and real-world nuances, resulting in suboptimal expert selection.","To address these limitations, we propose LLMoE, a novel framework that employs LLMs as the router within the MoE architecture.","Specifically, we replace the conventional neural network-based router with LLMs, leveraging their extensive world knowledge and reasoning capabilities to select experts based on historical price data and stock news.","This approach provides a more effective and interpretable selection mechanism.","Our experiments on multimodal real-world stock datasets demonstrate that LLMoE outperforms state-of-the-art MoE models and other deep neural network approaches.","Additionally, the flexible architecture of LLMoE allows for easy adaptation to various downstream tasks."],"url":"http://arxiv.org/abs/2501.09636v1"}
{"created":"2025-01-16 16:24:21","title":"Unified Face Matching and Physical-Digital Spoofing Attack Detection","abstract":"Face recognition technology has dramatically transformed the landscape of security, surveillance, and authentication systems, offering a user-friendly and non-invasive biometric solution. However, despite its significant advantages, face recognition systems face increasing threats from physical and digital spoofing attacks. Current research typically treats face recognition and attack detection as distinct classification challenges. This approach necessitates the implementation of separate models for each task, leading to considerable computational complexity, particularly on devices with limited resources. Such inefficiencies can stifle scalability and hinder performance. In response to these challenges, this paper introduces an innovative unified model designed for face recognition and detection of physical and digital attacks. By leveraging the advanced Swin Transformer backbone and incorporating HiLo attention in a convolutional neural network framework, we address unified face recognition and spoof attack detection more effectively. Moreover, we introduce augmentation techniques that replicate the traits of physical and digital spoofing cues, significantly enhancing our model robustness. Through comprehensive experimental evaluation across various datasets, we showcase the effectiveness of our model in unified face recognition and spoof detection. Additionally, we confirm its resilience against unseen physical and digital spoofing attacks, underscoring its potential for real-world applications.","sentences":["Face recognition technology has dramatically transformed the landscape of security, surveillance, and authentication systems, offering a user-friendly and non-invasive biometric solution.","However, despite its significant advantages, face recognition systems face increasing threats from physical and digital spoofing attacks.","Current research typically treats face recognition and attack detection as distinct classification challenges.","This approach necessitates the implementation of separate models for each task, leading to considerable computational complexity, particularly on devices with limited resources.","Such inefficiencies can stifle scalability and hinder performance.","In response to these challenges, this paper introduces an innovative unified model designed for face recognition and detection of physical and digital attacks.","By leveraging the advanced Swin Transformer backbone and incorporating HiLo attention in a convolutional neural network framework, we address unified face recognition and spoof attack detection more effectively.","Moreover, we introduce augmentation techniques that replicate the traits of physical and digital spoofing cues, significantly enhancing our model robustness.","Through comprehensive experimental evaluation across various datasets, we showcase the effectiveness of our model in unified face recognition and spoof detection.","Additionally, we confirm its resilience against unseen physical and digital spoofing attacks, underscoring its potential for real-world applications."],"url":"http://arxiv.org/abs/2501.09635v1"}
{"created":"2025-01-16 16:20:37","title":"Platform-Aware Mission Planning","abstract":"Planning for autonomous systems typically requires reasoning with models at different levels of abstraction, and the harmonization of two competing sets of objectives: high-level mission goals that refer to an interaction of the system with the external environment, and low-level platform constraints that aim to preserve the integrity and the correct interaction of the subsystems. The complicated interplay between these two models makes it very hard to reason on the system as a whole, especially when the objective is to find plans with robustness guarantees, considering the non-deterministic behavior of the lower layers of the system.   In this paper, we introduce the problem of Platform-Aware Mission Planning (PAMP), addressing it in the setting of temporal durative actions. The PAMP problem differs from standard temporal planning for its exists-forall nature: the high-level plan dealing with mission goals is required to satisfy safety and executability constraints, for all the possible non-deterministic executions of the low-level model of the platform and the environment. We propose two approaches for solving PAMP. The first baseline approach amalgamates the mission and platform levels, while the second is based on an abstraction-refinement loop that leverages the combination of a planner and a verification engine. We prove the soundness and completeness of the proposed approaches and validate them experimentally, demonstrating the importance of heterogeneous modeling and the superiority of the technique based on abstraction-refinement.","sentences":["Planning for autonomous systems typically requires reasoning with models at different levels of abstraction, and the harmonization of two competing sets of objectives: high-level mission goals that refer to an interaction of the system with the external environment, and low-level platform constraints that aim to preserve the integrity and the correct interaction of the subsystems.","The complicated interplay between these two models makes it very hard to reason on the system as a whole, especially when the objective is to find plans with robustness guarantees, considering the non-deterministic behavior of the lower layers of the system.   ","In this paper, we introduce the problem of Platform-Aware Mission Planning (PAMP), addressing it in the setting of temporal durative actions.","The PAMP problem differs from standard temporal planning for its exists-forall nature: the high-level plan dealing with mission goals is required to satisfy safety and executability constraints, for all the possible non-deterministic executions of the low-level model of the platform and the environment.","We propose two approaches for solving PAMP.","The first baseline approach amalgamates the mission and platform levels, while the second is based on an abstraction-refinement loop that leverages the combination of a planner and a verification engine.","We prove the soundness and completeness of the proposed approaches and validate them experimentally, demonstrating the importance of heterogeneous modeling and the superiority of the technique based on abstraction-refinement."],"url":"http://arxiv.org/abs/2501.09632v1"}
{"created":"2025-01-16 16:19:53","title":"Empowering Large Language Models in Wireless Communication: A Novel Dataset and Fine-Tuning Framework","abstract":"In this work, we develop a specialized dataset aimed at enhancing the evaluation and fine-tuning of large language models (LLMs) specifically for wireless communication applications. The dataset includes a diverse set of multi-hop questions, including true/false and multiple-choice types, spanning varying difficulty levels from easy to hard. By utilizing advanced language models for entity extraction and question generation, rigorous data curation processes are employed to maintain high quality and relevance. Additionally, we introduce a Pointwise V-Information (PVI) based fine-tuning method, providing a detailed theoretical analysis and justification for its use in quantifying the information content of training data with 2.24\\% and 1.31\\% performance boost for different models compared to baselines, respectively. To demonstrate the effectiveness of the fine-tuned models with the proposed methodologies on practical tasks, we also consider different tasks, including summarizing optimization problems from technical papers and solving the mathematical problems related to non-orthogonal multiple access (NOMA), which are generated by using the proposed multi-agent framework. Simulation results show significant performance gain in summarization tasks with 20.9\\% in the ROUGE-L metrics. We also study the scaling laws of fine-tuning LLMs and the challenges LLMs face in the field of wireless communications, offering insights into their adaptation to wireless communication tasks. This dataset and fine-tuning methodology aim to enhance the training and evaluation of LLMs, contributing to advancements in LLMs for wireless communication research and applications.","sentences":["In this work, we develop a specialized dataset aimed at enhancing the evaluation and fine-tuning of large language models (LLMs) specifically for wireless communication applications.","The dataset includes a diverse set of multi-hop questions, including true/false and multiple-choice types, spanning varying difficulty levels from easy to hard.","By utilizing advanced language models for entity extraction and question generation, rigorous data curation processes are employed to maintain high quality and relevance.","Additionally, we introduce a Pointwise V-Information (PVI) based fine-tuning method, providing a detailed theoretical analysis and justification for its use in quantifying the information content of training data with 2.24\\% and 1.31\\% performance boost for different models compared to baselines, respectively.","To demonstrate the effectiveness of the fine-tuned models with the proposed methodologies on practical tasks, we also consider different tasks, including summarizing optimization problems from technical papers and solving the mathematical problems related to non-orthogonal multiple access (NOMA), which are generated by using the proposed multi-agent framework.","Simulation results show significant performance gain in summarization tasks with 20.9\\% in the ROUGE-L metrics.","We also study the scaling laws of fine-tuning LLMs and the challenges LLMs face in the field of wireless communications, offering insights into their adaptation to wireless communication tasks.","This dataset and fine-tuning methodology aim to enhance the training and evaluation of LLMs, contributing to advancements in LLMs for wireless communication research and applications."],"url":"http://arxiv.org/abs/2501.09631v1"}
{"created":"2025-01-16 16:17:39","title":"Artificial Intelligence-Driven Clinical Decision Support Systems","abstract":"As artificial intelligence (AI) becomes increasingly embedded in healthcare delivery, this chapter explores the critical aspects of developing reliable and ethical Clinical Decision Support Systems (CDSS). Beginning with the fundamental transition from traditional statistical models to sophisticated machine learning approaches, this work examines rigorous validation strategies and performance assessment methods, including the crucial role of model calibration and decision curve analysis. The chapter emphasizes that creating trustworthy AI systems in healthcare requires more than just technical accuracy; it demands careful consideration of fairness, explainability, and privacy. The challenge of ensuring equitable healthcare delivery through AI is stressed, discussing methods to identify and mitigate bias in clinical predictive models. The chapter then delves into explainability as a cornerstone of human-centered CDSS. This focus reflects the understanding that healthcare professionals must not only trust AI recommendations but also comprehend their underlying reasoning. The discussion advances in an analysis of privacy vulnerabilities in medical AI systems, from data leakage in deep learning models to sophisticated attacks against model explanations. The text explores privacy-preservation strategies such as differential privacy and federated learning, while acknowledging the inherent trade-offs between privacy protection and model performance. This progression, from technical validation to ethical considerations, reflects the multifaceted challenges of developing AI systems that can be seamlessly and reliably integrated into daily clinical practice while maintaining the highest standards of patient care and data protection.","sentences":["As artificial intelligence (AI) becomes increasingly embedded in healthcare delivery, this chapter explores the critical aspects of developing reliable and ethical Clinical Decision Support Systems (CDSS).","Beginning with the fundamental transition from traditional statistical models to sophisticated machine learning approaches, this work examines rigorous validation strategies and performance assessment methods, including the crucial role of model calibration and decision curve analysis.","The chapter emphasizes that creating trustworthy AI systems in healthcare requires more than just technical accuracy; it demands careful consideration of fairness, explainability, and privacy.","The challenge of ensuring equitable healthcare delivery through AI is stressed, discussing methods to identify and mitigate bias in clinical predictive models.","The chapter then delves into explainability as a cornerstone of human-centered CDSS.","This focus reflects the understanding that healthcare professionals must not only trust AI recommendations but also comprehend their underlying reasoning.","The discussion advances in an analysis of privacy vulnerabilities in medical AI systems, from data leakage in deep learning models to sophisticated attacks against model explanations.","The text explores privacy-preservation strategies such as differential privacy and federated learning, while acknowledging the inherent trade-offs between privacy protection and model performance.","This progression, from technical validation to ethical considerations, reflects the multifaceted challenges of developing AI systems that can be seamlessly and reliably integrated into daily clinical practice while maintaining the highest standards of patient care and data protection."],"url":"http://arxiv.org/abs/2501.09628v1"}
{"created":"2025-01-16 16:00:52","title":"Weight for Robustness: A Comprehensive Approach towards Optimal Fault-Tolerant Asynchronous ML","abstract":"We address the challenges of Byzantine-robust training in asynchronous distributed machine learning systems, aiming to enhance efficiency amid massive parallelization and heterogeneous computing resources. Asynchronous systems, marked by independently operating workers and intermittent updates, uniquely struggle with maintaining integrity against Byzantine failures, which encompass malicious or erroneous actions that disrupt learning. The inherent delays in such settings not only introduce additional bias to the system but also obscure the disruptions caused by Byzantine faults. To tackle these issues, we adapt the Byzantine framework to asynchronous dynamics by introducing a novel weighted robust aggregation framework. This allows for the extension of robust aggregators and a recent meta-aggregator to their weighted versions, mitigating the effects of delayed updates. By further incorporating a recent variance-reduction technique, we achieve an optimal convergence rate for the first time in an asynchronous Byzantine environment. Our methodology is rigorously validated through empirical and theoretical analysis, demonstrating its effectiveness in enhancing fault tolerance and optimizing performance in asynchronous ML systems.","sentences":["We address the challenges of Byzantine-robust training in asynchronous distributed machine learning systems, aiming to enhance efficiency amid massive parallelization and heterogeneous computing resources.","Asynchronous systems, marked by independently operating workers and intermittent updates, uniquely struggle with maintaining integrity against Byzantine failures, which encompass malicious or erroneous actions that disrupt learning.","The inherent delays in such settings not only introduce additional bias to the system but also obscure the disruptions caused by Byzantine faults.","To tackle these issues, we adapt the Byzantine framework to asynchronous dynamics by introducing a novel weighted robust aggregation framework.","This allows for the extension of robust aggregators and a recent meta-aggregator to their weighted versions, mitigating the effects of delayed updates.","By further incorporating a recent variance-reduction technique, we achieve an optimal convergence rate for the first time in an asynchronous Byzantine environment.","Our methodology is rigorously validated through empirical and theoretical analysis, demonstrating its effectiveness in enhancing fault tolerance and optimizing performance in asynchronous ML systems."],"url":"http://arxiv.org/abs/2501.09621v1"}
{"created":"2025-01-16 16:00:37","title":"Beyond Reward Hacking: Causal Rewards for Large Language Model Alignment","abstract":"Recent advances in large language models (LLMs) have demonstrated significant progress in performing complex tasks. While Reinforcement Learning from Human Feedback (RLHF) has been effective in aligning LLMs with human preferences, it is susceptible to spurious correlations in reward modeling. Consequently, it often introduces biases-such as length bias, sycophancy, conceptual bias, and discrimination that hinder the model's ability to capture true causal relationships. To address this, we propose a novel causal reward modeling approach that integrates causal inference to mitigate these spurious correlations. Our method enforces counterfactual invariance, ensuring reward predictions remain consistent when irrelevant variables are altered. Through experiments on both synthetic and real-world datasets, we show that our approach mitigates various types of spurious correlations effectively, resulting in more reliable and fair alignment of LLMs with human preferences. As a drop-in enhancement to the existing RLHF workflow, our causal reward modeling provides a practical way to improve the trustworthiness and fairness of LLM finetuning.","sentences":["Recent advances in large language models (LLMs) have demonstrated significant progress in performing complex tasks.","While Reinforcement Learning from Human Feedback (RLHF) has been effective in aligning LLMs with human preferences, it is susceptible to spurious correlations in reward modeling.","Consequently, it often introduces biases-such as length bias, sycophancy, conceptual bias, and discrimination that hinder the model's ability to capture true causal relationships.","To address this, we propose a novel causal reward modeling approach that integrates causal inference to mitigate these spurious correlations.","Our method enforces counterfactual invariance, ensuring reward predictions remain consistent when irrelevant variables are altered.","Through experiments on both synthetic and real-world datasets, we show that our approach mitigates various types of spurious correlations effectively, resulting in more reliable and fair alignment of LLMs with human preferences.","As a drop-in enhancement to the existing RLHF workflow, our causal reward modeling provides a practical way to improve the trustworthiness and fairness of LLM finetuning."],"url":"http://arxiv.org/abs/2501.09620v1"}
{"created":"2025-01-16 15:44:24","title":"WMamba: Wavelet-based Mamba for Face Forgery Detection","abstract":"With the rapid advancement of deepfake generation technologies, the demand for robust and accurate face forgery detection algorithms has become increasingly critical. Recent studies have demonstrated that wavelet analysis can uncover subtle forgery artifacts that remain imperceptible in the spatial domain. Wavelets effectively capture important facial contours, which are often slender, fine-grained, and global in nature. However, existing wavelet-based approaches fail to fully leverage these unique characteristics, resulting in sub-optimal feature extraction and limited generalizability. To address this challenge, we introduce WMamba, a novel wavelet-based feature extractor built upon the Mamba architecture. WMamba maximizes the utility of wavelet information through two key innovations. First, we propose Dynamic Contour Convolution (DCConv), which employs specially crafted deformable kernels to adaptively model slender facial contours. Second, by leveraging the Mamba architecture, our method captures long-range spatial relationships with linear computational complexity. This efficiency allows for the extraction of fine-grained, global forgery artifacts from small image patches. Extensive experimental results show that WMamba achieves state-of-the-art (SOTA) performance, highlighting its effectiveness and superiority in face forgery detection.","sentences":["With the rapid advancement of deepfake generation technologies, the demand for robust and accurate face forgery detection algorithms has become increasingly critical.","Recent studies have demonstrated that wavelet analysis can uncover subtle forgery artifacts that remain imperceptible in the spatial domain.","Wavelets effectively capture important facial contours, which are often slender, fine-grained, and global in nature.","However, existing wavelet-based approaches fail to fully leverage these unique characteristics, resulting in sub-optimal feature extraction and limited generalizability.","To address this challenge, we introduce WMamba, a novel wavelet-based feature extractor built upon the Mamba architecture.","WMamba maximizes the utility of wavelet information through two key innovations.","First, we propose Dynamic Contour Convolution (DCConv), which employs specially crafted deformable kernels to adaptively model slender facial contours.","Second, by leveraging the Mamba architecture, our method captures long-range spatial relationships with linear computational complexity.","This efficiency allows for the extraction of fine-grained, global forgery artifacts from small image patches.","Extensive experimental results show that WMamba achieves state-of-the-art (SOTA) performance, highlighting its effectiveness and superiority in face forgery detection."],"url":"http://arxiv.org/abs/2501.09617v1"}
{"created":"2025-01-16 15:43:32","title":"ARMAX identification of low rank graphical models","abstract":"In large-scale systems, complex internal relationships are often present. Such interconnected systems can be effectively described by low rank stochastic processes. When identifying a predictive model of low rank processes from sampling data, the rank-deficient property of spectral densities is often obscured by the inevitable measurement noise in practice. However, existing low rank identification approaches often did not take noise into explicit consideration, leading to non-negligible inaccuracies even under weak noise. In this paper, we address the identification issue of low rank processes under measurement noise. We find that the noisy measurement model admits a sparse plus low rank structure in latent-variable graphical models. Specifically, we first decompose the problem into a maximum entropy covariance extension problem, and a low rank graphical estimation problem based on an autoregressive moving-average with exogenous input (ARMAX) model. To identify the ARMAX low rank graphical models, we propose an estimation approach based on maximum likelihood. The identifiability and consistency of this approach are proven under certain conditions. Simulation results confirm the reliable performance of the entire algorithm in both the parameter estimation and noisy data filtering.","sentences":["In large-scale systems, complex internal relationships are often present.","Such interconnected systems can be effectively described by low rank stochastic processes.","When identifying a predictive model of low rank processes from sampling data, the rank-deficient property of spectral densities is often obscured by the inevitable measurement noise in practice.","However, existing low rank identification approaches often did not take noise into explicit consideration, leading to non-negligible inaccuracies even under weak noise.","In this paper, we address the identification issue of low rank processes under measurement noise.","We find that the noisy measurement model admits a sparse plus low rank structure in latent-variable graphical models.","Specifically, we first decompose the problem into a maximum entropy covariance extension problem, and a low rank graphical estimation problem based on an autoregressive moving-average with exogenous input (ARMAX) model.","To identify the ARMAX low rank graphical models, we propose an estimation approach based on maximum likelihood.","The identifiability and consistency of this approach are proven under certain conditions.","Simulation results confirm the reliable performance of the entire algorithm in both the parameter estimation and noisy data filtering."],"url":"http://arxiv.org/abs/2501.09616v1"}
{"created":"2025-01-16 15:35:48","title":"EVaDE : Event-Based Variational Thompson Sampling for Model-Based Reinforcement Learning","abstract":"Posterior Sampling for Reinforcement Learning (PSRL) is a well-known algorithm that augments model-based reinforcement learning (MBRL) algorithms with Thompson sampling. PSRL maintains posterior distributions of the environment transition dynamics and the reward function, which are intractable for tasks with high-dimensional state and action spaces. Recent works show that dropout, used in conjunction with neural networks, induces variational distributions that can approximate these posteriors. In this paper, we propose Event-based Variational Distributions for Exploration (EVaDE), which are variational distributions that are useful for MBRL, especially when the underlying domain is object-based. We leverage the general domain knowledge of object-based domains to design three types of event-based convolutional layers to direct exploration. These layers rely on Gaussian dropouts and are inserted between the layers of the deep neural network model to help facilitate variational Thompson sampling. We empirically show the effectiveness of EVaDE-equipped Simulated Policy Learning (EVaDE-SimPLe) on the 100K Atari game suite.","sentences":["Posterior Sampling for Reinforcement Learning (PSRL) is a well-known algorithm that augments model-based reinforcement learning (MBRL) algorithms with Thompson sampling.","PSRL maintains posterior distributions of the environment transition dynamics and the reward function, which are intractable for tasks with high-dimensional state and action spaces.","Recent works show that dropout, used in conjunction with neural networks, induces variational distributions that can approximate these posteriors.","In this paper, we propose Event-based Variational Distributions for Exploration (EVaDE), which are variational distributions that are useful for MBRL, especially when the underlying domain is object-based.","We leverage the general domain knowledge of object-based domains to design three types of event-based convolutional layers to direct exploration.","These layers rely on Gaussian dropouts and are inserted between the layers of the deep neural network model to help facilitate variational Thompson sampling.","We empirically show the effectiveness of EVaDE-equipped Simulated Policy Learning (EVaDE-SimPLe) on the 100K Atari game suite."],"url":"http://arxiv.org/abs/2501.09611v1"}
{"created":"2025-01-16 15:34:00","title":"Adversarial-Ensemble Kolmogorov Arnold Networks for Enhancing Indoor Wi-Fi Positioning: A Defensive Approach Against Spoofing and Signal Manipulation Attacks","abstract":"The research presents a study on enhancing the robustness of Wi-Fi-based indoor positioning systems against adversarial attacks. The goal is to improve the positioning accuracy and resilience of these systems under two attack scenarios: Wi-Fi Spoofing and Signal Strength Manipulation. Three models are developed and evaluated: a baseline model (M_Base), an adversarially trained robust model (M_Rob), and an ensemble model (M_Ens). All models utilize a Kolmogorov-Arnold Network (KAN) architecture. The robust model is trained with adversarially perturbed data, while the ensemble model combines predictions from both the base and robust models. Experimental results show that the robust model reduces positioning error by approximately 10% compared to the baseline, achieving 2.03 meters error under Wi-Fi spoofing and 2.00 meters under signal strength manipulation. The ensemble model further outperforms with errors of 2.01 meters and 1.975 meters for the respective attack types. This analysis highlights the effectiveness of adversarial training techniques in mitigating attack impacts. The findings underscore the importance of considering adversarial scenarios in developing indoor positioning systems, as improved resilience can significantly enhance the accuracy and reliability of such systems in mission-critical environments.","sentences":["The research presents a study on enhancing the robustness of Wi-Fi-based indoor positioning systems against adversarial attacks.","The goal is to improve the positioning accuracy and resilience of these systems under two attack scenarios: Wi-Fi Spoofing and Signal Strength Manipulation.","Three models are developed and evaluated: a baseline model (M_Base), an adversarially trained robust model (M_Rob), and an ensemble model (M_Ens).","All models utilize a Kolmogorov-Arnold Network (KAN) architecture.","The robust model is trained with adversarially perturbed data, while the ensemble model combines predictions from both the base and robust models.","Experimental results show that the robust model reduces positioning error by approximately 10% compared to the baseline, achieving 2.03 meters error under Wi-Fi spoofing and 2.00 meters under signal strength manipulation.","The ensemble model further outperforms with errors of 2.01 meters and 1.975 meters for the respective attack types.","This analysis highlights the effectiveness of adversarial training techniques in mitigating attack impacts.","The findings underscore the importance of considering adversarial scenarios in developing indoor positioning systems, as improved resilience can significantly enhance the accuracy and reliability of such systems in mission-critical environments."],"url":"http://arxiv.org/abs/2501.09609v1"}
{"created":"2025-01-16 15:32:41","title":"Metric Learning with Progressive Self-Distillation for Audio-Visual Embedding Learning","abstract":"Metric learning projects samples into an embedded space, where similarities and dissimilarities are quantified based on their learned representations. However, existing methods often rely on label-guided representation learning, where representations of different modalities, such as audio and visual data, are aligned based on annotated labels. This approach tends to underutilize latent complex features and potential relationships inherent in the distributions of audio and visual data that are not directly tied to the labels, resulting in suboptimal performance in audio-visual embedding learning. To address this issue, we propose a novel architecture that integrates cross-modal triplet loss with progressive self-distillation. Our method enhances representation learning by leveraging inherent distributions and dynamically refining soft audio-visual alignments -- probabilistic alignments between audio and visual data that capture the inherent relationships beyond explicit labels. Specifically, the model distills audio-visual distribution-based knowledge from annotated labels in a subset of each batch. This self-distilled knowledge is used t","sentences":["Metric learning projects samples into an embedded space, where similarities and dissimilarities are quantified based on their learned representations.","However, existing methods often rely on label-guided representation learning, where representations of different modalities, such as audio and visual data, are aligned based on annotated labels.","This approach tends to underutilize latent complex features and potential relationships inherent in the distributions of audio and visual data that are not directly tied to the labels, resulting in suboptimal performance in audio-visual embedding learning.","To address this issue, we propose a novel architecture that integrates cross-modal triplet loss with progressive self-distillation.","Our method enhances representation learning by leveraging inherent distributions and dynamically refining soft audio-visual alignments -- probabilistic alignments between audio and visual data that capture the inherent relationships beyond explicit labels.","Specifically, the model distills audio-visual distribution-based knowledge from annotated labels in a subset of each batch.","This self-distilled knowledge is used t"],"url":"http://arxiv.org/abs/2501.09608v1"}
{"created":"2025-01-16 15:25:58","title":"Local US officials' views on the impacts and governance of AI: Evidence from 2022 and 2023 survey waves","abstract":"This paper presents a survey of local US policymakers' views on the future impact and regulation of AI. Our survey provides insight into US policymakers' expectations regarding the effects of AI on local communities and the nation, as well as their attitudes towards specific regulatory policies. Conducted in two waves (2022 and 2023), the survey captures changes in attitudes following the release of ChatGPT and the subsequent surge in public awareness of AI. Local policymakers express a mix of concern, optimism, and uncertainty about AI's impacts, anticipating significant societal risks such as increased surveillance, misinformation, and political polarization, alongside potential benefits in innovation and infrastructure. Many also report feeling underprepared and inadequately informed to make AI-related decisions. On regulation, a majority of policymakers support government oversight and favor specific policies addressing issues such as data privacy, AI-related unemployment, and AI safety and fairness. Democrats show stronger and more consistent support for regulation than Republicans, but the latter experienced a notable shift towards majority support between 2022 and 2023. Our study contributes to understanding the perspectives of local policymakers-key players in shaping state and federal AI legislation-by capturing evolving attitudes, partisan dynamics, and their implications for policy formation. The findings highlight the need for capacity-building initiatives and bi-partisan coordination to mitigate policy fragmentation and build a cohesive framework for AI governance in the US.","sentences":["This paper presents a survey of local US policymakers' views on the future impact and regulation of AI.","Our survey provides insight into US policymakers' expectations regarding the effects of AI on local communities and the nation, as well as their attitudes towards specific regulatory policies.","Conducted in two waves (2022 and 2023), the survey captures changes in attitudes following the release of ChatGPT and the subsequent surge in public awareness of AI.","Local policymakers express a mix of concern, optimism, and uncertainty about AI's impacts, anticipating significant societal risks such as increased surveillance, misinformation, and political polarization, alongside potential benefits in innovation and infrastructure.","Many also report feeling underprepared and inadequately informed to make AI-related decisions.","On regulation, a majority of policymakers support government oversight and favor specific policies addressing issues such as data privacy, AI-related unemployment, and AI safety and fairness.","Democrats show stronger and more consistent support for regulation than Republicans, but the latter experienced a notable shift towards majority support between 2022 and 2023.","Our study contributes to understanding the perspectives of local policymakers-key players in shaping state and federal AI legislation-by capturing evolving attitudes, partisan dynamics, and their implications for policy formation.","The findings highlight the need for capacity-building initiatives and bi-partisan coordination to mitigate policy fragmentation and build a cohesive framework for AI governance in the US."],"url":"http://arxiv.org/abs/2501.09606v1"}
{"created":"2025-01-16 15:25:44","title":"Managed-Retention Memory: A New Class of Memory for the AI Era","abstract":"AI clusters today are one of the major uses of High Bandwidth Memory (HBM). However, HBM is suboptimal for AI workloads for several reasons. Analysis shows HBM is overprovisioned on write performance, but underprovisioned on density and read bandwidth, and also has significant energy per bit overheads. It is also expensive, with lower yield than DRAM due to manufacturing complexity. We propose a new memory class: Managed-Retention Memory (MRM), which is more optimized to store key data structures for AI inference workloads. We believe that MRM may finally provide a path to viability for technologies that were originally proposed to support Storage Class Memory (SCM). These technologies traditionally offered long-term persistence (10+ years) but provided poor IO performance and/or endurance. MRM makes different trade-offs, and by understanding the workload IO patterns, MRM foregoes long-term data retention and write performance for better potential performance on the metrics important for these workloads.","sentences":["AI clusters today are one of the major uses of High Bandwidth Memory (HBM).","However, HBM is suboptimal for AI workloads for several reasons.","Analysis shows HBM is overprovisioned on write performance, but underprovisioned on density and read bandwidth, and also has significant energy per bit overheads.","It is also expensive, with lower yield than DRAM due to manufacturing complexity.","We propose a new memory class: Managed-Retention Memory (MRM), which is more optimized to store key data structures for AI inference workloads.","We believe that MRM may finally provide a path to viability for technologies that were originally proposed to support Storage Class Memory (SCM).","These technologies traditionally offered long-term persistence (10+ years) but provided poor IO performance and/or endurance.","MRM makes different trade-offs, and by understanding the workload IO patterns, MRM foregoes long-term data retention and write performance for better potential performance on the metrics important for these workloads."],"url":"http://arxiv.org/abs/2501.09605v1"}
{"created":"2025-01-16 15:24:41","title":"From Scarcity to Capability: Empowering Fake News Detection in Low-Resource Languages with LLMs","abstract":"The rapid spread of fake news presents a significant global challenge, particularly in low-resource languages like Bangla, which lack adequate datasets and detection tools. Although manual fact-checking is accurate, it is expensive and slow to prevent the dissemination of fake news. Addressing this gap, we introduce BanFakeNews-2.0, a robust dataset to enhance Bangla fake news detection. This version includes 11,700 additional, meticulously curated fake news articles validated from credible sources, creating a proportional dataset of 47,000 authentic and 13,000 fake news items across 13 categories. In addition, we created a manually curated independent test set of 460 fake and 540 authentic news items for rigorous evaluation. We invest efforts in collecting fake news from credible sources and manually verified while preserving the linguistic richness. We develop a benchmark system utilizing transformer-based architectures, including fine-tuned Bidirectional Encoder Representations from Transformers variants (F1-87\\%) and Large Language Models with Quantized Low-Rank Approximation (F1-89\\%), that significantly outperforms traditional methods. BanFakeNews-2.0 offers a valuable resource to advance research and application in fake news detection for low-resourced languages. We publicly release our dataset and model on Github to foster research in this direction.","sentences":["The rapid spread of fake news presents a significant global challenge, particularly in low-resource languages like Bangla, which lack adequate datasets and detection tools.","Although manual fact-checking is accurate, it is expensive and slow to prevent the dissemination of fake news.","Addressing this gap, we introduce BanFakeNews-2.0, a robust dataset to enhance Bangla fake news detection.","This version includes 11,700 additional, meticulously curated fake news articles validated from credible sources, creating a proportional dataset of 47,000 authentic and 13,000 fake news items across 13 categories.","In addition, we created a manually curated independent test set of 460 fake and 540 authentic news items for rigorous evaluation.","We invest efforts in collecting fake news from credible sources and manually verified while preserving the linguistic richness.","We develop a benchmark system utilizing transformer-based architectures, including fine-tuned Bidirectional Encoder Representations from Transformers variants (F1-87\\%) and Large Language Models with Quantized Low-Rank Approximation (F1-89\\%), that significantly outperforms traditional methods.","BanFakeNews-2.0 offers a valuable resource to advance research and application in fake news detection for low-resourced languages.","We publicly release our dataset and model on Github to foster research in this direction."],"url":"http://arxiv.org/abs/2501.09604v1"}
{"created":"2025-01-16 15:22:26","title":"Anatomy of a Digital Bubble: Lessons Learned from the NFT and Metaverse Frenzy","abstract":"In the past few years, \"metaverse\" and \"non-fungible tokens (NFT)\" have become buzzwords, and the prices of related assets have shown speculative bubble-like behavior. In this paper, we attempt to better understand the underlying economic dynamics. To do so, we look at Decentraland, a virtual world platform where land parcels are sold as NFT collections. We find that initially, land prices followed traditional real estate pricing models -- in particular, value decreased with distance from the most desirable areas -- suggesting Decentraland behaved much like a virtual city. However, these real estate pricing models stopped applying when both the metaverse and NFTs gained increased popular attention and enthusiasm in 2021, suggesting a new driving force for the underlying asset prices. At that time, following a substantial rise in NFT market values, short-term holders of multiple parcels began to take major selling positions in the Decentraland market, which hints that, rather than building a metaverse community, early Decentraland investors preferred to cash out when land valuations became overly inflated. Our analysis also shows that while the majority of buyers are new entrants to the market (many of whom joined during the bubble), liquidity (i.e., parcels) was mostly provided by early adopters selling, which caused stark differences in monetary gains. Early adopters made money -- more than 10,000 USD on average per parcel sold -- but users who joined later typically made no profit or even incurred losses in the order of 1,000 USD per parcel. Unlike established markets such as financial and real estate markets, newly emergent digital marketplaces are mostly self-regulated. As a result, the significant financial risks we identify indicate a strong need for establishing appropriate standards of business conduct and improving user awareness.","sentences":["In the past few years, \"metaverse\" and \"non-fungible tokens (NFT)\" have become buzzwords, and the prices of related assets have shown speculative bubble-like behavior.","In this paper, we attempt to better understand the underlying economic dynamics.","To do so, we look at Decentraland, a virtual world platform where land parcels are sold as NFT collections.","We find that initially, land prices followed traditional real estate pricing models -- in particular, value decreased with distance from the most desirable areas -- suggesting Decentraland behaved much like a virtual city.","However, these real estate pricing models stopped applying when both the metaverse and NFTs gained increased popular attention and enthusiasm in 2021, suggesting a new driving force for the underlying asset prices.","At that time, following a substantial rise in NFT market values, short-term holders of multiple parcels began to take major selling positions in the Decentraland market, which hints that, rather than building a metaverse community, early Decentraland investors preferred to cash out when land valuations became overly inflated.","Our analysis also shows that while the majority of buyers are new entrants to the market (many of whom joined during the bubble), liquidity (i.e., parcels) was mostly provided by early adopters selling, which caused stark differences in monetary gains.","Early adopters made money -- more than 10,000 USD on average per parcel sold -- but users who joined later typically made no profit or even incurred losses in the order of 1,000 USD per parcel.","Unlike established markets such as financial and real estate markets, newly emergent digital marketplaces are mostly self-regulated.","As a result, the significant financial risks we identify indicate a strong need for establishing appropriate standards of business conduct and improving user awareness."],"url":"http://arxiv.org/abs/2501.09601v1"}
{"created":"2025-01-16 15:22:06","title":"Mesh2SLAM in VR: A Fast Geometry-Based SLAM Framework for Rapid Prototyping in Virtual Reality Applications","abstract":"SLAM is a foundational technique with broad applications in robotics and AR/VR. SLAM simulations evaluate new concepts, but testing on resource-constrained devices, such as VR HMDs, faces challenges: high computational cost and restricted sensor data access. This work proposes a sparse framework using mesh geometry projections as features, which improves efficiency and circumvents direct sensor data access, advancing SLAM research as we demonstrate in VR and through numerical evaluation.","sentences":["SLAM is a foundational technique with broad applications in robotics and AR/VR.","SLAM simulations evaluate new concepts, but testing on resource-constrained devices, such as VR HMDs, faces challenges: high computational cost and restricted sensor data access.","This work proposes a sparse framework using mesh geometry projections as features, which improves efficiency and circumvents direct sensor data access, advancing SLAM research as we demonstrate in VR and through numerical evaluation."],"url":"http://arxiv.org/abs/2501.09600v1"}
{"created":"2025-01-16 15:21:18","title":"Reducing the Sensitivity of Neural Physics Simulators to Mesh Topology via Pretraining","abstract":"Meshes are used to represent complex objects in high fidelity physics simulators across a variety of domains, such as radar sensing and aerodynamics. There is growing interest in using neural networks to accelerate physics simulations, and also a growing body of work on applying neural networks directly to irregular mesh data. Since multiple mesh topologies can represent the same object, mesh augmentation is typically required to handle topological variation when training neural networks. Due to the sensitivity of physics simulators to small changes in mesh shape, it is challenging to use these augmentations when training neural network-based physics simulators. In this work, we show that variations in mesh topology can significantly reduce the performance of neural network simulators. We evaluate whether pretraining can be used to address this issue, and find that employing an established autoencoder pretraining technique with graph embedding models reduces the sensitivity of neural network simulators to variations in mesh topology. Finally, we highlight future research directions that may further reduce neural simulator sensitivity to mesh topology.","sentences":["Meshes are used to represent complex objects in high fidelity physics simulators across a variety of domains, such as radar sensing and aerodynamics.","There is growing interest in using neural networks to accelerate physics simulations, and also a growing body of work on applying neural networks directly to irregular mesh data.","Since multiple mesh topologies can represent the same object, mesh augmentation is typically required to handle topological variation when training neural networks.","Due to the sensitivity of physics simulators to small changes in mesh shape, it is challenging to use these augmentations when training neural network-based physics simulators.","In this work, we show that variations in mesh topology can significantly reduce the performance of neural network simulators.","We evaluate whether pretraining can be used to address this issue, and find that employing an established autoencoder pretraining technique with graph embedding models reduces the sensitivity of neural network simulators to variations in mesh topology.","Finally, we highlight future research directions that may further reduce neural simulator sensitivity to mesh topology."],"url":"http://arxiv.org/abs/2501.09597v1"}
{"created":"2025-01-16 15:20:22","title":"IFRA: a machine learning-based Instrumented Fall Risk Assessment Scale derived from Instrumented Timed Up and Go test in stroke patients","abstract":"Effective fall risk assessment is critical for post-stroke patients. The present study proposes a novel, data-informed fall risk assessment method based on the instrumented Timed Up and Go (ITUG) test data, bringing in many mobility measures that traditional clinical scales fail to capture. IFRA, which stands for Instrumented Fall Risk Assessment, has been developed using a two-step process: first, features with the highest predictive power among those collected in a ITUG test have been identified using machine learning techniques; then, a strategy is proposed to stratify patients into low, medium, or high-risk strata. The dataset used in our analysis consists of 142 participants, out of which 93 were used for training (15 synthetically generated), 17 for validation and 32 to test the resulting IFRA scale (22 non-fallers and 10 fallers). Features considered in the IFRA scale include gait speed, vertical acceleration during sit-to-walk transition, and turning angular velocity, which align well with established literature on the risk of fall in neurological patients. In a comparison with traditional clinical scales such as the traditional Timed Up & Go and the Mini-BESTest, IFRA demonstrates competitive performance, being the only scale to correctly assign more than half of the fallers to the high-risk stratum (Fischer's Exact test p = 0.004). Despite the dataset's limited size, this is the first proof-of-concept study to pave the way for future evidence regarding the use of IFRA tool for continuous patient monitoring and fall prevention both in clinical stroke rehabilitation and at home post-discharge.","sentences":["Effective fall risk assessment is critical for post-stroke patients.","The present study proposes a novel, data-informed fall risk assessment method based on the instrumented Timed Up and Go (ITUG) test data, bringing in many mobility measures that traditional clinical scales fail to capture.","IFRA, which stands for Instrumented Fall Risk Assessment, has been developed using a two-step process: first, features with the highest predictive power among those collected in a ITUG test have been identified using machine learning techniques; then, a strategy is proposed to stratify patients into low, medium, or high-risk strata.","The dataset used in our analysis consists of 142 participants, out of which 93 were used for training (15 synthetically generated), 17 for validation and 32 to test the resulting IFRA scale (22 non-fallers and 10 fallers).","Features considered in the IFRA scale include gait speed, vertical acceleration during sit-to-walk transition, and turning angular velocity, which align well with established literature on the risk of fall in neurological patients.","In a comparison with traditional clinical scales such as the traditional Timed Up & Go and the Mini-BESTest, IFRA demonstrates competitive performance, being the only scale to correctly assign more than half of the fallers to the high-risk stratum (Fischer's Exact test p = 0.004).","Despite the dataset's limited size, this is the first proof-of-concept study to pave the way for future evidence regarding the use of IFRA tool for continuous patient monitoring and fall prevention both in clinical stroke rehabilitation and at home post-discharge."],"url":"http://arxiv.org/abs/2501.09595v1"}
{"created":"2025-01-16 15:17:33","title":"Clinicians don't know what explanations they need: A case study on eliciting AI software explainability requirements","abstract":"This paper analyses how software developers elicit explainability requirements when creating a software application with an AI component, through a case study using AI in the medical context of predicting cerebral palsy (CP) risk in infants. Following a small software development team at a Norwegian hospital, we observe their process of simultaneously developing the AI application and discovering what explanations clinicians require from the AI predictions. Since clinicians struggled to articulate their explainability needs before interacting with the system, an iterative approach proved effective: the team started with minimal explanations and refined these based on clinicians' responses during real patient examinations. Our preliminary findings from the first two iterations show that clinicians valued \"interrogative explanations\" - i.e., tools that let them explore and compare the AI predictions with their own assessments - over detailed technical explanations of the AI model's inner workings. Based on our analysis, we suggest that successful explainability requirements emerge through iterative collaboration between developers and users rather than being fully specified upfront. To the best of our knowledge, this is the first empirical case study on eliciting explainability requirements in software engineering.","sentences":["This paper analyses how software developers elicit explainability requirements when creating a software application with an AI component, through a case study using AI in the medical context of predicting cerebral palsy (CP) risk in infants.","Following a small software development team at a Norwegian hospital, we observe their process of simultaneously developing the AI application and discovering what explanations clinicians require from the AI predictions.","Since clinicians struggled to articulate their explainability needs before interacting with the system, an iterative approach proved effective: the team started with minimal explanations and refined these based on clinicians' responses during real patient examinations.","Our preliminary findings from the first two iterations show that clinicians valued \"interrogative explanations\" - i.e., tools that let them explore and compare the AI predictions with their own assessments - over detailed technical explanations of the AI model's inner workings.","Based on our analysis, we suggest that successful explainability requirements emerge through iterative collaboration between developers and users rather than being fully specified upfront.","To the best of our knowledge, this is the first empirical case study on eliciting explainability requirements in software engineering."],"url":"http://arxiv.org/abs/2501.09592v1"}
{"created":"2025-01-16 15:17:27","title":"Metrics for Inter-Dataset Similarity with Example Applications in Synthetic Data and Feature Selection Evaluation -- Extended Version","abstract":"Measuring inter-dataset similarity is an important task in machine learning and data mining with various use cases and applications. Existing methods for measuring inter-dataset similarity are computationally expensive, limited, or sensitive to different entities and non-trivial choices for parameters. They also lack a holistic perspective on the entire dataset. In this paper, we propose two novel metrics for measuring inter-dataset similarity. We discuss the mathematical foundation and the theoretical basis of our proposed metrics. We demonstrate the effectiveness of the proposed metrics by investigating two applications in the evaluation of synthetic data and in the evaluation of feature selection methods. The theoretical and empirical studies conducted in this paper illustrate the effectiveness of the proposed metrics.","sentences":["Measuring inter-dataset similarity is an important task in machine learning and data mining with various use cases and applications.","Existing methods for measuring inter-dataset similarity are computationally expensive, limited, or sensitive to different entities and non-trivial choices for parameters.","They also lack a holistic perspective on the entire dataset.","In this paper, we propose two novel metrics for measuring inter-dataset similarity.","We discuss the mathematical foundation and the theoretical basis of our proposed metrics.","We demonstrate the effectiveness of the proposed metrics by investigating two applications in the evaluation of synthetic data and in the evaluation of feature selection methods.","The theoretical and empirical studies conducted in this paper illustrate the effectiveness of the proposed metrics."],"url":"http://arxiv.org/abs/2501.09591v1"}
{"created":"2025-01-16 15:11:33","title":"Atleus: Accelerating Transformers on the Edge Enabled by 3D Heterogeneous Manycore Architectures","abstract":"Transformer architectures have become the standard neural network model for various machine learning applications including natural language processing and computer vision. However, the compute and memory requirements introduced by transformer models make them challenging to adopt for edge applications. Furthermore, fine-tuning pre-trained transformers (e.g., foundation models) is a common task to enhance the model's predictive performance on specific tasks/applications. Existing transformer accelerators are oblivious to complexities introduced by fine-tuning. In this paper, we propose the design of a three-dimensional (3D) heterogeneous architecture referred to as Atleus that incorporates heterogeneous computing resources specifically optimized to accelerate transformer models for the dual purposes of fine-tuning and inference. Specifically, Atleus utilizes non-volatile memory and systolic array for accelerating transformer computational kernels using an integrated 3D platform. Moreover, we design a suitable NoC to achieve high performance and energy efficiency. Finally, Atleus adopts an effective quantization scheme to support model compression. Experimental results demonstrate that Atleus outperforms existing state-of-the-art by up to 56x and 64.5x in terms of performance and energy efficiency respectively","sentences":["Transformer architectures have become the standard neural network model for various machine learning applications including natural language processing and computer vision.","However, the compute and memory requirements introduced by transformer models make them challenging to adopt for edge applications.","Furthermore, fine-tuning pre-trained transformers (e.g., foundation models) is a common task to enhance the model's predictive performance on specific tasks/applications.","Existing transformer accelerators are oblivious to complexities introduced by fine-tuning.","In this paper, we propose the design of a three-dimensional (3D) heterogeneous architecture referred to as Atleus that incorporates heterogeneous computing resources specifically optimized to accelerate transformer models for the dual purposes of fine-tuning and inference.","Specifically, Atleus utilizes non-volatile memory and systolic array for accelerating transformer computational kernels using an integrated 3D platform.","Moreover, we design a suitable NoC to achieve high performance and energy efficiency.","Finally, Atleus adopts an effective quantization scheme to support model compression.","Experimental results demonstrate that Atleus outperforms existing state-of-the-art by up to 56x and 64.5x in terms of performance and energy efficiency respectively"],"url":"http://arxiv.org/abs/2501.09588v1"}
{"created":"2025-01-16 14:56:41","title":"Sequential PatchCore: Anomaly Detection for Surface Inspection using Synthetic Impurities","abstract":"The appearance of surface impurities (e.g., water stains, fingerprints, stickers) is an often-mentioned issue that causes degradation of automated visual inspection systems. At the same time, synthetic data generation techniques for visual surface inspection have focused primarily on generating perfect examples and defects, disregarding impurities. This study highlights the importance of considering impurities when generating synthetic data. We introduce a procedural method to include photorealistic water stains in synthetic data. The synthetic datasets are generated to correspond to real datasets and are further used to train an anomaly detection model and investigate the influence of water stains. The high-resolution images used for surface inspection lead to memory bottlenecks during anomaly detection training. To address this, we introduce Sequential PatchCore - a method to build coresets sequentially and make training on large images using consumer-grade hardware tractable. This allows us to perform transfer learning using coresets pre-trained on different dataset versions. Our results show the benefits of using synthetic data for pre-training an explicit coreset anomaly model and the extended performance benefits of finetuning the coreset using real data. We observed how the impurities and labelling ambiguity lower the model performance and have additionally reported the defect-wise recall to provide an industrially relevant perspective on model performance.","sentences":["The appearance of surface impurities (e.g., water stains, fingerprints, stickers) is an often-mentioned issue that causes degradation of automated visual inspection systems.","At the same time, synthetic data generation techniques for visual surface inspection have focused primarily on generating perfect examples and defects, disregarding impurities.","This study highlights the importance of considering impurities when generating synthetic data.","We introduce a procedural method to include photorealistic water stains in synthetic data.","The synthetic datasets are generated to correspond to real datasets and are further used to train an anomaly detection model and investigate the influence of water stains.","The high-resolution images used for surface inspection lead to memory bottlenecks during anomaly detection training.","To address this, we introduce Sequential PatchCore - a method to build coresets sequentially and make training on large images using consumer-grade hardware tractable.","This allows us to perform transfer learning using coresets pre-trained on different dataset versions.","Our results show the benefits of using synthetic data for pre-training an explicit coreset anomaly model and the extended performance benefits of finetuning the coreset using real data.","We observed how the impurities and labelling ambiguity lower the model performance and have additionally reported the defect-wise recall to provide an industrially relevant perspective on model performance."],"url":"http://arxiv.org/abs/2501.09579v1"}
{"created":"2025-01-16 14:45:12","title":"MatrixNet: Learning over symmetry groups using learned group representations","abstract":"Group theory has been used in machine learning to provide a theoretically grounded approach for incorporating known symmetry transformations in tasks from robotics to protein modeling. In these applications, equivariant neural networks use known symmetry groups with predefined representations to learn over geometric input data. We propose MatrixNet, a neural network architecture that learns matrix representations of group element inputs instead of using predefined representations. MatrixNet achieves higher sample efficiency and generalization over several standard baselines in prediction tasks over the several finite groups and the Artin braid group. We also show that MatrixNet respects group relations allowing generalization to group elements of greater word length than in the training set.","sentences":["Group theory has been used in machine learning to provide a theoretically grounded approach for incorporating known symmetry transformations in tasks from robotics to protein modeling.","In these applications, equivariant neural networks use known symmetry groups with predefined representations to learn over geometric input data.","We propose MatrixNet, a neural network architecture that learns matrix representations of group element inputs instead of using predefined representations.","MatrixNet achieves higher sample efficiency and generalization over several standard baselines in prediction tasks over the several finite groups and the Artin braid group.","We also show that MatrixNet respects group relations allowing generalization to group elements of greater word length than in the training set."],"url":"http://arxiv.org/abs/2501.09571v1"}
{"created":"2025-01-16 14:40:02","title":"A New Teacher-Reviewer-Student Framework for Semi-supervised 2D Human Pose Estimation","abstract":"Conventional 2D human pose estimation methods typically require extensive labeled annotations, which are both labor-intensive and expensive. In contrast, semi-supervised 2D human pose estimation can alleviate the above problems by leveraging a large amount of unlabeled data along with a small portion of labeled data. Existing semi-supervised 2D human pose estimation methods update the network through backpropagation, ignoring crucial historical information from the previous training process. Therefore, we propose a novel semi-supervised 2D human pose estimation method by utilizing a newly designed Teacher-Reviewer-Student framework. Specifically, we first mimic the phenomenon that human beings constantly review previous knowledge for consolidation to design our framework, in which the teacher predicts results to guide the student's learning and the reviewer stores important historical parameters to provide additional supervision signals. Secondly, we introduce a Multi-level Feature Learning strategy, which utilizes the outputs from different stages of the backbone to estimate the heatmap to guide network training, enriching the supervisory information while effectively capturing keypoint relationships. Finally, we design a data augmentation strategy, i.e., Keypoint-Mix, to perturb pose information by mixing different keypoints, thus enhancing the network's ability to discern keypoints. Extensive experiments on publicly available datasets, demonstrate our method achieves significant improvements compared to the existing methods.","sentences":["Conventional 2D human pose estimation methods typically require extensive labeled annotations, which are both labor-intensive and expensive.","In contrast, semi-supervised 2D human pose estimation can alleviate the above problems by leveraging a large amount of unlabeled data along with a small portion of labeled data.","Existing semi-supervised 2D human pose estimation methods update the network through backpropagation, ignoring crucial historical information from the previous training process.","Therefore, we propose a novel semi-supervised 2D human pose estimation method by utilizing a newly designed Teacher-Reviewer-Student framework.","Specifically, we first mimic the phenomenon that human beings constantly review previous knowledge for consolidation to design our framework, in which the teacher predicts results to guide the student's learning and the reviewer stores important historical parameters to provide additional supervision signals.","Secondly, we introduce a Multi-level Feature Learning strategy, which utilizes the outputs from different stages of the backbone to estimate the heatmap to guide network training, enriching the supervisory information while effectively capturing keypoint relationships.","Finally, we design a data augmentation strategy, i.e., Keypoint-Mix, to perturb pose information by mixing different keypoints, thus enhancing the network's ability to discern keypoints.","Extensive experiments on publicly available datasets, demonstrate our method achieves significant improvements compared to the existing methods."],"url":"http://arxiv.org/abs/2501.09565v1"}
{"created":"2025-01-16 14:31:03","title":"Rethinking cloud abstractions for tenant-provider cooperative optimization of AI workloads","abstract":"AI workloads, often hosted in multi-tenant cloud environments, require vast computational resources but suffer inefficiencies due to limited tenant-provider coordination. Tenants lack infrastructure insights, while providers lack workload details to optimize tasks like partitioning, scheduling, and fault tolerance. We propose the HarmonAIze project to redefine cloud abstractions, enabling cooperative optimization for improved performance, efficiency, resiliency, and sustainability. This paper outlines key opportunities, challenges, and a research agenda to realize this vision.","sentences":["AI workloads, often hosted in multi-tenant cloud environments, require vast computational resources but suffer inefficiencies due to limited tenant-provider coordination.","Tenants lack infrastructure insights, while providers lack workload details to optimize tasks like partitioning, scheduling, and fault tolerance.","We propose the HarmonAIze project to redefine cloud abstractions, enabling cooperative optimization for improved performance, efficiency, resiliency, and sustainability.","This paper outlines key opportunities, challenges, and a research agenda to realize this vision."],"url":"http://arxiv.org/abs/2501.09562v1"}
{"created":"2025-01-16 14:26:48","title":"Stylomech: Unveiling Authorship via Computational Stylometry in English and Romanized Sinhala","abstract":"With the advent of Web 2.0, the development in social technology coupled with global communication systematically brought positive and negative impacts to society. Copyright claims and Author identification are deemed crucial as there has been a considerable amount of increase in content violation owing to the lack of proper ethics in society. The Author's attribution in both English and Romanized Sinhala became a major requirement in the last few decades. As an area largely unexplored, particularly within the context of Romanized Sinhala, the research contributes significantly to the field of computational linguistics. The proposed author attribution system offers a unique approach, allowing for the comparison of only two sets of text: suspect author and anonymous text, a departure from traditional methodologies which often rely on larger corpora. This work focuses on using the numerical representation of various pairs of the same and different authors allowing for, the model to train on these representations as opposed to text, this allows for it to apply to a multitude of authors and contexts, given that the suspected author text, and the anonymous text are of reasonable quality. By expanding the scope of authorship attribution to encompass diverse linguistic contexts, the work contributes to fostering trust and accountability in digital communication, especially in Sri Lanka. This research presents a pioneering approach to author attribution in both English and Romanized Sinhala, addressing a critical need for content verification and intellectual property rights enforcement in the digital age.","sentences":["With the advent of Web 2.0, the development in social technology coupled with global communication systematically brought positive and negative impacts to society.","Copyright claims and Author identification are deemed crucial as there has been a considerable amount of increase in content violation owing to the lack of proper ethics in society.","The Author's attribution in both English and Romanized Sinhala became a major requirement in the last few decades.","As an area largely unexplored, particularly within the context of Romanized Sinhala, the research contributes significantly to the field of computational linguistics.","The proposed author attribution system offers a unique approach, allowing for the comparison of only two sets of text: suspect author and anonymous text, a departure from traditional methodologies which often rely on larger corpora.","This work focuses on using the numerical representation of various pairs of the same and different authors allowing for, the model to train on these representations as opposed to text, this allows for it to apply to a multitude of authors and contexts, given that the suspected author text, and the anonymous text are of reasonable quality.","By expanding the scope of authorship attribution to encompass diverse linguistic contexts, the work contributes to fostering trust and accountability in digital communication, especially in Sri Lanka.","This research presents a pioneering approach to author attribution in both English and Romanized Sinhala, addressing a critical need for content verification and intellectual property rights enforcement in the digital age."],"url":"http://arxiv.org/abs/2501.09561v1"}
{"created":"2025-01-16 14:24:40","title":"On a Variant of the Minimum Path Cover Problem in Acyclic Digraphs: Computational Complexity Results and Exact Method","abstract":"The Minimum Path Cover (MPC) problem consists of finding a minimum-cardinality set of node-disjoint paths that cover all nodes in a given graph. We explore a variant of the MPC problem on acyclic digraphs (DAGs) where, given a subset of arcs, each path within the MPC should contain at least one arc from this subset. We prove that the feasibility problem is strongly NP-hard on arbitrary DAGs, but the problem can be solved in polynomial time when the DAG is the transitive closure of a path.   Given that the problem may not always be feasible, our solution focuses on covering a maximum number of nodes with a minimum number of node-disjoint paths, such that each path includes at least one arc from the predefined subset of arcs. This paper introduces and investigates two integer programming formulations for this problem. We propose several valid inequalities to enhance the linear programming relaxations, employing them as cutting planes in a branch-and-cut approach. The procedure is implemented and tested on a wide range of instances, including real-world instances derived from an airline crew scheduling problem, demonstrating the effectiveness of the proposed approach.","sentences":["The Minimum Path Cover (MPC) problem consists of finding a minimum-cardinality set of node-disjoint paths that cover all nodes in a given graph.","We explore a variant of the MPC problem on acyclic digraphs (DAGs) where, given a subset of arcs, each path within the MPC should contain at least one arc from this subset.","We prove that the feasibility problem is strongly NP-hard on arbitrary DAGs, but the problem can be solved in polynomial time when the DAG is the transitive closure of a path.   ","Given that the problem may not always be feasible, our solution focuses on covering a maximum number of nodes with a minimum number of node-disjoint paths, such that each path includes at least one arc from the predefined subset of arcs.","This paper introduces and investigates two integer programming formulations for this problem.","We propose several valid inequalities to enhance the linear programming relaxations, employing them as cutting planes in a branch-and-cut approach.","The procedure is implemented and tested on a wide range of instances, including real-world instances derived from an airline crew scheduling problem, demonstrating the effectiveness of the proposed approach."],"url":"http://arxiv.org/abs/2501.09560v1"}
{"created":"2025-01-16 14:19:46","title":"Core Hours and Carbon Credits: Incentivizing Sustainability in HPC","abstract":"Realizing a shared responsibility between providers and consumers is critical to manage the sustainability of HPC. However, while cost may motivate efficiency improvements by infrastructure operators, broader progress is impeded by a lack of user incentives. We conduct a survey of HPC users that reveals fewer than 30 percent are aware of their energy consumption, and that energy efficiency is among users' lowest priority concerns. One explanation is that existing pricing models may encourage users to prioritize performance over energy efficiency. We propose two transparent multi-resource pricing schemes, Energy- and Carbon-Based Accounting, that seek to change this paradigm by incentivizing more efficient user behavior. These two schemes charge for computations based on their energy consumption or carbon footprint, respectively, rewarding users who leverage efficient hardware and software. We evaluate these two pricing schemes via simulation, in a prototype, and a user study.","sentences":["Realizing a shared responsibility between providers and consumers is critical to manage the sustainability of HPC.","However, while cost may motivate efficiency improvements by infrastructure operators, broader progress is impeded by a lack of user incentives.","We conduct a survey of HPC users that reveals fewer than 30 percent are aware of their energy consumption, and that energy efficiency is among users' lowest priority concerns.","One explanation is that existing pricing models may encourage users to prioritize performance over energy efficiency.","We propose two transparent multi-resource pricing schemes, Energy- and Carbon-Based Accounting, that seek to change this paradigm by incentivizing more efficient user behavior.","These two schemes charge for computations based on their energy consumption or carbon footprint, respectively, rewarding users who leverage efficient hardware and software.","We evaluate these two pricing schemes via simulation, in a prototype, and a user study."],"url":"http://arxiv.org/abs/2501.09557v1"}
{"created":"2025-01-16 14:18:10","title":"Overshoot: Taking advantage of future gradients in momentum-based stochastic optimization","abstract":"Overshoot is a novel, momentum-based stochastic gradient descent optimization method designed to enhance performance beyond standard and Nesterov's momentum. In conventional momentum methods, gradients from previous steps are aggregated with the gradient at current model weights before taking a step and updating the model. Rather than calculating gradient at the current model weights, Overshoot calculates the gradient at model weights shifted in the direction of the current momentum. This sacrifices the immediate benefit of using the gradient w.r.t. the exact model weights now, in favor of evaluating at a point, which will likely be more relevant for future updates. We show that incorporating this principle into momentum-based optimizers (SGD with momentum and Adam) results in faster convergence (saving on average at least 15% of steps). Overshoot consistently outperforms both standard and Nesterov's momentum across a wide range of tasks and integrates into popular momentum-based optimizers with zero memory and small computational overhead.","sentences":["Overshoot is a novel, momentum-based stochastic gradient descent optimization method designed to enhance performance beyond standard and Nesterov's momentum.","In conventional momentum methods, gradients from previous steps are aggregated with the gradient at current model weights before taking a step and updating the model.","Rather than calculating gradient at the current model weights, Overshoot calculates the gradient at model weights shifted in the direction of the current momentum.","This sacrifices the immediate benefit of using the gradient w.r.t.","the exact model weights now, in favor of evaluating at a point, which will likely be more relevant for future updates.","We show that incorporating this principle into momentum-based optimizers (SGD with momentum and Adam) results in faster convergence (saving on average at least 15% of steps).","Overshoot consistently outperforms both standard and Nesterov's momentum across a wide range of tasks and integrates into popular momentum-based optimizers with zero memory and small computational overhead."],"url":"http://arxiv.org/abs/2501.09556v1"}
{"created":"2025-01-16 14:18:06","title":"Text-driven Adaptation of Foundation Models for Few-shot Surgical Workflow Analysis","abstract":"Purpose: Surgical workflow analysis is crucial for improving surgical efficiency and safety. However, previous studies rely heavily on large-scale annotated datasets, posing challenges in cost, scalability, and reliance on expert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven Adaptation), designed to handle various surgical workflow analysis tasks with minimal paired image-label data.   Methods: Our approach has two key components. First, Few-shot selection-based modality alignment selects a small subset of images and aligns their embeddings with text embeddings from the downstream task, bridging the modality gap. Second, Text-driven adaptation leverages only text data to train a decoder, eliminating the need for paired image-text data. This decoder is then applied to aligned image embeddings, enabling image-related tasks without explicit image-text pairs.   Results: We evaluate our approach to generative tasks (image captioning) and discriminative tasks (triplet recognition and phase recognition). Results show that Surg-FTDA outperforms baselines and generalizes well across downstream tasks.   Conclusion: We propose a text-driven adaptation approach that mitigates the modality gap and handles multiple downstream tasks in surgical workflow analysis, with minimal reliance on large annotated datasets. The code and dataset will be released in https://github.com/TingxuanSix/Surg-FTDA.","sentences":["Purpose: Surgical workflow analysis is crucial for improving surgical efficiency and safety.","However, previous studies rely heavily on large-scale annotated datasets, posing challenges in cost, scalability, and reliance on expert annotations.","To address this, we propose Surg-FTDA (Few-shot Text-driven Adaptation), designed to handle various surgical workflow analysis tasks with minimal paired image-label data.   ","Methods: Our approach has two key components.","First, Few-shot selection-based modality alignment selects a small subset of images and aligns their embeddings with text embeddings from the downstream task, bridging the modality gap.","Second, Text-driven adaptation leverages only text data to train a decoder, eliminating the need for paired image-text data.","This decoder is then applied to aligned image embeddings, enabling image-related tasks without explicit image-text pairs.   ","Results: We evaluate our approach to generative tasks (image captioning) and discriminative tasks (triplet recognition and phase recognition).","Results show that Surg-FTDA outperforms baselines and generalizes well across downstream tasks.   ","Conclusion: We propose a text-driven adaptation approach that mitigates the modality gap and handles multiple downstream tasks in surgical workflow analysis, with minimal reliance on large annotated datasets.","The code and dataset will be released in https://github.com/TingxuanSix/Surg-FTDA."],"url":"http://arxiv.org/abs/2501.09555v1"}
