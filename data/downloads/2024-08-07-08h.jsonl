{"created":"2024-08-06 17:59:44","title":"LLaVA-OneVision: Easy Visual Task Transfer","abstract":"We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series. Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios. Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities. In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos.","sentences":["We present LLaVA-OneVision, a family of open large multimodal models (LMMs) developed by consolidating our insights into data, models, and visual representations in the LLaVA-NeXT blog series.","Our experimental results demonstrate that LLaVA-OneVision is the first single model that can simultaneously push the performance boundaries of open LMMs in three important computer vision scenarios: single-image, multi-image, and video scenarios.","Importantly, the design of LLaVA-OneVision allows strong transfer learning across different modalities/scenarios, yielding new emerging capabilities.","In particular, strong video understanding and cross-scenario capabilities are demonstrated through task transfer from images to videos."],"url":"http://arxiv.org/abs/2408.03326v1"}
{"created":"2024-08-06 17:58:53","title":"CoverBench: A Challenging Benchmark for Complex Claim Verification","abstract":"There is a growing line of research on verifying the correctness of language models' outputs. At the same time, LMs are being used to tackle complex queries that require reasoning. We introduce CoverBench, a challenging benchmark focused on verifying LM outputs in complex reasoning settings. Datasets that can be used for this purpose are often designed for other complex reasoning tasks (e.g., QA) targeting specific use-cases (e.g., financial tables), requiring transformations, negative sampling and selection of hard examples to collect such a benchmark. CoverBench provides a diversified evaluation for complex claim verification in a variety of domains, types of reasoning, relatively long inputs, and a variety of standardizations, such as multiple representations for tables where available, and a consistent schema. We manually vet the data for quality to ensure low levels of label noise. Finally, we report a variety of competitive baseline results to show CoverBench is challenging and has very significant headroom. The data is available at https://huggingface.co/datasets/google/coverbench .","sentences":["There is a growing line of research on verifying the correctness of language models' outputs.","At the same time, LMs are being used to tackle complex queries that require reasoning.","We introduce CoverBench, a challenging benchmark focused on verifying LM outputs in complex reasoning settings.","Datasets that can be used for this purpose are often designed for other complex reasoning tasks (e.g., QA) targeting specific use-cases (e.g., financial tables), requiring transformations, negative sampling and selection of hard examples to collect such a benchmark.","CoverBench provides a diversified evaluation for complex claim verification in a variety of domains, types of reasoning, relatively long inputs, and a variety of standardizations, such as multiple representations for tables where available, and a consistent schema.","We manually vet the data for quality to ensure low levels of label noise.","Finally, we report a variety of competitive baseline results to show CoverBench is challenging and has very significant headroom.","The data is available at https://huggingface.co/datasets/google/coverbench ."],"url":"http://arxiv.org/abs/2408.03325v1"}
{"created":"2024-08-06 17:58:29","title":"ClassiFIM: An Unsupervised Method To Detect Phase Transitions","abstract":"Estimation of the Fisher Information Metric (FIM-estimation) is an important task that arises in unsupervised learning of phase transitions, a problem proposed by physicists. This work completes the definition of the task by defining rigorous evaluation metrics distMSE, distMSEPS, and distRE and introduces ClassiFIM, a novel machine learning method designed to solve the FIM-estimation task. Unlike existing methods for unsupervised learning of phase transitions, ClassiFIM directly estimates a well-defined quantity (the FIM), allowing it to be rigorously compared to any present and future other methods that estimate the same. ClassiFIM transforms a dataset for the FIM-estimation task into a dataset for an auxiliary binary classification task and involves selecting and training a model for the latter. We prove that the output of ClassiFIM approaches the exact FIM in the limit of infinite dataset size and under certain regularity conditions. We implement ClassiFIM on multiple datasets, including datasets describing classical and quantum phase transitions, and find that it achieves a good ground truth approximation with modest computational resources. Furthermore, we independently implement two alternative state-of-the-art methods for unsupervised estimation of phase transition locations on the same datasets and find that ClassiFIM predicts such locations at least as well as these other methods. To emphasize the generality of our method, we also propose and generate the MNIST-CNN dataset, which consists of the output of CNNs trained on MNIST for different hyperparameter choices. Using ClassiFIM on this dataset suggests there is a phase transition in the distribution of image-prediction pairs for CNNs trained on MNIST, demonstrating the broad scope of FIM-estimation beyond physics.","sentences":["Estimation of the Fisher Information Metric (FIM-estimation) is an important task that arises in unsupervised learning of phase transitions, a problem proposed by physicists.","This work completes the definition of the task by defining rigorous evaluation metrics distMSE, distMSEPS, and distRE and introduces ClassiFIM, a novel machine learning method designed to solve the FIM-estimation task.","Unlike existing methods for unsupervised learning of phase transitions, ClassiFIM directly estimates a well-defined quantity (the FIM), allowing it to be rigorously compared to any present and future other methods that estimate the same.","ClassiFIM transforms a dataset for the FIM-estimation task into a dataset for an auxiliary binary classification task and involves selecting and training a model for the latter.","We prove that the output of ClassiFIM approaches the exact FIM in the limit of infinite dataset size and under certain regularity conditions.","We implement ClassiFIM on multiple datasets, including datasets describing classical and quantum phase transitions, and find that it achieves a good ground truth approximation with modest computational resources.","Furthermore, we independently implement two alternative state-of-the-art methods for unsupervised estimation of phase transition locations on the same datasets and find that ClassiFIM predicts such locations at least as well as these other methods.","To emphasize the generality of our method, we also propose and generate the MNIST-CNN dataset, which consists of the output of CNNs trained on MNIST for different hyperparameter choices.","Using ClassiFIM on this dataset suggests there is a phase transition in the distribution of image-prediction pairs for CNNs trained on MNIST, demonstrating the broad scope of FIM-estimation beyond physics."],"url":"http://arxiv.org/abs/2408.03323v1"}
{"created":"2024-08-06 17:51:42","title":"Training LLMs to Recognize Hedges in Spontaneous Narratives","abstract":"Hedges allow speakers to mark utterances as provisional, whether to signal non-prototypicality or \"fuzziness\", to indicate a lack of commitment to an utterance, to attribute responsibility for a statement to someone else, to invite input from a partner, or to soften critical feedback in the service of face-management needs. Here we focus on hedges in an experimentally parameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced from memory by 21 speakers for co-present addressees, transcribed to text (Galati and Brennan, 2010). We created a gold standard of hedges annotated by human coders (the Roadrunner-Hedge corpus) and compared three LLM-based approaches for hedge detection: fine-tuning BERT, and zero and few-shot prompting with GPT-4o and LLaMA-3. The best-performing approach was a fine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on the top performing approaches, we used an LLM-in-the-Loop approach to improve the gold standard coding, as well as to highlight cases in which hedges are ambiguous in linguistically interesting ways that will guide future research. This is the first step in our research program to train LLMs to interpret and generate collateral signals appropriately and meaningfully in conversation.","sentences":["Hedges allow speakers to mark utterances as provisional, whether to signal non-prototypicality or \"fuzziness\", to indicate a lack of commitment to an utterance, to attribute responsibility for a statement to someone else, to invite input from a partner, or to soften critical feedback in the service of face-management needs.","Here we focus on hedges in an experimentally parameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced from memory by 21 speakers for co-present addressees, transcribed to text (Galati and Brennan, 2010).","We created a gold standard of hedges annotated by human coders (the Roadrunner-Hedge corpus) and compared three LLM-based approaches for hedge detection: fine-tuning BERT, and zero and few-shot prompting with GPT-4o and LLaMA-3.","The best-performing approach was a fine-tuned BERT model, followed by few-shot GPT-4o.","After an error analysis on the top performing approaches, we used an LLM-in-the-Loop approach to improve the gold standard coding, as well as to highlight cases in which hedges are ambiguous in linguistically interesting ways that will guide future research.","This is the first step in our research program to train LLMs to interpret and generate collateral signals appropriately and meaningfully in conversation."],"url":"http://arxiv.org/abs/2408.03319v1"}
{"created":"2024-08-06 17:35:05","title":"Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters","abstract":"Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language. In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt? Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute. Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods. Moreover, current work largely provides negative results for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time. We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt. This observation motivates applying a \"compute-optimal\" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt. Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model.","sentences":["Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards building generally self-improving agents that can operate on open-ended natural language.","In this paper, we study the scaling of inference-time computation in LLMs, with a focus on answering the question: if an LLM is allowed to use a fixed but non-trivial amount of inference-time compute, how much can it improve its performance on a challenging prompt?","Answering this question has implications not only on the achievable performance of LLMs, but also on the future of LLM pretraining and how one should tradeoff inference-time and pre-training compute.","Despite its importance, little research attempted to understand the scaling behaviors of various test-time inference methods.","Moreover, current work largely provides negative results for a number of these strategies.","In this work, we analyze two primary mechanisms to scale test-time computation: (1) searching against dense, process-based verifier reward models; and (2) updating the model's distribution over a response adaptively, given the prompt at test time.","We find that in both cases, the effectiveness of different approaches to scaling test-time compute critically varies depending on the difficulty of the prompt.","This observation motivates applying a \"compute-optimal\" scaling strategy, which acts to most effectively allocate test-time compute adaptively per prompt.","Using this compute-optimal strategy, we can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline.","Additionally, in a FLOPs-matched evaluation, we find that on problems where a smaller base model attains somewhat non-trivial success rates, test-time compute can be used to outperform a 14x larger model."],"url":"http://arxiv.org/abs/2408.03314v1"}
{"created":"2024-08-06 17:29:01","title":"MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture Generation","abstract":"Recent advancements in the field of Diffusion Transformers have substantially improved the generation of high-quality 2D images, 3D videos, and 3D shapes. However, the effectiveness of the Transformer architecture in the domain of co-speech gesture generation remains relatively unexplored, as prior methodologies have predominantly employed the Convolutional Neural Network (CNNs) or simple a few transformer layers. In an attempt to bridge this research gap, we introduce a novel Masked Diffusion Transformer for co-speech gesture generation, referred to as MDT-A2G, which directly implements the denoising process on gesture sequences. To enhance the contextual reasoning capability of temporally aligned speech-driven gestures, we incorporate a novel Masked Diffusion Transformer. This model employs a mask modeling scheme specifically designed to strengthen temporal relation learning among sequence gestures, thereby expediting the learning process and leading to coherent and realistic motions. Apart from audio, Our MDT-A2G model also integrates multi-modal information, encompassing text, emotion, and identity. Furthermore, we propose an efficient inference strategy that diminishes the denoising computation by leveraging previously calculated results, thereby achieving a speedup with negligible performance degradation. Experimental results demonstrate that MDT-A2G excels in gesture generation, boasting a learning speed that is over 6$\\times$ faster than traditional diffusion transformers and an inference speed that is 5.7$\\times$ than the standard diffusion model.","sentences":["Recent advancements in the field of Diffusion Transformers have substantially improved the generation of high-quality 2D images, 3D videos, and 3D shapes.","However, the effectiveness of the Transformer architecture in the domain of co-speech gesture generation remains relatively unexplored, as prior methodologies have predominantly employed the Convolutional Neural Network (CNNs) or simple a few transformer layers.","In an attempt to bridge this research gap, we introduce a novel Masked Diffusion Transformer for co-speech gesture generation, referred to as MDT-A2G, which directly implements the denoising process on gesture sequences.","To enhance the contextual reasoning capability of temporally aligned speech-driven gestures, we incorporate a novel Masked Diffusion Transformer.","This model employs a mask modeling scheme specifically designed to strengthen temporal relation learning among sequence gestures, thereby expediting the learning process and leading to coherent and realistic motions.","Apart from audio, Our MDT-A2G model also integrates multi-modal information, encompassing text, emotion, and identity.","Furthermore, we propose an efficient inference strategy that diminishes the denoising computation by leveraging previously calculated results, thereby achieving a speedup with negligible performance degradation.","Experimental results demonstrate that MDT-A2G excels in gesture generation, boasting a learning speed that is over 6$\\times$ faster than traditional diffusion transformers and an inference speed that is 5.7$\\times$ than the standard diffusion model."],"url":"http://arxiv.org/abs/2408.03312v1"}
{"created":"2024-08-06 17:16:19","title":"Potential and Limitation of High-Frequency Cores and Caches","abstract":"This paper explores the potential of cryogenic computing and superconducting electronics as promising alternatives to traditional semiconductor devices. As semiconductor devices face challenges such as increased leakage currents and reduced performance at higher temperatures, these novel technologies offer high performance and low power computation. Cryogenic computing operates at ultra-low temperatures near 77 K, leading to lower leakage currents and improved electron mobility. On the other hand, superconducting electronics, operating near 0 K, allow electrons to flow without resistance, offering the potential for ultra-low-power, high-speed computation. This study presents a comprehensive performance modeling and analysis of these technologies and provides insights into their potential benefits and limitations. We implement models of in-order and out-of-order cores operating at high clock frequencies associated with superconducting electronics and cryogenic computing in gem5. We evaluate the performance of these components using workloads representative of real-world applications like NPB, SPEC CPU2006, and GAPBS. Our results show the potential speedups achievable by these components and the limitations posed by cache bandwidth. This work provides valuable insights into the performance implications and design trade-offs associated with cryogenic and superconducting technologies, laying the foundation for future research in this field using gem5.","sentences":["This paper explores the potential of cryogenic computing and superconducting electronics as promising alternatives to traditional semiconductor devices.","As semiconductor devices face challenges such as increased leakage currents and reduced performance at higher temperatures, these novel technologies offer high performance and low power computation.","Cryogenic computing operates at ultra-low temperatures near 77 K, leading to lower leakage currents and improved electron mobility.","On the other hand, superconducting electronics, operating near 0 K, allow electrons to flow without resistance, offering the potential for ultra-low-power, high-speed computation.","This study presents a comprehensive performance modeling and analysis of these technologies and provides insights into their potential benefits and limitations.","We implement models of in-order and out-of-order cores operating at high clock frequencies associated with superconducting electronics and cryogenic computing in gem5.","We evaluate the performance of these components using workloads representative of real-world applications like NPB, SPEC CPU2006, and GAPBS.","Our results show the potential speedups achievable by these components and the limitations posed by cache bandwidth.","This work provides valuable insights into the performance implications and design trade-offs associated with cryogenic and superconducting technologies, laying the foundation for future research in this field using gem5."],"url":"http://arxiv.org/abs/2408.03308v1"}
{"created":"2024-08-06 17:11:40","title":"Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks","abstract":"Etruscan mirrors constitute a significant category in Etruscan art, characterized by elaborate figurative illustrations featured on their backside. A laborious and costly aspect of their analysis and documentation is the task of manually tracing these illustrations. In previous work, a methodology has been proposed to automate this process, involving photometric-stereo scanning in combination with deep neural networks. While achieving quantitative performance akin to an expert annotator, some results still lack qualitative precision and, thus, require annotators for inspection and potential correction, maintaining resource intensity. In response, we propose a deep neural network trained to interactively refine existing annotations based on human guidance. Our human-in-the-loop approach streamlines annotation, achieving equal quality with up to 75% less manual input required. Moreover, during the refinement process, the relative improvement of our methodology over pure manual labeling reaches peak values of up to 26%, attaining drastically better quality quicker. By being tailored to the complex task of segmenting intricate lines, specifically distinguishing it from previous methods, our approach offers drastic improvements in efficacy, transferable to a broad spectrum of applications beyond Etruscan mirrors.","sentences":["Etruscan mirrors constitute a significant category in Etruscan art, characterized by elaborate figurative illustrations featured on their backside.","A laborious and costly aspect of their analysis and documentation is the task of manually tracing these illustrations.","In previous work, a methodology has been proposed to automate this process, involving photometric-stereo scanning in combination with deep neural networks.","While achieving quantitative performance akin to an expert annotator, some results still lack qualitative precision and, thus, require annotators for inspection and potential correction, maintaining resource intensity.","In response, we propose a deep neural network trained to interactively refine existing annotations based on human guidance.","Our human-in-the-loop approach streamlines annotation, achieving equal quality with up to 75% less manual input required.","Moreover, during the refinement process, the relative improvement of our methodology over pure manual labeling reaches peak values of up to 26%, attaining drastically better quality quicker.","By being tailored to the complex task of segmenting intricate lines, specifically distinguishing it from previous methods, our approach offers drastic improvements in efficacy, transferable to a broad spectrum of applications beyond Etruscan mirrors."],"url":"http://arxiv.org/abs/2408.03304v1"}
{"created":"2024-08-06 17:09:56","title":"Understanding How Blind Users Handle Object Recognition Errors: Strategies and Challenges","abstract":"Object recognition technologies hold the potential to support blind and low-vision people in navigating the world around them. However, the gap between benchmark performances and practical usability remains a significant challenge. This paper presents a study aimed at understanding blind users' interaction with object recognition systems for identifying and avoiding errors. Leveraging a pre-existing object recognition system, URCam, fine-tuned for our experiment, we conducted a user study involving 12 blind and low-vision participants. Through in-depth interviews and hands-on error identification tasks, we gained insights into users' experiences, challenges, and strategies for identifying errors in camera-based assistive technologies and object recognition systems. During interviews, many participants preferred independent error review, while expressing apprehension toward misrecognitions. In the error identification task, participants varied viewpoints, backgrounds, and object sizes in their images to avoid and overcome errors. Even after repeating the task, participants identified only half of the errors, and the proportion of errors identified did not significantly differ from their first attempts. Based on these insights, we offer implications for designing accessible interfaces tailored to the needs of blind and low-vision users in identifying object recognition errors.","sentences":["Object recognition technologies hold the potential to support blind and low-vision people in navigating the world around them.","However, the gap between benchmark performances and practical usability remains a significant challenge.","This paper presents a study aimed at understanding blind users' interaction with object recognition systems for identifying and avoiding errors.","Leveraging a pre-existing object recognition system, URCam, fine-tuned for our experiment, we conducted a user study involving 12 blind and low-vision participants.","Through in-depth interviews and hands-on error identification tasks, we gained insights into users' experiences, challenges, and strategies for identifying errors in camera-based assistive technologies and object recognition systems.","During interviews, many participants preferred independent error review, while expressing apprehension toward misrecognitions.","In the error identification task, participants varied viewpoints, backgrounds, and object sizes in their images to avoid and overcome errors.","Even after repeating the task, participants identified only half of the errors, and the proportion of errors identified did not significantly differ from their first attempts.","Based on these insights, we offer implications for designing accessible interfaces tailored to the needs of blind and low-vision users in identifying object recognition errors."],"url":"http://arxiv.org/abs/2408.03303v1"}
{"created":"2024-08-06 17:08:05","title":"TextIM: Part-aware Interactive Motion Synthesis from Text","abstract":"In this work, we propose TextIM, a novel framework for synthesizing TEXT-driven human Interactive Motions, with a focus on the precise alignment of part-level semantics. Existing methods often overlook the critical roles of interactive body parts and fail to adequately capture and align part-level semantics, resulting in inaccuracies and even erroneous movement outcomes. To address these issues, TextIM utilizes a decoupled conditional diffusion framework to enhance the detailed alignment between interactive movements and corresponding semantic intents from textual descriptions. Our approach leverages large language models, functioning as a human brain, to identify interacting human body parts and to comprehend interaction semantics to generate complicated and subtle interactive motion. Guided by the refined movements of the interacting parts, TextIM further extends these movements into a coherent whole-body motion. We design a spatial coherence module to complement the entire body movements while maintaining consistency and harmony across body parts using a part graph convolutional network. For training and evaluation, we carefully selected and re-labeled interactive motions from HUMANML3D to develop a specialized dataset. Experimental results demonstrate that TextIM produces semantically accurate human interactive motions, significantly enhancing the realism and applicability of synthesized interactive motions in diverse scenarios, even including interactions with deformable and dynamically changing objects.","sentences":["In this work, we propose TextIM, a novel framework for synthesizing TEXT-driven human Interactive Motions, with a focus on the precise alignment of part-level semantics.","Existing methods often overlook the critical roles of interactive body parts and fail to adequately capture and align part-level semantics, resulting in inaccuracies and even erroneous movement outcomes.","To address these issues, TextIM utilizes a decoupled conditional diffusion framework to enhance the detailed alignment between interactive movements and corresponding semantic intents from textual descriptions.","Our approach leverages large language models, functioning as a human brain, to identify interacting human body parts and to comprehend interaction semantics to generate complicated and subtle interactive motion.","Guided by the refined movements of the interacting parts, TextIM further extends these movements into a coherent whole-body motion.","We design a spatial coherence module to complement the entire body movements while maintaining consistency and harmony across body parts using a part graph convolutional network.","For training and evaluation, we carefully selected and re-labeled interactive motions from HUMANML3D to develop a specialized dataset.","Experimental results demonstrate that TextIM produces semantically accurate human interactive motions, significantly enhancing the realism and applicability of synthesized interactive motions in diverse scenarios, even including interactions with deformable and dynamically changing objects."],"url":"http://arxiv.org/abs/2408.03302v1"}
{"created":"2024-08-06 16:55:54","title":"KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models","abstract":"By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks. However, in the process of integrating external non-parametric supporting evidence with internal parametric knowledge, inevitable knowledge conflicts may arise, leading to confusion in the model's responses. To enhance the knowledge selection of LLMs in various contexts, some research has focused on refining their behavior patterns through instruction-tuning. Nonetheless, due to the absence of explicit negative signals and comparative objectives, models fine-tuned in this manner may still exhibit undesirable behaviors in the intricate and realistic retrieval scenarios. To this end, we propose a Knowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving controllable knowledge selection in real retrieval scenarios. Concretely, we explore and simulate error types across diverse context combinations and learn how to avoid these negative signals through preference optimization methods. Simultaneously, by adjusting the balance between response length and the proportion of preference data representing different behavior patterns, we enhance the adherence capabilities and noise robustness of LLMs in a balanced manner. Experimental results show that KaPO outperforms previous methods for handling knowledge conflicts by over 37%, while also exhibiting robust generalization across various out-of-distribution datasets.","sentences":["By integrating external knowledge, Retrieval-Augmented Generation (RAG) has become an effective strategy for mitigating the hallucination problems that large language models (LLMs) encounter when dealing with knowledge-intensive tasks.","However, in the process of integrating external non-parametric supporting evidence with internal parametric knowledge, inevitable knowledge conflicts may arise, leading to confusion in the model's responses.","To enhance the knowledge selection of LLMs in various contexts, some research has focused on refining their behavior patterns through instruction-tuning.","Nonetheless, due to the absence of explicit negative signals and comparative objectives, models fine-tuned in this manner may still exhibit undesirable behaviors in the intricate and realistic retrieval scenarios.","To this end, we propose a Knowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving controllable knowledge selection in real retrieval scenarios.","Concretely, we explore and simulate error types across diverse context combinations and learn how to avoid these negative signals through preference optimization methods.","Simultaneously, by adjusting the balance between response length and the proportion of preference data representing different behavior patterns, we enhance the adherence capabilities and noise robustness of LLMs in a balanced manner.","Experimental results show that KaPO outperforms previous methods for handling knowledge conflicts by over 37%, while also exhibiting robust generalization across various out-of-distribution datasets."],"url":"http://arxiv.org/abs/2408.03297v1"}
{"created":"2024-08-06 16:48:25","title":"Left of Fab: Securing Design and Collaboration in the Semiconductor Value Chain","abstract":"The purpose of this paper is to fill a gap in the general understanding -- and academic scrutiny -- of current and emerging workflows for designing and fabricating integrated circuits. The approach is to compare the IC design workflow with that for printed circuit boards, then to discern a classification for threats. The need to define and secure workflows is amplified by both U.S. investment in the semiconductor manufacturing and market forces affecting GPU production for AI applications. The origin of this knowledge gap can be the proprietary nature of solution spaces, but it can be the lack of demand for teaching and learning for engineers and technicians in this domain. This paper presents a framework for understanding the security of design workflows in a vendor- and tool-agnostic way.","sentences":["The purpose of this paper is to fill a gap in the general understanding -- and academic scrutiny -- of current and emerging workflows for designing and fabricating integrated circuits.","The approach is to compare the IC design workflow with that for printed circuit boards, then to discern a classification for threats.","The need to define and secure workflows is amplified by both U.S. investment in the semiconductor manufacturing and market forces affecting GPU production for AI applications.","The origin of this knowledge gap can be the proprietary nature of solution spaces, but it can be the lack of demand for teaching and learning for engineers and technicians in this domain.","This paper presents a framework for understanding the security of design workflows in a vendor- and tool-agnostic way."],"url":"http://arxiv.org/abs/2408.03295v1"}
{"created":"2024-08-06 16:41:33","title":"Static IR Drop Prediction with Attention U-Net and Saliency-Based Explainability","abstract":"There has been significant recent progress to reduce the computational effort of static IR drop analysis using neural networks, and modeling as an image-to-image translation task. A crucial issue is the lack of sufficient data from real industry designs to train these networks. Additionally, there is no methodology to explain a high-drop pixel in a predicted IR drop image to its specific root-causes. In this work, we first propose a U-Net neural network model with attention gates which is specifically tailored to achieve fast and accurate image-based static IR drop prediction. Attention gates allow selective emphasis on relevant parts of the input data without supervision which is desired because of the often sparse nature of the IR drop map. We propose a two-phase training process which utilizes a mix of artificially-generated data and a limited number of points from real designs. The results are, on-average, 18% (53%) better in MAE and 14% (113%) in F1 score compared to the winner of the ICCAD 2023 contest (and U-Net only) when tested on real designs. Second, we propose a fast method using saliency maps which can explain a predicted IR drop in terms of specific input pixels contributing the most to a drop. In our experiments, we show the number of high IR drop pixels can be reduced on-average by 18% by mimicking upsize of a tiny portion of PDN's resistive edges.","sentences":["There has been significant recent progress to reduce the computational effort of static IR drop analysis using neural networks, and modeling as an image-to-image translation task.","A crucial issue is the lack of sufficient data from real industry designs to train these networks.","Additionally, there is no methodology to explain a high-drop pixel in a predicted IR drop image to its specific root-causes.","In this work, we first propose a U-Net neural network model with attention gates which is specifically tailored to achieve fast and accurate image-based static IR drop prediction.","Attention gates allow selective emphasis on relevant parts of the input data without supervision which is desired because of the often sparse nature of the IR drop map.","We propose a two-phase training process which utilizes a mix of artificially-generated data and a limited number of points from real designs.","The results are, on-average, 18% (53%) better in MAE and 14% (113%) in F1 score compared to the winner of the ICCAD 2023 contest (and U-Net only) when tested on real designs.","Second, we propose a fast method using saliency maps which can explain a predicted IR drop in terms of specific input pixels contributing the most to a drop.","In our experiments, we show the number of high IR drop pixels can be reduced on-average by 18% by mimicking upsize of a tiny portion of PDN's resistive edges."],"url":"http://arxiv.org/abs/2408.03292v1"}
{"created":"2024-08-06 16:40:04","title":"DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training Quantization for Vision Transformers","abstract":"Vision transformers (ViTs) have garnered significant attention for their performance in vision tasks; however, the high computational cost and significant latency issues have hinder widespread adoption. Post-training quantization (PTQ), a promising method for model compression, still faces accuracy degradation challenges with ViTs. There are two reasons for this: the existing quantization paradigm does not fit the power-law distribution of post-Softmax activations well, and accuracy inevitably decreases after reparameterizing post-LayerNorm activations. We propose a Distribution-Friendly and Outlier-Aware Post-training Quantization method for Vision Transformers, named DopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers and introduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses more on values near 1, more accurately preserving the power-law distribution of post-Softmax activations, and achieves favorable results. Moreover, when reparameterizing post-LayerNorm activations from channel-wise to layer-wise quantization, the accuracy degradation is mainly due to the significant impact of outliers in the scaling factors. Therefore, DopQ-ViT proposes a method to Search for the Optimal Scaling Factor, denoted as SOSF, which compensates for the influence of outliers and preserves the performance of the quantization model. DopQ-ViT has undergone extensive validation and demonstrates significant performance improvements in quantization models, particularly in low-bit settings.","sentences":["Vision transformers (ViTs) have garnered significant attention for their performance in vision tasks; however, the high computational cost and significant latency issues have hinder widespread adoption.","Post-training quantization (PTQ), a promising method for model compression, still faces accuracy degradation challenges with ViTs.","There are two reasons for this: the existing quantization paradigm does not fit the power-law distribution of post-Softmax activations well, and accuracy inevitably decreases after reparameterizing post-LayerNorm activations.","We propose a Distribution-Friendly and Outlier-Aware Post-training Quantization method for Vision Transformers, named DopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers and introduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses more on values near 1, more accurately preserving the power-law distribution of post-Softmax activations, and achieves favorable results.","Moreover, when reparameterizing post-LayerNorm activations from channel-wise to layer-wise quantization, the accuracy degradation is mainly due to the significant impact of outliers in the scaling factors.","Therefore, DopQ-ViT proposes a method to Search for the Optimal Scaling Factor, denoted as SOSF, which compensates for the influence of outliers and preserves the performance of the quantization model.","DopQ-ViT has undergone extensive validation and demonstrates significant performance improvements in quantization models, particularly in low-bit settings."],"url":"http://arxiv.org/abs/2408.03291v1"}
{"created":"2024-08-06 16:39:42","title":"SARA: Singular-Value Based Adaptive Low-Rank Adaption","abstract":"With the increasing number of parameters in large pre-trained models, LoRA as a parameter-efficient fine-tuning(PEFT) method is widely used for not adding inference overhead. The LoRA method assumes that weight changes during fine-tuning can be approximated by low-rank matrices. However, the rank values need to be manually verified to match different downstream tasks, and they cannot accommodate the varying importance of different layers in the model. In this work, we first analyze the relationship between the performance of different layers and their ranks using SVD. Based on this, we design the Singular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds the rank during initialization by performing SVD on the pre-trained weights. Additionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly reduces the number of parameters by fine-tuning only multiple parallel sets of singular values controlled by a router. Extensive experiments on various complex tasks demonstrate the simplicity and parameter efficiency of our methods. They can effectively and adaptively find the most suitable rank for each layer of each model.","sentences":["With the increasing number of parameters in large pre-trained models, LoRA as a parameter-efficient fine-tuning(PEFT) method is widely used for not adding inference overhead.","The LoRA method assumes that weight changes during fine-tuning can be approximated by low-rank matrices.","However, the rank values need to be manually verified to match different downstream tasks, and they cannot accommodate the varying importance of different layers in the model.","In this work, we first analyze the relationship between the performance of different layers and their ranks using SVD.","Based on this, we design the Singular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds the rank during initialization by performing SVD on the pre-trained weights.","Additionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly reduces the number of parameters by fine-tuning only multiple parallel sets of singular values controlled by a router.","Extensive experiments on various complex tasks demonstrate the simplicity and parameter efficiency of our methods.","They can effectively and adaptively find the most suitable rank for each layer of each model."],"url":"http://arxiv.org/abs/2408.03290v1"}
{"created":"2024-08-06 16:35:25","title":"Malicious Internet Entity Detection Using Local Graph Inference","abstract":"Detection of malicious behavior in a large network is a challenging problem for machine learning in computer security, since it requires a model with high expressive power and scalable inference. Existing solutions struggle to achieve this feat -- current cybersec-tailored approaches are still limited in expressivity, and methods successful in other domains do not scale well for large volumes of data, rendering frequent retraining impossible. This work proposes a new perspective for learning from graph data that is modeling network entity interactions as a large heterogeneous graph. High expressivity of the method is achieved with neural network architecture HMILnet that naturally models this type of data and provides theoretical guarantees. The scalability is achieved by pursuing local graph inference, i.e., classifying individual vertices and their neighborhood as independent samples. Our experiments exhibit improvement over the state-of-the-art Probabilistic Threat Propagation (PTP) algorithm, show a further threefold accuracy improvement when additional data is used, which is not possible with the PTP algorithm, and demonstrate the generalization capabilities of the method to new, previously unseen entities.","sentences":["Detection of malicious behavior in a large network is a challenging problem for machine learning in computer security, since it requires a model with high expressive power and scalable inference.","Existing solutions struggle to achieve this feat -- current cybersec-tailored approaches are still limited in expressivity, and methods successful in other domains do not scale well for large volumes of data, rendering frequent retraining impossible.","This work proposes a new perspective for learning from graph data that is modeling network entity interactions as a large heterogeneous graph.","High expressivity of the method is achieved with neural network architecture HMILnet that naturally models this type of data and provides theoretical guarantees.","The scalability is achieved by pursuing local graph inference, i.e., classifying individual vertices and their neighborhood as independent samples.","Our experiments exhibit improvement over the state-of-the-art Probabilistic Threat Propagation (PTP) algorithm, show a further threefold accuracy improvement when additional data is used, which is not possible with the PTP algorithm, and demonstrate the generalization capabilities of the method to new, previously unseen entities."],"url":"http://arxiv.org/abs/2408.03287v1"}
{"created":"2024-08-06 16:34:04","title":"Biomedical SAM 2: Segment Anything in Biomedical Images and Videos","abstract":"Medical image segmentation and video object segmentation are essential for diagnosing and analyzing diseases by identifying and measuring biological structures. Recent advances in natural domain have been driven by foundation models like the Segment Anything Model 2 (SAM 2). To explore the performance of SAM 2 in biomedical applications, we designed two evaluation pipelines for single-frame image segmentation and multi-frame video segmentation with varied prompt designs, revealing SAM 2's limitations in medical contexts. Consequently, we developed BioSAM 2, an enhanced foundation model optimized for biomedical data based on SAM 2. Our experiments show that BioSAM 2 not only surpasses the performance of existing state-of-the-art foundation models but also matches or even exceeds specialist models, demonstrating its efficacy and potential in the medical domain.","sentences":["Medical image segmentation and video object segmentation are essential for diagnosing and analyzing diseases by identifying and measuring biological structures.","Recent advances in natural domain have been driven by foundation models like the Segment Anything Model 2 (SAM 2).","To explore the performance of SAM 2 in biomedical applications, we designed two evaluation pipelines for single-frame image segmentation and multi-frame video segmentation with varied prompt designs, revealing SAM 2's limitations in medical contexts.","Consequently, we developed BioSAM 2, an enhanced foundation model optimized for biomedical data based on SAM 2.","Our experiments show that BioSAM 2 not only surpasses the performance of existing state-of-the-art foundation models but also matches or even exceeds specialist models, demonstrating its efficacy and potential in the medical domain."],"url":"http://arxiv.org/abs/2408.03286v1"}
{"created":"2024-08-06 16:33:35","title":"JetUnit: Rendering Diverse Force Feedback in Virtual Reality Using Water Jets","abstract":"We propose JetUnit, a water-based VR haptic system designed to produce force feedback with a wide spectrum of intensities and frequencies through water jets. The key challenge in designing this system lies in optimizing parameters to enable the haptic device to generate force feedback that closely replicates the most intense force produced by direct water jets while ensuring the user remains dry. In this paper, we present the key design parameters of the JetUnit wearable device determined through a set of quantitative experiments and a perception study. We further conducted a user study to assess the impact of integrating our haptic solutions into virtual reality experiences. The results revealed that, by adhering to the design principles of JetUnit, the water-based haptic system is capable of delivering diverse force feedback sensations, significantly enhancing the immersive experience in virtual reality.","sentences":["We propose JetUnit, a water-based VR haptic system designed to produce force feedback with a wide spectrum of intensities and frequencies through water jets.","The key challenge in designing this system lies in optimizing parameters to enable the haptic device to generate force feedback that closely replicates the most intense force produced by direct water jets while ensuring the user remains dry.","In this paper, we present the key design parameters of the JetUnit wearable device determined through a set of quantitative experiments and a perception study.","We further conducted a user study to assess the impact of integrating our haptic solutions into virtual reality experiences.","The results revealed that, by adhering to the design principles of JetUnit, the water-based haptic system is capable of delivering diverse force feedback sensations, significantly enhancing the immersive experience in virtual reality."],"url":"http://arxiv.org/abs/2408.03285v1"}
{"created":"2024-08-06 16:31:45","title":"ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually Synced Facial Performer","abstract":"Lip-syncing videos with given audio is the foundation for various applications including the creation of virtual presenters or performers. While recent studies explore high-fidelity lip-sync with different techniques, their task-orientated models either require long-term videos for clip-specific training or retain visible artifacts. In this paper, we propose a unified and effective framework ReSyncer, that synchronizes generalized audio-visual facial information. The key design is revisiting and rewiring the Style-based generator to efficiently adopt 3D facial dynamics predicted by a principled style-injected Transformer. By simply re-configuring the information insertion mechanisms within the noise and style space, our framework fuses motion and appearance with unified training. Extensive experiments demonstrate that ReSyncer not only produces high-fidelity lip-synced videos according to audio, but also supports multiple appealing properties that are suitable for creating virtual presenters and performers, including fast personalized fine-tuning, video-driven lip-syncing, the transfer of speaking styles, and even face swapping. Resources can be found at https://guanjz20.github.io/projects/ReSyncer.","sentences":["Lip-syncing videos with given audio is the foundation for various applications including the creation of virtual presenters or performers.","While recent studies explore high-fidelity lip-sync with different techniques, their task-orientated models either require long-term videos for clip-specific training or retain visible artifacts.","In this paper, we propose a unified and effective framework ReSyncer, that synchronizes generalized audio-visual facial information.","The key design is revisiting and rewiring the Style-based generator to efficiently adopt 3D facial dynamics predicted by a principled style-injected Transformer.","By simply re-configuring the information insertion mechanisms within the noise and style space, our framework fuses motion and appearance with unified training.","Extensive experiments demonstrate that ReSyncer not only produces high-fidelity lip-synced videos according to audio, but also supports multiple appealing properties that are suitable for creating virtual presenters and performers, including fast personalized fine-tuning, video-driven lip-syncing, the transfer of speaking styles, and even face swapping.","Resources can be found at https://guanjz20.github.io/projects/ReSyncer."],"url":"http://arxiv.org/abs/2408.03284v1"}
{"created":"2024-08-06 16:29:51","title":"AMES: Asymmetric and Memory-Efficient Similarity Estimation for Instance-level Retrieval","abstract":"This work investigates the problem of instance-level image retrieval re-ranking with the constraint of memory efficiency, ultimately aiming to limit memory usage to 1KB per image. Departing from the prevalent focus on performance enhancements, this work prioritizes the crucial trade-off between performance and memory requirements. The proposed model uses a transformer-based architecture designed to estimate image-to-image similarity by capturing interactions within and across images based on their local descriptors. A distinctive property of the model is the capability for asymmetric similarity estimation. Database images are represented with a smaller number of descriptors compared to query images, enabling performance improvements without increasing memory consumption. To ensure adaptability across different applications, a universal model is introduced that adjusts to a varying number of local descriptors during the testing phase. Results on standard benchmarks demonstrate the superiority of our approach over both hand-crafted and learned models. In particular, compared with current state-of-the-art methods that overlook their memory footprint, our approach not only attains superior performance but does so with a significantly reduced memory footprint. The code and pretrained models are publicly available at: https://github.com/pavelsuma/ames","sentences":["This work investigates the problem of instance-level image retrieval re-ranking with the constraint of memory efficiency, ultimately aiming to limit memory usage to 1KB per image.","Departing from the prevalent focus on performance enhancements, this work prioritizes the crucial trade-off between performance and memory requirements.","The proposed model uses a transformer-based architecture designed to estimate image-to-image similarity by capturing interactions within and across images based on their local descriptors.","A distinctive property of the model is the capability for asymmetric similarity estimation.","Database images are represented with a smaller number of descriptors compared to query images, enabling performance improvements without increasing memory consumption.","To ensure adaptability across different applications, a universal model is introduced that adjusts to a varying number of local descriptors during the testing phase.","Results on standard benchmarks demonstrate the superiority of our approach over both hand-crafted and learned models.","In particular, compared with current state-of-the-art methods that overlook their memory footprint, our approach not only attains superior performance but does so with a significantly reduced memory footprint.","The code and pretrained models are publicly available at: https://github.com/pavelsuma/ames"],"url":"http://arxiv.org/abs/2408.03282v1"}
{"created":"2024-08-06 16:28:30","title":"StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation","abstract":"Evaluation is the baton for the development of large language models. Current evaluations typically employ a single-item assessment paradigm for each atomic test objective, which struggles to discern whether a model genuinely possesses the required capabilities or merely memorizes/guesses the answers to specific questions. To this end, we propose a novel evaluation framework referred to as StructEval. Starting from an atomic test objective, StructEval deepens and broadens the evaluation by conducting a structured assessment across multiple cognitive levels and critical concepts, and therefore offers a comprehensive, robust and consistent evaluation for LLMs. Experiments on three widely-used benchmarks demonstrate that StructEval serves as a reliable tool for resisting the risk of data contamination and reducing the interference of potential biases, thereby providing more reliable and consistent conclusions regarding model capabilities. Our framework also sheds light on the design of future principled and trustworthy LLM evaluation protocols.","sentences":["Evaluation is the baton for the development of large language models.","Current evaluations typically employ a single-item assessment paradigm for each atomic test objective, which struggles to discern whether a model genuinely possesses the required capabilities or merely memorizes/guesses the answers to specific questions.","To this end, we propose a novel evaluation framework referred to as StructEval.","Starting from an atomic test objective, StructEval deepens and broadens the evaluation by conducting a structured assessment across multiple cognitive levels and critical concepts, and therefore offers a comprehensive, robust and consistent evaluation for LLMs.","Experiments on three widely-used benchmarks demonstrate that StructEval serves as a reliable tool for resisting the risk of data contamination and reducing the interference of potential biases, thereby providing more reliable and consistent conclusions regarding model capabilities.","Our framework also sheds light on the design of future principled and trustworthy LLM evaluation protocols."],"url":"http://arxiv.org/abs/2408.03281v1"}
{"created":"2024-08-06 16:17:51","title":"Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments","abstract":"To deploy machine learning models on-device, practitioners use compression algorithms to shrink and speed up models while maintaining their high-quality output. A critical aspect of compression in practice is model comparison, including tracking many compression experiments, identifying subtle changes in model behavior, and negotiating complex accuracy-efficiency trade-offs. However, existing compression tools poorly support comparison, leading to tedious and, sometimes, incomplete analyses spread across disjoint tools. To support real-world comparative workflows, we develop an interactive visual system called Compress and Compare. Within a single interface, Compress and Compare surfaces promising compression strategies by visualizing provenance relationships between compressed models and reveals compression-induced behavior changes by comparing models' predictions, weights, and activations. We demonstrate how Compress and Compare supports common compression analysis tasks through two case studies, debugging failed compression on generative language models and identifying compression artifacts in image classification models. We further evaluate Compress and Compare in a user study with eight compression experts, illustrating its potential to provide structure to compression workflows, help practitioners build intuition about compression, and encourage thorough analysis of compression's effect on model behavior. Through these evaluations, we identify compression-specific challenges that future visual analytics tools should consider and Compress and Compare visualizations that may generalize to broader model comparison tasks.","sentences":["To deploy machine learning models on-device, practitioners use compression algorithms to shrink and speed up models while maintaining their high-quality output.","A critical aspect of compression in practice is model comparison, including tracking many compression experiments, identifying subtle changes in model behavior, and negotiating complex accuracy-efficiency trade-offs.","However, existing compression tools poorly support comparison, leading to tedious and, sometimes, incomplete analyses spread across disjoint tools.","To support real-world comparative workflows, we develop an interactive visual system called Compress and Compare.","Within a single interface, Compress and Compare surfaces promising compression strategies by visualizing provenance relationships between compressed models and reveals compression-induced behavior changes by comparing models' predictions, weights, and activations.","We demonstrate how Compress and Compare supports common compression analysis tasks through two case studies, debugging failed compression on generative language models and identifying compression artifacts in image classification models.","We further evaluate Compress and Compare in a user study with eight compression experts, illustrating its potential to provide structure to compression workflows, help practitioners build intuition about compression, and encourage thorough analysis of compression's effect on model behavior.","Through these evaluations, we identify compression-specific challenges that future visual analytics tools should consider and Compress and Compare visualizations that may generalize to broader model comparison tasks."],"url":"http://arxiv.org/abs/2408.03274v1"}
{"created":"2024-08-06 15:59:53","title":"Connections Beyond Data: Exploring Homophily With Visualizations","abstract":"Homophily refers to the tendency of individuals to associate with others who are similar to them in characteristics, such as, race, ethnicity, age, gender, or interests. In this paper, we investigate if individuals exhibit racial homophily when viewing visualizations, using mass shooting data in the United States as the example topic. We conducted a crowdsourced experiment (N=450) where each participant was shown a visualization displaying the counts of mass shooting victims, highlighting the counts for one of three racial groups (White, Black, or Hispanic). Participants were assigned to view visualizations highlighting their own race or a different race to assess the influence of racial concordance on changes in affect (emotion) and attitude towards gun control. While we did not find evidence of homophily, the results showed a significant negative shift in affect across all visualization conditions. Notably, political ideology significantly impacted changes in affect, with more liberal views correlating with a more negative affect change. Our findings underscore the complexity of reactions to mass shooting visualizations and suggest that future research should consider various methodological improvements to better assess homophily effects.","sentences":["Homophily refers to the tendency of individuals to associate with others who are similar to them in characteristics, such as, race, ethnicity, age, gender, or interests.","In this paper, we investigate if individuals exhibit racial homophily when viewing visualizations, using mass shooting data in the United States as the example topic.","We conducted a crowdsourced experiment (N=450) where each participant was shown a visualization displaying the counts of mass shooting victims, highlighting the counts for one of three racial groups (White, Black, or Hispanic).","Participants were assigned to view visualizations highlighting their own race or a different race to assess the influence of racial concordance on changes in affect (emotion) and attitude towards gun control.","While we did not find evidence of homophily, the results showed a significant negative shift in affect across all visualization conditions.","Notably, political ideology significantly impacted changes in affect, with more liberal views correlating with a more negative affect change.","Our findings underscore the complexity of reactions to mass shooting visualizations and suggest that future research should consider various methodological improvements to better assess homophily effects."],"url":"http://arxiv.org/abs/2408.03269v1"}
{"created":"2024-08-06 15:53:46","title":"Towards Fixing Panic Bugs for Real-world Rust Programs","abstract":"The Rust programming language has garnered significant attention due to its robust safety features and memory management capabilities. Despite its guaranteed memory safety, Rust programs still suffer from runtime errors that are unmanageable, i.e., panic errors. Notably, over half of the bugs in rustc, Rust's own compiler, are attributable to crash stemming from panic errors. However, understanding root causes and resolving these panics often requires substantial effort due to the limited information provided, and the stack backtrace could be intricate, often omitting the actual fault locations. Although numerous automated program repair techniques exist, we observe that the prevailing fix patterns do not readily apply to Rust programs due to natural differences in language mechanisms. To tackle the above challenges, this paper introduces a systematic study aimed at fixing Rust panic bugs. We commence by assembling a dataset, namely Panic4R, which includes 102 real panic bugs and their fixes from the top 500 most downloaded open-source crates. By analyzing Rust's implementation, we identify Rust-specific patterns for fixing panic bugs, which can aid in understanding and providing guidance for generating patches. Finally, we design and implement the first automated fixing tool, PanicKiller, for Rust panic bugs, which effectively generates correct patches on the real-world large-scale dataset, and has already assisted in the resolution of 28 panic bugs in open-source projects. Each resolved issue has been validated by the developers and merged into the respective codebases.","sentences":["The Rust programming language has garnered significant attention due to its robust safety features and memory management capabilities.","Despite its guaranteed memory safety, Rust programs still suffer from runtime errors that are unmanageable, i.e., panic errors.","Notably, over half of the bugs in rustc, Rust's own compiler, are attributable to crash stemming from panic errors.","However, understanding root causes and resolving these panics often requires substantial effort due to the limited information provided, and the stack backtrace could be intricate, often omitting the actual fault locations.","Although numerous automated program repair techniques exist, we observe that the prevailing fix patterns do not readily apply to Rust programs due to natural differences in language mechanisms.","To tackle the above challenges, this paper introduces a systematic study aimed at fixing Rust panic bugs.","We commence by assembling a dataset, namely Panic4R, which includes 102 real panic bugs and their fixes from the top 500 most downloaded open-source crates.","By analyzing Rust's implementation, we identify Rust-specific patterns for fixing panic bugs, which can aid in understanding and providing guidance for generating patches.","Finally, we design and implement the first automated fixing tool, PanicKiller, for Rust panic bugs, which effectively generates correct patches on the real-world large-scale dataset, and has already assisted in the resolution of 28 panic bugs in open-source projects.","Each resolved issue has been validated by the developers and merged into the respective codebases."],"url":"http://arxiv.org/abs/2408.03262v1"}
{"created":"2024-08-06 15:50:57","title":"Employing Vector Field Techniques on the Analysis of Memristor Cellular Nonlinear Networks Cell Dynamics","abstract":"This paper introduces an innovative graphical analysis tool for investigating the dynamics of Memristor Cellular Nonlinear Networks (M-CNNs) featuring 2nd-order processing elements, known as M-CNN cells. In the era of specialized hardware catering to the demands of intelligent autonomous systems, the integration of memristors within Cellular Nonlinear Networks (CNNs) has emerged as a promising paradigm due to their exceptional characteristics. However, the standard Dynamic Route Map (DRM) analysis, applicable to 1st-order systems, fails to address the intricacies of 2nd-order M-CNN cell dynamics, as well the 2nd-order DRM (DRM2) exhibits limitations on the graphical illustration of local dynamical properties of the M-CNN cells, e.g. state derivative's magnitude. To address this limitation, we propose a novel integration of M-CNN cell vector field into the cell's phase portrait, enhancing the analysis efficacy and enabling efficient M-CNN cell design. A comprehensive exploration of M-CNN cell dynamics is presented, showcasing the utility of the proposed graphical tool for various scenarios, including bistable and monostable behavior, and demonstrating its superior ability to reveal subtle variations in cell behavior. Through this work, we offer a refined perspective on the analysis and design of M-CNNs, paving the way for advanced applications in edge computing and specialized hardware.","sentences":["This paper introduces an innovative graphical analysis tool for investigating the dynamics of Memristor Cellular Nonlinear Networks (M-CNNs) featuring 2nd-order processing elements, known as M-CNN cells.","In the era of specialized hardware catering to the demands of intelligent autonomous systems, the integration of memristors within Cellular Nonlinear Networks (CNNs) has emerged as a promising paradigm due to their exceptional characteristics.","However, the standard Dynamic Route Map (DRM) analysis, applicable to 1st-order systems, fails to address the intricacies of 2nd-order M-CNN cell dynamics, as well the 2nd-order DRM (DRM2) exhibits limitations on the graphical illustration of local dynamical properties of the M-CNN cells, e.g. state derivative's magnitude.","To address this limitation, we propose a novel integration of M-CNN cell vector field into the cell's phase portrait, enhancing the analysis efficacy and enabling efficient M-CNN cell design.","A comprehensive exploration of M-CNN cell dynamics is presented, showcasing the utility of the proposed graphical tool for various scenarios, including bistable and monostable behavior, and demonstrating its superior ability to reveal subtle variations in cell behavior.","Through this work, we offer a refined perspective on the analysis and design of M-CNNs, paving the way for advanced applications in edge computing and specialized hardware."],"url":"http://arxiv.org/abs/2408.03260v1"}
{"created":"2024-08-06 15:40:32","title":"Synthesizing Text-to-SQL Data from Weak and Strong LLMs","abstract":"The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we introduce a synthetic data approach that combines data produced by larger, more powerful models (strong models) with error information data generated by smaller, not well-aligned models (weak models). The method not only enhances the domain generalization of text-to-SQL models but also explores the potential of error data supervision through preference learning. Furthermore, we employ the synthetic data approach for instruction tuning on open-source LLMs, resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is demonstrated through state-of-the-art results on the SPIDER and BIRD benchmarks, bridging the performance gap between open-source models and methods prompted by closed-source models.","sentences":["The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks.","In this paper, we introduce a synthetic data approach that combines data produced by larger, more powerful models (strong models) with error information data generated by smaller, not well-aligned models (weak models).","The method not only enhances the domain generalization of text-to-SQL models but also explores the potential of error data supervision through preference learning.","Furthermore, we employ the synthetic data approach for instruction tuning on open-source LLMs, resulting SENSE, a specialized text-to-SQL model.","The effectiveness of SENSE is demonstrated through state-of-the-art results on the SPIDER and BIRD benchmarks, bridging the performance gap between open-source models and methods prompted by closed-source models."],"url":"http://arxiv.org/abs/2408.03256v1"}
{"created":"2024-08-06 15:10:08","title":"Multi-User Mobile Augmented Reality for Cardiovascular Surgical Planning","abstract":"Collaborative planning for congenital heart diseases typically involves creating physical heart models through 3D printing, which are then examined by both surgeons and cardiologists. Recent developments in mobile augmented reality (AR) technologies have presented a viable alternative, known for their ease of use and portability. However, there is still a lack of research examining the utilization of multi-user mobile AR environments to support collaborative planning for cardiovascular surgeries. We created ARCollab, an iOS AR app designed for enabling multiple surgeons and cardiologists to interact with a patient's 3D heart model in a shared environment. ARCollab enables surgeons and cardiologists to import heart models, manipulate them through gestures and collaborate with other users, eliminating the need for fabricating physical heart models. Our evaluation of ARCollab's usability and usefulness in enhancing collaboration, conducted with three cardiothoracic surgeons and two cardiologists, marks the first human evaluation of a multi-user mobile AR tool for surgical planning. ARCollab is open-source, available at https://github.com/poloclub/arcollab.","sentences":["Collaborative planning for congenital heart diseases typically involves creating physical heart models through 3D printing, which are then examined by both surgeons and cardiologists.","Recent developments in mobile augmented reality (AR) technologies have presented a viable alternative, known for their ease of use and portability.","However, there is still a lack of research examining the utilization of multi-user mobile AR environments to support collaborative planning for cardiovascular surgeries.","We created ARCollab, an iOS AR app designed for enabling multiple surgeons and cardiologists to interact with a patient's 3D heart model in a shared environment.","ARCollab enables surgeons and cardiologists to import heart models, manipulate them through gestures and collaborate with other users, eliminating the need for fabricating physical heart models.","Our evaluation of ARCollab's usability and usefulness in enhancing collaboration, conducted with three cardiothoracic surgeons and two cardiologists, marks the first human evaluation of a multi-user mobile AR tool for surgical planning.","ARCollab is open-source, available at https://github.com/poloclub/arcollab."],"url":"http://arxiv.org/abs/2408.03249v1"}
{"created":"2024-08-06 15:07:08","title":"Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons","abstract":"In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks. Through an analysis of LLMs' internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances. Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions. By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation. Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks. Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning. Furthermore, we explored how contextual conflicts affect the retrieval of facts during the reasoning process to gain a comprehensive understanding of the factual recall behaviors of LLMs. Code and data will be available soon.","sentences":["In this paper, we investigate whether Large Language Models (LLMs) actively recall or retrieve their internal repositories of factual knowledge when faced with reasoning tasks.","Through an analysis of LLMs' internal factual recall at each reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness the critical factual associations under certain circumstances.","Instead, they tend to opt for alternative, shortcut-like pathways to answer reasoning questions.","By manually manipulating the recall process of parametric knowledge in LLMs, we demonstrate that enhancing this recall process directly improves reasoning performance whereas suppressing it leads to notable degradation.","Furthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a powerful technique for addressing complex reasoning tasks.","Our findings indicate that CoT can intensify the recall of factual knowledge by encouraging LLMs to engage in orderly and reliable reasoning.","Furthermore, we explored how contextual conflicts affect the retrieval of facts during the reasoning process to gain a comprehensive understanding of the factual recall behaviors of LLMs.","Code and data will be available soon."],"url":"http://arxiv.org/abs/2408.03247v1"}
{"created":"2024-08-06 15:06:40","title":"Making Long-Context Language Models Better Multi-Hop Reasoners","abstract":"Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased performance in the presence of noisy contexts. In this paper, we introduce Reasoning with Attributions, a novel approach that prompts LMs to supply attributions for each assertion during their reasoning. We validate our approach through experiments on three multi-hop datasets, employing both proprietary and open-source models, and demonstrate its efficacy and resilience. Furthermore, we explore methods to augment reasoning capabilities via fine-tuning and offer an attribution-annotated dataset and a specialized training strategy. Our fine-tuned model achieves competitive performance on multi-hop reasoning benchmarks, closely paralleling proprietary LMs such as ChatGPT and Claude-instant.","sentences":["Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications.","Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased performance in the presence of noisy contexts.","In this paper, we introduce Reasoning with Attributions, a novel approach that prompts LMs to supply attributions for each assertion during their reasoning.","We validate our approach through experiments on three multi-hop datasets, employing both proprietary and open-source models, and demonstrate its efficacy and resilience.","Furthermore, we explore methods to augment reasoning capabilities via fine-tuning and offer an attribution-annotated dataset and a specialized training strategy.","Our fine-tuned model achieves competitive performance on multi-hop reasoning benchmarks, closely paralleling proprietary LMs such as ChatGPT and Claude-instant."],"url":"http://arxiv.org/abs/2408.03246v1"}
{"created":"2024-08-06 15:05:27","title":"Modular assurance of an Autonomous Ferry using Contract-Based Design and Simulation-based Verification Principles","abstract":"With the introduction of autonomous technology into our society, e.g. autonomous shipping, it is important to assess and assure the safety of autonomous systems in a real-world context. Simulation-based testing is a common approach to attempt to verify performance of autonomous systems, but assurance also requires formal evidence. This paper introduces the Assurance of Digital Assets (ADA) framework, a structured method for the assurance of digital assets, i.e. novel, complex, or intelligent systems enabled by digital technologies, using contract-based design. Results are shown for an autonomous ferry assurance case, focusing on collision avoidance during the ferry's transit. Further, we discuss the role of simulation-based testing in verifying compliance to contract specifications, to build the necessary evidence for an assurance case.","sentences":["With the introduction of autonomous technology into our society, e.g. autonomous shipping, it is important to assess and assure the safety of autonomous systems in a real-world context.","Simulation-based testing is a common approach to attempt to verify performance of autonomous systems, but assurance also requires formal evidence.","This paper introduces the Assurance of Digital Assets (ADA) framework, a structured method for the assurance of digital assets, i.e. novel, complex, or intelligent systems enabled by digital technologies, using contract-based design.","Results are shown for an autonomous ferry assurance case, focusing on collision avoidance during the ferry's transit.","Further, we discuss the role of simulation-based testing in verifying compliance to contract specifications, to build the necessary evidence for an assurance case."],"url":"http://arxiv.org/abs/2408.03244v1"}
{"created":"2024-08-06 14:50:48","title":"LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for Accurate Robotic Grasping Under the Occlusion","abstract":"This paper addresses the challenge of perceiving complete object shapes through visual perception. While prior studies have demonstrated encouraging outcomes in segmenting the visible parts of objects within a scene, amodal segmentation, in particular, has the potential to allow robots to infer the occluded parts of objects. To this end, this paper introduces a new framework that explores amodal segmentation for robotic grasping in cluttered scenes, thus greatly enhancing robotic grasping abilities. Initially, we use a conventional segmentation algorithm to detect the visible segments of the target object, which provides shape priors for completing the full object mask. Particularly, to explore how to utilize semantic features from RGB images and geometric information from depth images, we propose a Linear-fusion Attention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the linear-fusion strategy to effectively fuse this cross-modal data, and then uses the prior visible mask as attention map to guide the network to focus on target feature locations for further complete mask recovery. Using the amodal mask of the target object provides advantages in selecting more accurate and robust grasp points compared to relying solely on the visible segments. The results on different datasets show that our method achieves state-of-the-art performance. Furthermore, the robot experiments validate the feasibility and robustness of this method in the real world. Our code and demonstrations are available on the project page: https://jrryzh.github.io/LAC-Net.","sentences":["This paper addresses the challenge of perceiving complete object shapes through visual perception.","While prior studies have demonstrated encouraging outcomes in segmenting the visible parts of objects within a scene, amodal segmentation, in particular, has the potential to allow robots to infer the occluded parts of objects.","To this end, this paper introduces a new framework that explores amodal segmentation for robotic grasping in cluttered scenes, thus greatly enhancing robotic grasping abilities.","Initially, we use a conventional segmentation algorithm to detect the visible segments of the target object, which provides shape priors for completing the full object mask.","Particularly, to explore how to utilize semantic features from RGB images and geometric information from depth images, we propose a Linear-fusion Attention-guided Convolutional Network (LAC-Net).","LAC-Net utilizes the linear-fusion strategy to effectively fuse this cross-modal data, and then uses the prior visible mask as attention map to guide the network to focus on target feature locations for further complete mask recovery.","Using the amodal mask of the target object provides advantages in selecting more accurate and robust grasp points compared to relying solely on the visible segments.","The results on different datasets show that our method achieves state-of-the-art performance.","Furthermore, the robot experiments validate the feasibility and robustness of this method in the real world.","Our code and demonstrations are available on the project page: https://jrryzh.github.io/LAC-Net."],"url":"http://arxiv.org/abs/2408.03238v1"}
{"created":"2024-08-06 14:50:11","title":"MacWilliams Identities for Rank metric codes over Galois rings","abstract":"The purpose of this paper is to study rank metric codes over Galois rings. In particular, we prove MacWilliams identities for finite chain rings relating the sequences of $q$-binomial moments of a code and its dual and for Gabidulin analogues of free codes over Galois rings we prove the corresponding MacWilliams identity as a functional expression relating the weight enumerator of the dual with the MacWilliams transform of the weight enumerator of the original code plus one extra term attached to the zero divisors.","sentences":["The purpose of this paper is to study rank metric codes over Galois rings.","In particular, we prove MacWilliams identities for finite chain rings relating the sequences of $q$-binomial moments of a code and its dual and for Gabidulin analogues of free codes over Galois rings we prove the corresponding MacWilliams identity as a functional expression relating the weight enumerator of the dual with the MacWilliams transform of the weight enumerator of the original code plus one extra term attached to the zero divisors."],"url":"http://arxiv.org/abs/2408.03237v1"}
{"created":"2024-08-06 14:48:34","title":"Analysis of Partially-Calibrated Sparse Subarrays for Direction Finding with Extended Degrees of Freedom","abstract":"This paper investigates the problem of direction-of-arrival (DOA) estimation using multiple partially-calibrated sparse subarrays. In particular, we present the Generalized Coarray Multiple Signal Classification (GCA-MUSIC) DOA estimation algorithm to scenarios with partially-calibrated sparse subarrays. The proposed GCA-MUSIC algorithm exploits the difference coarray for each subarray, followed by a specific pseudo-spectrum merging rule that is based on the intersection of the signal subspaces associated to each subarray. This rule assumes that there is no a priori knowledge about the cross-covariance between subarrays. In that way, only the second-order statistics of each subarray are used to estimate the directions with increased degrees of freedom, i.e., the estimation procedure preserves the coarray Multiple Signal Classification and sparse arrays properties to estimate more sources than the number of physical sensors in each subarray. Numerical simulations show that the proposed GCA-MUSIC has better performance than other similar strategies.","sentences":["This paper investigates the problem of direction-of-arrival (DOA) estimation using multiple partially-calibrated sparse subarrays.","In particular, we present the Generalized Coarray Multiple Signal Classification (GCA-MUSIC) DOA estimation algorithm to scenarios with partially-calibrated sparse subarrays.","The proposed GCA-MUSIC algorithm exploits the difference coarray for each subarray, followed by a specific pseudo-spectrum merging rule that is based on the intersection of the signal subspaces associated to each subarray.","This rule assumes that there is no a priori knowledge about the cross-covariance between subarrays.","In that way, only the second-order statistics of each subarray are used to estimate the directions with increased degrees of freedom, i.e., the estimation procedure preserves the coarray Multiple Signal Classification and sparse arrays properties to estimate more sources than the number of physical sensors in each subarray.","Numerical simulations show that the proposed GCA-MUSIC has better performance than other similar strategies."],"url":"http://arxiv.org/abs/2408.03236v1"}
{"created":"2024-08-06 14:44:55","title":"Contrastive Learning for Image Complexity Representation","abstract":"Quantifying and evaluating image complexity can be instrumental in enhancing the performance of various computer vision tasks. Supervised learning can effectively learn image complexity features from well-annotated datasets. However, creating such datasets requires expensive manual annotation costs. The models may learn human subjective biases from it. In this work, we introduce the MoCo v2 framework. We utilize contrastive learning to represent image complexity, named CLIC (Contrastive Learning for Image Complexity). We find that there are complexity differences between different local regions of an image, and propose Random Crop and Mix (RCM), which can produce positive samples consisting of multi-scale local crops. RCM can also expand the train set and increase data diversity without introducing additional data. We conduct extensive experiments with CLIC, comparing it with both unsupervised and supervised methods. The results demonstrate that the performance of CLIC is comparable to that of state-of-the-art supervised methods. In addition, we establish the pipelines that can apply CLIC to computer vision tasks to effectively improve their performance.","sentences":["Quantifying and evaluating image complexity can be instrumental in enhancing the performance of various computer vision tasks.","Supervised learning can effectively learn image complexity features from well-annotated datasets.","However, creating such datasets requires expensive manual annotation costs.","The models may learn human subjective biases from it.","In this work, we introduce the MoCo v2 framework.","We utilize contrastive learning to represent image complexity, named CLIC (Contrastive Learning for Image Complexity).","We find that there are complexity differences between different local regions of an image, and propose Random Crop and Mix (RCM), which can produce positive samples consisting of multi-scale local crops.","RCM can also expand the train set and increase data diversity without introducing additional data.","We conduct extensive experiments with CLIC, comparing it with both unsupervised and supervised methods.","The results demonstrate that the performance of CLIC is comparable to that of state-of-the-art supervised methods.","In addition, we establish the pipelines that can apply CLIC to computer vision tasks to effectively improve their performance."],"url":"http://arxiv.org/abs/2408.03230v1"}
{"created":"2024-08-06 14:36:43","title":"Line-based 6-DoF Object Pose Estimation and Tracking With an Event Camera","abstract":"Pose estimation and tracking of objects is a fundamental application in 3D vision. Event cameras possess remarkable attributes such as high dynamic range, low latency, and resilience against motion blur, which enables them to address challenging high dynamic range scenes or high-speed motion. These features make event cameras an ideal complement over standard cameras for object pose estimation. In this work, we propose a line-based robust pose estimation and tracking method for planar or non-planar objects using an event camera. Firstly, we extract object lines directly from events, then provide an initial pose using a globally-optimal Branch-and-Bound approach, where 2D-3D line correspondences are not known in advance. Subsequently, we utilize event-line matching to establish correspondences between 2D events and 3D models. Furthermore, object poses are refined and continuously tracked by minimizing event-line distances. Events are assigned different weights based on these distances, employing robust estimation algorithms. To evaluate the precision of the proposed methods in object pose estimation and tracking, we have devised and established an event-based moving object dataset. Compared against state-of-the-art methods, the robustness and accuracy of our methods have been validated both on synthetic experiments and the proposed dataset. The source code is available at https://github.com/Zibin6/LOPET.","sentences":["Pose estimation and tracking of objects is a fundamental application in 3D vision.","Event cameras possess remarkable attributes such as high dynamic range, low latency, and resilience against motion blur, which enables them to address challenging high dynamic range scenes or high-speed motion.","These features make event cameras an ideal complement over standard cameras for object pose estimation.","In this work, we propose a line-based robust pose estimation and tracking method for planar or non-planar objects using an event camera.","Firstly, we extract object lines directly from events, then provide an initial pose using a globally-optimal Branch-and-Bound approach, where 2D-3D line correspondences are not known in advance.","Subsequently, we utilize event-line matching to establish correspondences between 2D events and 3D models.","Furthermore, object poses are refined and continuously tracked by minimizing event-line distances.","Events are assigned different weights based on these distances, employing robust estimation algorithms.","To evaluate the precision of the proposed methods in object pose estimation and tracking, we have devised and established an event-based moving object dataset.","Compared against state-of-the-art methods, the robustness and accuracy of our methods have been validated both on synthetic experiments and the proposed dataset.","The source code is available at https://github.com/Zibin6/LOPET."],"url":"http://arxiv.org/abs/2408.03225v1"}
{"created":"2024-08-06 14:36:29","title":"Don't Think It Twice: Exploit Shift Invariance for Efficient Online Streaming Inference of CNNs","abstract":"Deep learning time-series processing often relies on convolutional neural networks with overlapping windows. This overlap allows the network to produce an output faster than the window length. However, it introduces additional computations. This work explores the potential to optimize computational efficiency during inference by exploiting convolution's shift-invariance properties to skip the calculation of layer activations between successive overlapping windows. Although convolutions are shift-invariant, zero-padding and pooling operations, widely used in such networks, are not efficient and complicate efficient streaming inference. We introduce StreamiNNC, a strategy to deploy Convolutional Neural Networks for online streaming inference. We explore the adverse effects of zero padding and pooling on the accuracy of streaming inference, deriving theoretical error upper bounds for pooling during streaming. We address these limitations by proposing signal padding and pooling alignment and provide guidelines for designing and deploying models for StreamiNNC. We validate our method in simulated data and on three real-world biomedical signal processing applications. StreamiNNC achieves a low deviation between streaming output and normal inference for all three networks (2.03 - 3.55% NRMSE). This work demonstrates that it is possible to linearly speed up the inference of streaming CNNs processing overlapping windows, negating the additional computation typically incurred by overlapping windows.","sentences":["Deep learning time-series processing often relies on convolutional neural networks with overlapping windows.","This overlap allows the network to produce an output faster than the window length.","However, it introduces additional computations.","This work explores the potential to optimize computational efficiency during inference by exploiting convolution's shift-invariance properties to skip the calculation of layer activations between successive overlapping windows.","Although convolutions are shift-invariant, zero-padding and pooling operations, widely used in such networks, are not efficient and complicate efficient streaming inference.","We introduce StreamiNNC, a strategy to deploy Convolutional Neural Networks for online streaming inference.","We explore the adverse effects of zero padding and pooling on the accuracy of streaming inference, deriving theoretical error upper bounds for pooling during streaming.","We address these limitations by proposing signal padding and pooling alignment and provide guidelines for designing and deploying models for StreamiNNC.","We validate our method in simulated data and on three real-world biomedical signal processing applications.","StreamiNNC achieves a low deviation between streaming output and normal inference for all three networks (2.03 - 3.55% NRMSE).","This work demonstrates that it is possible to linearly speed up the inference of streaming CNNs processing overlapping windows, negating the additional computation typically incurred by overlapping windows."],"url":"http://arxiv.org/abs/2408.03223v1"}
{"created":"2024-08-06 14:26:54","title":"DRL-Assisted Dynamic QoT-Aware Service Provisioning in Multi-Band Elastic Optical Networks","abstract":"We propose a DRL-assisted approach for service provisioning in multi-band elastic optical networks. Our simulation environment uses an accurate QoT estimator based on the GN/EGN model. Results show that the proposed approach reduces request blocking by 50% compared with heuristics from the literature.","sentences":["We propose a DRL-assisted approach for service provisioning in multi-band elastic optical networks.","Our simulation environment uses an accurate QoT estimator based on the GN/EGN model.","Results show that the proposed approach reduces request blocking by 50% compared with heuristics from the literature."],"url":"http://arxiv.org/abs/2408.03221v1"}
{"created":"2024-08-06 14:26:09","title":"Masked Random Noise for Communication Efficient Federaetd Learning","abstract":"Federated learning is a promising distributed training paradigm that effectively safeguards data privacy. However, it may involve significant communication costs, which hinders training efficiency. In this paper, we aim to enhance communication efficiency from a new perspective. Specifically, we request the distributed clients to find optimal model updates relative to global model parameters within predefined random noise. For this purpose, we propose Federated Masked Random Noise (FedMRN), a novel framework that enables clients to learn a 1-bit mask for each model parameter and apply masked random noise (i.e., the Hadamard product of random noise and masks) to represent model updates. To make FedMRN feasible, we propose an advanced mask training strategy, called progressive stochastic masking (PSM). After local training, each client only need to transmit local masks and a random seed to the server. Additionally, we provide theoretical guarantees for the convergence of FedMRN under both strongly convex and non-convex assumptions. Extensive experiments are conducted on four popular datasets. The results show that FedMRN exhibits superior convergence speed and test accuracy compared to relevant baselines, while attaining a similar level of accuracy as FedAvg.","sentences":["Federated learning is a promising distributed training paradigm that effectively safeguards data privacy.","However, it may involve significant communication costs, which hinders training efficiency.","In this paper, we aim to enhance communication efficiency from a new perspective.","Specifically, we request the distributed clients to find optimal model updates relative to global model parameters within predefined random noise.","For this purpose, we propose Federated Masked Random Noise (FedMRN), a novel framework that enables clients to learn a 1-bit mask for each model parameter and apply masked random noise (i.e., the Hadamard product of random noise and masks) to represent model updates.","To make FedMRN feasible, we propose an advanced mask training strategy, called progressive stochastic masking (PSM).","After local training, each client only need to transmit local masks and a random seed to the server.","Additionally, we provide theoretical guarantees for the convergence of FedMRN under both strongly convex and non-convex assumptions.","Extensive experiments are conducted on four popular datasets.","The results show that FedMRN exhibits superior convergence speed and test accuracy compared to relevant baselines, while attaining a similar level of accuracy as FedAvg."],"url":"http://arxiv.org/abs/2408.03220v1"}
{"created":"2024-08-06 14:25:23","title":"Learning to Learn without Forgetting using Attention","abstract":"Continual learning (CL) refers to the ability to continually learn over time by accommodating new knowledge while retaining previously learned experience. While this concept is inherent in human learning, current machine learning methods are highly prone to overwrite previously learned patterns and thus forget past experience. Instead, model parameters should be updated selectively and carefully, avoiding unnecessary forgetting while optimally leveraging previously learned patterns to accelerate future learning. Since hand-crafting effective update mechanisms is difficult, we propose meta-learning a transformer-based optimizer to enhance CL. This meta-learned optimizer uses attention to learn the complex relationships between model parameters across a stream of tasks, and is designed to generate effective weight updates for the current task while preventing catastrophic forgetting on previously encountered tasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and SplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both forward and backward transfer, even on small sets of labeled data, highlighting the advantages of integrating a meta-learned optimizer within the continual learning framework.","sentences":["Continual learning (CL) refers to the ability to continually learn over time by accommodating new knowledge while retaining previously learned experience.","While this concept is inherent in human learning, current machine learning methods are highly prone to overwrite previously learned patterns and thus forget past experience.","Instead, model parameters should be updated selectively and carefully, avoiding unnecessary forgetting while optimally leveraging previously learned patterns to accelerate future learning.","Since hand-crafting effective update mechanisms is difficult, we propose meta-learning a transformer-based optimizer to enhance CL.","This meta-learned optimizer uses attention to learn the complex relationships between model parameters across a stream of tasks, and is designed to generate effective weight updates for the current task while preventing catastrophic forgetting on previously encountered tasks.","Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and SplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both forward and backward transfer, even on small sets of labeled data, highlighting the advantages of integrating a meta-learned optimizer within the continual learning framework."],"url":"http://arxiv.org/abs/2408.03219v1"}
{"created":"2024-08-06 14:19:06","title":"FedBAT: Communication-Efficient Federated Learning via Learnable Binarization","abstract":"Federated learning is a promising distributed machine learning paradigm that can effectively exploit large-scale data without exposing users' privacy. However, it may incur significant communication overhead, thereby potentially impairing the training efficiency. To address this challenge, numerous studies suggest binarizing the model updates. Nonetheless, traditional methods usually binarize model updates in a post-training manner, resulting in significant approximation errors and consequent degradation in model accuracy. To this end, we propose Federated Binarization-Aware Training (FedBAT), a novel framework that directly learns binary model updates during the local training process, thus inherently reducing the approximation errors. FedBAT incorporates an innovative binarization operator, along with meticulously designed derivatives to facilitate efficient learning. In addition, we establish theoretical guarantees regarding the convergence of FedBAT. Extensive experiments are conducted on four popular datasets. The results show that FedBAT significantly accelerates the convergence and exceeds the accuracy of baselines by up to 9\\%, even surpassing that of FedAvg in some cases.","sentences":["Federated learning is a promising distributed machine learning paradigm that can effectively exploit large-scale data without exposing users' privacy.","However, it may incur significant communication overhead, thereby potentially impairing the training efficiency.","To address this challenge, numerous studies suggest binarizing the model updates.","Nonetheless, traditional methods usually binarize model updates in a post-training manner, resulting in significant approximation errors and consequent degradation in model accuracy.","To this end, we propose Federated Binarization-Aware Training (FedBAT), a novel framework that directly learns binary model updates during the local training process, thus inherently reducing the approximation errors.","FedBAT incorporates an innovative binarization operator, along with meticulously designed derivatives to facilitate efficient learning.","In addition, we establish theoretical guarantees regarding the convergence of FedBAT.","Extensive experiments are conducted on four popular datasets.","The results show that FedBAT significantly accelerates the convergence and exceeds the accuracy of baselines by up to 9\\%, even surpassing that of FedAvg in some cases."],"url":"http://arxiv.org/abs/2408.03215v1"}
{"created":"2024-08-06 14:08:22","title":"IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning using Instruct Prompts","abstract":"Diffusion models continuously push the boundary of state-of-the-art image generation, but the process is hard to control with any nuance: practice proves that textual prompts are inadequate for accurately describing image style or fine structural details (such as faces). ControlNet and IPAdapter address this shortcoming by conditioning the generative process on imagery instead, but each individual instance is limited to modeling a single conditional posterior: for practical use-cases, where multiple different posteriors are desired within the same workflow, training and using multiple adapters is cumbersome. We propose IPAdapter-Instruct, which combines natural-image conditioning with ``Instruct'' prompts to swap between interpretations for the same conditioning image: style transfer, object extraction, both, or something else still? IPAdapterInstruct efficiently learns multiple tasks with minimal loss in quality compared to dedicated per-task models.","sentences":["Diffusion models continuously push the boundary of state-of-the-art image generation, but the process is hard to control with any nuance: practice proves that textual prompts are inadequate for accurately describing image style or fine structural details (such as faces).","ControlNet and IPAdapter address this shortcoming by conditioning the generative process on imagery instead, but each individual instance is limited to modeling a single conditional posterior: for practical use-cases, where multiple different posteriors are desired within the same workflow, training and using multiple adapters is cumbersome.","We propose IPAdapter-Instruct, which combines natural-image conditioning with ``Instruct'' prompts to swap between interpretations for the same conditioning image: style transfer, object extraction, both, or something else still?","IPAdapterInstruct efficiently learns multiple tasks with minimal loss in quality compared to dedicated per-task models."],"url":"http://arxiv.org/abs/2408.03209v1"}
{"created":"2024-08-06 14:06:53","title":"Personalizing Federated Instrument Segmentation with Visual Trait Priors in Robotic Surgery","abstract":"Personalized federated learning (PFL) for surgical instrument segmentation (SIS) is a promising approach. It enables multiple clinical sites to collaboratively train a series of models in privacy, with each model tailored to the individual distribution of each site. Existing PFL methods rarely consider the personalization of multi-headed self-attention, and do not account for appearance diversity and instrument shape similarity, both inherent in surgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait priors for SIS, incorporating global-personalized disentanglement (GPD), appearance-regulation personalized enhancement (APE), and shape-similarity global enhancement (SGE), to boost SIS performance in each site. GPD represents the first attempt at head-wise assignment for multi-headed self-attention personalization. To preserve the unique appearance representation of each site and gradually leverage the inter-site difference, APE introduces appearance regulation and provides customized layer-wise aggregation solutions via hypernetworks for each site's personalized parameters. The mutual shape information of instruments is maintained and shared via SGE, which enhances the cross-style shape consistency on the image level and computes the shape-similarity contribution of each site on the prediction level for updating the global parameters. PFedSIS outperforms state-of-the-art methods with +1.51% Dice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding code and models will be released at https://github.com/wzjialang/PFedSIS.","sentences":["Personalized federated learning (PFL) for surgical instrument segmentation (SIS) is a promising approach.","It enables multiple clinical sites to collaboratively train a series of models in privacy, with each model tailored to the individual distribution of each site.","Existing PFL methods rarely consider the personalization of multi-headed self-attention, and do not account for appearance diversity and instrument shape similarity, both inherent in surgical scenes.","We thus propose PFedSIS, a novel PFL method with visual trait priors for SIS, incorporating global-personalized disentanglement (GPD), appearance-regulation personalized enhancement (APE), and shape-similarity global enhancement (SGE), to boost SIS performance in each site.","GPD represents the first attempt at head-wise assignment for multi-headed self-attention personalization.","To preserve the unique appearance representation of each site and gradually leverage the inter-site difference, APE introduces appearance regulation and provides customized layer-wise aggregation solutions via hypernetworks for each site's personalized parameters.","The mutual shape information of instruments is maintained and shared via SGE, which enhances the cross-style shape consistency on the image level and computes the shape-similarity contribution of each site on the prediction level for updating the global parameters.","PFedSIS outperforms state-of-the-art methods with +1.51% Dice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains.","The corresponding code and models will be released at https://github.com/wzjialang/PFedSIS."],"url":"http://arxiv.org/abs/2408.03208v1"}
{"created":"2024-08-06 14:03:06","title":"GRAFX: An Open-Source Library for Audio Processing Graphs in PyTorch","abstract":"We present GRAFX, an open-source library designed for handling audio processing graphs in PyTorch. Along with various library functionalities, we describe technical details on the efficient parallel computation of input graphs, signals, and processor parameters in GPU. Then, we show its example use under a music mixing scenario, where parameters of every differentiable processor in a large graph are optimized via gradient descent. The code is available at https://github.com/sh-lee97/grafx.","sentences":["We present GRAFX, an open-source library designed for handling audio processing graphs in PyTorch.","Along with various library functionalities, we describe technical details on the efficient parallel computation of input graphs, signals, and processor parameters in GPU.","Then, we show its example use under a music mixing scenario, where parameters of every differentiable processor in a large graph are optimized via gradient descent.","The code is available at https://github.com/sh-lee97/grafx."],"url":"http://arxiv.org/abs/2408.03204v1"}
{"created":"2024-08-06 14:00:23","title":"A Debiased Nearest Neighbors Framework for Multi-Label Text Classification","abstract":"Multi-Label Text Classification (MLTC) is a practical yet challenging task that involves assigning multiple non-exclusive labels to each document. Previous studies primarily focus on capturing label correlations to assist label prediction by introducing special labeling schemes, designing specific model structures, or adding auxiliary tasks. Recently, the $k$ Nearest Neighbor ($k$NN) framework has shown promise by retrieving labeled samples as references to mine label co-occurrence information in the embedding space. However, two critical biases, namely embedding alignment bias and confidence estimation bias, are often overlooked, adversely affecting prediction performance. In this paper, we introduce a DEbiased Nearest Neighbors (DENN) framework for MLTC, specifically designed to mitigate these biases. To address embedding alignment bias, we propose a debiased contrastive learning strategy, enhancing neighbor consistency on label co-occurrence. For confidence estimation bias, we present a debiased confidence estimation strategy, improving the adaptive combination of predictions from $k$NN and inductive binary classifications. Extensive experiments conducted on four public benchmark datasets (i.e., AAPD, RCV1-V2, Amazon-531, and EUR-LEX57K) showcase the effectiveness of our proposed method. Besides, our method does not introduce any extra parameters.","sentences":["Multi-Label Text Classification (MLTC) is a practical yet challenging task that involves assigning multiple non-exclusive labels to each document.","Previous studies primarily focus on capturing label correlations to assist label prediction by introducing special labeling schemes, designing specific model structures, or adding auxiliary tasks.","Recently, the $k$ Nearest Neighbor ($k$NN) framework has shown promise by retrieving labeled samples as references to mine label co-occurrence information in the embedding space.","However, two critical biases, namely embedding alignment bias and confidence estimation bias, are often overlooked, adversely affecting prediction performance.","In this paper, we introduce a DEbiased Nearest Neighbors (DENN) framework for MLTC, specifically designed to mitigate these biases.","To address embedding alignment bias, we propose a debiased contrastive learning strategy, enhancing neighbor consistency on label co-occurrence.","For confidence estimation bias, we present a debiased confidence estimation strategy, improving the adaptive combination of predictions from $k$NN and inductive binary classifications.","Extensive experiments conducted on four public benchmark datasets (i.e., AAPD, RCV1-V2, Amazon-531, and EUR-LEX57K) showcase the effectiveness of our proposed method.","Besides, our method does not introduce any extra parameters."],"url":"http://arxiv.org/abs/2408.03202v1"}
{"created":"2024-08-06 13:58:56","title":"Adversarial Safety-Critical Scenario Generation using Naturalistic Human Driving Priors","abstract":"Evaluating the decision-making system is indispensable in developing autonomous vehicles, while realistic and challenging safety-critical test scenarios play a crucial role. Obtaining these scenarios is non-trivial, thanks to the long-tailed distribution, sparsity, and rarity in real-world data sets. To tackle this problem, in this paper, we introduce a natural adversarial scenario generation solution using naturalistic human driving priors and reinforcement learning techniques. By doing this, we can obtain large-scale test scenarios that are both diverse and realistic. Specifically, we build a simulation environment that mimics natural traffic interaction scenarios. Informed by this environment, we implement a two-stage procedure. The first stage incorporates conventional rule-based models, e.g., IDM~(Intelligent Driver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes) model, to coarsely and discretely capture and calibrate key control parameters from the real-world dataset. Next, we leverage GAIL~(Generative Adversarial Imitation Learning) to represent driver behaviors continuously. The derived GAIL can be further used to design a PPO~(Proximal Policy Optimization)-based actor-critic network framework to fine-tune the reward function, and then optimizes our natural adversarial scenario generation solution. Extensive experiments have been conducted in the NGSIM dataset including the trajectory of 3,000 vehicles. Essential traffic parameters were measured in comparison with the baseline model, e.g., the collision rate, accelerations, steering, and the number of lane changes. Our findings demonstrate that the proposed model can generate realistic safety-critical test scenarios covering both naturalness and adversariality, which can be a cornerstone for the development of autonomous vehicles.","sentences":["Evaluating the decision-making system is indispensable in developing autonomous vehicles, while realistic and challenging safety-critical test scenarios play a crucial role.","Obtaining these scenarios is non-trivial, thanks to the long-tailed distribution, sparsity, and rarity in real-world data sets.","To tackle this problem, in this paper, we introduce a natural adversarial scenario generation solution using naturalistic human driving priors and reinforcement learning techniques.","By doing this, we can obtain large-scale test scenarios that are both diverse and realistic.","Specifically, we build a simulation environment that mimics natural traffic interaction scenarios.","Informed by this environment, we implement a two-stage procedure.","The first stage incorporates conventional rule-based models, e.g., IDM~(Intelligent Driver Model) and MOBIL~(Minimizing Overall Braking Induced by Lane changes) model, to coarsely and discretely capture and calibrate key control parameters from the real-world dataset.","Next, we leverage GAIL~(Generative Adversarial Imitation Learning) to represent driver behaviors continuously.","The derived GAIL can be further used to design a PPO~(Proximal Policy Optimization)-based actor-critic network framework to fine-tune the reward function, and then optimizes our natural adversarial scenario generation solution.","Extensive experiments have been conducted in the NGSIM dataset including the trajectory of 3,000 vehicles.","Essential traffic parameters were measured in comparison with the baseline model, e.g., the collision rate, accelerations, steering, and the number of lane changes.","Our findings demonstrate that the proposed model can generate realistic safety-critical test scenarios covering both naturalness and adversariality, which can be a cornerstone for the development of autonomous vehicles."],"url":"http://arxiv.org/abs/2408.03200v1"}
{"created":"2024-08-06 13:55:51","title":"RELIEF: Reinforcement Learning Empowered Graph Feature Prompt Tuning","abstract":"The advent of the \"pre-train, prompt\" paradigm has recently extended its generalization ability and data efficiency to graph representation learning, following its achievements in Natural Language Processing (NLP). Initial graph prompt tuning approaches tailored specialized prompting functions for Graph Neural Network (GNN) models pre-trained with specific strategies, such as edge prediction, thus limiting their applicability. In contrast, another pioneering line of research has explored universal prompting via adding prompts to the input graph's feature space, thereby removing the reliance on specific pre-training strategies. However, the necessity to add feature prompts to all nodes remains an open question. Motivated by findings from prompt tuning research in the NLP domain, which suggest that highly capable pre-trained models need less conditioning signal to achieve desired behaviors, we advocate for strategically incorporating necessary and lightweight feature prompts to certain graph nodes to enhance downstream task performance. This introduces a combinatorial optimization problem, requiring a policy to decide 1) which nodes to prompt and 2) what specific feature prompts to attach. We then address the problem by framing the prompt incorporation process as a sequential decision-making problem and propose our method, RELIEF, which employs Reinforcement Learning (RL) to optimize it. At each step, the RL agent selects a node (discrete action) and determines the prompt content (continuous action), aiming to maximize cumulative performance gain. Extensive experiments on graph and node-level tasks with various pre-training strategies in few-shot scenarios demonstrate that our RELIEF outperforms fine-tuning and other prompt-based approaches in classification performance and data efficiency.","sentences":["The advent of the \"pre-train, prompt\" paradigm has recently extended its generalization ability and data efficiency to graph representation learning, following its achievements in Natural Language Processing (NLP).","Initial graph prompt tuning approaches tailored specialized prompting functions for Graph Neural Network (GNN) models pre-trained with specific strategies, such as edge prediction, thus limiting their applicability.","In contrast, another pioneering line of research has explored universal prompting via adding prompts to the input graph's feature space, thereby removing the reliance on specific pre-training strategies.","However, the necessity to add feature prompts to all nodes remains an open question.","Motivated by findings from prompt tuning research in the NLP domain, which suggest that highly capable pre-trained models need less conditioning signal to achieve desired behaviors, we advocate for strategically incorporating necessary and lightweight feature prompts to certain graph nodes to enhance downstream task performance.","This introduces a combinatorial optimization problem, requiring a policy to decide 1) which nodes to prompt and 2) what specific feature prompts to attach.","We then address the problem by framing the prompt incorporation process as a sequential decision-making problem and propose our method, RELIEF, which employs Reinforcement Learning (RL) to optimize it.","At each step, the RL agent selects a node (discrete action) and determines the prompt content (continuous action), aiming to maximize cumulative performance gain.","Extensive experiments on graph and node-level tasks with various pre-training strategies in few-shot scenarios demonstrate that our RELIEF outperforms fine-tuning and other prompt-based approaches in classification performance and data efficiency."],"url":"http://arxiv.org/abs/2408.03195v1"}
{"created":"2024-08-06 13:49:01","title":"Efficient NeRF Optimization -- Not All Samples Remain Equally Hard","abstract":"We propose an application of online hard sample mining for efficient training of Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality for many 3D reconstruction and rendering tasks but require substantial computational resources. The encoding of the scene information within the NeRF network parameters necessitates stochastic sampling. We observe that during the training, a major part of the compute time and memory usage is spent on processing already learnt samples, which no longer affect the model update significantly. We identify the backward pass on the stochastic samples as the computational bottleneck during the optimization. We thus perform the first forward pass in inference mode as a relatively low-cost search for hard samples. This is followed by building the computational graph and updating the NeRF network parameters using only the hard samples. To demonstrate the effectiveness of the proposed approach, we apply our method to Instant-NGP, resulting in significant improvements of the view-synthesis quality over the baseline (1 dB improvement on average per training time, or 2x speedup to reach the same PSNR level) along with approx. 40% memory savings coming from using only the hard samples to build the computational graph. As our method only interfaces with the network module, we expect it to be widely applicable.","sentences":["We propose an application of online hard sample mining for efficient training of Neural Radiance Fields (NeRF).","NeRF models produce state-of-the-art quality for many 3D reconstruction and rendering tasks but require substantial computational resources.","The encoding of the scene information within the NeRF network parameters necessitates stochastic sampling.","We observe that during the training, a major part of the compute time and memory usage is spent on processing already learnt samples, which no longer affect the model update significantly.","We identify the backward pass on the stochastic samples as the computational bottleneck during the optimization.","We thus perform the first forward pass in inference mode as a relatively low-cost search for hard samples.","This is followed by building the computational graph and updating the NeRF network parameters using only the hard samples.","To demonstrate the effectiveness of the proposed approach, we apply our method to Instant-NGP, resulting in significant improvements of the view-synthesis quality over the baseline (1 dB improvement on average per training time, or 2x speedup to reach the same PSNR level) along with approx.","40% memory savings coming from using only the hard samples to build the computational graph.","As our method only interfaces with the network module, we expect it to be widely applicable."],"url":"http://arxiv.org/abs/2408.03193v1"}
{"created":"2024-08-06 13:43:52","title":"Integrated Intention Prediction and Decision-Making with Spectrum Attention Net and Proximal Policy Optimization","abstract":"For autonomous driving in highly dynamic environments, it is anticipated to predict the future behaviors of surrounding vehicles (SVs) and make safe and effective decisions. However, modeling the inherent coupling effect between the prediction and decision-making modules has been a long-standing challenge, especially when there is a need to maintain appropriate computational efficiency. To tackle these problems, we propose a novel integrated intention prediction and decision-making approach, which explicitly models the coupling relationship and achieves efficient computation. Specifically, a spectrum attention net is designed to predict the intentions of SVs by capturing the trends of each frequency component over time and their interrelations. Fast computation of the intention prediction module is attained as the predicted intentions are not decoded to trajectories in the executing process. Furthermore, the proximal policy optimization (PPO) algorithm is employed to address the non-stationary problem in the framework through a modest policy update enabled by a clipping mechanism within its objective function. On the basis of these developments, the intention prediction and decision-making modules are integrated through joint learning. Experiments are conducted in representative traffic scenarios, and the results reveal that the proposed integrated framework demonstrates superior performance over several deep reinforcement learning (DRL) baselines in terms of success rate, efficiency, and safety in driving tasks.","sentences":["For autonomous driving in highly dynamic environments, it is anticipated to predict the future behaviors of surrounding vehicles (SVs) and make safe and effective decisions.","However, modeling the inherent coupling effect between the prediction and decision-making modules has been a long-standing challenge, especially when there is a need to maintain appropriate computational efficiency.","To tackle these problems, we propose a novel integrated intention prediction and decision-making approach, which explicitly models the coupling relationship and achieves efficient computation.","Specifically, a spectrum attention net is designed to predict the intentions of SVs by capturing the trends of each frequency component over time and their interrelations.","Fast computation of the intention prediction module is attained as the predicted intentions are not decoded to trajectories in the executing process.","Furthermore, the proximal policy optimization (PPO) algorithm is employed to address the non-stationary problem in the framework through a modest policy update enabled by a clipping mechanism within its objective function.","On the basis of these developments, the intention prediction and decision-making modules are integrated through joint learning.","Experiments are conducted in representative traffic scenarios, and the results reveal that the proposed integrated framework demonstrates superior performance over several deep reinforcement learning (DRL) baselines in terms of success rate, efficiency, and safety in driving tasks."],"url":"http://arxiv.org/abs/2408.03191v1"}
{"created":"2024-08-06 13:39:54","title":"DaVE -- A Curated Database of Visualization Examples","abstract":"Visualization, from simple line plots to complex high-dimensional visual analysis systems, has established itself throughout numerous domains to explore, analyze, and evaluate data. Applying such visualizations in the context of simulation science where High-Performance Computing (HPC) produces ever-growing amounts of data that is more complex, potentially multidimensional, and multimodal, takes up resources and a high level of technological experience often not available to domain experts. In this work, we present DaVE -- a curated database of visualization examples, which aims to provide state-of-the-art and advanced visualization methods that arise in the context of HPC applications. Based on domain- or data-specific descriptors entered by the user, DaVE provides a list of appropriate visualization techniques, each accompanied by descriptions, examples, references, and resources. Sample code, adaptable container templates, and recipes for easy integration in HPC applications can be downloaded for easy access to high-fidelity visualizations. While the database is currently filled with a limited number of entries based on a broad evaluation of needs and challenges of current HPC users, DaVE is designed to be easily extended by experts from both the visualization and HPC communities.","sentences":["Visualization, from simple line plots to complex high-dimensional visual analysis systems, has established itself throughout numerous domains to explore, analyze, and evaluate data.","Applying such visualizations in the context of simulation science where High-Performance Computing (HPC) produces ever-growing amounts of data that is more complex, potentially multidimensional, and multimodal, takes up resources and a high level of technological experience often not available to domain experts.","In this work, we present DaVE -- a curated database of visualization examples, which aims to provide state-of-the-art and advanced visualization methods that arise in the context of HPC applications.","Based on domain- or data-specific descriptors entered by the user, DaVE provides a list of appropriate visualization techniques, each accompanied by descriptions, examples, references, and resources.","Sample code, adaptable container templates, and recipes for easy integration in HPC applications can be downloaded for easy access to high-fidelity visualizations.","While the database is currently filled with a limited number of entries based on a broad evaluation of needs and challenges of current HPC users, DaVE is designed to be easily extended by experts from both the visualization and HPC communities."],"url":"http://arxiv.org/abs/2408.03188v1"}
{"created":"2024-08-06 13:35:27","title":"MaskAnyone Toolkit: Offering Strategies for Minimizing Privacy Risks and Maximizing Utility in Audio-Visual Data Archiving","abstract":"This paper introduces MaskAnyone, a novel toolkit designed to navigate some privacy and ethical concerns of sharing audio-visual data in research. MaskAnyone offers a scalable, user-friendly solution for de-identifying individuals in video and audio content through face-swapping and voice alteration, supporting multi-person masking and real-time bulk processing. By integrating this tool within research practices, we aim to enhance data reproducibility and utility in social science research. Our approach draws on Design Science Research, proposing that MaskAnyone can facilitate safer data sharing and potentially reduce the storage of fully identifiable data. We discuss the development and capabilities of MaskAnyone, explore its integration into ethical research practices, and consider the broader implications of audio-visual data masking, including issues of consent and the risk of misuse. The paper concludes with a preliminary evaluation framework for assessing the effectiveness and ethical integration of masking tools in such research settings.","sentences":["This paper introduces MaskAnyone, a novel toolkit designed to navigate some privacy and ethical concerns of sharing audio-visual data in research.","MaskAnyone offers a scalable, user-friendly solution for de-identifying individuals in video and audio content through face-swapping and voice alteration, supporting multi-person masking and real-time bulk processing.","By integrating this tool within research practices, we aim to enhance data reproducibility and utility in social science research.","Our approach draws on Design Science Research, proposing that MaskAnyone can facilitate safer data sharing and potentially reduce the storage of fully identifiable data.","We discuss the development and capabilities of MaskAnyone, explore its integration into ethical research practices, and consider the broader implications of audio-visual data masking, including issues of consent and the risk of misuse.","The paper concludes with a preliminary evaluation framework for assessing the effectiveness and ethical integration of masking tools in such research settings."],"url":"http://arxiv.org/abs/2408.03185v1"}
{"created":"2024-08-06 13:22:51","title":"An Object is Worth 64x64 Pixels: Generating 3D Object via Image Diffusion","abstract":"We introduce a new approach for generating realistic 3D models with UV maps through a representation termed \"Object Images.\" This approach encapsulates surface geometry, appearance, and patch structures within a 64x64 pixel image, effectively converting complex 3D shapes into a more manageable 2D format. By doing so, we address the challenges of both geometric and semantic irregularity inherent in polygonal meshes. This method allows us to use image generation models, such as Diffusion Transformers, directly for 3D shape generation. Evaluated on the ABO dataset, our generated shapes with patch structures achieve point cloud FID comparable to recent 3D generative models, while naturally supporting PBR material generation.","sentences":["We introduce a new approach for generating realistic 3D models with UV maps through a representation termed \"Object Images.\"","This approach encapsulates surface geometry, appearance, and patch structures within a 64x64 pixel image, effectively converting complex 3D shapes into a more manageable 2D format.","By doing so, we address the challenges of both geometric and semantic irregularity inherent in polygonal meshes.","This method allows us to use image generation models, such as Diffusion Transformers, directly for 3D shape generation.","Evaluated on the ABO dataset, our generated shapes with patch structures achieve point cloud FID comparable to recent 3D generative models, while naturally supporting PBR material generation."],"url":"http://arxiv.org/abs/2408.03178v1"}
{"created":"2024-08-06 13:16:16","title":"Leveraging Parameter Efficient Training Methods for Low Resource Text Classification: A Case Study in Marathi","abstract":"With the surge in digital content in low-resource languages, there is an escalating demand for advanced Natural Language Processing (NLP) techniques tailored to these languages. BERT (Bidirectional Encoder Representations from Transformers), serving as the foundational framework for numerous NLP architectures and language models, is increasingly employed for the development of low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method for fine-tuning Large Language Models (LLMs) and reducing the training parameters to some extent to decrease the computational costs needed for training the model and achieve results comparable to a fully fine-tuned model. In this work, we present a study of PEFT methods for the Indic low-resource language Marathi. We conduct a comprehensive analysis of PEFT methods applied to various monolingual and multilingual Marathi BERT models. These approaches are evaluated on prominent text classification datasets like MahaSent, MahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to significantly expedite the training speed of the models, addressing a critical aspect of model development and deployment. In this study, we explore Low-Rank Adaptation of Large Language Models (LoRA) and adapter methods for low-resource text classification. We show that these methods are competitive with full fine-tuning and can be used without loss in accuracy. This study contributes valuable insights into the effectiveness of Marathi BERT models, offering a foundation for the continued advancement of NLP capabilities in Marathi and similar Indic languages.","sentences":["With the surge in digital content in low-resource languages, there is an escalating demand for advanced Natural Language Processing (NLP) techniques tailored to these languages.","BERT (Bidirectional Encoder Representations from Transformers), serving as the foundational framework for numerous NLP architectures and language models, is increasingly employed for the development of low-resource NLP models.","Parameter Efficient Fine-Tuning (PEFT) is a method for fine-tuning Large Language Models (LLMs) and reducing the training parameters to some extent to decrease the computational costs needed for training the model and achieve results comparable to a fully fine-tuned model.","In this work, we present a study of PEFT methods for the Indic low-resource language Marathi.","We conduct a comprehensive analysis of PEFT methods applied to various monolingual and multilingual Marathi BERT models.","These approaches are evaluated on prominent text classification datasets like MahaSent, MahaHate, and MahaNews.","The incorporation of PEFT techniques is demonstrated to significantly expedite the training speed of the models, addressing a critical aspect of model development and deployment.","In this study, we explore Low-Rank Adaptation of Large Language Models (LoRA) and adapter methods for low-resource text classification.","We show that these methods are competitive with full fine-tuning and can be used without loss in accuracy.","This study contributes valuable insights into the effectiveness of Marathi BERT models, offering a foundation for the continued advancement of NLP capabilities in Marathi and similar Indic languages."],"url":"http://arxiv.org/abs/2408.03172v1"}
{"created":"2024-08-06 13:13:54","title":"typedKanren: Statically Typed Relational Programming with Exhaustive Matching in Haskell","abstract":"We present a statically typed embedding of relational programming (specifically a dialect of miniKanren with disequality constraints) in Haskell. Apart from handling types, our dialect extends standard relational combinator repertoire with a variation of relational matching that supports static exhaustiveness checks. To hide the boilerplate definitions and support comfortable logic programming with user-defined data types we use generic programming via GHC.Generics as well as metaprogramming via Template Haskell. We demonstrate our dialect on several examples and compare its performance against some other known implementations of miniKanren.","sentences":["We present a statically typed embedding of relational programming (specifically a dialect of miniKanren with disequality constraints) in Haskell.","Apart from handling types, our dialect extends standard relational combinator repertoire with a variation of relational matching that supports static exhaustiveness checks.","To hide the boilerplate definitions and support comfortable logic programming with user-defined data types we use generic programming via GHC.Generics as well as metaprogramming via Template Haskell.","We demonstrate our dialect on several examples and compare its performance against some other known implementations of miniKanren."],"url":"http://arxiv.org/abs/2408.03170v1"}
{"created":"2024-08-06 13:11:36","title":"Training on the Fly: On-device Self-supervised Learning aboard Nano-drones within 20 mW","abstract":"Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning (TinyML), such as nano-drones, are becoming an increasingly attractive technology. Their small form factor (i.e., ~10cm diameter) ensures vast applicability, ranging from the exploration of narrow disaster scenarios to safe human-robot interaction. Simple electronics make these CPSes inexpensive, but strongly limit the computational, memory, and sensing resources available on board. In real-world applications, these limitations are further exacerbated by domain shift. This fundamental machine learning problem implies that model perception performance drops when moving from the training domain to a different deployment one. To cope with and mitigate this general problem, we present a novel on-device fine-tuning approach that relies only on the limited ultra-low power resources available aboard nano-drones. Then, to overcome the lack of ground-truth training labels aboard our CPS, we also employ a self-supervised method based on ego-motion consistency. Albeit our work builds on top of a specific real-world vision-based human pose estimation task, it is widely applicable for many embedded TinyML use cases. Our 512-image on-device training procedure is fully deployed aboard an ultra-low power GWT GAP9 System-on-Chip and requires only 1MB of memory while consuming as low as 19mW or running in just 510ms (at 38mW). Finally, we demonstrate the benefits of our on-device learning approach by field-testing our closed-loop CPS, showing a reduction in horizontal position error of up to 26% vs. a non-fine-tuned state-of-the-art baseline. In the most challenging never-seen-before environment, our on-device learning procedure makes the difference between succeeding or failing the mission.","sentences":["Miniaturized cyber-physical systems (CPSes) powered by tiny machine learning (TinyML), such as nano-drones, are becoming an increasingly attractive technology.","Their small form factor (i.e., ~10cm diameter) ensures vast applicability, ranging from the exploration of narrow disaster scenarios to safe human-robot interaction.","Simple electronics make these CPSes inexpensive, but strongly limit the computational, memory, and sensing resources available on board.","In real-world applications, these limitations are further exacerbated by domain shift.","This fundamental machine learning problem implies that model perception performance drops when moving from the training domain to a different deployment one.","To cope with and mitigate this general problem, we present a novel on-device fine-tuning approach that relies only on the limited ultra-low power resources available aboard nano-drones.","Then, to overcome the lack of ground-truth training labels aboard our CPS, we also employ a self-supervised method based on ego-motion consistency.","Albeit our work builds on top of a specific real-world vision-based human pose estimation task, it is widely applicable for many embedded TinyML use cases.","Our 512-image on-device training procedure is fully deployed aboard an ultra-low power GWT GAP9 System-on-Chip and requires only 1MB of memory while consuming as low as 19mW or running in just 510ms (at 38mW).","Finally, we demonstrate the benefits of our on-device learning approach by field-testing our closed-loop CPS, showing a reduction in horizontal position error of up to 26% vs. a non-fine-tuned state-of-the-art baseline.","In the most challenging never-seen-before environment, our on-device learning procedure makes the difference between succeeding or failing the mission."],"url":"http://arxiv.org/abs/2408.03168v1"}
{"created":"2024-08-06 13:07:08","title":"CADRL: Category-aware Dual-agent Reinforcement Learning for Explainable Recommendations over Knowledge Graphs","abstract":"Knowledge graphs (KGs) have been widely adopted to mitigate data sparsity and address cold-start issues in recommender systems. While existing KGs-based recommendation methods can predict user preferences and demands, they fall short in generating explicit recommendation paths and lack explainability. As a step beyond the above methods, recent advancements utilize reinforcement learning (RL) to find suitable items for a given user via explainable recommendation paths. However, the performance of these solutions is still limited by the following two points. (1) Lack of ability to capture contextual dependencies from neighboring information. (2) The excessive reliance on short recommendation paths due to efficiency concerns. To surmount these challenges, we propose a category-aware dual-agent reinforcement learning (CADRL) model for explainable recommendations over KGs. Specifically, our model comprises two components: (1) a category-aware gated graph neural network that jointly captures context-aware item representations from neighboring entities and categories, and (2) a dual-agent RL framework where two agents efficiently traverse long paths to search for suitable items. Finally, experimental results show that CADRL outperforms state-of-the-art models in terms of both effectiveness and efficiency on large-scale datasets.","sentences":["Knowledge graphs (KGs) have been widely adopted to mitigate data sparsity and address cold-start issues in recommender systems.","While existing KGs-based recommendation methods can predict user preferences and demands, they fall short in generating explicit recommendation paths and lack explainability.","As a step beyond the above methods, recent advancements utilize reinforcement learning (RL) to find suitable items for a given user via explainable recommendation paths.","However, the performance of these solutions is still limited by the following two points.","(1) Lack of ability to capture contextual dependencies from neighboring information.","(2) The excessive reliance on short recommendation paths due to efficiency concerns.","To surmount these challenges, we propose a category-aware dual-agent reinforcement learning (CADRL) model for explainable recommendations over KGs.","Specifically, our model comprises two components: (1) a category-aware gated graph neural network that jointly captures context-aware item representations from neighboring entities and categories, and (2) a dual-agent RL framework where two agents efficiently traverse long paths to search for suitable items.","Finally, experimental results show that CADRL outperforms state-of-the-art models in terms of both effectiveness and efficiency on large-scale datasets."],"url":"http://arxiv.org/abs/2408.03166v1"}
{"created":"2024-08-06 13:05:32","title":"Dilated Convolution with Learnable Spacings makes visual models more aligned with humans: a Grad-CAM study","abstract":"Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced convolution method that allows enlarging the receptive fields (RF) without increasing the number of parameters, like the dilated convolution, yet without imposing a regular grid. DCLS has been shown to outperform the standard and dilated convolutions on several computer vision benchmarks. Here, we show that, in addition, DCLS increases the models' interpretability, defined as the alignment with human visual strategies. To quantify it, we use the Spearman correlation between the models' GradCAM heatmaps and the ClickMe dataset heatmaps, which reflect human visual attention. We took eight reference models - ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and 36) - and drop-in replaced the standard convolution layers with DCLS ones. This improved the interpretability score in seven of them. Moreover, we observed that Grad-CAM generated random heatmaps for two models in our study: CAFormer and ConvFormer models, leading to low interpretability scores. We addressed this issue by introducing Threshold-Grad-CAM, a modification built on top of Grad-CAM that enhanced interpretability across nearly all models. The code and checkpoints to reproduce this study are available at: https://github.com/rabihchamas/DCLS-GradCAM-Eval.","sentences":["Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced convolution method that allows enlarging the receptive fields (RF) without increasing the number of parameters, like the dilated convolution, yet without imposing a regular grid.","DCLS has been shown to outperform the standard and dilated convolutions on several computer vision benchmarks.","Here, we show that, in addition, DCLS increases the models' interpretability, defined as the alignment with human visual strategies.","To quantify it, we use the Spearman correlation between the models' GradCAM heatmaps and the ClickMe dataset heatmaps, which reflect human visual attention.","We took eight reference models - ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and 36) - and drop-in replaced the standard convolution layers with DCLS ones.","This improved the interpretability score in seven of them.","Moreover, we observed that Grad-CAM generated random heatmaps for two models in our study: CAFormer and ConvFormer models, leading to low interpretability scores.","We addressed this issue by introducing Threshold-Grad-CAM, a modification built on top of Grad-CAM that enhanced interpretability across nearly all models.","The code and checkpoints to reproduce this study are available at: https://github.com/rabihchamas/DCLS-GradCAM-Eval."],"url":"http://arxiv.org/abs/2408.03164v1"}
{"created":"2024-08-06 12:55:20","title":"Narrowband-IoT (NB-IoT) and IoT Use Cases in Universities, Campuses, and Educational Institutions: A Research Analysis","abstract":"The main objective of this research paper is to analyze the available use cases of Narrowband-IoT and IoT in universities, campuses, and educational institutions. A literature review was conducted using multiple databases such as IEEE Xplore, ACM Digital Library, and Scopus. The study explores the benefits of IoT adoption in higher education. Various use cases of NB-IoT in educational institutions were analyzed, including smart campus management, asset tracking, monitoring, and safety and security systems. Of the six use cases assessed, three focused on the deployment of IoT Things, while three focused on NB-IoT Connectivity. The research paper concludes that NB-IoT technology has significant potential to enhance various aspects of educational institutions, from smart campus management to improving safety and security systems. The study recommends further exploration and implementation of NB-IoT technology in educational settings to improve efficiency, security, and overall campus management. The research highlights the potential applications of NB-IoT in universities and educational institutions, paving the way for future studies in this area. The social implications of this research could involve enhancing the overall learning experience for students, improving campus safety, and promoting technological advancements in educational settings.   Keywords: narrowband-IoT, Internet-of-Things, smart campus, smart institutions","sentences":["The main objective of this research paper is to analyze the available use cases of Narrowband-IoT and IoT in universities, campuses, and educational institutions.","A literature review was conducted using multiple databases such as IEEE Xplore, ACM Digital Library, and Scopus.","The study explores the benefits of IoT adoption in higher education.","Various use cases of NB-IoT in educational institutions were analyzed, including smart campus management, asset tracking, monitoring, and safety and security systems.","Of the six use cases assessed, three focused on the deployment of IoT Things, while three focused on NB-IoT Connectivity.","The research paper concludes that NB-IoT technology has significant potential to enhance various aspects of educational institutions, from smart campus management to improving safety and security systems.","The study recommends further exploration and implementation of NB-IoT technology in educational settings to improve efficiency, security, and overall campus management.","The research highlights the potential applications of NB-IoT in universities and educational institutions, paving the way for future studies in this area.","The social implications of this research could involve enhancing the overall learning experience for students, improving campus safety, and promoting technological advancements in educational settings.   ","Keywords: narrowband-IoT, Internet-of-Things, smart campus, smart institutions"],"url":"http://arxiv.org/abs/2408.03157v1"}
{"created":"2024-08-06 12:55:17","title":"Iterative CT Reconstruction via Latent Variable Optimization of Shallow Diffusion Models","abstract":"Image generative AI has garnered significant attention in recent years. In particular, the diffusion model, a core component of recent generative AI, produces high-quality images with rich diversity. In this study, we propose a novel CT reconstruction method by combining the denoising diffusion probabilistic model with iterative CT reconstruction. In sharp contrast to previous studies, we optimize the fidelity loss of CT reconstruction with respect to the latent variable of the diffusion model, instead of the image and model parameters. To suppress anatomical structure changes produced by the diffusion model, we shallow the diffusion and reverse processes, and fix a set of added noises in the reverse process to make it deterministic during inference. We demonstrate the effectiveness of the proposed method through sparse view CT reconstruction of 1/10 view projection data. Despite the simplicity of the implementation, the proposed method shows the capability of reconstructing high-quality images while preserving the patient's anatomical structure, and outperforms existing methods including iterative reconstruction, iterative reconstruction with total variation, and the diffusion model alone in terms of quantitative indices such as SSIM and PSNR. We also explore further sparse view CT using 1/20 view projection data with the same trained diffusion model. As the number of iterations increases, image quality improvement comparable to that of 1/10 sparse view CT reconstruction is achieved. In principle, the proposed method can be widely applied not only to CT but also to other imaging modalities such as MRI, PET, and SPECT.","sentences":["Image generative AI has garnered significant attention in recent years.","In particular, the diffusion model, a core component of recent generative AI, produces high-quality images with rich diversity.","In this study, we propose a novel CT reconstruction method by combining the denoising diffusion probabilistic model with iterative CT reconstruction.","In sharp contrast to previous studies, we optimize the fidelity loss of CT reconstruction with respect to the latent variable of the diffusion model, instead of the image and model parameters.","To suppress anatomical structure changes produced by the diffusion model, we shallow the diffusion and reverse processes, and fix a set of added noises in the reverse process to make it deterministic during inference.","We demonstrate the effectiveness of the proposed method through sparse view CT reconstruction of 1/10 view projection data.","Despite the simplicity of the implementation, the proposed method shows the capability of reconstructing high-quality images while preserving the patient's anatomical structure, and outperforms existing methods including iterative reconstruction, iterative reconstruction with total variation, and the diffusion model alone in terms of quantitative indices such as SSIM and PSNR.","We also explore further sparse view CT using 1/20 view projection data with the same trained diffusion model.","As the number of iterations increases, image quality improvement comparable to that of 1/10 sparse view CT reconstruction is achieved.","In principle, the proposed method can be widely applied not only to CT but also to other imaging modalities such as MRI, PET, and SPECT."],"url":"http://arxiv.org/abs/2408.03156v1"}
{"created":"2024-08-06 12:54:41","title":"Fake News Detection via Wisdom of Synthetic & Representative Crowds","abstract":"Social media companies have struggled to provide a democratically legitimate definition of \"Fake News\". Reliance on expert judgment has attracted criticism due to a general trust deficit and political polarisation. Approaches reliant on the ``wisdom of the crowds'' are a cost-effective, transparent and inclusive alternative. This paper provides a novel end-to-end methodology to detect fake news on X via \"wisdom of the synthetic & representative crowds\". We deploy an online survey on the Lucid platform to gather veracity assessments for a number of pandemic-related tweets from crowd-workers. Borrowing from the MrP literature, we train a Hierarchical Bayesian model to predict the veracity of each tweet from the perspective of different personae from the population of interest.   We then weight the predicted veracity assessments according to a representative stratification frame, such that decisions about ``fake'' tweets are representative of the overall polity of interest. Based on these aggregated scores, we analyse a corpus of tweets and perform a second MrP to generate state-level estimates of the number of people who share fake news. We find small but statistically meaningful heterogeneity in fake news sharing across US states. At the individual-level: i. sharing fake news is generally rare, with an average sharing probability interval [0.07,0.14]; ii. strong evidence that Democrats share less fake news, accounting for a reduction in the sharing odds of [57.3%,3.9%] relative to the average user; iii. when Republican definitions of fake news are used, it is the latter who show a decrease in the propensity to share fake news worth [50.8%, 2.0%]; iv. some evidence that women share less fake news than men, an effect worth a [29.5%,4.9%] decrease.","sentences":["Social media companies have struggled to provide a democratically legitimate definition of \"Fake News\".","Reliance on expert judgment has attracted criticism due to a general trust deficit and political polarisation.","Approaches reliant on the ``wisdom of the crowds'' are a cost-effective, transparent and inclusive alternative.","This paper provides a novel end-to-end methodology to detect fake news on X via \"wisdom of the synthetic & representative crowds\".","We deploy an online survey on the Lucid platform to gather veracity assessments for a number of pandemic-related tweets from crowd-workers.","Borrowing from the MrP literature, we train a Hierarchical Bayesian model to predict the veracity of each tweet from the perspective of different personae from the population of interest.   ","We then weight the predicted veracity assessments according to a representative stratification frame, such that decisions about ``fake'' tweets are representative of the overall polity of interest.","Based on these aggregated scores, we analyse a corpus of tweets and perform a second MrP to generate state-level estimates of the number of people who share fake news.","We find small but statistically meaningful heterogeneity in fake news sharing across US states.","At the individual-level: i. sharing fake news is generally rare, with an average sharing probability interval [0.07,0.14]; ii.","strong evidence that Democrats share less fake news, accounting for a reduction in the sharing odds of [57.3%,3.9%] relative to the average user; iii.","when Republican definitions of fake news are used, it is the latter who show a decrease in the propensity to share fake news worth [50.8%, 2.0%]; iv.","some evidence that women share less fake news than men, an effect worth a [29.5%,4.9%] decrease."],"url":"http://arxiv.org/abs/2408.03154v1"}
{"created":"2024-08-06 12:52:03","title":"TSC: A Simple Two-Sided Constraint against Over-Smoothing","abstract":"Graph Convolutional Neural Network (GCN), a widely adopted method for analyzing relational data, enhances node discriminability through the aggregation of neighboring information. Usually, stacking multiple layers can improve the performance of GCN by leveraging information from high-order neighbors. However, the increase of the network depth will induce the over-smoothing problem, which can be attributed to the quality and quantity of neighbors changing: (a) neighbor quality, node's neighbors become overlapping in high order, leading to aggregated information becoming indistinguishable, (b) neighbor quantity, the exponentially growing aggregated neighbors submerges the node's initial feature by recursively aggregating operations. Current solutions mainly focus on one of the above causes and seldom consider both at once.   Aiming at tackling both causes of over-smoothing in one shot, we introduce a simple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet potent techniques: random masking and contrastive constraint. The random masking acts on the representation matrix's columns to regulate the degree of information aggregation from neighbors, thus preventing the convergence of node representations. Meanwhile, the contrastive constraint, applied to the representation matrix's rows, enhances the discriminability of the nodes. Designed as a plug-in module, TSC can be easily coupled with GCN or SGC architectures. Experimental analyses on diverse real-world graph datasets verify that our approach markedly reduces the convergence of node's representation and the performance degradation in deeper GCN.","sentences":["Graph Convolutional Neural Network (GCN), a widely adopted method for analyzing relational data, enhances node discriminability through the aggregation of neighboring information.","Usually, stacking multiple layers can improve the performance of GCN by leveraging information from high-order neighbors.","However, the increase of the network depth will induce the over-smoothing problem, which can be attributed to the quality and quantity of neighbors changing: (a) neighbor quality, node's neighbors become overlapping in high order, leading to aggregated information becoming indistinguishable, (b) neighbor quantity, the exponentially growing aggregated neighbors submerges the node's initial feature by recursively aggregating operations.","Current solutions mainly focus on one of the above causes and seldom consider both at once.   ","Aiming at tackling both causes of over-smoothing in one shot, we introduce a simple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet potent techniques: random masking and contrastive constraint.","The random masking acts on the representation matrix's columns to regulate the degree of information aggregation from neighbors, thus preventing the convergence of node representations.","Meanwhile, the contrastive constraint, applied to the representation matrix's rows, enhances the discriminability of the nodes.","Designed as a plug-in module, TSC can be easily coupled with GCN or SGC architectures.","Experimental analyses on diverse real-world graph datasets verify that our approach markedly reduces the convergence of node's representation and the performance degradation in deeper GCN."],"url":"http://arxiv.org/abs/2408.03152v1"}
{"created":"2024-08-06 12:49:33","title":"Conditioning LLMs with Emotion in Neural Machine Translation","abstract":"Large Language Models (LLMs) have shown remarkable performance in Natural Language Processing tasks, including Machine Translation (MT). In this work, we propose a novel MT pipeline that integrates emotion information extracted from a Speech Emotion Recognition (SER) model into LLMs to enhance translation quality. We first fine-tune five existing LLMs on the Libri-trans dataset and select the most performant model. Subsequently, we augment LLM prompts with different dimensional emotions and train the selected LLM under these different configurations. Our experiments reveal that integrating emotion information, especially arousal, into LLM prompts leads to notable improvements in translation quality.","sentences":["Large Language Models (LLMs) have shown remarkable performance in Natural Language Processing tasks, including Machine Translation (MT).","In this work, we propose a novel MT pipeline that integrates emotion information extracted from a Speech Emotion Recognition (SER) model into LLMs to enhance translation quality.","We first fine-tune five existing LLMs on the Libri-trans dataset and select the most performant model.","Subsequently, we augment LLM prompts with different dimensional emotions and train the selected LLM under these different configurations.","Our experiments reveal that integrating emotion information, especially arousal, into LLM prompts leads to notable improvements in translation quality."],"url":"http://arxiv.org/abs/2408.03150v1"}
{"created":"2024-08-06 12:45:56","title":"Leveraging Entity Information for Cross-Modality Correlation Learning: The Entity-Guided Multimodal Summarization","abstract":"The rapid increase in multimedia data has spurred advancements in Multimodal Summarization with Multimodal Output (MSMO), which aims to produce a multimodal summary that integrates both text and relevant images. The inherent heterogeneity of content within multimodal inputs and outputs presents a significant challenge to the execution of MSMO. Traditional approaches typically adopt a holistic perspective on coarse image-text data or individual visual objects, overlooking the essential connections between objects and the entities they represent. To integrate the fine-grained entity knowledge, we propose an Entity-Guided Multimodal Summarization model (EGMS). Our model, building on BART, utilizes dual multimodal encoders with shared weights to process text-image and entity-image information concurrently. A gating mechanism then combines visual data for enhanced textual summary generation, while image selection is refined through knowledge distillation from a pre-trained vision-language model. Extensive experiments on public MSMO dataset validate the superiority of the EGMS method, which also prove the necessity to incorporate entity information into MSMO problem.","sentences":["The rapid increase in multimedia data has spurred advancements in Multimodal Summarization with Multimodal Output (MSMO), which aims to produce a multimodal summary that integrates both text and relevant images.","The inherent heterogeneity of content within multimodal inputs and outputs presents a significant challenge to the execution of MSMO.","Traditional approaches typically adopt a holistic perspective on coarse image-text data or individual visual objects, overlooking the essential connections between objects and the entities they represent.","To integrate the fine-grained entity knowledge, we propose an Entity-Guided Multimodal Summarization model (EGMS).","Our model, building on BART, utilizes dual multimodal encoders with shared weights to process text-image and entity-image information concurrently.","A gating mechanism then combines visual data for enhanced textual summary generation, while image selection is refined through knowledge distillation from a pre-trained vision-language model.","Extensive experiments on public MSMO dataset validate the superiority of the EGMS method, which also prove the necessity to incorporate entity information into MSMO problem."],"url":"http://arxiv.org/abs/2408.03149v1"}
{"created":"2024-08-06 12:42:37","title":"The Dawn of Decentralized Social Media: An Exploration of Bluesky's Public Opening","abstract":"Bluesky is a Twitter-like decentralized social media platform that has recently grown in popularity. After an invite-only period, it opened to the public worldwide on February 6th, 2024. In this paper, we provide a longitudinal analysis of user activity in the two months around the opening, studying changes in the general characteristics of the platform due to the rapid growth of the user base. We observe a broad distribution of activity similar to more established platforms, but a higher volume of original than reshared content, and very low toxicity. After opening to the public, Bluesky experienced a large surge in new users and activity, especially posting English and Japanese content. In particular, several accounts entered the discussion with suspicious behavior, like following many accounts and sharing content from low-credibility news outlets. Some of these have already been classified as spam or suspended, suggesting effective moderation.","sentences":["Bluesky is a Twitter-like decentralized social media platform that has recently grown in popularity.","After an invite-only period, it opened to the public worldwide on February 6th, 2024.","In this paper, we provide a longitudinal analysis of user activity in the two months around the opening, studying changes in the general characteristics of the platform due to the rapid growth of the user base.","We observe a broad distribution of activity similar to more established platforms, but a higher volume of original than reshared content, and very low toxicity.","After opening to the public, Bluesky experienced a large surge in new users and activity, especially posting English and Japanese content.","In particular, several accounts entered the discussion with suspicious behavior, like following many accounts and sharing content from low-credibility news outlets.","Some of these have already been classified as spam or suspended, suggesting effective moderation."],"url":"http://arxiv.org/abs/2408.03146v1"}
{"created":"2024-08-06 12:37:47","title":"SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast and Reliable Surface Defect Detection","abstract":"The aim of surface defect detection is to identify and localise abnormal regions on the surfaces of captured objects, a task that's increasingly demanded across various industries. Current approaches frequently fail to fulfil the extensive demands of these industries, which encompass high performance, consistency, and fast operation, along with the capacity to leverage the entirety of the available training data. Addressing these gaps, we introduce SuperSimpleNet, an innovative discriminative model that evolved from SimpleNet. This advanced model significantly enhances its predecessor's training consistency, inference time, as well as detection performance. SuperSimpleNet operates in an unsupervised manner using only normal training images but also benefits from labelled abnormal training images when they are available. SuperSimpleNet achieves state-of-the-art results in both the supervised and the unsupervised settings, as demonstrated by experiments across four challenging benchmark datasets. Code: https://github.com/blaz-r/SuperSimpleNet .","sentences":["The aim of surface defect detection is to identify and localise abnormal regions on the surfaces of captured objects, a task that's increasingly demanded across various industries.","Current approaches frequently fail to fulfil the extensive demands of these industries, which encompass high performance, consistency, and fast operation, along with the capacity to leverage the entirety of the available training data.","Addressing these gaps, we introduce SuperSimpleNet, an innovative discriminative model that evolved from SimpleNet.","This advanced model significantly enhances its predecessor's training consistency, inference time, as well as detection performance.","SuperSimpleNet operates in an unsupervised manner using only normal training images but also benefits from labelled abnormal training images when they are available.","SuperSimpleNet achieves state-of-the-art results in both the supervised and the unsupervised settings, as demonstrated by experiments across four challenging benchmark datasets.","Code: https://github.com/blaz-r/SuperSimpleNet ."],"url":"http://arxiv.org/abs/2408.03143v1"}
{"created":"2024-08-06 12:30:04","title":"Measuring interconnectedness of infectious diseases in funded and unfunded research: a temporal network analysis on bibliometric data 1995-2022","abstract":"Despite substantial investments in infectious disease research over the past decades, the field continues to struggle with inadequate long-term investment strategies and resource disparities, which highlights the critical need for a better understanding of funding and research landscapes to support evidence-based policymaking. Our study presents a novel perspective on the interconnectedness of evolving infectious disease knowledge. Through identifying publications based on funded and unfunded research, the analysis of temporal network of infectious disease associations reveals (i) growing compartmentalisation of funded research, i.e., it focuses on the groups of infectious diseases with readily established connections, and (ii) the growth in global integration in unfunded research, i.e., it tends to be more widely exploratory and links distant diseases. Moreover, we find that in both funded and unfunded research prominent diseases like HIV, malaria and tuberculosis have strong bridging effects facilitating global integration, while diphtheria, tetanus, and pertussis are characterised with strong local connectivity between themselves. We also find that although coronavirus has seen a surge in publications since COVID-19, its systemic impact on the interconnectedness of infectious disease knowledge remains relatively low. Our work highlights the importance of considering the interconnectedness of infectious diseases in health policy making and has potential to contribute to more efficient health resource allocation.","sentences":["Despite substantial investments in infectious disease research over the past decades, the field continues to struggle with inadequate long-term investment strategies and resource disparities, which highlights the critical need for a better understanding of funding and research landscapes to support evidence-based policymaking.","Our study presents a novel perspective on the interconnectedness of evolving infectious disease knowledge.","Through identifying publications based on funded and unfunded research, the analysis of temporal network of infectious disease associations reveals (i) growing compartmentalisation of funded research, i.e., it focuses on the groups of infectious diseases with readily established connections, and (ii) the growth in global integration in unfunded research, i.e., it tends to be more widely exploratory and links distant diseases.","Moreover, we find that in both funded and unfunded research prominent diseases like HIV, malaria and tuberculosis have strong bridging effects facilitating global integration, while diphtheria, tetanus, and pertussis are characterised with strong local connectivity between themselves.","We also find that although coronavirus has seen a surge in publications since COVID-19, its systemic impact on the interconnectedness of infectious disease knowledge remains relatively low.","Our work highlights the importance of considering the interconnectedness of infectious diseases in health policy making and has potential to contribute to more efficient health resource allocation."],"url":"http://arxiv.org/abs/2408.03140v1"}
{"created":"2024-08-06 12:16:15","title":"Stochastic Trajectory Optimization for Demonstration Imitation","abstract":"Humans often learn new skills by imitating the experts and gradually developing their proficiency. In this work, we introduce Stochastic Trajectory Optimization for Demonstration Imitation (STODI), a trajectory optimization framework for robots to imitate the shape of demonstration trajectories with improved dynamic performance. Consistent with the human learning process, demonstration imitation serves as an initial step, while trajectory optimization aims to enhance robot motion performance. By generating random noise and constructing proper cost functions, the STODI effectively explores and exploits generated noisy trajectories while preserving the demonstration shape characteristics. We employ three metrics to measure the similarity of trajectories in both the time and frequency domains to help with demonstration imitation. Theoretical analysis reveals relationships among these metrics, emphasizing the benefits of frequency-domain analysis for specific tasks. Experiments on a 7-DOF robotic arm in the PyBullet simulator validate the efficacy of the STODI framework, showcasing the improved optimization performance and stability compared to previous methods.","sentences":["Humans often learn new skills by imitating the experts and gradually developing their proficiency.","In this work, we introduce Stochastic Trajectory Optimization for Demonstration Imitation (STODI), a trajectory optimization framework for robots to imitate the shape of demonstration trajectories with improved dynamic performance.","Consistent with the human learning process, demonstration imitation serves as an initial step, while trajectory optimization aims to enhance robot motion performance.","By generating random noise and constructing proper cost functions, the STODI effectively explores and exploits generated noisy trajectories while preserving the demonstration shape characteristics.","We employ three metrics to measure the similarity of trajectories in both the time and frequency domains to help with demonstration imitation.","Theoretical analysis reveals relationships among these metrics, emphasizing the benefits of frequency-domain analysis for specific tasks.","Experiments on a 7-DOF robotic arm in the PyBullet simulator validate the efficacy of the STODI framework, showcasing the improved optimization performance and stability compared to previous methods."],"url":"http://arxiv.org/abs/2408.03131v1"}
{"created":"2024-08-06 12:07:32","title":"Inference Optimizations for Large Language Models: Effects, Challenges, and Practical Considerations","abstract":"Large language models are ubiquitous in natural language processing because they can adapt to new tasks without retraining. However, their sheer scale and complexity present unique challenges and opportunities, prompting researchers and practitioners to explore novel model training, optimization, and deployment methods. This literature review focuses on various techniques for reducing resource requirements and compressing large language models, including quantization, pruning, knowledge distillation, and architectural optimizations. The primary objective is to explore each method in-depth and highlight its unique challenges and practical applications. The discussed methods are categorized into a taxonomy that presents an overview of the optimization landscape and helps navigate it to understand the research trajectory better.","sentences":["Large language models are ubiquitous in natural language processing because they can adapt to new tasks without retraining.","However, their sheer scale and complexity present unique challenges and opportunities, prompting researchers and practitioners to explore novel model training, optimization, and deployment methods.","This literature review focuses on various techniques for reducing resource requirements and compressing large language models, including quantization, pruning, knowledge distillation, and architectural optimizations.","The primary objective is to explore each method in-depth and highlight its unique challenges and practical applications.","The discussed methods are categorized into a taxonomy that presents an overview of the optimization landscape and helps navigate it to understand the research trajectory better."],"url":"http://arxiv.org/abs/2408.03130v1"}
{"created":"2024-08-06 11:59:09","title":"Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral 7B Model and Data Augmentation","abstract":"This paper describes our approach to the SemEval-2024 safe biomedical Natural Language Inference for Clinical Trials (NLI4CT) task, which concerns classifying statements about Clinical Trial Reports (CTRs). We explored the capabilities of Mistral-7B, a generalist open-source Large Language Model (LLM). We developed a prompt for the NLI4CT task, and fine-tuned a quantized version of the model using an augmented version of the training dataset. The experimental results show that this approach can produce notable results in terms of the macro F1-score, while having limitations in terms of faithfulness and consistency. All the developed code is publicly available on a GitHub repository","sentences":["This paper describes our approach to the SemEval-2024 safe biomedical Natural Language Inference for Clinical Trials (NLI4CT) task, which concerns classifying statements about Clinical Trial Reports (CTRs).","We explored the capabilities of Mistral-7B, a generalist open-source Large Language Model (LLM).","We developed a prompt for the NLI4CT task, and fine-tuned a quantized version of the model using an augmented version of the training dataset.","The experimental results show that this approach can produce notable results in terms of the macro F1-score, while having limitations in terms of faithfulness and consistency.","All the developed code is publicly available on a GitHub repository"],"url":"http://arxiv.org/abs/2408.03127v1"}
{"created":"2024-08-06 11:56:26","title":"COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework","abstract":"As the NLP community increasingly addresses challenges associated with multilingualism, robust annotation tools are essential to handle multilingual datasets efficiently. In this paper, we introduce a code-mixed multilingual text annotation framework, COMMENTATOR, specifically designed for annotating code-mixed text. The tool demonstrates its effectiveness in token-level and sentence-level language annotation tasks for Hinglish text. We perform robust qualitative human-based evaluations to showcase COMMENTATOR led to 5x faster annotations than the best baseline. Our code is publicly available at \\url{https://github.com/lingo-iitgn/commentator}. The demonstration video is available at \\url{https://bit.ly/commentator_video}.","sentences":["As the NLP community increasingly addresses challenges associated with multilingualism, robust annotation tools are essential to handle multilingual datasets efficiently.","In this paper, we introduce a code-mixed multilingual text annotation framework, COMMENTATOR, specifically designed for annotating code-mixed text.","The tool demonstrates its effectiveness in token-level and sentence-level language annotation tasks for Hinglish text.","We perform robust qualitative human-based evaluations to showcase COMMENTATOR led to 5x faster annotations than the best baseline.","Our code is publicly available at \\url{https://github.com/lingo-iitgn/commentator}.","The demonstration video is available at \\url{https://bit.ly/commentator_video}."],"url":"http://arxiv.org/abs/2408.03125v1"}
{"created":"2024-08-06 11:49:25","title":"Flexible Type-Based Resource Estimation in Quantum Circuit Description Languages","abstract":"We introduce a type system for the Quipper language designed to derive upper bounds on the size of the circuits produced by the typed program. This size can be measured according to various metrics, including width, depth and gate count, but also variations thereof obtained by considering only some wire types or some gate kinds. The key ingredients for achieving this level of flexibility are effects and refinement types, both relying on indices, that is, generic arithmetic expressions whose operators are interpreted differently depending on the target metric. The approach is shown to be correct through logical predicates, under reasonable assumptions about the chosen resource metric. This approach is empirically evaluated through the QuRA tool, showing that, in many cases, inferring tight bounds is possible in a fully automatic way.","sentences":["We introduce a type system for the Quipper language designed to derive upper bounds on the size of the circuits produced by the typed program.","This size can be measured according to various metrics, including width, depth and gate count, but also variations thereof obtained by considering only some wire types or some gate kinds.","The key ingredients for achieving this level of flexibility are effects and refinement types, both relying on indices, that is, generic arithmetic expressions whose operators are interpreted differently depending on the target metric.","The approach is shown to be correct through logical predicates, under reasonable assumptions about the chosen resource metric.","This approach is empirically evaluated through the QuRA tool, showing that, in many cases, inferring tight bounds is possible in a fully automatic way."],"url":"http://arxiv.org/abs/2408.03121v1"}
{"created":"2024-08-06 11:49:13","title":"Benchmarking In-the-wild Multimodal Disease Recognition and A Versatile Baseline","abstract":"Existing plant disease classification models have achieved remarkable performance in recognizing in-laboratory diseased images. However, their performance often significantly degrades in classifying in-the-wild images. Furthermore, we observed that in-the-wild plant images may exhibit similar appearances across various diseases (i.e., small inter-class discrepancy) while the same diseases may look quite different (i.e., large intra-class variance). Motivated by this observation, we propose an in-the-wild multimodal plant disease recognition dataset that contains the largest number of disease classes but also text-based descriptions for each disease. Particularly, the newly provided text descriptions are introduced to provide rich information in textual modality and facilitate in-the-wild disease classification with small inter-class discrepancy and large intra-class variance issues. Therefore, our proposed dataset can be regarded as an ideal testbed for evaluating disease recognition methods in the real world. In addition, we further present a strong yet versatile baseline that models text descriptions and visual data through multiple prototypes for a given class. By fusing the contributions of multimodal prototypes in classification, our baseline can effectively address the small inter-class discrepancy and large intra-class variance issues. Remarkably, our baseline model can not only classify diseases but also recognize diseases in few-shot or training-free scenarios. Extensive benchmarking results demonstrate that our proposed in-the-wild multimodal dataset sets many new challenges to the plant disease recognition task and there is a large space to improve for future works.","sentences":["Existing plant disease classification models have achieved remarkable performance in recognizing in-laboratory diseased images.","However, their performance often significantly degrades in classifying in-the-wild images.","Furthermore, we observed that in-the-wild plant images may exhibit similar appearances across various diseases (i.e., small inter-class discrepancy) while the same diseases may look quite different (i.e., large intra-class variance).","Motivated by this observation, we propose an in-the-wild multimodal plant disease recognition dataset that contains the largest number of disease classes but also text-based descriptions for each disease.","Particularly, the newly provided text descriptions are introduced to provide rich information in textual modality and facilitate in-the-wild disease classification with small inter-class discrepancy and large intra-class variance issues.","Therefore, our proposed dataset can be regarded as an ideal testbed for evaluating disease recognition methods in the real world.","In addition, we further present a strong yet versatile baseline that models text descriptions and visual data through multiple prototypes for a given class.","By fusing the contributions of multimodal prototypes in classification, our baseline can effectively address the small inter-class discrepancy and large intra-class variance issues.","Remarkably, our baseline model can not only classify diseases but also recognize diseases in few-shot or training-free scenarios.","Extensive benchmarking results demonstrate that our proposed in-the-wild multimodal dataset sets many new challenges to the plant disease recognition task and there is a large space to improve for future works."],"url":"http://arxiv.org/abs/2408.03120v1"}
{"created":"2024-08-06 11:49:11","title":"Evaluating the Translation Performance of Large Language Models Based on Euas-20","abstract":"In recent years, with the rapid development of deep learning technology, large language models (LLMs) such as BERT and GPT have achieved breakthrough results in natural language processing tasks. Machine translation (MT), as one of the core tasks of natural language processing, has also benefited from the development of large language models and achieved a qualitative leap. Despite the significant progress in translation performance achieved by large language models, machine translation still faces many challenges. Therefore, in this paper, we construct the dataset Euas-20 to evaluate the performance of large language models on translation tasks, the translation ability on different languages, and the effect of pre-training data on the translation ability of LLMs for researchers and developers.","sentences":["In recent years, with the rapid development of deep learning technology, large language models (LLMs) such as BERT and GPT have achieved breakthrough results in natural language processing tasks.","Machine translation (MT), as one of the core tasks of natural language processing, has also benefited from the development of large language models and achieved a qualitative leap.","Despite the significant progress in translation performance achieved by large language models, machine translation still faces many challenges.","Therefore, in this paper, we construct the dataset Euas-20 to evaluate the performance of large language models on translation tasks, the translation ability on different languages, and the effect of pre-training data on the translation ability of LLMs for researchers and developers."],"url":"http://arxiv.org/abs/2408.03119v1"}
