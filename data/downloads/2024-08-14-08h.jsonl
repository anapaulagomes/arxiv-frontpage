{"created":"2024-08-13 17:57:14","title":"Fingerspelling within Sign Language Translation","abstract":"Fingerspelling poses challenges for sign language processing due to its high-frequency motion and use for open-vocabulary terms. While prior work has studied fingerspelling recognition, there has been little attention to evaluating how well sign language translation models understand fingerspelling in the context of entire sentences -- and improving this capability. We manually annotate instances of fingerspelling within FLEURS-ASL and use them to evaluate the effect of two simple measures to improve fingerspelling recognition within American Sign Language to English translation: 1) use a model family (ByT5) with character- rather than subword-level tokenization, and 2) mix fingerspelling recognition data into the translation training mixture. We find that 1) substantially improves understanding of fingerspelling (and therefore translation quality overall), but the effect of 2) is mixed.","sentences":["Fingerspelling poses challenges for sign language processing due to its high-frequency motion and use for open-vocabulary terms.","While prior work has studied fingerspelling recognition, there has been little attention to evaluating how well sign language translation models understand fingerspelling in the context of entire sentences -- and improving this capability.","We manually annotate instances of fingerspelling within FLEURS-ASL and use them to evaluate the effect of two simple measures to improve fingerspelling recognition within American Sign Language to English translation: 1) use a model family (ByT5) with character- rather than subword-level tokenization, and 2) mix fingerspelling recognition data into the translation training mixture.","We find that 1) substantially improves understanding of fingerspelling (and therefore translation quality overall), but the effect of 2) is mixed."],"url":"http://arxiv.org/abs/2408.07065v1"}
{"created":"2024-08-13 17:56:51","title":"HADRON: Human-friendly Control and Artificial Intelligence for Military Drone Operations","abstract":"As drones are getting more and more entangled in our society, more untrained users require the capability to operate them. This scenario is to be achieved through the development of artificial intelligence capabilities assisting the human operator in controlling the Unmanned Aerial System (UAS) and processing the sensor data, thereby alleviating the need for extensive operator training. This paper presents the HADRON project that seeks to develop and test multiple novel technologies to enable human-friendly control of drone swarms. This project is divided into three main parts. The first part consists of the integration of different technologies for the intuitive control of drones, focusing on novice or inexperienced pilots and operators. The second part focuses on the development of a multi-drone system that will be controlled from a command and control station, in which an expert pilot can supervise the operations of the multiple drones. The third part of the project will focus on reducing the cognitive load on human operators, whether they are novice or expert pilots. For this, we will develop AI tools that will assist drone operators with semi-automated real-time data processing.","sentences":["As drones are getting more and more entangled in our society, more untrained users require the capability to operate them.","This scenario is to be achieved through the development of artificial intelligence capabilities assisting the human operator in controlling the Unmanned Aerial System (UAS) and processing the sensor data, thereby alleviating the need for extensive operator training.","This paper presents the HADRON project that seeks to develop and test multiple novel technologies to enable human-friendly control of drone swarms.","This project is divided into three main parts.","The first part consists of the integration of different technologies for the intuitive control of drones, focusing on novice or inexperienced pilots and operators.","The second part focuses on the development of a multi-drone system that will be controlled from a command and control station, in which an expert pilot can supervise the operations of the multiple drones.","The third part of the project will focus on reducing the cognitive load on human operators, whether they are novice or expert pilots.","For this, we will develop AI tools that will assist drone operators with semi-automated real-time data processing."],"url":"http://arxiv.org/abs/2408.07063v1"}
{"created":"2024-08-13 17:50:28","title":"Diversity Empowers Intelligence: Integrating Expertise of Software Engineering Agents","abstract":"Large language model (LLM) agents have shown great potential in solving real-world software engineering (SWE) problems. The most advanced open-source SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite. However, these sophisticated agent frameworks exhibit varying strengths, excelling in certain tasks while underperforming in others. To fully harness the diversity of these agents, we propose DEI (Diversity Empowered Intelligence), a framework that leverages their unique expertise. DEI functions as a meta-module atop existing SWE agent frameworks, managing agent collectives for enhanced problem-solving. Experimental results show that a DEI-guided committee of agents is able to surpass the best individual agent's performance by a large margin. For instance, a group of open-source SWE agents, with a maximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3% resolve rate with DEI, making a 25% improvement and beating most closed-source solutions. Our best-performing group excels with a 55% resolve rate, securing the highest ranking on SWE-Bench Lite. Our findings contribute to the growing body of research on collaborative AI systems and their potential to solve complex software engineering challenges.","sentences":["Large language model (LLM) agents have shown great potential in solving real-world software engineering (SWE) problems.","The most advanced open-source SWE agent can resolve over 27% of real GitHub issues in SWE-Bench Lite.","However, these sophisticated agent frameworks exhibit varying strengths, excelling in certain tasks while underperforming in others.","To fully harness the diversity of these agents, we propose DEI (Diversity Empowered Intelligence), a framework that leverages their unique expertise.","DEI functions as a meta-module atop existing SWE agent frameworks, managing agent collectives for enhanced problem-solving.","Experimental results show that a DEI-guided committee of agents is able to surpass the best individual agent's performance by a large margin.","For instance, a group of open-source SWE agents, with a maximum individual resolve rate of 27.3% on SWE-Bench Lite, can achieve a 34.3% resolve rate with DEI, making a 25% improvement and beating most closed-source solutions.","Our best-performing group excels with a 55% resolve rate, securing the highest ranking on SWE-Bench Lite.","Our findings contribute to the growing body of research on collaborative AI systems and their potential to solve complex software engineering challenges."],"url":"http://arxiv.org/abs/2408.07060v1"}
{"created":"2024-08-13 17:49:46","title":"Model Counting in the Wild","abstract":"Model counting is a fundamental problem in automated reasoning with applications in probabilistic inference, network reliability, neural network verification, and more. Although model counting is computationally intractable from a theoretical perspective due to its #P-completeness, the past decade has seen significant progress in developing state-of-the-art model counters to address scalability challenges.   In this work, we conduct a rigorous assessment of the scalability of model counters in the wild. To this end, we surveyed 11 application domains and collected an aggregate of 2262 benchmarks from these domains. We then evaluated six state-of-the-art model counters on these instances to assess scalability and runtime performance.   Our empirical evaluation demonstrates that the performance of model counters varies significantly across different application domains, underscoring the need for careful selection by the end user. Additionally, we investigated the behavior of different counters with respect to two parameters suggested by the model counting community, finding only a weak correlation. Our analysis highlights the challenges and opportunities for portfolio-based approaches in model counting.","sentences":["Model counting is a fundamental problem in automated reasoning with applications in probabilistic inference, network reliability, neural network verification, and more.","Although model counting is computationally intractable from a theoretical perspective due to its #P-completeness, the past decade has seen significant progress in developing state-of-the-art model counters to address scalability challenges.   ","In this work, we conduct a rigorous assessment of the scalability of model counters in the wild.","To this end, we surveyed 11 application domains and collected an aggregate of 2262 benchmarks from these domains.","We then evaluated six state-of-the-art model counters on these instances to assess scalability and runtime performance.   ","Our empirical evaluation demonstrates that the performance of model counters varies significantly across different application domains, underscoring the need for careful selection by the end user.","Additionally, we investigated the behavior of different counters with respect to two parameters suggested by the model counting community, finding only a weak correlation.","Our analysis highlights the challenges and opportunities for portfolio-based approaches in model counting."],"url":"http://arxiv.org/abs/2408.07059v1"}
{"created":"2024-08-13 17:49:00","title":"A Survey on Model MoErging: Recycling and Routing Among Specialized Experts for Collaborative Learning","abstract":"The availability of performant pre-trained models has led to a proliferation of fine-tuned expert models that are specialized to a particular domain or task. Model MoErging methods aim to recycle expert models to create an aggregate system with improved performance or generalization. A key component of MoErging methods is the creation of a router that decides which expert model(s) to use for a particular input or application. The promise, effectiveness, and large design space of MoErging has spurred the development of many new methods over the past few years. This rapid pace of development has made it challenging to compare different MoErging methods, which are rarely compared to one another and are often validated in different experimental setups. To remedy such gaps, we present a comprehensive survey of MoErging methods that includes a novel taxonomy for cataloging key design choices and clarifying suitable applications for each method. Apart from surveying MoErging research, we inventory software tools and applications that make use of MoErging. We additionally discuss related fields of study such as model merging, multitask learning, and mixture-of-experts models. Taken as a whole, our survey provides a unified overview of existing MoErging methods and creates a solid foundation for future work in this burgeoning field.","sentences":["The availability of performant pre-trained models has led to a proliferation of fine-tuned expert models that are specialized to a particular domain or task.","Model MoErging methods aim to recycle expert models to create an aggregate system with improved performance or generalization.","A key component of MoErging methods is the creation of a router that decides which expert model(s) to use for a particular input or application.","The promise, effectiveness, and large design space of MoErging has spurred the development of many new methods over the past few years.","This rapid pace of development has made it challenging to compare different MoErging methods, which are rarely compared to one another and are often validated in different experimental setups.","To remedy such gaps, we present a comprehensive survey of MoErging methods that includes a novel taxonomy for cataloging key design choices and clarifying suitable applications for each method.","Apart from surveying MoErging research, we inventory software tools and applications that make use of MoErging.","We additionally discuss related fields of study such as model merging, multitask learning, and mixture-of-experts models.","Taken as a whole, our survey provides a unified overview of existing MoErging methods and creates a solid foundation for future work in this burgeoning field."],"url":"http://arxiv.org/abs/2408.07057v1"}
{"created":"2024-08-13 17:46:12","title":"LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs","abstract":"Current long context large language models (LLMs) can process inputs up to 100,000 tokens, yet struggle to generate outputs exceeding even a modest length of 2,000 words. Through controlled experiments, we find that the model's effective generation length is inherently bounded by the sample it has seen during supervised fine-tuning (SFT). In other words, their output limitation is due to the scarcity of long-output examples in existing SFT datasets. To address this, we introduce AgentWrite, an agent-based pipeline that decomposes ultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to generate coherent outputs exceeding 20,000 words. Leveraging AgentWrite, we construct LongWriter-6k, a dataset containing 6,000 SFT data with output lengths ranging from 2k to 32k words. By incorporating this dataset into model training, we successfully scale the output length of existing models to over 10,000 words while maintaining output quality. We also develop LongBench-Write, a comprehensive benchmark for evaluating ultra-long generation capabilities. Our 9B parameter model, further improved through DPO, achieves state-of-the-art performance on this benchmark, surpassing even much larger proprietary models. In general, our work demonstrates that existing long context LLM already possesses the potential for a larger output window--all you need is data with extended output during model alignment to unlock this capability. Our code & models are at: https://github.com/THUDM/LongWriter.","sentences":["Current long context large language models (LLMs) can process inputs up to 100,000 tokens, yet struggle to generate outputs exceeding even a modest length of 2,000 words.","Through controlled experiments, we find that the model's effective generation length is inherently bounded by the sample it has seen during supervised fine-tuning (SFT).","In other words, their output limitation is due to the scarcity of long-output examples in existing SFT datasets.","To address this, we introduce AgentWrite, an agent-based pipeline that decomposes ultra-long generation tasks into subtasks, enabling off-the-shelf LLMs to generate coherent outputs exceeding 20,000 words.","Leveraging AgentWrite, we construct LongWriter-6k, a dataset containing 6,000 SFT data with output lengths ranging from 2k to 32k words.","By incorporating this dataset into model training, we successfully scale the output length of existing models to over 10,000 words while maintaining output quality.","We also develop LongBench-Write, a comprehensive benchmark for evaluating ultra-long generation capabilities.","Our 9B parameter model, further improved through DPO, achieves state-of-the-art performance on this benchmark, surpassing even much larger proprietary models.","In general, our work demonstrates that existing long context LLM already possesses the potential for a larger output window--all you need is data with extended output during model alignment to unlock this capability.","Our code & models are at: https://github.com/THUDM/LongWriter."],"url":"http://arxiv.org/abs/2408.07055v1"}
{"created":"2024-08-13 17:45:12","title":"Exploiting Leakage in Password Managers via Injection Attacks","abstract":"This work explores injection attacks against password managers. In this setting, the adversary (only) controls their own application client, which they use to \"inject\" chosen payloads to a victim's client via, for example, sharing credentials with them. The injections are interleaved with adversarial observations of some form of protected state (such as encrypted vault exports or the network traffic received by the application servers), from which the adversary backs out confidential information. We uncover a series of general design patterns in popular password managers that lead to vulnerabilities allowing an adversary to efficiently recover passwords, URLs, usernames, and attachments. We develop general attack templates to exploit these design patterns and experimentally showcase their practical efficacy via analysis of ten distinct password manager applications. We disclosed our findings to these vendors, many of which deployed mitigations.","sentences":["This work explores injection attacks against password managers.","In this setting, the adversary (only) controls their own application client, which they use to \"inject\" chosen payloads to a victim's client via, for example, sharing credentials with them.","The injections are interleaved with adversarial observations of some form of protected state (such as encrypted vault exports or the network traffic received by the application servers), from which the adversary backs out confidential information.","We uncover a series of general design patterns in popular password managers that lead to vulnerabilities allowing an adversary to efficiently recover passwords, URLs, usernames, and attachments.","We develop general attack templates to exploit these design patterns and experimentally showcase their practical efficacy via analysis of ten distinct password manager applications.","We disclosed our findings to these vendors, many of which deployed mitigations."],"url":"http://arxiv.org/abs/2408.07054v1"}
{"created":"2024-08-13 17:43:32","title":"The News Comment Gap and Algorithmic Agenda Setting in Online Forums","abstract":"The disparity between news stories valued by journalists and those preferred by readers, known as the \"News Gap\", is well-documented. However, the difference in expectations regarding news related user-generated content is less studied. Comment sections, hosted by news websites, are popular venues for reader engagement, yet still subject to editorial decisions. It is thus important to understand journalist vs reader comment preferences and how these are served by various comment ranking algorithms that represent discussions differently. We analyse 1.2 million comments from Austrian newspaper Der Standard to understand the \"News Comment Gap\" and the effects of different ranking algorithms. We find that journalists prefer positive, timely, complex, direct responses, while readers favour comments similar to article content from elite authors. We introduce the versatile Feature-Oriented Ranking Utility Metric (FORUM) to assess the impact of different ranking algorithms and find dramatic differences in how they prioritise the display of comments by sentiment, topical relevance, lexical diversity, and readability. Journalists can exert substantial influence over the discourse through both curatorial and algorithmic means. Understanding these choices' implications is vital in fostering engaging and civil discussions while aligning with journalistic objectives, especially given the increasing legal scrutiny and societal importance of online discourse.","sentences":["The disparity between news stories valued by journalists and those preferred by readers, known as the \"News Gap\", is well-documented.","However, the difference in expectations regarding news related user-generated content is less studied.","Comment sections, hosted by news websites, are popular venues for reader engagement, yet still subject to editorial decisions.","It is thus important to understand journalist vs reader comment preferences and how these are served by various comment ranking algorithms that represent discussions differently.","We analyse 1.2 million comments from Austrian newspaper Der Standard to understand the \"News Comment Gap\" and the effects of different ranking algorithms.","We find that journalists prefer positive, timely, complex, direct responses, while readers favour comments similar to article content from elite authors.","We introduce the versatile Feature-Oriented Ranking Utility Metric (FORUM) to assess the impact of different ranking algorithms and find dramatic differences in how they prioritise the display of comments by sentiment, topical relevance, lexical diversity, and readability.","Journalists can exert substantial influence over the discourse through both curatorial and algorithmic means.","Understanding these choices' implications is vital in fostering engaging and civil discussions while aligning with journalistic objectives, especially given the increasing legal scrutiny and societal importance of online discourse."],"url":"http://arxiv.org/abs/2408.07052v1"}
{"created":"2024-08-13 17:37:40","title":"PSM: Learning Probabilistic Embeddings for Multi-scale Zero-Shot Soundscape Mapping","abstract":"A soundscape is defined by the acoustic environment a person perceives at a location. In this work, we propose a framework for mapping soundscapes across the Earth. Since soundscapes involve sound distributions that span varying spatial scales, we represent locations with multi-scale satellite imagery and learn a joint representation among this imagery, audio, and text. To capture the inherent uncertainty in the soundscape of a location, we design the representation space to be probabilistic. We also fuse ubiquitous metadata (including geolocation, time, and data source) to enable learning of spatially and temporally dynamic representations of soundscapes. We demonstrate the utility of our framework by creating large-scale soundscape maps integrating both audio and text with temporal control. To facilitate future research on this task, we also introduce a large-scale dataset, GeoSound, containing over $300k$ geotagged audio samples paired with both low- and high-resolution satellite imagery. We demonstrate that our method outperforms the existing state-of-the-art on both GeoSound and the existing SoundingEarth dataset. Our dataset and code is available at https://github.com/mvrl/PSM.","sentences":["A soundscape is defined by the acoustic environment a person perceives at a location.","In this work, we propose a framework for mapping soundscapes across the Earth.","Since soundscapes involve sound distributions that span varying spatial scales, we represent locations with multi-scale satellite imagery and learn a joint representation among this imagery, audio, and text.","To capture the inherent uncertainty in the soundscape of a location, we design the representation space to be probabilistic.","We also fuse ubiquitous metadata (including geolocation, time, and data source) to enable learning of spatially and temporally dynamic representations of soundscapes.","We demonstrate the utility of our framework by creating large-scale soundscape maps integrating both audio and text with temporal control.","To facilitate future research on this task, we also introduce a large-scale dataset, GeoSound, containing over $300k$ geotagged audio samples paired with both low- and high-resolution satellite imagery.","We demonstrate that our method outperforms the existing state-of-the-art on both GeoSound and the existing SoundingEarth dataset.","Our dataset and code is available at https://github.com/mvrl/PSM."],"url":"http://arxiv.org/abs/2408.07050v1"}
{"created":"2024-08-13 17:20:52","title":"TableGuard -- Securing Structured & Unstructured Data","abstract":"With the increasing demand for data sharing across platforms and organizations, ensuring the privacy and security of sensitive information has become a critical challenge. This paper introduces \"TableGuard\". An innovative approach to data obfuscation tailored for relational databases. Building on the principles and techniques developed in prior work on context-sensitive obfuscation, TableGuard applies these methods to ensure that API calls return only obfuscated data, thereby safeguarding privacy when sharing data with third parties. TableGuard leverages advanced context-sensitive obfuscation techniques to replace sensitive data elements with contextually appropriate alternatives. By maintaining the relational integrity and coherence of the data, our approach mitigates the risks of cognitive dissonance and data leakage. We demonstrate the implementation of TableGuard using a BERT based transformer model, which identifies and obfuscates sensitive entities within relational tables. Our evaluation shows that TableGuard effectively balances privacy protection with data utility, minimizing information loss while ensuring that the obfuscated data remains functionally useful for downstream applications. The results highlight the importance of domain-specific obfuscation strategies and the role of context length in preserving data integrity. The implications of this research are significant for organizations that need to share data securely with external parties. TableGuard offers a robust framework for implementing privacy-preserving data sharing mechanisms, thereby contributing to the broader field of data privacy and security.","sentences":["With the increasing demand for data sharing across platforms and organizations, ensuring the privacy and security of sensitive information has become a critical challenge.","This paper introduces \"TableGuard\".","An innovative approach to data obfuscation tailored for relational databases.","Building on the principles and techniques developed in prior work on context-sensitive obfuscation, TableGuard applies these methods to ensure that API calls return only obfuscated data, thereby safeguarding privacy when sharing data with third parties.","TableGuard leverages advanced context-sensitive obfuscation techniques to replace sensitive data elements with contextually appropriate alternatives.","By maintaining the relational integrity and coherence of the data, our approach mitigates the risks of cognitive dissonance and data leakage.","We demonstrate the implementation of TableGuard using a BERT based transformer model, which identifies and obfuscates sensitive entities within relational tables.","Our evaluation shows that TableGuard effectively balances privacy protection with data utility, minimizing information loss while ensuring that the obfuscated data remains functionally useful for downstream applications.","The results highlight the importance of domain-specific obfuscation strategies and the role of context length in preserving data integrity.","The implications of this research are significant for organizations that need to share data securely with external parties.","TableGuard offers a robust framework for implementing privacy-preserving data sharing mechanisms, thereby contributing to the broader field of data privacy and security."],"url":"http://arxiv.org/abs/2408.07045v1"}
{"created":"2024-08-13 17:07:29","title":"KAN You See It? KANs and Sentinel for Effective and Explainable Crop Field Segmentation","abstract":"Segmentation of crop fields is essential for enhancing agricultural productivity, monitoring crop health, and promoting sustainable practices. Deep learning models adopted for this task must ensure accurate and reliable predictions to avoid economic losses and environmental impact. The newly proposed Kolmogorov-Arnold networks (KANs) offer promising advancements in the performance of neural networks. This paper analyzes the integration of KAN layers into the U-Net architecture (U-KAN) to segment crop fields using Sentinel-2 and Sentinel-1 satellite images and provides an analysis of the performance and explainability of these networks. Our findings indicate a 2\\% improvement in IoU compared to the traditional full-convolutional U-Net model in fewer GFLOPs. Furthermore, gradient-based explanation techniques show that U-KAN predictions are highly plausible and that the network has a very high ability to focus on the boundaries of cultivated areas rather than on the areas themselves. The per-channel relevance analysis also reveals that some channels are irrelevant to this task.","sentences":["Segmentation of crop fields is essential for enhancing agricultural productivity, monitoring crop health, and promoting sustainable practices.","Deep learning models adopted for this task must ensure accurate and reliable predictions to avoid economic losses and environmental impact.","The newly proposed Kolmogorov-Arnold networks (KANs) offer promising advancements in the performance of neural networks.","This paper analyzes the integration of KAN layers into the U-Net architecture (U-KAN) to segment crop fields using Sentinel-2 and Sentinel-1 satellite images and provides an analysis of the performance and explainability of these networks.","Our findings indicate a 2\\% improvement in IoU compared to the traditional full-convolutional U-Net model in fewer GFLOPs.","Furthermore, gradient-based explanation techniques show that U-KAN predictions are highly plausible and that the network has a very high ability to focus on the boundaries of cultivated areas rather than on the areas themselves.","The per-channel relevance analysis also reveals that some channels are irrelevant to this task."],"url":"http://arxiv.org/abs/2408.07040v1"}
{"created":"2024-08-13 17:05:06","title":"PathInsight: Instruction Tuning of Multimodal Datasets and Models for Intelligence Assisted Diagnosis in Histopathology","abstract":"Pathological diagnosis remains the definitive standard for identifying tumors. The rise of multimodal large models has simplified the process of integrating image analysis with textual descriptions. Despite this advancement, the substantial costs associated with training and deploying these complex multimodal models, together with a scarcity of high-quality training datasets, create a significant divide between cutting-edge technology and its application in the clinical setting. We had meticulously compiled a dataset of approximately 45,000 cases, covering over 6 different tasks, including the classification of organ tissues, generating pathology report descriptions, and addressing pathology-related questions and answers. We have fine-tuned multimodal large models, specifically LLaVA, Qwen-VL, InternLM, with this dataset to enhance instruction-based performance. We conducted a qualitative assessment of the capabilities of the base model and the fine-tuned model in performing image captioning and classification tasks on the specific dataset. The evaluation results demonstrate that the fine-tuned model exhibits proficiency in addressing typical pathological questions. We hope that by making both our models and datasets publicly available, they can be valuable to the medical and research communities.","sentences":["Pathological diagnosis remains the definitive standard for identifying tumors.","The rise of multimodal large models has simplified the process of integrating image analysis with textual descriptions.","Despite this advancement, the substantial costs associated with training and deploying these complex multimodal models, together with a scarcity of high-quality training datasets, create a significant divide between cutting-edge technology and its application in the clinical setting.","We had meticulously compiled a dataset of approximately 45,000 cases, covering over 6 different tasks, including the classification of organ tissues, generating pathology report descriptions, and addressing pathology-related questions and answers.","We have fine-tuned multimodal large models, specifically LLaVA, Qwen-VL, InternLM, with this dataset to enhance instruction-based performance.","We conducted a qualitative assessment of the capabilities of the base model and the fine-tuned model in performing image captioning and classification tasks on the specific dataset.","The evaluation results demonstrate that the fine-tuned model exhibits proficiency in addressing typical pathological questions.","We hope that by making both our models and datasets publicly available, they can be valuable to the medical and research communities."],"url":"http://arxiv.org/abs/2408.07037v1"}
{"created":"2024-08-13 16:36:33","title":"Improved Counting under Continual Observation with Pure Differential Privacy","abstract":"Counting under continual observation is a well-studied problem in the area of differential privacy. Given a stream of updates $x_1,x_2,\\dots,x_T \\in \\{0,1\\}$ the problem is to continuously release estimates of the prefix sums $\\sum_{i=1}^t x_i$ for $t=1,\\dots,T$ while protecting each input $x_i$ in the stream with differential privacy. Recently, significant leaps have been made in our understanding of this problem under $\\textit{approximate}$ differential privacy, aka. $(\\varepsilon,\\delta)$$\\textit{-differential privacy}$. However, for the classical case of $\\varepsilon$-differential privacy, we are not aware of any improvement in mean squared error since the work of Honaker (TPDP 2015). In this paper we present such an improvement, reducing the mean squared error by a factor of about 4, asymptotically. The key technique is a new generalization of the binary tree mechanism that uses a $k$-ary number system with $\\textit{negative digits}$ to improve the privacy-accuracy trade-off. Our mechanism improves the mean squared error over all 'optimal' $(\\varepsilon,\\delta)$-differentially private factorization mechanisms based on Gaussian noise whenever $\\delta$ is sufficiently small. Specifically, using $k=19$ we get an asymptotic improvement over the bound given in the work by Henzinger, Upadhyay and Upadhyay (SODA 2023) when $\\delta = O(T^{-0.92})$.","sentences":["Counting under continual observation is a well-studied problem in the area of differential privacy.","Given a stream of updates $x_1,x_2,\\dots,x_T","\\in \\{0,1\\}$ the problem is to continuously release estimates of the prefix sums $\\sum_{i=1}^t x_i$ for $t=1,\\dots,T$ while protecting each input $x_i$ in the stream with differential privacy.","Recently, significant leaps have been made in our understanding of this problem under $\\textit{approximate}$ differential privacy, aka.","$(\\varepsilon,\\delta)$$\\textit{-differential privacy}$.","However, for the classical case of $\\varepsilon$-differential privacy, we are not aware of any improvement in mean squared error since the work of Honaker (TPDP 2015).","In this paper we present such an improvement, reducing the mean squared error by a factor of about 4, asymptotically.","The key technique is a new generalization of the binary tree mechanism that uses a $k$-ary number system with $\\textit{negative digits}$ to improve the privacy-accuracy trade-off.","Our mechanism improves the mean squared error over all 'optimal' $(\\varepsilon,\\delta)$-differentially private factorization mechanisms based on Gaussian noise whenever $\\delta$ is sufficiently small.","Specifically, using $k=19$ we get an asymptotic improvement over the bound given in the work by Henzinger, Upadhyay and Upadhyay (SODA 2023) when $\\delta = O(T^{-0.92})$."],"url":"http://arxiv.org/abs/2408.07021v1"}
{"created":"2024-08-13 16:35:22","title":"A $5/4$ Approximation for Two-Edge-Connectivity","abstract":"The $2$-Edge Connected Spanning Subgraph problem (2ECSS) is among the most basic survivable network design problems: given an undirected unweighted graph, find a subgraph with the minimum number of edges which is 2-edge-connected (i.e., it remains connected after the removal of any single edge). This NP-hard problem is well-studied in terms of approximation algorithms. The current-best approximation factor for 2ECSS is $1.3+\\varepsilon$ for any constant $\\varepsilon >0$ [Garg, Grandoni, Jabal-Ameli'23; Kobayashi,Noguchi'23]. In this paper we present a much simpler $9/7$ approximation algorithm, and a more complex $5/4$ one. Our algorithms are also faster: their running time is $n^{O(1)}$ instead of $n^{O(1/\\varepsilon)}$.","sentences":["The $2$-Edge Connected Spanning Subgraph problem (2ECSS) is among the most basic survivable network design problems: given an undirected unweighted graph, find a subgraph with the minimum number of edges which is 2-edge-connected (i.e., it remains connected after the removal of any single edge).","This NP-hard problem is well-studied in terms of approximation algorithms.","The current-best approximation factor for 2ECSS is $1.3+\\varepsilon$ for any constant $\\varepsilon >0$","[Garg, Grandoni, Jabal-Ameli'23; Kobayashi,Noguchi'23].","In this paper we present a much simpler $9/7$ approximation algorithm, and a more complex $5/4$ one.","Our algorithms are also faster: their running time is $n^{O(1)}$ instead of $n^{O(1/\\varepsilon)}$."],"url":"http://arxiv.org/abs/2408.07019v1"}
{"created":"2024-08-13 16:34:06","title":"Efficient Human-Object-Interaction (EHOI) Detection via Interaction Label Coding and Conditional Decision","abstract":"Human-Object Interaction (HOI) detection is a fundamental task in image understanding. While deep-learning-based HOI methods provide high performance in terms of mean Average Precision (mAP), they are computationally expensive and opaque in training and inference processes. An Efficient HOI (EHOI) detector is proposed in this work to strike a good balance between detection performance, inference complexity, and mathematical transparency. EHOI is a two-stage method. In the first stage, it leverages a frozen object detector to localize the objects and extract various features as intermediate outputs. In the second stage, the first-stage outputs predict the interaction type using the XGBoost classifier. Our contributions include the application of error correction codes (ECCs) to encode rare interaction cases, which reduces the model size and the complexity of the XGBoost classifier in the second stage. Additionally, we provide a mathematical formulation of the relabeling and decision-making process. Apart from the architecture, we present qualitative results to explain the functionalities of the feedforward modules. Experimental results demonstrate the advantages of ECC-coded interaction labels and the excellent balance of detection performance and complexity of the proposed EHOI method.","sentences":["Human-Object Interaction (HOI) detection is a fundamental task in image understanding.","While deep-learning-based HOI methods provide high performance in terms of mean Average Precision (mAP), they are computationally expensive and opaque in training and inference processes.","An Efficient HOI (EHOI) detector is proposed in this work to strike a good balance between detection performance, inference complexity, and mathematical transparency.","EHOI is a two-stage method.","In the first stage, it leverages a frozen object detector to localize the objects and extract various features as intermediate outputs.","In the second stage, the first-stage outputs predict the interaction type using the XGBoost classifier.","Our contributions include the application of error correction codes (ECCs) to encode rare interaction cases, which reduces the model size and the complexity of the XGBoost classifier in the second stage.","Additionally, we provide a mathematical formulation of the relabeling and decision-making process.","Apart from the architecture, we present qualitative results to explain the functionalities of the feedforward modules.","Experimental results demonstrate the advantages of ECC-coded interaction labels and the excellent balance of detection performance and complexity of the proposed EHOI method."],"url":"http://arxiv.org/abs/2408.07018v1"}
{"created":"2024-08-13 16:30:36","title":"Defining and Measuring Disentanglement for non-Independent Factors of Variation","abstract":"Representation learning is an approach that allows to discover and extract the factors of variation from the data. Intuitively, a representation is said to be disentangled if it separates the different factors of variation in a way that is understandable to humans. Definitions of disentanglement and metrics to measure it usually assume that the factors of variation are independent of each other. However, this is generally false in the real world, which limits the use of these definitions and metrics to very specific and unrealistic scenarios. In this paper we give a definition of disentanglement based on information theory that is also valid when the factors of variation are not independent. Furthermore, we relate this definition to the Information Bottleneck Method. Finally, we propose a method to measure the degree of disentanglement from the given definition that works when the factors of variation are not independent. We show through different experiments that the method proposed in this paper correctly measures disentanglement with non-independent factors of variation, while other methods fail in this scenario.","sentences":["Representation learning is an approach that allows to discover and extract the factors of variation from the data.","Intuitively, a representation is said to be disentangled if it separates the different factors of variation in a way that is understandable to humans.","Definitions of disentanglement and metrics to measure it usually assume that the factors of variation are independent of each other.","However, this is generally false in the real world, which limits the use of these definitions and metrics to very specific and unrealistic scenarios.","In this paper we give a definition of disentanglement based on information theory that is also valid when the factors of variation are not independent.","Furthermore, we relate this definition to the Information Bottleneck Method.","Finally, we propose a method to measure the degree of disentanglement from the given definition that works when the factors of variation are not independent.","We show through different experiments that the method proposed in this paper correctly measures disentanglement with non-independent factors of variation, while other methods fail in this scenario."],"url":"http://arxiv.org/abs/2408.07016v1"}
{"created":"2024-08-13 16:15:50","title":"Imagen 3","abstract":"We introduce Imagen 3, a latent diffusion model that generates high quality images from text prompts. We describe our quality and responsibility evaluations. Imagen 3 is preferred over other state-of-the-art (SOTA) models at the time of evaluation. In addition, we discuss issues around safety and representation, as well as methods we used to minimize the potential harm of our models.","sentences":["We introduce Imagen 3, a latent diffusion model that generates high quality images from text prompts.","We describe our quality and responsibility evaluations.","Imagen 3 is preferred over other state-of-the-art (SOTA) models at the time of evaluation.","In addition, we discuss issues around safety and representation, as well as methods we used to minimize the potential harm of our models."],"url":"http://arxiv.org/abs/2408.07009v1"}
{"created":"2024-08-13 16:12:25","title":"Content and Style Aware Audio-Driven Facial Animation","abstract":"Audio-driven 3D facial animation has several virtual humans applications for content creation and editing. While several existing methods provide solutions for speech-driven animation, precise control over content (what) and style (how) of the final performance is still challenging. We propose a novel approach that takes as input an audio, and the corresponding text to extract temporally-aligned content and disentangled style representations, in order to provide controls over 3D facial animation. Our method is trained in two stages, that evolves from audio prominent styles (how it sounds) to visual prominent styles (how it looks). We leverage a high-resource audio dataset in stage I to learn styles that control speech generation in a self-supervised learning framework, and then fine-tune this model with low-resource audio/3D mesh pairs in stage II to control 3D vertex generation. We employ a non-autoregressive seq2seq formulation to model sentence-level dependencies, and better mouth articulations. Our method provides flexibility that the style of a reference audio and the content of a source audio can be combined to enable audio style transfer. Similarly, the content can be modified, e.g. muting or swapping words, that enables style-preserving content editing.","sentences":["Audio-driven 3D facial animation has several virtual humans applications for content creation and editing.","While several existing methods provide solutions for speech-driven animation, precise control over content (what) and style (how) of the final performance is still challenging.","We propose a novel approach that takes as input an audio, and the corresponding text to extract temporally-aligned content and disentangled style representations, in order to provide controls over 3D facial animation.","Our method is trained in two stages, that evolves from audio prominent styles (how it sounds) to visual prominent styles (how it looks).","We leverage a high-resource audio dataset in stage I to learn styles that control speech generation in a self-supervised learning framework, and then fine-tune this model with low-resource audio/3D mesh pairs in stage II to control 3D vertex generation.","We employ a non-autoregressive seq2seq formulation to model sentence-level dependencies, and better mouth articulations.","Our method provides flexibility that the style of a reference audio and the content of a source audio can be combined to enable audio style transfer.","Similarly, the content can be modified, e.g. muting or swapping words, that enables style-preserving content editing."],"url":"http://arxiv.org/abs/2408.07005v1"}
{"created":"2024-08-13 16:08:37","title":"Casper: Prompt Sanitization for Protecting User Privacy in Web-Based Large Language Models","abstract":"Web-based Large Language Model (LLM) services have been widely adopted and have become an integral part of our Internet experience. Third-party plugins enhance the functionalities of LLM by enabling access to real-world data and services. However, the privacy consequences associated with these services and their third-party plugins are not well understood. Sensitive prompt data are stored, processed, and shared by cloud-based LLM providers and third-party plugins. In this paper, we propose Casper, a prompt sanitization technique that aims to protect user privacy by detecting and removing sensitive information from user inputs before sending them to LLM services. Casper runs entirely on the user's device as a browser extension and does not require any changes to the online LLM services. At the core of Casper is a three-layered sanitization mechanism consisting of a rule-based filter, a Machine Learning (ML)-based named entity recognizer, and a browser-based local LLM topic identifier. We evaluate Casper on a dataset of 4000 synthesized prompts and show that it can effectively filter out Personal Identifiable Information (PII) and privacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively.","sentences":["Web-based Large Language Model (LLM) services have been widely adopted and have become an integral part of our Internet experience.","Third-party plugins enhance the functionalities of LLM by enabling access to real-world data and services.","However, the privacy consequences associated with these services and their third-party plugins are not well understood.","Sensitive prompt data are stored, processed, and shared by cloud-based LLM providers and third-party plugins.","In this paper, we propose Casper, a prompt sanitization technique that aims to protect user privacy by detecting and removing sensitive information from user inputs before sending them to LLM services.","Casper runs entirely on the user's device as a browser extension and does not require any changes to the online LLM services.","At the core of Casper is a three-layered sanitization mechanism consisting of a rule-based filter, a Machine Learning (ML)-based named entity recognizer, and a browser-based local LLM topic identifier.","We evaluate Casper on a dataset of 4000 synthesized prompts and show that it can effectively filter out Personal Identifiable Information (PII) and privacy-sensitive topics with high accuracy, at 98.5% and 89.9%, respectively."],"url":"http://arxiv.org/abs/2408.07004v1"}
{"created":"2024-08-13 16:07:16","title":"Generative AI for automatic topic labelling","abstract":"Topic Modeling has become a prominent tool for the study of scientific fields, as they allow for a large scale interpretation of research trends. Nevertheless, the output of these models is structured as a list of keywords which requires a manual interpretation for the labelling. This paper proposes to assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini for topic labelling. Drawing on previous research leveraging BERTopic, we generate topics from a dataset of all the scientific articles (n=34,797) authored by all biology professors in Switzerland (n=465) between 2008 and 2020, as recorded in the Web of Science database. We assess the output of the three models both quantitatively and qualitatively and find that, first, both GPT models are capable of accurately and precisely label topics from the models' output keywords. Second, 3-word labels are preferable to grasp the complexity of research topics.","sentences":["Topic Modeling has become a prominent tool for the study of scientific fields, as they allow for a large scale interpretation of research trends.","Nevertheless, the output of these models is structured as a list of keywords which requires a manual interpretation for the labelling.","This paper proposes to assess the reliability of three LLMs, namely flan, GPT-4o, and GPT-4 mini for topic labelling.","Drawing on previous research leveraging BERTopic, we generate topics from a dataset of all the scientific articles (n=34,797) authored by all biology professors in Switzerland (n=465) between 2008 and 2020, as recorded in the Web of Science database.","We assess the output of the three models both quantitatively and qualitatively and find that, first, both GPT models are capable of accurately and precisely label topics from the models' output keywords.","Second, 3-word labels are preferable to grasp the complexity of research topics."],"url":"http://arxiv.org/abs/2408.07003v1"}
{"created":"2024-08-13 16:00:30","title":"Faster Private Minimum Spanning Trees","abstract":"Motivated by applications in clustering and synthetic data generation, we consider the problem of releasing a minimum spanning tree (MST) under edge-weight differential privacy constraints where a graph topology $G=(V,E)$ with $n$ vertices and $m$ edges is public, the weight matrix $\\vec{W}\\in \\mathbb{R}^{n \\times n}$ is private, and we wish to release an approximate MST under $\\rho$-zero-concentrated differential privacy. Weight matrices are considered neighboring if they differ by at most $\\Delta_\\infty$ in each entry, i.e., we consider an $\\ell_\\infty$ neighboring relationship. Existing private MST algorithms either add noise to each entry in $\\vec{W}$ and estimate the MST by post-processing or add noise to weights in-place during the execution of a specific MST algorithm. Using the post-processing approach with an efficient MST algorithm takes $O(n^2)$ time on dense graphs but results in an additive error on the weight of the MST of magnitude $O(n^2\\log n)$. In-place algorithms give asymptotically better utility, but the running time of existing in-place algorithms is $O(n^3)$ for dense graphs. Our main result is a new differentially private MST algorithm that matches the utility of existing in-place methods while running in time $O(m + n^{3/2}\\log n)$ for fixed privacy parameter $\\rho$. The technical core of our algorithm is an efficient sublinear time simulation of Report-Noisy-Max that works by discretizing all edge weights to a multiple of $\\Delta_\\infty$ and forming groups of edges with identical weights. Specifically, we present a data structure that allows us to sample a noisy minimum weight edge among at most $O(n^2)$ cut edges in $O(\\sqrt{n} \\log n)$ time. Experimental evaluations support our claims that our algorithm significantly improves previous algorithms either in utility or running time.","sentences":["Motivated by applications in clustering and synthetic data generation, we consider the problem of releasing a minimum spanning tree (MST) under edge-weight differential privacy constraints where a graph topology $G=(V,E)$ with $n$ vertices and $m$ edges is public, the weight matrix $\\vec{W}\\in \\mathbb{R}^{n \\times n}$ is private, and we wish to release an approximate MST under $\\rho$-zero-concentrated differential privacy.","Weight matrices are considered neighboring if they differ by at most $\\Delta_\\infty$ in each entry, i.e., we consider an $\\ell_\\infty$ neighboring relationship.","Existing private MST algorithms either add noise to each entry in $\\vec{W}$ and estimate the MST by post-processing or add noise to weights in-place during the execution of a specific MST algorithm.","Using the post-processing approach with an efficient MST algorithm takes $O(n^2)$ time on dense graphs but results in an additive error on the weight of the MST of magnitude $O(n^2\\log n)$.","In-place algorithms give asymptotically better utility, but the running time of existing in-place algorithms is $O(n^3)$ for dense graphs.","Our main result is a new differentially private MST algorithm that matches the utility of existing in-place methods while running in time $O(m + n^{3/2}\\log n)$ for fixed privacy parameter $\\rho$. The technical core of our algorithm is an efficient sublinear time simulation of Report-Noisy-Max that works by discretizing all edge weights to a multiple of $\\Delta_\\infty$ and forming groups of edges with identical weights.","Specifically, we present a data structure that allows us to sample a noisy minimum weight edge among at most $O(n^2)$ cut edges in $O(\\sqrt{n} \\log n)$ time.","Experimental evaluations support our claims that our algorithm significantly improves previous algorithms either in utility or running time."],"url":"http://arxiv.org/abs/2408.06997v1"}
{"created":"2024-08-13 15:56:42","title":"Blessing of Dimensionality for Approximating Sobolev Classes on Manifolds","abstract":"The manifold hypothesis says that natural high-dimensional data is actually supported on or around a low-dimensional manifold. Recent success of statistical and learning-based methods empirically supports this hypothesis, due to outperforming classical statistical intuition in very high dimensions. A natural step for analysis is thus to assume the manifold hypothesis and derive bounds that are independent of any embedding space. Theoretical implications in this direction have recently been explored in terms of generalization of ReLU networks and convergence of Langevin methods. We complement existing results by providing theoretical statistical complexity results, which directly relates to generalization properties. In particular, we demonstrate that the statistical complexity required to approximate a class of bounded Sobolev functions on a compact manifold is bounded from below, and moreover that this bound is dependent only on the intrinsic properties of the manifold. These provide complementary bounds for existing approximation results for ReLU networks on manifolds, which give upper bounds on generalization capacity.","sentences":["The manifold hypothesis says that natural high-dimensional data is actually supported on or around a low-dimensional manifold.","Recent success of statistical and learning-based methods empirically supports this hypothesis, due to outperforming classical statistical intuition in very high dimensions.","A natural step for analysis is thus to assume the manifold hypothesis and derive bounds that are independent of any embedding space.","Theoretical implications in this direction have recently been explored in terms of generalization of ReLU networks and convergence of Langevin methods.","We complement existing results by providing theoretical statistical complexity results, which directly relates to generalization properties.","In particular, we demonstrate that the statistical complexity required to approximate a class of bounded Sobolev functions on a compact manifold is bounded from below, and moreover that this bound is dependent only on the intrinsic properties of the manifold.","These provide complementary bounds for existing approximation results for ReLU networks on manifolds, which give upper bounds on generalization capacity."],"url":"http://arxiv.org/abs/2408.06996v1"}
{"created":"2024-08-13 15:56:20","title":"Low-Bitwidth Floating Point Quantization for Efficient High-Quality Diffusion Models","abstract":"Diffusion models are emerging models that generate images by iteratively denoising random Gaussian noise using deep neural networks. These models typically exhibit high computational and memory demands, necessitating effective post-training quantization for high-performance inference. Recent works propose low-bitwidth (e.g., 8-bit or 4-bit) quantization for diffusion models, however 4-bit integer quantization typically results in low-quality images. We observe that on several widely used hardware platforms, there is little or no difference in compute capability between floating-point and integer arithmetic operations of the same bitwidth (e.g., 8-bit or 4-bit). Therefore, we propose an effective floating-point quantization method for diffusion models that provides better image quality compared to integer quantization methods. We employ a floating-point quantization method that was effective for other processing tasks, specifically computer vision and natural language tasks, and tailor it for diffusion models by integrating weight rounding learning during the mapping of the full-precision values to the quantized values in the quantization process. We comprehensively study integer and floating-point quantization methods in state-of-the-art diffusion models. Our floating-point quantization method not only generates higher-quality images than that of integer quantization methods, but also shows no noticeable degradation compared to full-precision models (32-bit floating-point), when both weights and activations are quantized to 8-bit floating-point values, while has minimal degradation with 4-bit weights and 8-bit activations.","sentences":["Diffusion models are emerging models that generate images by iteratively denoising random Gaussian noise using deep neural networks.","These models typically exhibit high computational and memory demands, necessitating effective post-training quantization for high-performance inference.","Recent works propose low-bitwidth (e.g., 8-bit or 4-bit) quantization for diffusion models, however 4-bit integer quantization typically results in low-quality images.","We observe that on several widely used hardware platforms, there is little or no difference in compute capability between floating-point and integer arithmetic operations of the same bitwidth (e.g., 8-bit or 4-bit).","Therefore, we propose an effective floating-point quantization method for diffusion models that provides better image quality compared to integer quantization methods.","We employ a floating-point quantization method that was effective for other processing tasks, specifically computer vision and natural language tasks, and tailor it for diffusion models by integrating weight rounding learning during the mapping of the full-precision values to the quantized values in the quantization process.","We comprehensively study integer and floating-point quantization methods in state-of-the-art diffusion models.","Our floating-point quantization method not only generates higher-quality images than that of integer quantization methods, but also shows no noticeable degradation compared to full-precision models (32-bit floating-point), when both weights and activations are quantized to 8-bit floating-point values, while has minimal degradation with 4-bit weights and 8-bit activations."],"url":"http://arxiv.org/abs/2408.06995v1"}
{"created":"2024-08-13 15:53:58","title":"LLMs can Schedule","abstract":"The job shop scheduling problem (JSSP) remains a significant hurdle in optimizing production processes. This challenge involves efficiently allocating jobs to a limited number of machines while minimizing factors like total processing time or job delays. While recent advancements in artificial intelligence have yielded promising solutions, such as reinforcement learning and graph neural networks, this paper explores the potential of Large Language Models (LLMs) for JSSP. We introduce the very first supervised 120k dataset specifically designed to train LLMs for JSSP. Surprisingly, our findings demonstrate that LLM-based scheduling can achieve performance comparable to other neural approaches. Furthermore, we propose a sampling method that enhances the effectiveness of LLMs in tackling JSSP.","sentences":["The job shop scheduling problem (JSSP) remains a significant hurdle in optimizing production processes.","This challenge involves efficiently allocating jobs to a limited number of machines while minimizing factors like total processing time or job delays.","While recent advancements in artificial intelligence have yielded promising solutions, such as reinforcement learning and graph neural networks, this paper explores the potential of Large Language Models (LLMs) for JSSP.","We introduce the very first supervised 120k dataset specifically designed to train LLMs for JSSP.","Surprisingly, our findings demonstrate that LLM-based scheduling can achieve performance comparable to other neural approaches.","Furthermore, we propose a sampling method that enhances the effectiveness of LLMs in tackling JSSP."],"url":"http://arxiv.org/abs/2408.06993v1"}
{"created":"2024-08-13 15:47:10","title":"Catamorphic Abstractions for Constrained Horn Clause Satisfiability","abstract":"Catamorphisms are functions that are recursively defined on list and trees and, in general, on Algebraic Data Types (ADTs), and are often used to compute suitable abstractions of programs that manipulate ADTs. Examples of catamorphisms include functions that compute size of lists, orderedness of lists, and height of trees. It is well known that program properties specified through catamorphisms can be proved by showing the satisfiability of suitable sets of Constrained Horn Clauses (CHCs). We address the problem of checking the satisfiability of those sets of CHCs, and we propose a method for transforming sets of CHCs into equisatisfiable sets where catamorphisms are no longer present. As a consequence, clauses with catamorphisms can be handled without extending the satisfiability algorithms used by existing CHC solvers. Through an experimental evaluation on a non-trivial benchmark consisting of many list and tree processing algorithms expressed as sets of CHCs, we show that our technique is indeed effective and significantly enhances the performance of state-of-the-art CHC solvers.","sentences":["Catamorphisms are functions that are recursively defined on list and trees and, in general, on Algebraic Data Types (ADTs), and are often used to compute suitable abstractions of programs that manipulate ADTs.","Examples of catamorphisms include functions that compute size of lists, orderedness of lists, and height of trees.","It is well known that program properties specified through catamorphisms can be proved by showing the satisfiability of suitable sets of Constrained Horn Clauses (CHCs).","We address the problem of checking the satisfiability of those sets of CHCs, and we propose a method for transforming sets of CHCs into equisatisfiable sets where catamorphisms are no longer present.","As a consequence, clauses with catamorphisms can be handled without extending the satisfiability algorithms used by existing CHC solvers.","Through an experimental evaluation on a non-trivial benchmark consisting of many list and tree processing algorithms expressed as sets of CHCs, we show that our technique is indeed effective and significantly enhances the performance of state-of-the-art CHC solvers."],"url":"http://arxiv.org/abs/2408.06988v1"}
{"created":"2024-08-13 15:32:54","title":"SpectralGaussians: Semantic, spectral 3D Gaussian splatting for multi-spectral scene representation, visualization and analysis","abstract":"We propose a novel cross-spectral rendering framework based on 3D Gaussian Splatting (3DGS) that generates realistic and semantically meaningful splats from registered multi-view spectrum and segmentation maps. This extension enhances the representation of scenes with multiple spectra, providing insights into the underlying materials and segmentation. We introduce an improved physically-based rendering approach for Gaussian splats, estimating reflectance and lights per spectra, thereby enhancing accuracy and realism. In a comprehensive quantitative and qualitative evaluation, we demonstrate the superior performance of our approach with respect to other recent learning-based spectral scene representation approaches (i.e., XNeRF and SpectralNeRF) as well as other non-spectral state-of-the-art learning-based approaches. Our work also demonstrates the potential of spectral scene understanding for precise scene editing techniques like style transfer, inpainting, and removal. Thereby, our contributions address challenges in multi-spectral scene representation, rendering, and editing, offering new possibilities for diverse applications.","sentences":["We propose a novel cross-spectral rendering framework based on 3D Gaussian Splatting (3DGS) that generates realistic and semantically meaningful splats from registered multi-view spectrum and segmentation maps.","This extension enhances the representation of scenes with multiple spectra, providing insights into the underlying materials and segmentation.","We introduce an improved physically-based rendering approach for Gaussian splats, estimating reflectance and lights per spectra, thereby enhancing accuracy and realism.","In a comprehensive quantitative and qualitative evaluation, we demonstrate the superior performance of our approach with respect to other recent learning-based spectral scene representation approaches (i.e., XNeRF and SpectralNeRF) as well as other non-spectral state-of-the-art learning-based approaches.","Our work also demonstrates the potential of spectral scene understanding for precise scene editing techniques like style transfer, inpainting, and removal.","Thereby, our contributions address challenges in multi-spectral scene representation, rendering, and editing, offering new possibilities for diverse applications."],"url":"http://arxiv.org/abs/2408.06975v1"}
{"created":"2024-08-13 15:27:43","title":"Prompt-Based Segmentation at Multiple Resolutions and Lighting Conditions using Segment Anything Model 2","abstract":"This paper provides insight into the effectiveness of zero-shot, prompt-based, Segment Anything Model (SAM), and its updated version, SAM 2, and the non-promptable, conventional convolutional network (CNN), in segmenting solar panels, in RGB aerial imagery, across lighting conditions, spatial resolutions, and prompt strategies. SAM 2 demonstrates improvements over SAM, particularly in sub-optimal lighting conditions when prompted by points. Both SAMs, prompted by user-box, outperformed CNN, in all scenarios. Additionally, YOLOv9 prompting outperformed user points prompting. In high-resolution imagery, both in optimal and sub-optimal lighting conditions, Eff-UNet outperformed both SAM models prompted by YOLOv9 boxes, positioning Eff-UNet as the appropriate model for automatic segmentation in high-resolution data. In low-resolution data, user box prompts were found crucial to achieve a reasonable performance. This paper provides details on strengths and limitations of each model and outlines robustness of user prompted image segmentation models in inconsistent resolution and lighting conditions of remotely sensed data.","sentences":["This paper provides insight into the effectiveness of zero-shot, prompt-based, Segment Anything Model (SAM), and its updated version, SAM 2, and the non-promptable, conventional convolutional network (CNN), in segmenting solar panels, in RGB aerial imagery, across lighting conditions, spatial resolutions, and prompt strategies.","SAM 2 demonstrates improvements over SAM, particularly in sub-optimal lighting conditions when prompted by points.","Both SAMs, prompted by user-box, outperformed CNN, in all scenarios.","Additionally, YOLOv9 prompting outperformed user points prompting.","In high-resolution imagery, both in optimal and sub-optimal lighting conditions, Eff-UNet outperformed both SAM models prompted by YOLOv9 boxes, positioning Eff-UNet as the appropriate model for automatic segmentation in high-resolution data.","In low-resolution data, user box prompts were found crucial to achieve a reasonable performance.","This paper provides details on strengths and limitations of each model and outlines robustness of user prompted image segmentation models in inconsistent resolution and lighting conditions of remotely sensed data."],"url":"http://arxiv.org/abs/2408.06970v1"}
{"created":"2024-08-13 15:27:30","title":"IRS-Assisted Lossy Communications Under Correlated Rayleigh Fading: Outage Probability Analysis and Optimization","abstract":"This paper focuses on an intelligent reflecting surface (IRS)-assisted lossy communication system with correlated Rayleigh fading. We analyze the correlated channel model and derive the outage probability of the system. Then, we design a deep reinforce learning (DRL) method to optimize the phase shift of IRS, in order to maximize the received signal power. Moreover, this paper presents results of the simulations conducted to evaluate the performance of the DRL-based method. The simulation results indicate that the outage probability of the considered system increases significantly with more correlated channel coefficients. Moreover, the performance gap between DRL and theoretical limit increases with higher transmit power and/or larger distortion requirement.","sentences":["This paper focuses on an intelligent reflecting surface (IRS)-assisted lossy communication system with correlated Rayleigh fading.","We analyze the correlated channel model and derive the outage probability of the system.","Then, we design a deep reinforce learning (DRL) method to optimize the phase shift of IRS, in order to maximize the received signal power.","Moreover, this paper presents results of the simulations conducted to evaluate the performance of the DRL-based method.","The simulation results indicate that the outage probability of the considered system increases significantly with more correlated channel coefficients.","Moreover, the performance gap between DRL and theoretical limit increases with higher transmit power and/or larger distortion requirement."],"url":"http://arxiv.org/abs/2408.06969v1"}
{"created":"2024-08-13 15:21:46","title":"DyG-Mamba: Continuous State Space Modeling on Dynamic Graphs","abstract":"Dynamic graph learning aims to uncover evolutionary laws in real-world systems, enabling accurate social recommendation (link prediction) or early detection of cancer cells (classification). Inspired by the success of state space models, e.g., Mamba, for efficiently capturing long-term dependencies in language modeling, we propose DyG-Mamba, a new continuous state space model (SSM) for dynamic graph learning. Specifically, we first found that using inputs as control signals for SSM is not suitable for continuous-time dynamic network data with irregular sampling intervals, resulting in models being insensitive to time information and lacking generalization properties. Drawing inspiration from the Ebbinghaus forgetting curve, which suggests that memory of past events is strongly correlated with time intervals rather than specific details of the events themselves, we directly utilize irregular time spans as control signals for SSM to achieve significant robustness and generalization. Through exhaustive experiments on 12 datasets for dynamic link prediction and dynamic node classification tasks, we found that DyG-Mamba achieves state-of-the-art performance on most of the datasets, while also demonstrating significantly improved computation and memory efficiency.","sentences":["Dynamic graph learning aims to uncover evolutionary laws in real-world systems, enabling accurate social recommendation (link prediction) or early detection of cancer cells (classification).","Inspired by the success of state space models, e.g., Mamba, for efficiently capturing long-term dependencies in language modeling, we propose DyG-Mamba, a new continuous state space model (SSM) for dynamic graph learning.","Specifically, we first found that using inputs as control signals for SSM is not suitable for continuous-time dynamic network data with irregular sampling intervals, resulting in models being insensitive to time information and lacking generalization properties.","Drawing inspiration from the Ebbinghaus forgetting curve, which suggests that memory of past events is strongly correlated with time intervals rather than specific details of the events themselves, we directly utilize irregular time spans as control signals for SSM to achieve significant robustness and generalization.","Through exhaustive experiments on 12 datasets for dynamic link prediction and dynamic node classification tasks, we found that DyG-Mamba achieves state-of-the-art performance on most of the datasets, while also demonstrating significantly improved computation and memory efficiency."],"url":"http://arxiv.org/abs/2408.06966v1"}
{"created":"2024-08-13 15:17:48","title":"ASPEN: ASP-Based System for Collective Entity Resolution","abstract":"In this paper, we present ASPEN, an answer set programming (ASP) implementation of a recently proposed declarative framework for collective entity resolution (ER). While an ASP encoding had been previously suggested, several practical issues had been neglected, most notably, the question of how to efficiently compute the (externally defined) similarity facts that are used in rule bodies. This leads us to propose new variants of the encodings (including Datalog approximations) and show how to employ different functionalities of ASP solvers to compute (maximal) solutions, and (approximations of) the sets of possible and certain merges. A comprehensive experimental evaluation of ASPEN on real-world datasets shows that the approach is promising, achieving high accuracy in real-life ER scenarios. Our experiments also yield useful insights into the relative merits of different types of (approximate) ER solutions, the impact of recursion, and factors influencing performance.","sentences":["In this paper, we present ASPEN, an answer set programming (ASP) implementation of a recently proposed declarative framework for collective entity resolution (ER).","While an ASP encoding had been previously suggested, several practical issues had been neglected, most notably, the question of how to efficiently compute the (externally defined) similarity facts that are used in rule bodies.","This leads us to propose new variants of the encodings (including Datalog approximations) and show how to employ different functionalities of ASP solvers to compute (maximal) solutions, and (approximations of) the sets of possible and certain merges.","A comprehensive experimental evaluation of ASPEN on real-world datasets shows that the approach is promising, achieving high accuracy in real-life ER scenarios.","Our experiments also yield useful insights into the relative merits of different types of (approximate) ER solutions, the impact of recursion, and factors influencing performance."],"url":"http://arxiv.org/abs/2408.06961v1"}
{"created":"2024-08-13 15:17:03","title":"Measuring User Understanding in Dialogue-based XAI Systems","abstract":"The field of eXplainable Artificial Intelligence (XAI) is increasingly recognizing the need to personalize and/or interactively adapt the explanation to better reflect users' explanation needs. While dialogue-based approaches to XAI have been proposed recently, the state-of-the-art in XAI is still characterized by what we call one-shot, non-personalized and one-way explanations. In contrast, dialogue-based systems that can adapt explanations through interaction with a user promise to be superior to GUI-based or dashboard explanations as they offer a more intuitive way of requesting information. In general, while interactive XAI systems are often evaluated in terms of user satisfaction, there are limited studies that access user's objective model understanding. This is in particular the case for dialogue-based XAI approaches. In this paper, we close this gap by carrying out controlled experiments within a dialogue framework in which we measure understanding of users in three phases by asking them to simulate the predictions of the model they are learning about. By this, we can quantify the level of (improved) understanding w.r.t. how the model works, comparing the state prior, and after the interaction. We further analyze the data to reveal patterns of how the interaction between groups with high vs. low understanding gain differ. Overall, our work thus contributes to our understanding about the effectiveness of XAI approaches.","sentences":["The field of eXplainable Artificial Intelligence (XAI) is increasingly recognizing the need to personalize and/or interactively adapt the explanation to better reflect users' explanation needs.","While dialogue-based approaches to XAI have been proposed recently, the state-of-the-art in XAI is still characterized by what we call one-shot, non-personalized and one-way explanations.","In contrast, dialogue-based systems that can adapt explanations through interaction with a user promise to be superior to GUI-based or dashboard explanations as they offer a more intuitive way of requesting information.","In general, while interactive XAI systems are often evaluated in terms of user satisfaction, there are limited studies that access user's objective model understanding.","This is in particular the case for dialogue-based XAI approaches.","In this paper, we close this gap by carrying out controlled experiments within a dialogue framework in which we measure understanding of users in three phases by asking them to simulate the predictions of the model they are learning about.","By this, we can quantify the level of (improved) understanding w.r.t.","how the model works, comparing the state prior, and after the interaction.","We further analyze the data to reveal patterns of how the interaction between groups with high vs. low understanding gain differ.","Overall, our work thus contributes to our understanding about the effectiveness of XAI approaches."],"url":"http://arxiv.org/abs/2408.06960v1"}
{"created":"2024-08-13 15:15:37","title":"AuToMATo: A Parameter-Free Persistence-Based Clustering Algorithm","abstract":"We present AuToMATo, a novel parameter-free clustering algorithm based on persistent homology. AuToMATo combines the existing ToMATo clustering algorithm with a bootstrapping procedure in order to separate significant peaks of an estimated density function from non-significant ones. We perform a thorough comparison of AuToMATo against many other state-of-the-art clustering algorithms. We find that not only that AuToMATo compares favorably against other parameter-free clustering algorithms, but in many instances also significantly outperforms even the best selection of parameters for other algorithms. AuToMATo is motivated by applications in topological data analysis, in particular the Mapper algorithm, where it is desirable to work with a parameter-free clustering algorithm. Indeed, we provide evidence that AuToMATo performs well when used with Mapper. Finally, we provide an open-source implementation of AuToMATo in Python that is fully compatible with the standardscikit-learn architecture.","sentences":["We present AuToMATo, a novel parameter-free clustering algorithm based on persistent homology.","AuToMATo combines the existing ToMATo clustering algorithm with a bootstrapping procedure in order to separate significant peaks of an estimated density function from non-significant ones.","We perform a thorough comparison of AuToMATo against many other state-of-the-art clustering algorithms.","We find that not only that AuToMATo compares favorably against other parameter-free clustering algorithms, but in many instances also significantly outperforms even the best selection of parameters for other algorithms.","AuToMATo is motivated by applications in topological data analysis, in particular the Mapper algorithm, where it is desirable to work with a parameter-free clustering algorithm.","Indeed, we provide evidence that AuToMATo performs well when used with Mapper.","Finally, we provide an open-source implementation of AuToMATo in Python that is fully compatible with the standardscikit-learn architecture."],"url":"http://arxiv.org/abs/2408.06958v1"}
{"created":"2024-08-13 15:15:10","title":"Rural Handover Parameter Tuning to Achieve End to End Latency Requirements of Future Railway Mobile Communication Systems","abstract":"GSM-R (GSM for Railways) is a 2G-based standardized ground-to-train communications system that enabled interoperability across different countries. However, as a 2G-based system, it is nearing its lifetime and therefore, it will be replaced with 5G-based Future Railway Mobile Communications System (FRMCS). FRMCS is expected to bring in new use cases that demand low latency and high reliability. However, from a mobility perspective, it is not clear how the low latency and high reliability will be achieved. This paper investigates the effect of handover procedure on latency and reliability and analyzes which use cases of FRMCS can be satisfied using baseline handover. We also sweep through different handover parameter configurations and analyze their effect on mobility performance. Then, we analyze the effect of mobility performance on packet latency and reliability. Our results show that, with baseline handover, Standard Data Communications Scenario is met and optimizing for baseline handover performance can reduce latency by up to 18.5%, indicating that optimizing for mobility performance is crucial in FRMCS.","sentences":["GSM-R (GSM for Railways) is a 2G-based standardized ground-to-train communications system that enabled interoperability across different countries.","However, as a 2G-based system, it is nearing its lifetime and therefore, it will be replaced with 5G-based Future Railway Mobile Communications System (FRMCS).","FRMCS is expected to bring in new use cases that demand low latency and high reliability.","However, from a mobility perspective, it is not clear how the low latency and high reliability will be achieved.","This paper investigates the effect of handover procedure on latency and reliability and analyzes which use cases of FRMCS can be satisfied using baseline handover.","We also sweep through different handover parameter configurations and analyze their effect on mobility performance.","Then, we analyze the effect of mobility performance on packet latency and reliability.","Our results show that, with baseline handover, Standard Data Communications Scenario is met and optimizing for baseline handover performance can reduce latency by up to 18.5%, indicating that optimizing for mobility performance is crucial in FRMCS."],"url":"http://arxiv.org/abs/2408.06957v1"}
{"created":"2024-08-13 15:15:06","title":"PayOff: A Regulated Central Bank Digital Currency with Private Offline Payments","abstract":"The European Central Bank is preparing for the potential issuance of a central bank digital currency (CBDC), called the digital euro. A recent regulatory proposal by the European Commission defines several requirements for the digital euro, such as support for both online and offline payments. Offline payments are expected to enable cash-like privacy, local payment settlement, and the enforcement of holding limits. While other central banks have expressed similar desired functionality, achieving such offline payments poses a novel technical challenge. We observe that none of the existing research solutions, including offline E-cash schemes, are fully compliant. Proposed solutions based on secure elements offer no guarantees in case of compromise and can therefore lead to significant payment fraud.   The main contribution of this paper is PayOff, a novel CBDC design motivated by the digital euro regulation, which focuses on offline payments. We analyze the security implications of local payment settlement and identify new security objectives. PayOff protects user privacy, supports complex regulations such as holding limits, and implements safeguards to increase robustness against secure element failure. Our analysis shows that PayOff provides strong privacy and identifies residual leakages that may arise in real-world deployments. Our evaluation shows that offline payments can be fast and that the central bank can handle high payment loads with moderate computing resources. However, the main limitation of PayOff is that offline payment messages and storage requirements grow in the number of payments that the sender makes or receives without going online in between.","sentences":["The European Central Bank is preparing for the potential issuance of a central bank digital currency (CBDC), called the digital euro.","A recent regulatory proposal by the European Commission defines several requirements for the digital euro, such as support for both online and offline payments.","Offline payments are expected to enable cash-like privacy, local payment settlement, and the enforcement of holding limits.","While other central banks have expressed similar desired functionality, achieving such offline payments poses a novel technical challenge.","We observe that none of the existing research solutions, including offline E-cash schemes, are fully compliant.","Proposed solutions based on secure elements offer no guarantees in case of compromise and can therefore lead to significant payment fraud.   ","The main contribution of this paper is PayOff, a novel CBDC design motivated by the digital euro regulation, which focuses on offline payments.","We analyze the security implications of local payment settlement and identify new security objectives.","PayOff protects user privacy, supports complex regulations such as holding limits, and implements safeguards to increase robustness against secure element failure.","Our analysis shows that PayOff provides strong privacy and identifies residual leakages that may arise in real-world deployments.","Our evaluation shows that offline payments can be fast and that the central bank can handle high payment loads with moderate computing resources.","However, the main limitation of PayOff is that offline payment messages and storage requirements grow in the number of payments that the sender makes or receives without going online in between."],"url":"http://arxiv.org/abs/2408.06956v1"}
{"created":"2024-08-13 15:14:14","title":"Crowdsourcing: A Framework for Usability Evaluation","abstract":"Objective: This research explores using crowdsourcing for software usability evaluation.   Background: Usability studies are essential for designing user-friendly software, but traditional methods are often costly and time-consuming. Crowdsourcing offers a quicker, cost-effective alternative for remote usability evaluation, though ensuring quality feedback remains a challenge.   Method: A systematic mapping study was conducted to review current usability evaluation research. Subsequently, multi-experiments were performed, comparing novice crowd usability inspectors to experts using expert heuristic evaluation as a benchmark. These results were used to create and validate a framework for crowd usability inspection through a case study.   Results: The mapping study identified expert heuristic evaluation as a prevalent method, especially for websites. Experimental findings showed that novice crowd usability inspections, guided by expert heuristics, can match experts in identifying usability issues in content, quality, severity, and time efficiency. The case study demonstrated that the framework allows effective usability inspections, leading to successful software redesigns. Iterations of 3-5 novice inspections effectively resolved key usability issues within three cycles.   Conclusion: Crowdsourcing is an effective alternative to expert heuristic evaluation for usability assessment. The proposed framework for crowd usability inspection is a viable solution for budget-constrained software companies.   Keywords: crowdsourcing, crowd usability evaluation, expert heuristic evaluation, framework.","sentences":["Objective: This research explores using crowdsourcing for software usability evaluation.   ","Background: Usability studies are essential for designing user-friendly software, but traditional methods are often costly and time-consuming.","Crowdsourcing offers a quicker, cost-effective alternative for remote usability evaluation, though ensuring quality feedback remains a challenge.   ","Method: A systematic mapping study was conducted to review current usability evaluation research.","Subsequently, multi-experiments were performed, comparing novice crowd usability inspectors to experts using expert heuristic evaluation as a benchmark.","These results were used to create and validate a framework for crowd usability inspection through a case study.   ","Results:","The mapping study identified expert heuristic evaluation as a prevalent method, especially for websites.","Experimental findings showed that novice crowd usability inspections, guided by expert heuristics, can match experts in identifying usability issues in content, quality, severity, and time efficiency.","The case study demonstrated that the framework allows effective usability inspections, leading to successful software redesigns.","Iterations of 3-5 novice inspections effectively resolved key usability issues within three cycles.   ","Conclusion: Crowdsourcing is an effective alternative to expert heuristic evaluation for usability assessment.","The proposed framework for crowd usability inspection is a viable solution for budget-constrained software companies.   ","Keywords: crowdsourcing, crowd usability evaluation, expert heuristic evaluation, framework."],"url":"http://arxiv.org/abs/2408.06955v1"}
{"created":"2024-08-13 15:13:21","title":"Neural Speech and Audio Coding","abstract":"This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems. It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods. The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements. Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs' output, along with the autoencoder-based end-to-end models and LPCNet--hybrid systems that combine linear predictive coding (LPC) with neural networks. Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or predefined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs. Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques.","sentences":["This paper explores the integration of model-based and data-driven approaches within the realm of neural speech and audio coding systems.","It highlights the challenges posed by the subjective evaluation processes of speech and audio codecs and discusses the limitations of purely data-driven approaches, which often require inefficiently large architectures to match the performance of model-based methods.","The study presents hybrid systems as a viable solution, offering significant improvements to the performance of conventional codecs through meticulously chosen design enhancements.","Specifically, it introduces a neural network-based signal enhancer designed to post-process existing codecs' output, along with the autoencoder-based end-to-end models and LPCNet--hybrid systems that combine linear predictive coding (LPC) with neural networks.","Furthermore, the paper delves into predictive models operating within custom feature spaces (TF-Codec) or predefined transform domains (MDCTNet) and examines the use of psychoacoustically calibrated loss functions to train end-to-end neural audio codecs.","Through these investigations, the paper demonstrates the potential of hybrid systems to advance the field of speech and audio coding by bridging the gap between traditional model-based approaches and modern data-driven techniques."],"url":"http://arxiv.org/abs/2408.06954v1"}
{"created":"2024-08-13 15:03:46","title":"Heavy-Ball Momentum Accelerated Actor-Critic With Function Approximation","abstract":"By using an parametric value function to replace the Monte-Carlo rollouts for value estimation, the actor-critic (AC) algorithms can reduce the variance of stochastic policy gradient so that to improve the convergence rate. While existing works mainly focus on analyzing convergence rate of AC algorithms under Markovian noise, the impacts of momentum on AC algorithms remain largely unexplored. In this work, we first propose a heavy-ball momentum based advantage actor-critic (\\mbox{HB-A2C}) algorithm by integrating the heavy-ball momentum into the critic recursion that is parameterized by a linear function. When the sample trajectory follows a Markov decision process, we quantitatively certify the acceleration capability of the proposed HB-A2C algorithm. Our theoretical results demonstrate that the proposed HB-A2C finds an $\\epsilon$-approximate stationary point with $\\oo{\\epsilon^{-2}}$ iterations for reinforcement learning tasks with Markovian noise. Moreover, we also reveal the dependence of learning rates on the length of the sample trajectory. By carefully selecting the momentum factor of the critic recursion, the proposed HB-A2C can balance the errors introduced by the initialization and the stoschastic approximation.","sentences":["By using an parametric value function to replace the Monte-Carlo rollouts for value estimation, the actor-critic (AC) algorithms can reduce the variance of stochastic policy gradient so that to improve the convergence rate.","While existing works mainly focus on analyzing convergence rate of AC algorithms under Markovian noise, the impacts of momentum on AC algorithms remain largely unexplored.","In this work, we first propose a heavy-ball momentum based advantage actor-critic (\\mbox{HB-A2C}) algorithm by integrating the heavy-ball momentum into the critic recursion that is parameterized by a linear function.","When the sample trajectory follows a Markov decision process, we quantitatively certify the acceleration capability of the proposed HB-A2C algorithm.","Our theoretical results demonstrate that the proposed HB-A2C finds an $\\epsilon$-approximate stationary point with $\\oo{\\epsilon^{-2}}$ iterations for reinforcement learning tasks with Markovian noise.","Moreover, we also reveal the dependence of learning rates on the length of the sample trajectory.","By carefully selecting the momentum factor of the critic recursion, the proposed HB-A2C can balance the errors introduced by the initialization and the stoschastic approximation."],"url":"http://arxiv.org/abs/2408.06945v1"}
{"created":"2024-08-13 15:01:50","title":"Mesh Simplification For Unfolding","abstract":"We present a computational approach for unfolding 3D shapes isometrically into the plane as a single patch without overlapping triangles. This is a hard, sometimes impossible, problem, which existing methods are forced to soften by allowing for map distortions or multiple patches. Instead, we propose a geometric relaxation of the problem: we modify the input shape until it admits an overlap-free unfolding. We achieve this by locally displacing vertices and collapsing edges, guided by the unfolding process. We validate our algorithm quantitatively and qualitatively on a large dataset of complex shapes and show its proficiency by fabricating real shapes from paper.","sentences":["We present a computational approach for unfolding 3D shapes isometrically into the plane as a single patch without overlapping triangles.","This is a hard, sometimes impossible, problem, which existing methods are forced to soften by allowing for map distortions or multiple patches.","Instead, we propose a geometric relaxation of the problem: we modify the input shape until it admits an overlap-free unfolding.","We achieve this by locally displacing vertices and collapsing edges, guided by the unfolding process.","We validate our algorithm quantitatively and qualitatively on a large dataset of complex shapes and show its proficiency by fabricating real shapes from paper."],"url":"http://arxiv.org/abs/2408.06944v1"}
{"created":"2024-08-13 15:01:33","title":"Towards Holistic Disease Risk Prediction using Small Language Models","abstract":"Data in the healthcare domain arise from a variety of sources and modalities, such as x-ray images, continuous measurements, and clinical notes. Medical practitioners integrate these diverse data types daily to make informed and accurate decisions. With recent advancements in language models capable of handling multimodal data, it is a logical progression to apply these models to the healthcare sector. In this work, we introduce a framework that connects small language models to multiple data sources, aiming to predict the risk of various diseases simultaneously. Our experiments encompass 12 different tasks within a multitask learning setup. Although our approach does not surpass state-of-the-art methods specialized for single tasks, it demonstrates competitive performance and underscores the potential of small language models for multimodal reasoning in healthcare.","sentences":["Data in the healthcare domain arise from a variety of sources and modalities, such as x-ray images, continuous measurements, and clinical notes.","Medical practitioners integrate these diverse data types daily to make informed and accurate decisions.","With recent advancements in language models capable of handling multimodal data, it is a logical progression to apply these models to the healthcare sector.","In this work, we introduce a framework that connects small language models to multiple data sources, aiming to predict the risk of various diseases simultaneously.","Our experiments encompass 12 different tasks within a multitask learning setup.","Although our approach does not surpass state-of-the-art methods specialized for single tasks, it demonstrates competitive performance and underscores the potential of small language models for multimodal reasoning in healthcare."],"url":"http://arxiv.org/abs/2408.06943v1"}
{"created":"2024-08-13 15:01:03","title":"Speech-based Mark for Data Sonification","abstract":"Sonification serves as a powerful tool for data accessibility, especially for people with vision loss. Among various modalities, speech is a familiar means of communication similar to the role of text in visualization. However, speech-based sonification is underexplored. We introduce SpeechTone, a novel speech-based mark for data sonification and extension to the existing Erie declarative grammar for sonification. It encodes data into speech attributes such as pitch, speed, voice and speech content. We demonstrate the efficacy of SpeechTone through three examples.","sentences":["Sonification serves as a powerful tool for data accessibility, especially for people with vision loss.","Among various modalities, speech is a familiar means of communication similar to the role of text in visualization.","However, speech-based sonification is underexplored.","We introduce SpeechTone, a novel speech-based mark for data sonification and extension to the existing Erie declarative grammar for sonification.","It encodes data into speech attributes such as pitch, speed, voice and speech content.","We demonstrate the efficacy of SpeechTone through three examples."],"url":"http://arxiv.org/abs/2408.06942v1"}
{"created":"2024-08-13 14:59:44","title":"OpenResearcher: Unleashing AI for Accelerated Scientific Research","abstract":"The rapid growth of scientific literature imposes significant challenges for researchers endeavoring to stay updated with the latest advancements in their fields and delve into new areas. We introduce OpenResearcher, an innovative platform that leverages Artificial Intelligence (AI) techniques to accelerate the research process by answering diverse questions from researchers. OpenResearcher is built based on Retrieval-Augmented Generation (RAG) to integrate Large Language Models (LLMs) with up-to-date, domain-specific knowledge. Moreover, we develop various tools for OpenResearcher to understand researchers' queries, search from the scientific literature, filter retrieved information, provide accurate and comprehensive answers, and self-refine these answers. OpenResearcher can flexibly use these tools to balance efficiency and effectiveness. As a result, OpenResearcher enables researchers to save time and increase their potential to discover new insights and drive scientific breakthroughs. Demo, video, and code are available at: https://github.com/GAIR-NLP/OpenResearcher.","sentences":["The rapid growth of scientific literature imposes significant challenges for researchers endeavoring to stay updated with the latest advancements in their fields and delve into new areas.","We introduce OpenResearcher, an innovative platform that leverages Artificial Intelligence (AI) techniques to accelerate the research process by answering diverse questions from researchers.","OpenResearcher is built based on Retrieval-Augmented Generation (RAG) to integrate Large Language Models (LLMs) with up-to-date, domain-specific knowledge.","Moreover, we develop various tools for OpenResearcher to understand researchers' queries, search from the scientific literature, filter retrieved information, provide accurate and comprehensive answers, and self-refine these answers.","OpenResearcher can flexibly use these tools to balance efficiency and effectiveness.","As a result, OpenResearcher enables researchers to save time and increase their potential to discover new insights and drive scientific breakthroughs.","Demo, video, and code are available at: https://github.com/GAIR-NLP/OpenResearcher."],"url":"http://arxiv.org/abs/2408.06941v1"}
{"created":"2024-08-13 14:42:16","title":"UFO-MAC: A Unified Framework for Optimization of High-Performance Multipliers and Multiply-Accumulators","abstract":"Multipliers and multiply-accumulators (MACs) are critical arithmetic circuit components in the modern era. As essential components of AI accelerators, they significantly influence the area and performance of compute-intensive circuits. This paper presents UFO-MAC, a unified framework for the optimization of multipliers and MACs. Specifically, UFO-MAC employs an optimal compressor tree structure and utilizes integer linear programming (ILP) to refine the stage assignment and interconnection of the compressors. Additionally, it explicitly exploits the non-uniform arrival time profile of the carry propagate adder (CPA) within multipliers to achieve targeted optimization. Moreover, the framework also supports the optimization of fused MAC architectures. Experimental results demonstrate that multipliers and MACs optimized by UFO-MAC Pareto-dominate state-of-the-art baselines and commercial IP libraries. The performance gain of UFO-MAC is further validated through the implementation of multipliers and MACs within functional modules, underlining its efficacy in real scenarios.","sentences":["Multipliers and multiply-accumulators (MACs) are critical arithmetic circuit components in the modern era.","As essential components of AI accelerators, they significantly influence the area and performance of compute-intensive circuits.","This paper presents UFO-MAC, a unified framework for the optimization of multipliers and MACs.","Specifically, UFO-MAC employs an optimal compressor tree structure and utilizes integer linear programming (ILP) to refine the stage assignment and interconnection of the compressors.","Additionally, it explicitly exploits the non-uniform arrival time profile of the carry propagate adder (CPA) within multipliers to achieve targeted optimization.","Moreover, the framework also supports the optimization of fused MAC architectures.","Experimental results demonstrate that multipliers and MACs optimized by UFO-MAC Pareto-dominate state-of-the-art baselines and commercial IP libraries.","The performance gain of UFO-MAC is further validated through the implementation of multipliers and MACs within functional modules, underlining its efficacy in real scenarios."],"url":"http://arxiv.org/abs/2408.06935v1"}
{"created":"2024-08-13 14:34:59","title":"The advantages of context specific language models: the case of the Erasmian Language Model","abstract":"The current trend to improve language model performance seems to be based on scaling up with the number of parameters (e.g. the state of the art GPT4 model has approximately 1.7 trillion parameters) or the amount of training data fed into the model. However this comes at significant costs in terms of computational resources and energy costs that compromise the sustainability of AI solutions, as well as risk relating to privacy and misuse. In this paper we present the Erasmian Language Model (ELM) a small context specific, 900 million parameter model, pre-trained and fine-tuned by and for Erasmus University Rotterdam. We show how the model performs adequately in a classroom context for essay writing, and how it achieves superior performance in subjects that are part of its context. This has implications for a wide range of institutions and organizations, showing that context specific language models may be a viable alternative for resource constrained, privacy sensitive use cases.","sentences":["The current trend to improve language model performance seems to be based on scaling up with the number of parameters (e.g. the state of the art GPT4 model has approximately 1.7 trillion parameters) or the amount of training data fed into the model.","However this comes at significant costs in terms of computational resources and energy costs that compromise the sustainability of AI solutions, as well as risk relating to privacy and misuse.","In this paper we present the Erasmian Language Model (ELM) a small context specific, 900 million parameter model, pre-trained and fine-tuned by and for Erasmus University Rotterdam.","We show how the model performs adequately in a classroom context for essay writing, and how it achieves superior performance in subjects that are part of its context.","This has implications for a wide range of institutions and organizations, showing that context specific language models may be a viable alternative for resource constrained, privacy sensitive use cases."],"url":"http://arxiv.org/abs/2408.06931v1"}
{"created":"2024-08-13 14:33:32","title":"Diagnosis extraction from unstructured Dutch echocardiogram reports using span- and document-level characteristic classification","abstract":"Clinical machine learning research and AI driven clinical decision support models rely on clinically accurate labels. Manually extracting these labels with the help of clinical specialists is often time-consuming and expensive. This study tests the feasibility of automatic span- and document-level diagnosis extraction from unstructured Dutch echocardiogram reports.   We included 115,692 unstructured echocardiogram reports from the UMCU a large university hospital in the Netherlands. A randomly selected subset was manually annotated for the occurrence and severity of eleven commonly described cardiac characteristics. We developed and tested several automatic labelling techniques at both span and document levels, using weighted and macro F1-score, precision, and recall for performance evaluation. We compared the performance of span labelling against document labelling methods, which included both direct document classifiers and indirect document classifiers that rely on span classification results.   The SpanCategorizer and MedRoBERTa.nl models outperformed all other span and document classifiers, respectively. The weighted F1-score varied between characteristics, ranging from 0.60 to 0.93 in SpanCategorizer and 0.96 to 0.98 in MedRoBERTa.nl. Direct document classification was superior to indirect document classification using span classifiers. SetFit achieved competitive document classification performance using only 10\\% of the training data. Utilizing a reduced label set yielded near-perfect document classification results.   We recommend using our published SpanCategorizer and MedRoBERTa.nl models for span- and document-level diagnosis extraction from Dutch echocardiography reports. For settings with limited training data, SetFit may be a promising alternative for document classification.","sentences":["Clinical machine learning research and AI driven clinical decision support models rely on clinically accurate labels.","Manually extracting these labels with the help of clinical specialists is often time-consuming and expensive.","This study tests the feasibility of automatic span- and document-level diagnosis extraction from unstructured Dutch echocardiogram reports.   ","We included 115,692 unstructured echocardiogram reports from the UMCU a large university hospital in the Netherlands.","A randomly selected subset was manually annotated for the occurrence and severity of eleven commonly described cardiac characteristics.","We developed and tested several automatic labelling techniques at both span and document levels, using weighted and macro F1-score, precision, and recall for performance evaluation.","We compared the performance of span labelling against document labelling methods, which included both direct document classifiers and indirect document classifiers that rely on span classification results.   ","The SpanCategorizer and MedRoBERTa.nl models outperformed all other span and document classifiers, respectively.","The weighted F1-score varied between characteristics, ranging from 0.60 to 0.93 in SpanCategorizer and 0.96 to 0.98 in MedRoBERTa.nl.","Direct document classification was superior to indirect document classification using span classifiers.","SetFit achieved competitive document classification performance using only 10\\% of the training data.","Utilizing a reduced label set yielded near-perfect document classification results.   ","We recommend using our published SpanCategorizer and MedRoBERTa.nl models for span- and document-level diagnosis extraction from Dutch echocardiography reports.","For settings with limited training data, SetFit may be a promising alternative for document classification."],"url":"http://arxiv.org/abs/2408.06930v1"}
{"created":"2024-08-13 14:32:43","title":"Evaluating Cultural Adaptability of a Large Language Model via Simulation of Synthetic Personas","abstract":"The success of Large Language Models (LLMs) in multicultural environments hinges on their ability to understand users' diverse cultural backgrounds. We measure this capability by having an LLM simulate human profiles representing various nationalities within the scope of a questionnaire-style psychological experiment. Specifically, we employ GPT-3.5 to reproduce reactions to persuasive news articles of 7,286 participants from 15 countries; comparing the results with a dataset of real participants sharing the same demographic traits. Our analysis shows that specifying a person's country of residence improves GPT-3.5's alignment with their responses. In contrast, using native language prompting introduces shifts that significantly reduce overall alignment, with some languages particularly impairing performance. These findings suggest that while direct nationality information enhances the model's cultural adaptability, native language cues do not reliably improve simulation fidelity and can detract from the model's effectiveness.","sentences":["The success of Large Language Models (LLMs) in multicultural environments hinges on their ability to understand users' diverse cultural backgrounds.","We measure this capability by having an LLM simulate human profiles representing various nationalities within the scope of a questionnaire-style psychological experiment.","Specifically, we employ GPT-3.5 to reproduce reactions to persuasive news articles of 7,286 participants from 15 countries; comparing the results with a dataset of real participants sharing the same demographic traits.","Our analysis shows that specifying a person's country of residence improves GPT-3.5's alignment with their responses.","In contrast, using native language prompting introduces shifts that significantly reduce overall alignment, with some languages particularly impairing performance.","These findings suggest that while direct nationality information enhances the model's cultural adaptability, native language cues do not reliably improve simulation fidelity and can detract from the model's effectiveness."],"url":"http://arxiv.org/abs/2408.06929v1"}
{"created":"2024-08-13 14:29:00","title":"Breaking Class Barriers: Efficient Dataset Distillation via Inter-Class Feature Compensator","abstract":"Dataset distillation has emerged as a technique aiming to condense informative features from large, natural datasets into a compact and synthetic form. While recent advancements have refined this technique, its performance is bottlenecked by the prevailing class-specific synthesis paradigm. Under this paradigm, synthetic data is optimized exclusively for a pre-assigned one-hot label, creating an implicit class barrier in feature condensation. This leads to inefficient utilization of the distillation budget and oversight of inter-class feature distributions, which ultimately limits the effectiveness and efficiency, as demonstrated in our analysis.   To overcome these constraints, this paper presents the Inter-class Feature Compensator (INFER), an innovative distillation approach that transcends the class-specific data-label framework widely utilized in current dataset distillation methods. Specifically, INFER leverages a Universal Feature Compensator (UFC) to enhance feature integration across classes, enabling the generation of multiple additional synthetic instances from a single UFC input. This significantly improves the efficiency of the distillation budget.   Moreover, INFER enriches inter-class interactions during the distillation, thereby enhancing the effectiveness and generalizability of the distilled data. By allowing for the linear interpolation of labels similar to those in the original dataset, INFER meticulously optimizes the synthetic data and dramatically reduces the size of soft labels in the synthetic dataset to almost zero, establishing a new benchmark for efficiency and effectiveness in dataset distillation.","sentences":["Dataset distillation has emerged as a technique aiming to condense informative features from large, natural datasets into a compact and synthetic form.","While recent advancements have refined this technique, its performance is bottlenecked by the prevailing class-specific synthesis paradigm.","Under this paradigm, synthetic data is optimized exclusively for a pre-assigned one-hot label, creating an implicit class barrier in feature condensation.","This leads to inefficient utilization of the distillation budget and oversight of inter-class feature distributions, which ultimately limits the effectiveness and efficiency, as demonstrated in our analysis.   ","To overcome these constraints, this paper presents the Inter-class Feature Compensator (INFER), an innovative distillation approach that transcends the class-specific data-label framework widely utilized in current dataset distillation methods.","Specifically, INFER leverages a Universal Feature Compensator (UFC) to enhance feature integration across classes, enabling the generation of multiple additional synthetic instances from a single UFC input.","This significantly improves the efficiency of the distillation budget.   ","Moreover, INFER enriches inter-class interactions during the distillation, thereby enhancing the effectiveness and generalizability of the distilled data.","By allowing for the linear interpolation of labels similar to those in the original dataset, INFER meticulously optimizes the synthetic data and dramatically reduces the size of soft labels in the synthetic dataset to almost zero, establishing a new benchmark for efficiency and effectiveness in dataset distillation."],"url":"http://arxiv.org/abs/2408.06927v1"}
{"created":"2024-08-13 14:26:30","title":"SceneGPT: A Language Model for 3D Scene Understanding","abstract":"Building models that can understand and reason about 3D scenes is difficult owing to the lack of data sources for 3D supervised training and large-scale training regimes. In this work we ask - How can the knowledge in a pre-trained language model be leveraged for 3D scene understanding without any 3D pre-training. The aim of this work is to establish whether pre-trained LLMs possess priors/knowledge required for reasoning in 3D space and how can we prompt them such that they can be used for general purpose spatial reasoning and object understanding in 3D. To this end, we present SceneGPT, an LLM based scene understanding system which can perform 3D spatial reasoning without training or explicit 3D supervision. The key components of our framework are - 1) a 3D scene graph, that serves as scene representation, encoding the objects in the scene and their spatial relationships 2) a pre-trained LLM that can be adapted with in context learning for 3D spatial reasoning. We evaluate our framework qualitatively on object and scene understanding tasks including object semantics, physical properties and affordances (object-level) and spatial understanding (scene-level).","sentences":["Building models that can understand and reason about 3D scenes is difficult owing to the lack of data sources for 3D supervised training and large-scale training regimes.","In this work we ask - How can the knowledge in a pre-trained language model be leveraged for 3D scene understanding without any 3D pre-training.","The aim of this work is to establish whether pre-trained LLMs possess priors/knowledge required for reasoning in 3D space and how can we prompt them such that they can be used for general purpose spatial reasoning and object understanding in 3D. To this end, we present SceneGPT, an LLM based scene understanding system which can perform 3D spatial reasoning without training or explicit 3D supervision.","The key components of our framework are - 1) a 3D scene graph, that serves as scene representation, encoding the objects in the scene and their spatial relationships 2) a pre-trained LLM that can be adapted with in context learning for 3D spatial reasoning.","We evaluate our framework qualitatively on object and scene understanding tasks including object semantics, physical properties and affordances (object-level) and spatial understanding (scene-level)."],"url":"http://arxiv.org/abs/2408.06926v1"}
{"created":"2024-08-13 14:18:50","title":"Engineering Hypergraph $b$-Matching Algorithms","abstract":"Recently, researchers have extended the concept of matchings to the more general problem of finding $b$-matchings in hypergraphs broadening the scope of potential applications and challenges. The concept of $b$-matchings, where $b$ is a function that assigns positive integers to the vertices of the graph, is a natural extension of matchings in graphs, where each vertex $v$ is allowed to be matched to up to $b(v)$ edges, rather than just one. The weighted $b$-matching problem then seeks to select a subset of the hyperedges that fulfills the constraint and maximizes the weight.   In this work, we engineer novel algorithms for this generalized problem. More precisely, we introduce exact data reductions for the problem as well as a novel greedy initial solution and local search algorithms. These data reductions allow us to significantly shrink the input size. This is done by either determining if a hyperedge is guaranteed to be in an optimum $b$-matching and thus can be added to our solution or if it can be safely ignored. Our iterated local search algorithm provides a framework for finding suitable improvement swaps of edges.   Experiments on a wide range of real-world hypergraphs show that our new set of data reductions are highly practical, and our initial solutions are competitive for graphs and hypergraphs as well.","sentences":["Recently, researchers have extended the concept of matchings to the more general problem of finding $b$-matchings in hypergraphs broadening the scope of potential applications and challenges.","The concept of $b$-matchings, where $b$ is a function that assigns positive integers to the vertices of the graph, is a natural extension of matchings in graphs, where each vertex $v$ is allowed to be matched to up to $b(v)$ edges, rather than just one.","The weighted $b$-matching problem then seeks to select a subset of the hyperedges that fulfills the constraint and maximizes the weight.   ","In this work, we engineer novel algorithms for this generalized problem.","More precisely, we introduce exact data reductions for the problem as well as a novel greedy initial solution and local search algorithms.","These data reductions allow us to significantly shrink the input size.","This is done by either determining if a hyperedge is guaranteed to be in an optimum $b$-matching and thus can be added to our solution or if it can be safely ignored.","Our iterated local search algorithm provides a framework for finding suitable improvement swaps of edges.   ","Experiments on a wide range of real-world hypergraphs show that our new set of data reductions are highly practical, and our initial solutions are competitive for graphs and hypergraphs as well."],"url":"http://arxiv.org/abs/2408.06924v1"}
{"created":"2024-08-13 14:15:15","title":"Temporal Variability and Multi-Viewed Self-Supervised Representations to Tackle the ASVspoof5 Deepfake Challenge","abstract":"ASVspoof5, the fifth edition of the ASVspoof series, is one of the largest global audio security challenges. It aims to advance the development of countermeasure (CM) to discriminate bonafide and spoofed speech utterances. In this paper, we focus on addressing the problem of open-domain audio deepfake detection, which corresponds directly to the ASVspoof5 Track1 open condition. At first, we comprehensively investigate various CM on ASVspoof5, including data expansion, data augmentation, and self-supervised learning (SSL) features. Due to the high-frequency gaps characteristic of the ASVspoof5 dataset, we introduce Frequency Mask, a data augmentation method that masks specific frequency bands to improve CM robustness. Combining various scale of temporal information with multiple SSL features, our experiments achieved a minDCF of 0.0158 and an EER of 0.55% on the ASVspoof 5 Track 1 evaluation progress set.","sentences":["ASVspoof5, the fifth edition of the ASVspoof series, is one of the largest global audio security challenges.","It aims to advance the development of countermeasure (CM) to discriminate bonafide and spoofed speech utterances.","In this paper, we focus on addressing the problem of open-domain audio deepfake detection, which corresponds directly to the ASVspoof5","Track1 open condition.","At first, we comprehensively investigate various CM on ASVspoof5, including data expansion, data augmentation, and self-supervised learning (SSL) features.","Due to the high-frequency gaps characteristic of the ASVspoof5 dataset, we introduce Frequency Mask, a data augmentation method that masks specific frequency bands to improve CM robustness.","Combining various scale of temporal information with multiple SSL features, our experiments achieved a minDCF of 0.0158 and an EER of 0.55% on the ASVspoof 5 Track 1 evaluation progress set."],"url":"http://arxiv.org/abs/2408.06922v1"}
{"created":"2024-08-13 14:12:03","title":"Multi-Agent Continuous Control with Generative Flow Networks","abstract":"Generative Flow Networks (GFlowNets) aim to generate diverse trajectories from a distribution in which the final states of the trajectories are proportional to the reward, serving as a powerful alternative to reinforcement learning for exploratory control tasks. However, the individual-flow matching constraint in GFlowNets limits their applications for multi-agent systems, especially continuous joint-control problems. In this paper, we propose a novel Multi-Agent generative Continuous Flow Networks (MACFN) method to enable multiple agents to perform cooperative exploration for various compositional continuous objects. Technically, MACFN trains decentralized individual-flow-based policies in a centralized global-flow-based matching fashion. During centralized training, MACFN introduces a continuous flow decomposition network to deduce the flow contributions of each agent in the presence of only global rewards. Then agents can deliver actions solely based on their assigned local flow in a decentralized way, forming a joint policy distribution proportional to the rewards. To guarantee the expressiveness of continuous flow decomposition, we theoretically derive a consistency condition on the decomposition network. Experimental results demonstrate that the proposed method yields results superior to the state-of-the-art counterparts and better exploration capability. Our code is available at https://github.com/isluoshuang/MACFN.","sentences":["Generative Flow Networks (GFlowNets) aim to generate diverse trajectories from a distribution in which the final states of the trajectories are proportional to the reward, serving as a powerful alternative to reinforcement learning for exploratory control tasks.","However, the individual-flow matching constraint in GFlowNets limits their applications for multi-agent systems, especially continuous joint-control problems.","In this paper, we propose a novel Multi-Agent generative Continuous Flow Networks (MACFN) method to enable multiple agents to perform cooperative exploration for various compositional continuous objects.","Technically, MACFN trains decentralized individual-flow-based policies in a centralized global-flow-based matching fashion.","During centralized training, MACFN introduces a continuous flow decomposition network to deduce the flow contributions of each agent in the presence of only global rewards.","Then agents can deliver actions solely based on their assigned local flow in a decentralized way, forming a joint policy distribution proportional to the rewards.","To guarantee the expressiveness of continuous flow decomposition, we theoretically derive a consistency condition on the decomposition network.","Experimental results demonstrate that the proposed method yields results superior to the state-of-the-art counterparts and better exploration capability.","Our code is available at https://github.com/isluoshuang/MACFN."],"url":"http://arxiv.org/abs/2408.06920v1"}
{"created":"2024-08-13 14:06:07","title":"Quantitative analysis of attack-fault trees via Markov decision processes","abstract":"Adequate risk assessment of safety critical systems needs to take both safety and security into account, as well as their interaction. A prominent methodology for modeling safety and security are attack-fault trees (AFTs), which combine the well-established fault tree and attack tree methodologies for safety and security, respectively. AFTs can be used for quantitative analysis as well, capturing the interplay between safety and security metrics. However, existing approaches are based on modeling the AFT as a priced-timed automaton. This allows for a wide range of analyses, but Pareto analsis is still lacking, and analyses that exist are computationally expensive. In this paper, we combine safety and security analysis techniques to introduce a novel method to find the Pareto front between the metrics reliability (safety) and attack cost (security) using Markov decision processes. This gives us the full interplay between safety and security while being considerably more lightweight and faster than the automaton approach. We validate our approach on a case study of cyberattacks on an oil pipe line.","sentences":["Adequate risk assessment of safety critical systems needs to take both safety and security into account, as well as their interaction.","A prominent methodology for modeling safety and security are attack-fault trees (AFTs), which combine the well-established fault tree and attack tree methodologies for safety and security, respectively.","AFTs can be used for quantitative analysis as well, capturing the interplay between safety and security metrics.","However, existing approaches are based on modeling the AFT as a priced-timed automaton.","This allows for a wide range of analyses, but Pareto analsis is still lacking, and analyses that exist are computationally expensive.","In this paper, we combine safety and security analysis techniques to introduce a novel method to find the Pareto front between the metrics reliability (safety) and attack cost (security) using Markov decision processes.","This gives us the full interplay between safety and security while being considerably more lightweight and faster than the automaton approach.","We validate our approach on a case study of cyberattacks on an oil pipe line."],"url":"http://arxiv.org/abs/2408.06914v1"}
{"created":"2024-08-13 14:00:44","title":"An Information Geometry Interpretation for Approximate Message Passing","abstract":"In this paper, we propose an information geometry (IG) framework to solve the standard linear regression problem. The proposed framework is an extension of the one for computing the mean of complex multivariate Gaussian distribution. By applying the proposed framework, the information geometry approach (IGA) and the approximate information geometry approach (AIGA) for basis pursuit de-noising (BPDN) in standard linear regression are derived. The framework can also be applied to other standard linear regression problems. With the transformations of natural and expectation parameters of Gaussian distributions, we then show the relationship between the IGA and the message passing (MP) algorithm. Finally, we prove that the AIGA is equivalent to the approximate message passing (AMP) algorithm. These intrinsic results offer a new perspective for the AMP algorithm, and clues for understanding and improving stochastic reasoning methods.","sentences":["In this paper, we propose an information geometry (IG) framework to solve the standard linear regression problem.","The proposed framework is an extension of the one for computing the mean of complex multivariate Gaussian distribution.","By applying the proposed framework, the information geometry approach (IGA) and the approximate information geometry approach (AIGA) for basis pursuit de-noising (BPDN) in standard linear regression are derived.","The framework can also be applied to other standard linear regression problems.","With the transformations of natural and expectation parameters of Gaussian distributions, we then show the relationship between the IGA and the message passing (MP) algorithm.","Finally, we prove that the AIGA is equivalent to the approximate message passing (AMP) algorithm.","These intrinsic results offer a new perspective for the AMP algorithm, and clues for understanding and improving stochastic reasoning methods."],"url":"http://arxiv.org/abs/2408.06907v1"}
{"created":"2024-08-13 13:58:23","title":"Re-TASK: Revisiting LLM Tasks from Capability, Skill, and Knowledge Perspectives","abstract":"As large language models (LLMs) continue to scale, their enhanced performance often proves insufficient for solving domain-specific tasks. Systematically analyzing their failures and effectively enhancing their performance remain significant challenges. This paper introduces the Re-TASK framework, a novel theoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge perspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space Theory. The Re-TASK framework provides a systematic methodology to deepen our understanding, evaluation, and enhancement of LLMs for domain-specific tasks. It explores the interplay among an LLM's capabilities, the knowledge it processes, and the skills it applies, elucidating how these elements are interconnected and impact task performance. Our application of the Re-TASK framework reveals that many failures in domain-specific tasks can be attributed to insufficient knowledge or inadequate skill adaptation. With this insight, we propose structured strategies for enhancing LLMs through targeted knowledge injection and skill adaptation. Specifically, we identify key capability items associated with tasks and employ a deliberately designed prompting strategy to enhance task performance, thereby reducing the need for extensive fine-tuning. Alternatively, we fine-tune the LLM using capability-specific instructions, further validating the efficacy of our framework. Experimental results confirm the framework's effectiveness, demonstrating substantial improvements in both the performance and applicability of LLMs.","sentences":["As large language models (LLMs) continue to scale, their enhanced performance often proves insufficient for solving domain-specific tasks.","Systematically analyzing their failures and effectively enhancing their performance remain significant challenges.","This paper introduces the Re-TASK framework, a novel theoretical model that Revisits LLM Tasks from cApability, Skill, Knowledge perspectives, guided by the principles of Bloom's Taxonomy and Knowledge Space Theory.","The Re-TASK framework provides a systematic methodology to deepen our understanding, evaluation, and enhancement of LLMs for domain-specific tasks.","It explores the interplay among an LLM's capabilities, the knowledge it processes, and the skills it applies, elucidating how these elements are interconnected and impact task performance.","Our application of the Re-TASK framework reveals that many failures in domain-specific tasks can be attributed to insufficient knowledge or inadequate skill adaptation.","With this insight, we propose structured strategies for enhancing LLMs through targeted knowledge injection and skill adaptation.","Specifically, we identify key capability items associated with tasks and employ a deliberately designed prompting strategy to enhance task performance, thereby reducing the need for extensive fine-tuning.","Alternatively, we fine-tune the LLM using capability-specific instructions, further validating the efficacy of our framework.","Experimental results confirm the framework's effectiveness, demonstrating substantial improvements in both the performance and applicability of LLMs."],"url":"http://arxiv.org/abs/2408.06904v1"}
{"created":"2024-08-13 13:56:17","title":"Heterogeneity: An Open Challenge for Federated On-board Machine Learning","abstract":"The design of satellite missions is currently undergoing a paradigm shift from the historical approach of individualised monolithic satellites towards distributed mission configurations, consisting of multiple small satellites. With a rapidly growing number of such satellites now deployed in orbit, each collecting large amounts of data, interest in on-board orbital edge computing is rising. Federated Learning is a promising distributed computing approach in this context, allowing multiple satellites to collaborate efficiently in training on-board machine learning models. Though recent works on the use of Federated Learning in orbital edge computing have focused largely on homogeneous satellite constellations, Federated Learning could also be employed to allow heterogeneous satellites to form ad-hoc collaborations, e.g. in the case of communications satellites operated by different providers. Such an application presents additional challenges to the Federated Learning paradigm, arising largely from the heterogeneity of such a system. In this position paper, we offer a systematic review of these challenges in the context of the cross-provider use case, giving a brief overview of the state-of-the-art for each, and providing an entry point for deeper exploration of each issue.","sentences":["The design of satellite missions is currently undergoing a paradigm shift from the historical approach of individualised monolithic satellites towards distributed mission configurations, consisting of multiple small satellites.","With a rapidly growing number of such satellites now deployed in orbit, each collecting large amounts of data, interest in on-board orbital edge computing is rising.","Federated Learning is a promising distributed computing approach in this context, allowing multiple satellites to collaborate efficiently in training on-board machine learning models.","Though recent works on the use of Federated Learning in orbital edge computing have focused largely on homogeneous satellite constellations, Federated Learning could also be employed to allow heterogeneous satellites to form ad-hoc collaborations, e.g. in the case of communications satellites operated by different providers.","Such an application presents additional challenges to the Federated Learning paradigm, arising largely from the heterogeneity of such a system.","In this position paper, we offer a systematic review of these challenges in the context of the cross-provider use case, giving a brief overview of the state-of-the-art for each, and providing an entry point for deeper exploration of each issue."],"url":"http://arxiv.org/abs/2408.06903v1"}
{"created":"2024-08-13 13:51:34","title":"Divide and Conquer: Improving Multi-Camera 3D Perception with 2D Semantic-Depth Priors and Input-Dependent Queries","abstract":"3D perception tasks, such as 3D object detection and Bird's-Eye-View (BEV) segmentation using multi-camera images, have drawn significant attention recently. Despite the fact that accurately estimating both semantic and 3D scene layouts are crucial for this task, existing techniques often neglect the synergistic effects of semantic and depth cues, leading to the occurrence of classification and position estimation errors. Additionally, the input-independent nature of initial queries also limits the learning capacity of Transformer-based models. To tackle these challenges, we propose an input-aware Transformer framework that leverages Semantics and Depth as priors (named SDTR). Our approach involves the use of an S-D Encoder that explicitly models semantic and depth priors, thereby disentangling the learning process of object categorization and position estimation. Moreover, we introduce a Prior-guided Query Builder that incorporates the semantic prior into the initial queries of the Transformer, resulting in more effective input-aware queries. Extensive experiments on the nuScenes and Lyft benchmarks demonstrate the state-of-the-art performance of our method in both 3D object detection and BEV segmentation tasks.","sentences":["3D perception tasks, such as 3D object detection and Bird's-Eye-View (BEV) segmentation using multi-camera images, have drawn significant attention recently.","Despite the fact that accurately estimating both semantic and 3D scene layouts are crucial for this task, existing techniques often neglect the synergistic effects of semantic and depth cues, leading to the occurrence of classification and position estimation errors.","Additionally, the input-independent nature of initial queries also limits the learning capacity of Transformer-based models.","To tackle these challenges, we propose an input-aware Transformer framework that leverages Semantics and Depth as priors (named SDTR).","Our approach involves the use of an S-D Encoder that explicitly models semantic and depth priors, thereby disentangling the learning process of object categorization and position estimation.","Moreover, we introduce a Prior-guided Query Builder that incorporates the semantic prior into the initial queries of the Transformer, resulting in more effective input-aware queries.","Extensive experiments on the nuScenes and Lyft benchmarks demonstrate the state-of-the-art performance of our method in both 3D object detection and BEV segmentation tasks."],"url":"http://arxiv.org/abs/2408.06901v1"}
{"created":"2024-08-13 13:50:49","title":"Entendre, a Social Bot Detection Tool for Niche, Fringe, and Extreme Social Media","abstract":"Social bots-automated accounts that generate and spread content on social media-are exploiting vulnerabilities in these platforms to manipulate public perception and disseminate disinformation. This has prompted the development of public bot detection services; however, most of these services focus primarily on Twitter, leaving niche platforms vulnerable. Fringe social media platforms such as Parler, Gab, and Gettr often have minimal moderation, which facilitates the spread of hate speech and misinformation. To address this gap, we introduce Entendre, an open-access, scalable, and platform-agnostic bot detection framework. Entendre can process a labeled dataset from any social platform to produce a tailored bot detection model using a random forest classification approach, ensuring robust social bot detection. We exploit the idea that most social platforms share a generic template, where users can post content, approve content, and provide a bio (common data features). By emphasizing general data features over platform-specific ones, Entendre offers rapid extensibility at the expense of some accuracy. To demonstrate Entendre's effectiveness, we used it to explore the presence of bots among accounts posting racist content on the now-defunct right-wing platform Parler. We examined 233,000 posts from 38,379 unique users and found that 1,916 unique users (4.99%) exhibited bot-like behavior. Visualization techniques further revealed that these bots significantly impacted the network, amplifying influential rhetoric and hashtags (e.g., #qanon, #trump, #antilgbt). These preliminary findings underscore the need for tools like Entendre to monitor and assess bot activity across diverse platforms.","sentences":["Social bots-automated accounts that generate and spread content on social media-are exploiting vulnerabilities in these platforms to manipulate public perception and disseminate disinformation.","This has prompted the development of public bot detection services; however, most of these services focus primarily on Twitter, leaving niche platforms vulnerable.","Fringe social media platforms such as Parler, Gab, and Gettr often have minimal moderation, which facilitates the spread of hate speech and misinformation.","To address this gap, we introduce Entendre, an open-access, scalable, and platform-agnostic bot detection framework.","Entendre can process a labeled dataset from any social platform to produce a tailored bot detection model using a random forest classification approach, ensuring robust social bot detection.","We exploit the idea that most social platforms share a generic template, where users can post content, approve content, and provide a bio (common data features).","By emphasizing general data features over platform-specific ones, Entendre offers rapid extensibility at the expense of some accuracy.","To demonstrate Entendre's effectiveness, we used it to explore the presence of bots among accounts posting racist content on the now-defunct right-wing platform Parler.","We examined 233,000 posts from 38,379 unique users and found that 1,916 unique users (4.99%) exhibited bot-like behavior.","Visualization techniques further revealed that these bots significantly impacted the network, amplifying influential rhetoric and hashtags (e.g., #qanon, #trump, #antilgbt).","These preliminary findings underscore the need for tools like Entendre to monitor and assess bot activity across diverse platforms."],"url":"http://arxiv.org/abs/2408.06900v1"}
{"created":"2024-08-13 13:50:46","title":"EE3P3D: Event-based Estimation of Periodic Phenomena Frequency using 3D Correlation","abstract":"We present a novel method for measuring the frequency of periodic phenomena, e.g., rotation, flicker and vibration, by an event camera, a device asynchronously reporting brightness changes at independently operating pixels with high temporal resolution. The approach assumes that for a periodic phenomenon, a highly similar set of events is generated within a specific spatio-temporal window at a time difference corresponding to the phenomenon's period. The sets of similar events are detected by 3D spatio-temporal correlation in the event stream space. The proposed method, EE3P3D, is evaluated on a dataset of 12 sequences of periodic phenomena, i.e. flashing light and vibration, and periodic motion, e.g., rotation, ranging from 3.2 Hz to 2 kHz (equivalent to 192 - 120 000 RPM). EE3P3D significantly outperforms published methods on this dataset, achieving a mean relative error of 0.1%.","sentences":["We present a novel method for measuring the frequency of periodic phenomena, e.g., rotation, flicker and vibration, by an event camera, a device asynchronously reporting brightness changes at independently operating pixels with high temporal resolution.","The approach assumes that for a periodic phenomenon, a highly similar set of events is generated within a specific spatio-temporal window at a time difference corresponding to the phenomenon's period.","The sets of similar events are detected by 3D spatio-temporal correlation in the event stream space.","The proposed method, EE3P3D, is evaluated on a dataset of 12 sequences of periodic phenomena, i.e. flashing light and vibration, and periodic motion, e.g., rotation, ranging from 3.2 Hz to 2 kHz (equivalent to 192 - 120 000 RPM).","EE3P3D significantly outperforms published methods on this dataset, achieving a mean relative error of 0.1%."],"url":"http://arxiv.org/abs/2408.06899v1"}
{"created":"2024-08-13 13:38:32","title":"Automatic Feature Recognition and Dimensional Attributes Extraction From CAD Models for Hybrid Additive-Subtractive Manufacturing","abstract":"The integration of Computer-Aided Design (CAD), Computer-Aided Process Planning (CAPP), and Computer-Aided Manufacturing (CAM) plays a crucial role in modern manufacturing, facilitating seamless transitions from digital designs to physical products. However, a significant challenge within this integration is the Automatic Feature Recognition (AFR) of CAD models, especially in the context of hybrid manufacturing that combines subtractive and additive manufacturing processes. Traditional AFR methods, focused mainly on the identification of subtractive (machined) features including holes, fillets, chamfers, pockets, and slots, fail to recognize features pertinent to additive manufacturing. Furthermore, the traditional methods fall short in accurately extracting geometric dimensions and orientations, which are also key factors for effective manufacturing process planning. This paper presents a novel approach for creating a synthetic CAD dataset that encompasses features relevant to both additive and subtractive machining through Python Open Cascade. The Hierarchical Graph Convolutional Neural Network (HGCNN) model is implemented to accurately identify the composite additive-subtractive features within the synthetic CAD dataset. The key novelty and contribution of the proposed methodology lie in its ability to recognize a wide range of manufacturing features, and precisely extracting their dimensions, orientations, and stock sizes. The proposed model demonstrates remarkable feature recognition accuracy exceeding 97% and a dimension extraction accuracy of 100% for identified features. Therefore, the proposed methodology enhances the integration of CAD, CAPP, and CAM within hybrid manufacturing by providing precise feature recognition and dimension extraction. It facilitates improved manufacturing process planning, by enabling more informed decision-making.","sentences":["The integration of Computer-Aided Design (CAD), Computer-Aided Process Planning (CAPP), and Computer-Aided Manufacturing (CAM) plays a crucial role in modern manufacturing, facilitating seamless transitions from digital designs to physical products.","However, a significant challenge within this integration is the Automatic Feature Recognition (AFR) of CAD models, especially in the context of hybrid manufacturing that combines subtractive and additive manufacturing processes.","Traditional AFR methods, focused mainly on the identification of subtractive (machined) features including holes, fillets, chamfers, pockets, and slots, fail to recognize features pertinent to additive manufacturing.","Furthermore, the traditional methods fall short in accurately extracting geometric dimensions and orientations, which are also key factors for effective manufacturing process planning.","This paper presents a novel approach for creating a synthetic CAD dataset that encompasses features relevant to both additive and subtractive machining through Python Open Cascade.","The Hierarchical Graph Convolutional Neural Network (HGCNN) model is implemented to accurately identify the composite additive-subtractive features within the synthetic CAD dataset.","The key novelty and contribution of the proposed methodology lie in its ability to recognize a wide range of manufacturing features, and precisely extracting their dimensions, orientations, and stock sizes.","The proposed model demonstrates remarkable feature recognition accuracy exceeding 97% and a dimension extraction accuracy of 100% for identified features.","Therefore, the proposed methodology enhances the integration of CAD, CAPP, and CAM within hybrid manufacturing by providing precise feature recognition and dimension extraction.","It facilitates improved manufacturing process planning, by enabling more informed decision-making."],"url":"http://arxiv.org/abs/2408.06891v1"}
{"created":"2024-08-13 13:36:48","title":"BMFT: Achieving Fairness via Bias-based Weight Masking Fine-tuning","abstract":"Developing models with robust group fairness properties is paramount, particularly in ethically sensitive domains such as medical diagnosis. Recent approaches to achieving fairness in machine learning require a substantial amount of training data and depend on model retraining, which may not be practical in real-world scenarios. To mitigate these challenges, we propose Bias-based Weight Masking Fine-Tuning (BMFT), a novel post-processing method that enhances the fairness of a trained model in significantly fewer epochs without requiring access to the original training data. BMFT produces a mask over model parameters, which efficiently identifies the weights contributing the most towards biased predictions. Furthermore, we propose a two-step debiasing strategy, wherein the feature extractor undergoes initial fine-tuning on the identified bias-influenced weights, succeeded by a fine-tuning phase on a reinitialised classification layer to uphold discriminative performance. Extensive experiments across four dermatological datasets and two sensitive attributes demonstrate that BMFT outperforms existing state-of-the-art (SOTA) techniques in both diagnostic accuracy and fairness metrics. Our findings underscore the efficacy and robustness of BMFT in advancing fairness across various out-of-distribution (OOD) settings. Our code is available at: https://github.com/vios-s/BMFT","sentences":["Developing models with robust group fairness properties is paramount, particularly in ethically sensitive domains such as medical diagnosis.","Recent approaches to achieving fairness in machine learning require a substantial amount of training data and depend on model retraining, which may not be practical in real-world scenarios.","To mitigate these challenges, we propose Bias-based Weight Masking Fine-Tuning (BMFT), a novel post-processing method that enhances the fairness of a trained model in significantly fewer epochs without requiring access to the original training data.","BMFT produces a mask over model parameters, which efficiently identifies the weights contributing the most towards biased predictions.","Furthermore, we propose a two-step debiasing strategy, wherein the feature extractor undergoes initial fine-tuning on the identified bias-influenced weights, succeeded by a fine-tuning phase on a reinitialised classification layer to uphold discriminative performance.","Extensive experiments across four dermatological datasets and two sensitive attributes demonstrate that BMFT outperforms existing state-of-the-art (SOTA) techniques in both diagnostic accuracy and fairness metrics.","Our findings underscore the efficacy and robustness of BMFT in advancing fairness across various out-of-distribution (OOD) settings.","Our code is available at: https://github.com/vios-s/BMFT"],"url":"http://arxiv.org/abs/2408.06890v1"}
{"created":"2024-08-13 13:33:35","title":"Voltran: Unlocking Trust and Confidentiality in Decentralized Federated Learning Aggregation","abstract":"The decentralized Federated Learning (FL) paradigm built upon blockchain architectures leverages distributed node clusters to replace the single server for executing FL model aggregation. This paradigm tackles the vulnerability of the centralized malicious server in vanilla FL and inherits the trustfulness and robustness offered by blockchain. However, existing blockchain-enabled schemes face challenges related to inadequate confidentiality on models and limited computational resources of blockchains to perform large-scale FL computations. In this paper, we present Voltran, an innovative hybrid platform designed to achieve trust, confidentiality, and robustness for FL based on the combination of the Trusted Execution Environment (TEE) and blockchain technology. We offload the FL aggregation computation into TEE to provide an isolated, trusted and customizable off-chain execution, and then guarantee the authenticity and verifiability of aggregation results on the blockchain. Moreover, we provide strong scalability on multiple FL scenarios by introducing a multi-SGX parallel execution strategy to amortize the large-scale FL workload. We implement a prototype of Voltran and conduct a comprehensive performance evaluation. Extensive experimental results demonstrate that Voltran incurs minimal additional overhead while guaranteeing trust, confidentiality, and authenticity, and it significantly brings a significant speed-up compared to state-of-the-art ciphertext aggregation schemes.","sentences":["The decentralized Federated Learning (FL) paradigm built upon blockchain architectures leverages distributed node clusters to replace the single server for executing FL model aggregation.","This paradigm tackles the vulnerability of the centralized malicious server in vanilla FL and inherits the trustfulness and robustness offered by blockchain.","However, existing blockchain-enabled schemes face challenges related to inadequate confidentiality on models and limited computational resources of blockchains to perform large-scale FL computations.","In this paper, we present Voltran, an innovative hybrid platform designed to achieve trust, confidentiality, and robustness for FL based on the combination of the Trusted Execution Environment (TEE) and blockchain technology.","We offload the FL aggregation computation into TEE to provide an isolated, trusted and customizable off-chain execution, and then guarantee the authenticity and verifiability of aggregation results on the blockchain.","Moreover, we provide strong scalability on multiple FL scenarios by introducing a multi-SGX parallel execution strategy to amortize the large-scale FL workload.","We implement a prototype of Voltran and conduct a comprehensive performance evaluation.","Extensive experimental results demonstrate that Voltran incurs minimal additional overhead while guaranteeing trust, confidentiality, and authenticity, and it significantly brings a significant speed-up compared to state-of-the-art ciphertext aggregation schemes."],"url":"http://arxiv.org/abs/2408.06885v1"}
{"created":"2024-08-13 13:29:57","title":"Diffusion Model for Slate Recommendation","abstract":"Slate recommendation is a technique commonly used on streaming platforms and e-commerce sites to present multiple items together. A significant challenge with slate recommendation is managing the complex combinatorial choice space. Traditional methods often simplify this problem by assuming users engage with only one item at a time. However, this simplification does not reflect the reality, as users often interact with multiple items simultaneously. In this paper, we address the general slate recommendation problem, which accounts for simultaneous engagement with multiple items. We propose a generative approach using Diffusion Models, leveraging their ability to learn structures in high-dimensional data. Our model generates high-quality slates that maximize user satisfaction by overcoming the challenges of the combinatorial choice space. Furthermore, our approach enhances the diversity of recommendations. Extensive offline evaluations on applications such as music playlist generation and e-commerce bundle recommendations show that our model outperforms state-of-the-art baselines in both relevance and diversity.","sentences":["Slate recommendation is a technique commonly used on streaming platforms and e-commerce sites to present multiple items together.","A significant challenge with slate recommendation is managing the complex combinatorial choice space.","Traditional methods often simplify this problem by assuming users engage with only one item at a time.","However, this simplification does not reflect the reality, as users often interact with multiple items simultaneously.","In this paper, we address the general slate recommendation problem, which accounts for simultaneous engagement with multiple items.","We propose a generative approach using Diffusion Models, leveraging their ability to learn structures in high-dimensional data.","Our model generates high-quality slates that maximize user satisfaction by overcoming the challenges of the combinatorial choice space.","Furthermore, our approach enhances the diversity of recommendations.","Extensive offline evaluations on applications such as music playlist generation and e-commerce bundle recommendations show that our model outperforms state-of-the-art baselines in both relevance and diversity."],"url":"http://arxiv.org/abs/2408.06883v1"}
{"created":"2024-08-13 13:26:38","title":"Architecture Specific Generation of Large Scale Lattice Boltzmann Methods for Sparse Complex Geometries","abstract":"We implement and analyse a sparse / indirect-addressing data structure for the Lattice Boltzmann Method to support efficient compute kernels for fluid dynamics problems with a high number of non-fluid nodes in the domain, such as in porous media flows. The data structure is integrated into a code generation pipeline to enable sparse Lattice Boltzmann Methods with a variety of stencils and collision operators and to generate efficient code for kernels for CPU as well as for AMD and NVIDIA accelerator cards. We optimize these sparse kernels with an in-place streaming pattern to save memory accesses and memory consumption and we implement a communication hiding technique to prove scalability. We present single GPU performance results with up to 99% of maximal bandwidth utilization. We integrate the optimized generated kernels in the high performance framework WALBERLA and achieve a scaling efficiency of at least 82% on up to 1024 NVIDIA A100 GPUs and up to 4096 AMD MI250X GPUs on modern HPC systems. Further, we set up three different applications to test the sparse data structure for realistic demonstrator problems. We show performance results for flow through porous media, free flow over a particle bed, and blood flow in a coronary artery. We achieve a maximal performance speed-up of 2 and a significantly reduced memory consumption by up to 75% with the sparse / indirect-addressing data structure compared to the direct-addressing data structure for these applications.","sentences":["We implement and analyse a sparse / indirect-addressing data structure for the Lattice Boltzmann Method to support efficient compute kernels for fluid dynamics problems with a high number of non-fluid nodes in the domain, such as in porous media flows.","The data structure is integrated into a code generation pipeline to enable sparse Lattice Boltzmann Methods with a variety of stencils and collision operators and to generate efficient code for kernels for CPU as well as for AMD and NVIDIA accelerator cards.","We optimize these sparse kernels with an in-place streaming pattern to save memory accesses and memory consumption and we implement a communication hiding technique to prove scalability.","We present single GPU performance results with up to 99% of maximal bandwidth utilization.","We integrate the optimized generated kernels in the high performance framework WALBERLA and achieve a scaling efficiency of at least 82% on up to 1024 NVIDIA A100 GPUs and up to 4096 AMD MI250X GPUs on modern HPC systems.","Further, we set up three different applications to test the sparse data structure for realistic demonstrator problems.","We show performance results for flow through porous media, free flow over a particle bed, and blood flow in a coronary artery.","We achieve a maximal performance speed-up of 2 and a significantly reduced memory consumption by up to 75% with the sparse / indirect-addressing data structure compared to the direct-addressing data structure for these applications."],"url":"http://arxiv.org/abs/2408.06880v1"}
{"created":"2024-08-13 13:26:24","title":"PBIR-NIE: Glossy Object Capture under Non-Distant Lighting","abstract":"Glossy objects present a significant challenge for 3D reconstruction from multi-view input images under natural lighting. In this paper, we introduce PBIR-NIE, an inverse rendering framework designed to holistically capture the geometry, material attributes, and surrounding illumination of such objects. We propose a novel parallax-aware non-distant environment map as a lightweight and efficient lighting representation, accurately modeling the near-field background of the scene, which is commonly encountered in real-world capture setups. This feature allows our framework to accommodate complex parallax effects beyond the capabilities of standard infinite-distance environment maps. Our method optimizes an underlying signed distance field (SDF) through physics-based differentiable rendering, seamlessly connecting surface gradients between a triangle mesh and the SDF via neural implicit evolution (NIE). To address the intricacies of highly glossy BRDFs in differentiable rendering, we integrate the antithetic sampling algorithm to mitigate variance in the Monte Carlo gradient estimator. Consequently, our framework exhibits robust capabilities in handling glossy object reconstruction, showcasing superior quality in geometry, relighting, and material estimation.","sentences":["Glossy objects present a significant challenge for 3D reconstruction from multi-view input images under natural lighting.","In this paper, we introduce PBIR-NIE, an inverse rendering framework designed to holistically capture the geometry, material attributes, and surrounding illumination of such objects.","We propose a novel parallax-aware non-distant environment map as a lightweight and efficient lighting representation, accurately modeling the near-field background of the scene, which is commonly encountered in real-world capture setups.","This feature allows our framework to accommodate complex parallax effects beyond the capabilities of standard infinite-distance environment maps.","Our method optimizes an underlying signed distance field (SDF) through physics-based differentiable rendering, seamlessly connecting surface gradients between a triangle mesh and the SDF via neural implicit evolution (NIE).","To address the intricacies of highly glossy BRDFs in differentiable rendering, we integrate the antithetic sampling algorithm to mitigate variance in the Monte Carlo gradient estimator.","Consequently, our framework exhibits robust capabilities in handling glossy object reconstruction, showcasing superior quality in geometry, relighting, and material estimation."],"url":"http://arxiv.org/abs/2408.06878v1"}
{"created":"2024-08-13 13:14:54","title":"Decision-Focused Learning to Predict Action Costs for Planning","abstract":"In many automated planning applications, action costs can be hard to specify. An example is the time needed to travel through a certain road segment, which depends on many factors, such as the current weather conditions. A natural way to address this issue is to learn to predict these parameters based on input features (e.g., weather forecasts) and use the predicted action costs in automated planning afterward. Decision-Focused Learning (DFL) has been successful in learning to predict the parameters of combinatorial optimization problems in a way that optimizes solution quality rather than prediction quality. This approach yields better results than treating prediction and optimization as separate tasks. In this paper, we investigate for the first time the challenges of implementing DFL for automated planning in order to learn to predict the action costs. There are two main challenges to overcome: (1) planning systems are called during gradient descent learning, to solve planning problems with negative action costs, which are not supported in planning. We propose novel methods for gradient computation to avoid this issue. (2) DFL requires repeated planner calls during training, which can limit the scalability of the method. We experiment with different methods approximating the optimal plan as well as an easy-to-implement caching mechanism to speed up the learning process. As the first work that addresses DFL for automated planning, we demonstrate that the proposed gradient computation consistently yields significantly better plans than predictions aimed at minimizing prediction error; and that caching can temper the computation requirements.","sentences":["In many automated planning applications, action costs can be hard to specify.","An example is the time needed to travel through a certain road segment, which depends on many factors, such as the current weather conditions.","A natural way to address this issue is to learn to predict these parameters based on input features (e.g., weather forecasts) and use the predicted action costs in automated planning afterward.","Decision-Focused Learning (DFL) has been successful in learning to predict the parameters of combinatorial optimization problems in a way that optimizes solution quality rather than prediction quality.","This approach yields better results than treating prediction and optimization as separate tasks.","In this paper, we investigate for the first time the challenges of implementing DFL for automated planning in order to learn to predict the action costs.","There are two main challenges to overcome: (1) planning systems are called during gradient descent learning, to solve planning problems with negative action costs, which are not supported in planning.","We propose novel methods for gradient computation to avoid this issue.","(2) DFL requires repeated planner calls during training, which can limit the scalability of the method.","We experiment with different methods approximating the optimal plan as well as an easy-to-implement caching mechanism to speed up the learning process.","As the first work that addresses DFL for automated planning, we demonstrate that the proposed gradient computation consistently yields significantly better plans than predictions aimed at minimizing prediction error; and that caching can temper the computation requirements."],"url":"http://arxiv.org/abs/2408.06876v1"}
{"created":"2024-08-13 13:11:56","title":"Advancing Interactive Explainable AI via Belief Change Theory","abstract":"As AI models become ever more complex and intertwined in humans' daily lives, greater levels of interactivity of explainable AI (XAI) methods are needed. In this paper, we propose the use of belief change theory as a formal foundation for operators that model the incorporation of new information, i.e. user feedback in interactive XAI, to logical representations of data-driven classifiers. We argue that this type of formalisation provides a framework and a methodology to develop interactive explanations in a principled manner, providing warranted behaviour and favouring transparency and accountability of such interactions. Concretely, we first define a novel, logic-based formalism to represent explanatory information shared between humans and machines. We then consider real world scenarios for interactive XAI, with different prioritisations of new and existing knowledge, where our formalism may be instantiated. Finally, we analyse a core set of belief change postulates, discussing their suitability for our real world settings and pointing to particular challenges that may require the relaxation or reinterpretation of some of the theoretical assumptions underlying existing operators.","sentences":["As AI models become ever more complex and intertwined in humans' daily lives, greater levels of interactivity of explainable AI (XAI) methods are needed.","In this paper, we propose the use of belief change theory as a formal foundation for operators that model the incorporation of new information, i.e. user feedback in interactive XAI, to logical representations of data-driven classifiers.","We argue that this type of formalisation provides a framework and a methodology to develop interactive explanations in a principled manner, providing warranted behaviour and favouring transparency and accountability of such interactions.","Concretely, we first define a novel, logic-based formalism to represent explanatory information shared between humans and machines.","We then consider real world scenarios for interactive XAI, with different prioritisations of new and existing knowledge, where our formalism may be instantiated.","Finally, we analyse a core set of belief change postulates, discussing their suitability for our real world settings and pointing to particular challenges that may require the relaxation or reinterpretation of some of the theoretical assumptions underlying existing operators."],"url":"http://arxiv.org/abs/2408.06875v1"}
{"created":"2024-08-13 13:11:53","title":"Leveraging Language Models for Emotion and Behavior Analysis in Education","abstract":"The analysis of students' emotions and behaviors is crucial for enhancing learning outcomes and personalizing educational experiences. Traditional methods often rely on intrusive visual and physiological data collection, posing privacy concerns and scalability issues. This paper proposes a novel method leveraging large language models (LLMs) and prompt engineering to analyze textual data from students. Our approach utilizes tailored prompts to guide LLMs in detecting emotional and engagement states, providing a non-intrusive and scalable solution. We conducted experiments using Qwen, ChatGPT, Claude2, and GPT-4, comparing our method against baseline models and chain-of-thought (CoT) prompting. Results demonstrate that our method significantly outperforms the baselines in both accuracy and contextual understanding. This study highlights the potential of LLMs combined with prompt engineering to offer practical and effective tools for educational emotion and behavior analysis.","sentences":["The analysis of students' emotions and behaviors is crucial for enhancing learning outcomes and personalizing educational experiences.","Traditional methods often rely on intrusive visual and physiological data collection, posing privacy concerns and scalability issues.","This paper proposes a novel method leveraging large language models (LLMs) and prompt engineering to analyze textual data from students.","Our approach utilizes tailored prompts to guide LLMs in detecting emotional and engagement states, providing a non-intrusive and scalable solution.","We conducted experiments using Qwen, ChatGPT, Claude2, and GPT-4, comparing our method against baseline models and chain-of-thought (CoT) prompting.","Results demonstrate that our method significantly outperforms the baselines in both accuracy and contextual understanding.","This study highlights the potential of LLMs combined with prompt engineering to offer practical and effective tools for educational emotion and behavior analysis."],"url":"http://arxiv.org/abs/2408.06874v1"}
{"created":"2024-08-13 13:11:28","title":"Margin of Victory for Weighted Tournament Solutions","abstract":"Determining how close a winner of an election is to becoming a loser, or distinguishing between different possible winners of an election, are major problems in computational social choice. We tackle these problems for so-called weighted tournament solutions by generalizing the notion of margin of victory (MoV) for tournament solutions by Brill et. al to weighted tournament solutions. For these, the MoV of a winner (resp. loser) is the total weight that needs to be changed in the tournament to make them a loser (resp. winner). We study three weighted tournament solutions: Borda's rule, the weighted Uncovered Set, and Split Cycle. For all three rules, we determine whether the MoV for winners and non-winners is tractable and give upper and lower bounds on the possible values of the MoV. Further, we axiomatically study and generalize properties from the unweighted tournament setting to weighted tournaments.","sentences":["Determining how close a winner of an election is to becoming a loser, or distinguishing between different possible winners of an election, are major problems in computational social choice.","We tackle these problems for so-called weighted tournament solutions by generalizing the notion of margin of victory (MoV) for tournament solutions by Brill et.","al to weighted tournament solutions.","For these, the MoV of a winner (resp. loser) is the total weight that needs to be changed in the tournament to make them a loser (resp. winner).","We study three weighted tournament solutions: Borda's rule, the weighted Uncovered Set, and Split Cycle.","For all three rules, we determine whether the MoV for winners and non-winners is tractable and give upper and lower bounds on the possible values of the MoV. Further, we axiomatically study and generalize properties from the unweighted tournament setting to weighted tournaments."],"url":"http://arxiv.org/abs/2408.06873v1"}
{"created":"2024-08-13 13:10:03","title":"Generative AI Tools in Academic Research: Applications and Implications for Qualitative and Quantitative Research Methodologies","abstract":"This study examines the impact of Generative Artificial Intelligence (GenAI) on academic research, focusing on its application to qualitative and quantitative data analysis. As GenAI tools evolve rapidly, they offer new possibilities for enhancing research productivity and democratising complex analytical processes. However, their integration into academic practice raises significant questions regarding research integrity and security, authorship, and the changing nature of scholarly work. Through an examination of current capabilities and potential future applications, this study provides insights into how researchers may utilise GenAI tools responsibly and ethically.   We present case studies that demonstrate the application of GenAI in various research methodologies, discuss the challenges of replicability and consistency in AI-assisted research, and consider the ethical implications of increased AI integration in academia. This study explores both qualitative and quantitative applications of GenAI, highlighting tools for transcription, coding, thematic analysis, visual analytics, and statistical analysis. By addressing these issues, we aim to contribute to the ongoing discourse on the role of AI in shaping the future of academic research and provide guidance for researchers exploring the rapidly evolving landscape of AI-assisted research tools and research.","sentences":["This study examines the impact of Generative Artificial Intelligence (GenAI) on academic research, focusing on its application to qualitative and quantitative data analysis.","As GenAI tools evolve rapidly, they offer new possibilities for enhancing research productivity and democratising complex analytical processes.","However, their integration into academic practice raises significant questions regarding research integrity and security, authorship, and the changing nature of scholarly work.","Through an examination of current capabilities and potential future applications, this study provides insights into how researchers may utilise GenAI tools responsibly and ethically.   ","We present case studies that demonstrate the application of GenAI in various research methodologies, discuss the challenges of replicability and consistency in AI-assisted research, and consider the ethical implications of increased AI integration in academia.","This study explores both qualitative and quantitative applications of GenAI, highlighting tools for transcription, coding, thematic analysis, visual analytics, and statistical analysis.","By addressing these issues, we aim to contribute to the ongoing discourse on the role of AI in shaping the future of academic research and provide guidance for researchers exploring the rapidly evolving landscape of AI-assisted research tools and research."],"url":"http://arxiv.org/abs/2408.06872v1"}
{"created":"2024-08-13 13:06:50","title":"A Comprehensive Survey on Synthetic Infrared Image synthesis","abstract":"Synthetic infrared (IR) scene and target generation is an important computer vision problem as it allows the generation of realistic IR images and targets for training and testing of various applications, such as remote sensing, surveillance, and target recognition. It also helps reduce the cost and risk associated with collecting real-world IR data. This survey paper aims to provide a comprehensive overview of the conventional mathematical modelling-based methods and deep learning-based methods used for generating synthetic IR scenes and targets. The paper discusses the importance of synthetic IR scene and target generation and briefly covers the mathematics of blackbody and grey body radiations, as well as IR image-capturing methods. The potential use cases of synthetic IR scenes and target generation are also described, highlighting the significance of these techniques in various fields. Additionally, the paper explores possible new ways of developing new techniques to enhance the efficiency and effectiveness of synthetic IR scenes and target generation while highlighting the need for further research to advance this field.","sentences":["Synthetic infrared (IR) scene and target generation is an important computer vision problem as it allows the generation of realistic IR images and targets for training and testing of various applications, such as remote sensing, surveillance, and target recognition.","It also helps reduce the cost and risk associated with collecting real-world IR data.","This survey paper aims to provide a comprehensive overview of the conventional mathematical modelling-based methods and deep learning-based methods used for generating synthetic IR scenes and targets.","The paper discusses the importance of synthetic IR scene and target generation and briefly covers the mathematics of blackbody and grey body radiations, as well as IR image-capturing methods.","The potential use cases of synthetic IR scenes and target generation are also described, highlighting the significance of these techniques in various fields.","Additionally, the paper explores possible new ways of developing new techniques to enhance the efficiency and effectiveness of synthetic IR scenes and target generation while highlighting the need for further research to advance this field."],"url":"http://arxiv.org/abs/2408.06868v1"}
{"created":"2024-08-13 13:05:36","title":"Optimal Bound for PCA with Outliers using Higher-Degree Voronoi Diagrams","abstract":"In this paper, we introduce new algorithms for Principal Component Analysis (PCA) with outliers. Utilizing techniques from computational geometry, specifically higher-degree Voronoi diagrams, we navigate to the optimal subspace for PCA even in the presence of outliers. This approach achieves an optimal solution with a time complexity of $n^{d+\\mathcal{O}(1)}\\text{poly}(n,d)$. Additionally, we present a randomized algorithm with a complexity of $2^{\\mathcal{O}(r(d-r))} \\times \\text{poly}(n, d)$. This algorithm samples subspaces characterized in terms of a Grassmannian manifold. By employing such sampling method, we ensure a high likelihood of capturing the optimal subspace, with the success probability $(1 - \\delta)^T$. Where $\\delta$ represents the probability that a sampled subspace does not contain the optimal solution, and $T$ is the number of subspaces sampled, proportional to $2^{r(d-r)}$. Our use of higher-degree Voronoi diagrams and Grassmannian based sampling offers a clearer conceptual pathway and practical advantages, particularly in handling large datasets or higher-dimensional settings.","sentences":["In this paper, we introduce new algorithms for Principal Component Analysis (PCA) with outliers.","Utilizing techniques from computational geometry, specifically higher-degree Voronoi diagrams, we navigate to the optimal subspace for PCA even in the presence of outliers.","This approach achieves an optimal solution with a time complexity of $n^{d+\\mathcal{O}(1)}\\text{poly}(n,d)$.","Additionally, we present a randomized algorithm with a complexity of $2^{\\mathcal{O}(r(d-r))}","\\times \\text{poly}(n, d)$.","This algorithm samples subspaces characterized in terms of a Grassmannian manifold.","By employing such sampling method, we ensure a high likelihood of capturing the optimal subspace, with the success probability $(1 - \\delta)^T$. Where $\\delta$ represents the probability that a sampled subspace does not contain the optimal solution, and $T$ is the number of subspaces sampled, proportional to $2^{r(d-r)}$.","Our use of higher-degree Voronoi diagrams and Grassmannian based sampling offers a clearer conceptual pathway and practical advantages, particularly in handling large datasets or higher-dimensional settings."],"url":"http://arxiv.org/abs/2408.06867v1"}
