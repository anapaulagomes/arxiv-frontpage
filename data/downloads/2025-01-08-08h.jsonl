{"created":"2025-01-07 18:59:59","title":"LargeAD: Large-Scale Cross-Sensor Data Pretraining for Autonomous Driving","abstract":"Recent advancements in vision foundation models (VFMs) have revolutionized visual perception in 2D, yet their potential for 3D scene understanding, particularly in autonomous driving applications, remains underexplored. In this paper, we introduce LargeAD, a versatile and scalable framework designed for large-scale 3D pretraining across diverse real-world driving datasets. Our framework leverages VFMs to extract semantically rich superpixels from 2D images, which are aligned with LiDAR point clouds to generate high-quality contrastive samples. This alignment facilitates cross-modal representation learning, enhancing the semantic consistency between 2D and 3D data. We introduce several key innovations: i) VFM-driven superpixel generation for detailed semantic representation, ii) a VFM-assisted contrastive learning strategy to align multimodal features, iii) superpoint temporal consistency to maintain stable representations across time, and iv) multi-source data pretraining to generalize across various LiDAR configurations. Our approach delivers significant performance improvements over state-of-the-art methods in both linear probing and fine-tuning tasks for both LiDAR-based segmentation and object detection. Extensive experiments on eleven large-scale multi-modal datasets highlight our superior performance, demonstrating the adaptability, efficiency, and robustness in real-world autonomous driving scenarios.","sentences":["Recent advancements in vision foundation models (VFMs) have revolutionized visual perception in 2D, yet their potential for 3D scene understanding, particularly in autonomous driving applications, remains underexplored.","In this paper, we introduce LargeAD, a versatile and scalable framework designed for large-scale 3D pretraining across diverse real-world driving datasets.","Our framework leverages VFMs to extract semantically rich superpixels from 2D images, which are aligned with LiDAR point clouds to generate high-quality contrastive samples.","This alignment facilitates cross-modal representation learning, enhancing the semantic consistency between 2D and 3D data.","We introduce several key innovations: i) VFM-driven superpixel generation for detailed semantic representation, ii) a VFM-assisted contrastive learning strategy to align multimodal features, iii) superpoint temporal consistency to maintain stable representations across time, and iv) multi-source data pretraining to generalize across various LiDAR configurations.","Our approach delivers significant performance improvements over state-of-the-art methods in both linear probing and fine-tuning tasks for both LiDAR-based segmentation and object detection.","Extensive experiments on eleven large-scale multi-modal datasets highlight our superior performance, demonstrating the adaptability, efficiency, and robustness in real-world autonomous driving scenarios."],"url":"http://arxiv.org/abs/2501.04005v1"}
{"created":"2025-01-07 18:59:58","title":"LiMoE: Mixture of LiDAR Representation Learners from Automotive Scenes","abstract":"LiDAR data pretraining offers a promising approach to leveraging large-scale, readily available datasets for enhanced data utilization. However, existing methods predominantly focus on sparse voxel representation, overlooking the complementary attributes provided by other LiDAR representations. In this work, we propose LiMoE, a framework that integrates the Mixture of Experts (MoE) paradigm into LiDAR data representation learning to synergistically combine multiple representations, such as range images, sparse voxels, and raw points. Our approach consists of three stages: i) Image-to-LiDAR Pretraining, which transfers prior knowledge from images to point clouds across different representations; ii) Contrastive Mixture Learning (CML), which uses MoE to adaptively activate relevant attributes from each representation and distills these mixed features into a unified 3D network; iii) Semantic Mixture Supervision (SMS), which combines semantic logits from multiple representations to boost downstream segmentation performance. Extensive experiments across 11 large-scale LiDAR datasets demonstrate our effectiveness and superiority. The code and model checkpoints have been made publicly accessible.","sentences":["LiDAR data pretraining offers a promising approach to leveraging large-scale, readily available datasets for enhanced data utilization.","However, existing methods predominantly focus on sparse voxel representation, overlooking the complementary attributes provided by other LiDAR representations.","In this work, we propose LiMoE, a framework that integrates the Mixture of Experts (MoE) paradigm into LiDAR data representation learning to synergistically combine multiple representations, such as range images, sparse voxels, and raw points.","Our approach consists of three stages: i) Image-to-LiDAR Pretraining, which transfers prior knowledge from images to point clouds across different representations; ii) Contrastive Mixture Learning (CML), which uses MoE to adaptively activate relevant attributes from each representation and distills these mixed features into a unified 3D network; iii) Semantic Mixture Supervision (SMS), which combines semantic logits from multiple representations to boost downstream segmentation performance.","Extensive experiments across 11 large-scale LiDAR datasets demonstrate our effectiveness and superiority.","The code and model checkpoints have been made publicly accessible."],"url":"http://arxiv.org/abs/2501.04004v1"}
{"created":"2025-01-07 18:59:55","title":"Are VLMs Ready for Autonomous Driving? An Empirical Study from the Reliability, Data, and Metric Perspectives","abstract":"Recent advancements in Vision-Language Models (VLMs) have sparked interest in their use for autonomous driving, particularly in generating interpretable driving decisions through natural language. However, the assumption that VLMs inherently provide visually grounded, reliable, and interpretable explanations for driving remains largely unexamined. To address this gap, we introduce DriveBench, a benchmark dataset designed to evaluate VLM reliability across 17 settings (clean, corrupted, and text-only inputs), encompassing 19,200 frames, 20,498 question-answer pairs, three question types, four mainstream driving tasks, and a total of 12 popular VLMs. Our findings reveal that VLMs often generate plausible responses derived from general knowledge or textual cues rather than true visual grounding, especially under degraded or missing visual inputs. This behavior, concealed by dataset imbalances and insufficient evaluation metrics, poses significant risks in safety-critical scenarios like autonomous driving. We further observe that VLMs struggle with multi-modal reasoning and display heightened sensitivity to input corruptions, leading to inconsistencies in performance. To address these challenges, we propose refined evaluation metrics that prioritize robust visual grounding and multi-modal understanding. Additionally, we highlight the potential of leveraging VLMs' awareness of corruptions to enhance their reliability, offering a roadmap for developing more trustworthy and interpretable decision-making systems in real-world autonomous driving contexts. The benchmark toolkit is publicly accessible.","sentences":["Recent advancements in Vision-Language Models (VLMs) have sparked interest in their use for autonomous driving, particularly in generating interpretable driving decisions through natural language.","However, the assumption that VLMs inherently provide visually grounded, reliable, and interpretable explanations for driving remains largely unexamined.","To address this gap, we introduce DriveBench, a benchmark dataset designed to evaluate VLM reliability across 17 settings (clean, corrupted, and text-only inputs), encompassing 19,200 frames, 20,498 question-answer pairs, three question types, four mainstream driving tasks, and a total of 12 popular VLMs.","Our findings reveal that VLMs often generate plausible responses derived from general knowledge or textual cues rather than true visual grounding, especially under degraded or missing visual inputs.","This behavior, concealed by dataset imbalances and insufficient evaluation metrics, poses significant risks in safety-critical scenarios like autonomous driving.","We further observe that VLMs struggle with multi-modal reasoning and display heightened sensitivity to input corruptions, leading to inconsistencies in performance.","To address these challenges, we propose refined evaluation metrics that prioritize robust visual grounding and multi-modal understanding.","Additionally, we highlight the potential of leveraging VLMs' awareness of corruptions to enhance their reliability, offering a roadmap for developing more trustworthy and interpretable decision-making systems in real-world autonomous driving contexts.","The benchmark toolkit is publicly accessible."],"url":"http://arxiv.org/abs/2501.04003v1"}
{"created":"2025-01-07 18:59:28","title":"Extraction Of Cumulative Blobs From Dynamic Gestures","abstract":"Gesture recognition is a perceptual user interface, which is based on CV technology that allows the computer to interpret human motions as commands, allowing users to communicate with a computer without the use of hands, thus making the mouse and keyboard superfluous. Gesture recognition's main weakness is a light condition because gesture control is based on computer vision, which heavily relies on cameras. These cameras are used to interpret gestures in 2D and 3D, so the extracted information can vary depending on the source of light. The limitation of the system cannot work in a dark environment. A simple night vision camera can be used as our camera for motion capture as they also blast out infrared light which is not visible to humans but can be clearly seen with a camera that has no infrared filter this majorly overcomes the limitation of systems which cannot work in a dark environment. So, the video stream from the camera is fed into a Raspberry Pi which has a Python program running OpenCV module which is used for detecting, isolating and tracking the path of dynamic gesture, then we use an algorithm of machine learning to recognize the pattern drawn and accordingly control the GPIOs of the raspberry pi to perform some activities.","sentences":["Gesture recognition is a perceptual user interface, which is based on CV technology that allows the computer to interpret human motions as commands, allowing users to communicate with a computer without the use of hands, thus making the mouse and keyboard superfluous.","Gesture recognition's main weakness is a light condition because gesture control is based on computer vision, which heavily relies on cameras.","These cameras are used to interpret gestures in 2D and 3D, so the extracted information can vary depending on the source of light.","The limitation of the system cannot work in a dark environment.","A simple night vision camera can be used as our camera for motion capture as they also blast out infrared light which is not visible to humans but can be clearly seen with a camera that has no infrared filter this majorly overcomes the limitation of systems which cannot work in a dark environment.","So, the video stream from the camera is fed into a Raspberry Pi which has a Python program running OpenCV module which is used for detecting, isolating and tracking the path of dynamic gesture, then we use an algorithm of machine learning to recognize the pattern drawn and accordingly control the GPIOs of the raspberry pi to perform some activities."],"url":"http://arxiv.org/abs/2501.04002v1"}
{"created":"2025-01-07 18:58:54","title":"Sa2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos","abstract":"This work presents Sa2VA, the first unified model for dense grounded understanding of both images and videos. Unlike existing multi-modal large language models, which are often limited to specific modalities and tasks, Sa2VA supports a wide range of image and video tasks, including referring segmentation and conversation, with minimal one-shot instruction tuning. Sa2VA combines SAM-2, a foundation video segmentation model, with LLaVA, an advanced vision-language model, and unifies text, image, and video into a shared LLM token space. Using the LLM, Sa2VA generates instruction tokens that guide SAM-2 in producing precise masks, enabling a grounded, multi-modal understanding of both static and dynamic visual content. Additionally, we introduce Ref-SAV, an auto-labeled dataset containing over 72k object expressions in complex video scenes, designed to boost model performance. We also manually validate 2k video objects in the Ref-SAV datasets to benchmark referring video object segmentation in complex environments. Experiments show that Sa2VA achieves state-of-the-art across multiple tasks, particularly in referring video object segmentation, highlighting its potential for complex real-world applications.","sentences":["This work presents Sa2VA, the first unified model for dense grounded understanding of both images and videos.","Unlike existing multi-modal large language models, which are often limited to specific modalities and tasks, Sa2VA supports a wide range of image and video tasks, including referring segmentation and conversation, with minimal one-shot instruction tuning.","Sa2VA combines SAM-2, a foundation video segmentation model, with LLaVA, an advanced vision-language model, and unifies text, image, and video into a shared LLM token space.","Using the LLM, Sa2VA generates instruction tokens that guide SAM-2 in producing precise masks, enabling a grounded, multi-modal understanding of both static and dynamic visual content.","Additionally, we introduce Ref-SAV, an auto-labeled dataset containing over 72k object expressions in complex video scenes, designed to boost model performance.","We also manually validate 2k video objects in the Ref-SAV datasets to benchmark referring video object segmentation in complex environments.","Experiments show that Sa2VA achieves state-of-the-art across multiple tasks, particularly in referring video object segmentation, highlighting its potential for complex real-world applications."],"url":"http://arxiv.org/abs/2501.04001v1"}
{"created":"2025-01-07 18:56:14","title":"A Survey on Federated Learning in Human Sensing","abstract":"Human Sensing, a field that leverages technology to monitor human activities, psycho-physiological states, and interactions with the environment, enhances our understanding of human behavior and drives the development of advanced services that improve overall quality of life. However, its reliance on detailed and often privacy-sensitive data as the basis for its machine learning (ML) models raises significant legal and ethical concerns. The recently proposed ML approach of Federated Learning (FL) promises to alleviate many of these concerns, as it is able to create accurate ML models without sending raw user data to a central server. While FL has demonstrated its usefulness across a variety of areas, such as text prediction and cyber security, its benefits in Human Sensing are under-explored, given the particular challenges in this domain. This survey conducts a comprehensive analysis of the current state-of-the-art studies on FL in Human Sensing, and proposes a taxonomy and an eight-dimensional assessment for FL approaches. Through the eight-dimensional assessment, we then evaluate whether the surveyed studies consider a specific FL-in-Human-Sensing challenge or not. Finally, based on the overall analysis, we discuss open challenges and highlight five research aspects related to FL in Human Sensing that require urgent research attention. Our work provides a comprehensive corpus of FL studies and aims to assist FL practitioners in developing and evaluating solutions that effectively address the real-world complexities of Human Sensing.","sentences":["Human Sensing, a field that leverages technology to monitor human activities, psycho-physiological states, and interactions with the environment, enhances our understanding of human behavior and drives the development of advanced services that improve overall quality of life.","However, its reliance on detailed and often privacy-sensitive data as the basis for its machine learning (ML) models raises significant legal and ethical concerns.","The recently proposed ML approach of Federated Learning (FL) promises to alleviate many of these concerns, as it is able to create accurate ML models without sending raw user data to a central server.","While FL has demonstrated its usefulness across a variety of areas, such as text prediction and cyber security, its benefits in Human Sensing are under-explored, given the particular challenges in this domain.","This survey conducts a comprehensive analysis of the current state-of-the-art studies on FL in Human Sensing, and proposes a taxonomy and an eight-dimensional assessment for FL approaches.","Through the eight-dimensional assessment, we then evaluate whether the surveyed studies consider a specific FL-in-Human-Sensing challenge or not.","Finally, based on the overall analysis, we discuss open challenges and highlight five research aspects related to FL in Human Sensing that require urgent research attention.","Our work provides a comprehensive corpus of FL studies and aims to assist FL practitioners in developing and evaluating solutions that effectively address the real-world complexities of Human Sensing."],"url":"http://arxiv.org/abs/2501.04000v1"}
{"created":"2025-01-07 18:55:02","title":"WAPTS: A Weighted Allocation Probability Adjusted Thompson Sampling Algorithm for High-Dimensional and Sparse Experiment Settings","abstract":"Aiming for more effective experiment design, such as in video content advertising where different content options compete for user engagement, these scenarios can be modeled as multi-arm bandit problems. In cases where limited interactions are available due to external factors, such as the cost of conducting experiments, recommenders often face constraints due to the small number of user interactions. In addition, there is a trade-off between selecting the best treatment and the ability to personalize and contextualize based on individual factors. A popular solution to this dilemma is the Contextual Bandit framework. It aims to maximize outcomes while incorporating personalization (contextual) factors, customizing treatments such as a user's profile to individual preferences. Despite their advantages, Contextual Bandit algorithms face challenges like measurement bias and the 'curse of dimensionality.' These issues complicate the management of numerous interventions and often lead to data sparsity through participant segmentation. To address these problems, we introduce the Weighted Allocation Probability Adjusted Thompson Sampling (WAPTS) algorithm. WAPTS builds on the contextual Thompson Sampling method by using a dynamic weighting parameter. This improves the allocation process for interventions and enables rapid optimization in data-sparse environments. We demonstrate the performance of our approach on different numbers of arms and effect sizes.","sentences":["Aiming for more effective experiment design, such as in video content advertising where different content options compete for user engagement, these scenarios can be modeled as multi-arm bandit problems.","In cases where limited interactions are available due to external factors, such as the cost of conducting experiments, recommenders often face constraints due to the small number of user interactions.","In addition, there is a trade-off between selecting the best treatment and the ability to personalize and contextualize based on individual factors.","A popular solution to this dilemma is the Contextual Bandit framework.","It aims to maximize outcomes while incorporating personalization (contextual) factors, customizing treatments such as a user's profile to individual preferences.","Despite their advantages, Contextual Bandit algorithms face challenges like measurement bias and the 'curse of dimensionality.'","These issues complicate the management of numerous interventions and often lead to data sparsity through participant segmentation.","To address these problems, we introduce the Weighted Allocation Probability Adjusted Thompson Sampling (WAPTS) algorithm.","WAPTS builds on the contextual Thompson Sampling method by using a dynamic weighting parameter.","This improves the allocation process for interventions and enables rapid optimization in data-sparse environments.","We demonstrate the performance of our approach on different numbers of arms and effect sizes."],"url":"http://arxiv.org/abs/2501.03999v1"}
{"created":"2025-01-07 18:52:05","title":"RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance","abstract":"Retrieval-augmented generation (RAG) improves large language models (LLMs) by using external knowledge to guide response generation, reducing hallucinations. However, RAG, particularly multi-modal RAG, can introduce new hallucination sources: (i) the retrieval process may select irrelevant pieces (e.g., documents, images) as raw context from the database, and (ii) retrieved images are processed into text-based context via vision-language models (VLMs) or directly used by multi-modal language models (MLLMs) like GPT-4o, which may hallucinate. To address this, we propose a novel framework to evaluate the reliability of multi-modal RAG using two performance measures: (i) the relevancy score (RS), assessing the relevance of retrieved entries to the query, and (ii) the correctness score (CS), evaluating the accuracy of the generated response. We train RS and CS models using a ChatGPT-derived database and human evaluator samples. Results show that both models achieve ~88% accuracy on test data. Additionally, we construct a 5000-sample human-annotated database evaluating the relevancy of retrieved pieces and the correctness of response statements. Our RS model aligns with human preferences 20% more often than CLIP in retrieval, and our CS model matches human preferences ~91% of the time. Finally, we assess various RAG systems' selection and generation performances using RS and CS.","sentences":["Retrieval-augmented generation (RAG) improves large language models (LLMs) by using external knowledge to guide response generation, reducing hallucinations.","However, RAG, particularly multi-modal RAG, can introduce new hallucination sources: (i) the retrieval process may select irrelevant pieces (e.g., documents, images) as raw context from the database, and (ii) retrieved images are processed into text-based context via vision-language models (VLMs) or directly used by multi-modal language models (MLLMs) like GPT-4o, which may hallucinate.","To address this, we propose a novel framework to evaluate the reliability of multi-modal RAG using two performance measures: (i) the relevancy score (RS), assessing the relevance of retrieved entries to the query, and (ii) the correctness score (CS), evaluating the accuracy of the generated response.","We train RS and CS models using a ChatGPT-derived database and human evaluator samples.","Results show that both models achieve ~88% accuracy on test data.","Additionally, we construct a 5000-sample human-annotated database evaluating the relevancy of retrieved pieces and the correctness of response statements.","Our RS model aligns with human preferences 20% more often than CLIP in retrieval, and our CS model matches human preferences ~91% of the time.","Finally, we assess various RAG systems' selection and generation performances using RS and CS."],"url":"http://arxiv.org/abs/2501.03995v1"}
{"created":"2025-01-07 18:50:06","title":"NeuralSVG: An Implicit Representation for Text-to-Vector Generation","abstract":"Vector graphics are essential in design, providing artists with a versatile medium for creating resolution-independent and highly editable visual content. Recent advancements in vision-language and diffusion models have fueled interest in text-to-vector graphics generation. However, existing approaches often suffer from over-parameterized outputs or treat the layered structure - a core feature of vector graphics - as a secondary goal, diminishing their practical use. Recognizing the importance of layered SVG representations, we propose NeuralSVG, an implicit neural representation for generating vector graphics from text prompts. Inspired by Neural Radiance Fields (NeRFs), NeuralSVG encodes the entire scene into the weights of a small MLP network, optimized using Score Distillation Sampling (SDS). To encourage a layered structure in the generated SVG, we introduce a dropout-based regularization technique that strengthens the standalone meaning of each shape. We additionally demonstrate that utilizing a neural representation provides an added benefit of inference-time control, enabling users to dynamically adapt the generated SVG based on user-provided inputs, all with a single learned representation. Through extensive qualitative and quantitative evaluations, we demonstrate that NeuralSVG outperforms existing methods in generating structured and flexible SVG.","sentences":["Vector graphics are essential in design, providing artists with a versatile medium for creating resolution-independent and highly editable visual content.","Recent advancements in vision-language and diffusion models have fueled interest in text-to-vector graphics generation.","However, existing approaches often suffer from over-parameterized outputs or treat the layered structure - a core feature of vector graphics - as a secondary goal, diminishing their practical use.","Recognizing the importance of layered SVG representations, we propose NeuralSVG, an implicit neural representation for generating vector graphics from text prompts.","Inspired by Neural Radiance Fields (NeRFs), NeuralSVG encodes the entire scene into the weights of a small MLP network, optimized using Score Distillation Sampling (SDS).","To encourage a layered structure in the generated SVG, we introduce a dropout-based regularization technique that strengthens the standalone meaning of each shape.","We additionally demonstrate that utilizing a neural representation provides an added benefit of inference-time control, enabling users to dynamically adapt the generated SVG based on user-provided inputs, all with a single learned representation.","Through extensive qualitative and quantitative evaluations, we demonstrate that NeuralSVG outperforms existing methods in generating structured and flexible SVG."],"url":"http://arxiv.org/abs/2501.03992v1"}
{"created":"2025-01-07 18:48:42","title":"Influences on LLM Calibration: A Study of Response Agreement, Loss Functions, and Prompt Styles","abstract":"Calibration, the alignment between model confidence and prediction accuracy, is critical for the reliable deployment of large language models (LLMs). Existing works neglect to measure the generalization of their methods to other prompt styles and different sizes of LLMs. To address this, we define a controlled experimental setting covering 12 LLMs and four prompt styles. We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance. Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement. To optimize calibration, we integrate focal and AUC surrogate losses alongside binary cross-entropy. Experiments across four datasets demonstrate that both response agreement and focal loss improve calibration from baselines. We find that few-shot prompts are the most effective for auxiliary model-based methods, and auxiliary models demonstrate robust calibration performance across accuracy variations, outperforming LLMs' internal probabilities and verbalized confidences. These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications.","sentences":["Calibration, the alignment between model confidence and prediction accuracy, is critical for the reliable deployment of large language models (LLMs).","Existing works neglect to measure the generalization of their methods to other prompt styles and different sizes of LLMs.","To address this, we define a controlled experimental setting covering 12 LLMs and four prompt styles.","We additionally investigate if incorporating the response agreement of multiple LLMs and an appropriate loss function can improve calibration performance.","Concretely, we build Calib-n, a novel framework that trains an auxiliary model for confidence estimation that aggregates responses from multiple LLMs to capture inter-model agreement.","To optimize calibration, we integrate focal and AUC surrogate losses alongside binary cross-entropy.","Experiments across four datasets demonstrate that both response agreement and focal loss improve calibration from baselines.","We find that few-shot prompts are the most effective for auxiliary model-based methods, and auxiliary models demonstrate robust calibration performance across accuracy variations, outperforming LLMs' internal probabilities and verbalized confidences.","These insights deepen the understanding of influence factors in LLM calibration, supporting their reliable deployment in diverse applications."],"url":"http://arxiv.org/abs/2501.03991v1"}
{"created":"2025-01-07 18:46:34","title":"(De)-Indexing and the Right to be Forgotten","abstract":"In the digital age, the challenge of forgetfulness has emerged as a significant concern, particularly regarding the management of personal data and its accessibility online. The right to be forgotten (RTBF) allows individuals to request the removal of outdated or harmful information from public access, yet implementing this right poses substantial technical difficulties for search engines. This paper aims to introduce non-experts to the foundational concepts of information retrieval (IR) and de-indexing, which are critical for understanding how search engines can effectively \"forget\" certain content. We will explore various IR models, including boolean, probabilistic, vector space, and embedding-based approaches, as well as the role of Large Language Models (LLMs) in enhancing data processing capabilities. By providing this overview, we seek to highlight the complexities involved in balancing individual privacy rights with the operational challenges faced by search engines in managing information visibility.","sentences":["In the digital age, the challenge of forgetfulness has emerged as a significant concern, particularly regarding the management of personal data and its accessibility online.","The right to be forgotten (RTBF) allows individuals to request the removal of outdated or harmful information from public access, yet implementing this right poses substantial technical difficulties for search engines.","This paper aims to introduce non-experts to the foundational concepts of information retrieval (IR) and de-indexing, which are critical for understanding how search engines can effectively \"forget\" certain content.","We will explore various IR models, including boolean, probabilistic, vector space, and embedding-based approaches, as well as the role of Large Language Models (LLMs) in enhancing data processing capabilities.","By providing this overview, we seek to highlight the complexities involved in balancing individual privacy rights with the operational challenges faced by search engines in managing information visibility."],"url":"http://arxiv.org/abs/2501.03989v1"}
{"created":"2025-01-07 18:46:17","title":"Semantically Cohesive Word Grouping in Indian Languages","abstract":"Indian languages are inflectional and agglutinative and typically follow clause-free word order. The structure of sentences across most major Indian languages are similar when their dependency parse trees are considered. While some differences in the parsing structure occur due to peculiarities of a language or its preferred natural way of conveying meaning, several apparent differences are simply due to the granularity of representation of the smallest semantic unit of processing in a sentence. The semantic unit is typically a word, typographically separated by whitespaces. A single whitespace-separated word in one language may correspond to a group of words in another. Hence, grouping of words based on semantics helps unify the parsing structure of parallel sentences across languages and, in the process, morphology. In this work, we propose word grouping as a major preprocessing step for any computational or linguistic processing of sentences for Indian languages. Among Indian languages, since Hindi is one of the least agglutinative, we expect it to benefit the most from word-grouping. Hence, in this paper, we focus on Hindi to study the effects of grouping. We perform quantitative assessment of our proposal with an intrinsic method that perturbs sentences by shuffling words as well as an extrinsic evaluation that verifies the importance of word grouping for the task of Machine Translation (MT) using decomposed prompting. We also qualitatively analyze certain aspects of the syntactic structure of sentences. Our experiments and analyses show that the proposed grouping technique brings uniformity in the syntactic structures, as well as aids underlying NLP tasks.","sentences":["Indian languages are inflectional and agglutinative and typically follow clause-free word order.","The structure of sentences across most major Indian languages are similar when their dependency parse trees are considered.","While some differences in the parsing structure occur due to peculiarities of a language or its preferred natural way of conveying meaning, several apparent differences are simply due to the granularity of representation of the smallest semantic unit of processing in a sentence.","The semantic unit is typically a word, typographically separated by whitespaces.","A single whitespace-separated word in one language may correspond to a group of words in another.","Hence, grouping of words based on semantics helps unify the parsing structure of parallel sentences across languages and, in the process, morphology.","In this work, we propose word grouping as a major preprocessing step for any computational or linguistic processing of sentences for Indian languages.","Among Indian languages, since Hindi is one of the least agglutinative, we expect it to benefit the most from word-grouping.","Hence, in this paper, we focus on Hindi to study the effects of grouping.","We perform quantitative assessment of our proposal with an intrinsic method that perturbs sentences by shuffling words as well as an extrinsic evaluation that verifies the importance of word grouping for the task of Machine Translation (MT) using decomposed prompting.","We also qualitatively analyze certain aspects of the syntactic structure of sentences.","Our experiments and analyses show that the proposed grouping technique brings uniformity in the syntactic structures, as well as aids underlying NLP tasks."],"url":"http://arxiv.org/abs/2501.03988v1"}
{"created":"2025-01-07 18:29:15","title":"Cyber Spectrum Intelligence: Security Applications, Challenges and Road Ahead","abstract":"Cyber Spectrum Intelligence (SpecInt) is emerging as a concept that extends beyond basic {\\em spectrum sensing} and {\\em signal intelligence} to encompass a broader set of capabilities and technologies aimed at monitoring the use of the radio spectrum and extracting information. SpecInt merges traditional spectrum sensing techniques with Artificial Intelligence (AI) and parallel processing to enhance the ability to extract and correlate simultaneous events occurring on various frequencies, allowing for a new wave of intelligence applications.   This paper provides an overview of the emerging SpecInt research area, characterizing the system architecture and the most relevant applications for cyber-physical security. We identify five subcategories of spectrum intelligence for cyber-physical security, encompassing Device Intelligence, Channel Intelligence, Location Intelligence, Communication Intelligence, and Ambient Intelligence. We also provide preliminary results based on an experimental testbed showing the viability, feasibility, and potential of this emerging application area. Finally, we point out current research challenges and future directions paving the way for further research in this domain.","sentences":["Cyber Spectrum Intelligence (SpecInt) is emerging as a concept that extends beyond basic {\\em spectrum sensing} and {\\em signal intelligence} to encompass a broader set of capabilities and technologies aimed at monitoring the use of the radio spectrum and extracting information.","SpecInt merges traditional spectrum sensing techniques with Artificial Intelligence (AI) and parallel processing to enhance the ability to extract and correlate simultaneous events occurring on various frequencies, allowing for a new wave of intelligence applications.   ","This paper provides an overview of the emerging SpecInt research area, characterizing the system architecture and the most relevant applications for cyber-physical security.","We identify five subcategories of spectrum intelligence for cyber-physical security, encompassing Device Intelligence, Channel Intelligence, Location Intelligence, Communication Intelligence, and Ambient Intelligence.","We also provide preliminary results based on an experimental testbed showing the viability, feasibility, and potential of this emerging application area.","Finally, we point out current research challenges and future directions paving the way for further research in this domain."],"url":"http://arxiv.org/abs/2501.03977v1"}
{"created":"2025-01-07 18:22:44","title":"MAD-BA: 3D LiDAR Bundle Adjustment -- from Uncertainty Modelling to Structure Optimization","abstract":"The joint optimization of sensor poses and 3D structure is fundamental for state estimation in robotics and related fields. Current LiDAR systems often prioritize pose optimization, with structure refinement either omitted or treated separately using representations like signed distance functions or neural networks. This paper introduces a framework for simultaneous optimization of sensor poses and 3D map, represented as surfels. A generalized LiDAR uncertainty model is proposed to address degraded or less reliable measurements in varying scenarios. Experimental results on public datasets demonstrate improved performance over most comparable state-of-the-art methods. The system is provided as open-source software to support further research.","sentences":["The joint optimization of sensor poses and 3D structure is fundamental for state estimation in robotics and related fields.","Current LiDAR systems often prioritize pose optimization, with structure refinement either omitted or treated separately using representations like signed distance functions or neural networks.","This paper introduces a framework for simultaneous optimization of sensor poses and 3D map, represented as surfels.","A generalized LiDAR uncertainty model is proposed to address degraded or less reliable measurements in varying scenarios.","Experimental results on public datasets demonstrate improved performance over most comparable state-of-the-art methods.","The system is provided as open-source software to support further research."],"url":"http://arxiv.org/abs/2501.03972v1"}
{"created":"2025-01-07 18:22:23","title":"Impact of Leg Stiffness on Energy Efficiency in One Legged Hopping","abstract":"In the fields of robotics and biomechanics, the integration of elastic elements such as springs and tendons in legged systems has long been recognized for enabling energy-efficient locomotion. Yet, a significant challenge persists: designing a robotic leg that perform consistently across diverse operating conditions, especially varying average forward speeds. It remains unclear whether, for such a range of operating conditions, the stiffness of the elastic elements needs to be varied or if a similar performance can be obtained by changing the motion and actuation while keeping the stiffness fixed. This work explores the influence of the leg stiffness on the energy efficiency of a monopedal robot through an extensive parametric study of its periodic hopping motion. To this end, we formulate an optimal control problem parameterized by average forward speed and leg stiffness, solving it numerically using direct collocation. Our findings indicate that, compared to the use of a fixed stiffness, employing variable stiffness in legged systems improves energy efficiency by 20 % maximally and by 6.8 % on average across a range of speeds.","sentences":["In the fields of robotics and biomechanics, the integration of elastic elements such as springs and tendons in legged systems has long been recognized for enabling energy-efficient locomotion.","Yet, a significant challenge persists: designing a robotic leg that perform consistently across diverse operating conditions, especially varying average forward speeds.","It remains unclear whether, for such a range of operating conditions, the stiffness of the elastic elements needs to be varied or if a similar performance can be obtained by changing the motion and actuation while keeping the stiffness fixed.","This work explores the influence of the leg stiffness on the energy efficiency of a monopedal robot through an extensive parametric study of its periodic hopping motion.","To this end, we formulate an optimal control problem parameterized by average forward speed and leg stiffness, solving it numerically using direct collocation.","Our findings indicate that, compared to the use of a fixed stiffness, employing variable stiffness in legged systems improves energy efficiency by 20 % maximally and by 6.8 % on average across a range of speeds."],"url":"http://arxiv.org/abs/2501.03971v1"}
{"created":"2025-01-07 18:06:27","title":"VLM-driven Behavior Tree for Context-aware Task Planning","abstract":"The use of Large Language Models (LLMs) for generating Behavior Trees (BTs) has recently gained attention in the robotics community, yet remains in its early stages of development. In this paper, we propose a novel framework that leverages Vision-Language Models (VLMs) to interactively generate and edit BTs that address visual conditions, enabling context-aware robot operations in visually complex environments. A key feature of our approach lies in the conditional control through self-prompted visual conditions. Specifically, the VLM generates BTs with visual condition nodes, where conditions are expressed as free-form text. Another VLM process integrates the text into its prompt and evaluates the conditions against real-world images during robot execution. We validated our framework in a real-world cafe scenario, demonstrating both its feasibility and limitations.","sentences":["The use of Large Language Models (LLMs) for generating Behavior Trees (BTs) has recently gained attention in the robotics community, yet remains in its early stages of development.","In this paper, we propose a novel framework that leverages Vision-Language Models (VLMs) to interactively generate and edit BTs that address visual conditions, enabling context-aware robot operations in visually complex environments.","A key feature of our approach lies in the conditional control through self-prompted visual conditions.","Specifically, the VLM generates BTs with visual condition nodes, where conditions are expressed as free-form text.","Another VLM process integrates the text into its prompt and evaluates the conditions against real-world images during robot execution.","We validated our framework in a real-world cafe scenario, demonstrating both its feasibility and limitations."],"url":"http://arxiv.org/abs/2501.03968v1"}
{"created":"2025-01-07 18:05:24","title":"Temporal Feature Weaving for Neonatal Echocardiographic Viewpoint Video Classification","abstract":"Automated viewpoint classification in echocardiograms can help under-resourced clinics and hospitals in providing faster diagnosis and screening when expert technicians may not be available. We propose a novel approach towards echocardiographic viewpoint classification. We show that treating viewpoint classification as video classification rather than image classification yields advantage. We propose a CNN-GRU architecture with a novel temporal feature weaving method, which leverages both spatial and temporal information to yield a 4.33\\% increase in accuracy over baseline image classification while using only four consecutive frames. The proposed approach incurs minimal computational overhead. Additionally, we publish the Neonatal Echocardiogram Dataset (NED), a professionally-annotated dataset providing sixteen viewpoints and associated echocardipgraphy videos to encourage future work and development in this field. Code available at: https://github.com/satchelfrench/NED","sentences":["Automated viewpoint classification in echocardiograms can help under-resourced clinics and hospitals in providing faster diagnosis and screening when expert technicians may not be available.","We propose a novel approach towards echocardiographic viewpoint classification.","We show that treating viewpoint classification as video classification rather than image classification yields advantage.","We propose a CNN-GRU architecture with a novel temporal feature weaving method, which leverages both spatial and temporal information to yield a 4.33\\% increase in accuracy over baseline image classification while using only four consecutive frames.","The proposed approach incurs minimal computational overhead.","Additionally, we publish the Neonatal Echocardiogram Dataset (NED), a professionally-annotated dataset providing sixteen viewpoints and associated echocardipgraphy videos to encourage future work and development in this field.","Code available at: https://github.com/satchelfrench/NED"],"url":"http://arxiv.org/abs/2501.03967v1"}
{"created":"2025-01-07 17:56:47","title":"A comparative study of uncertainty quantification methods in gust response analysis of a Lift-Plus-Cruise eVTOL aircraft wing","abstract":"Wind gusts, being inherently stochastic, can significantly influence the safety and performance of aircraft. This study investigates a three-dimensional uncertainty quantification (UQ) problem to explore how uncertainties in gust and flight conditions affect the structural response of a Lift-Plus-Cruise eVTOL aircraft wing. The analysis employs an unsteady aeroelastic model with a one-way coupling between a panel method aerodynamic solver and a shell analysis structural solver to predict the wing's response under varying conditions. Additionally, this paper presents a comparative evaluation of commonly used non-intrusive UQ methods, including non-intrusive polynomial chaos, kriging, Monte Carlo, univariate dimension reduction, and gradient-enhanced univariate dimension reduction. These methods are assessed based on their effectiveness in estimating various risk measures-mean, standard deviation, and 95th percentile-of critical structural response outputs such as maximum tip displacement and average strain energy. The numerical results reveal significant variability in the structural response outputs, even under relatively small ranges of uncertain inputs. This highlights the sensitivity of the system to uncertainties in gust and flight conditions. Furthermore, the performance of the implemented UQ methods varies significantly depending on the specific risk measures and the quantity of interest being analyzed.","sentences":["Wind gusts, being inherently stochastic, can significantly influence the safety and performance of aircraft.","This study investigates a three-dimensional uncertainty quantification (UQ) problem to explore how uncertainties in gust and flight conditions affect the structural response of a Lift-Plus-Cruise eVTOL aircraft wing.","The analysis employs an unsteady aeroelastic model with a one-way coupling between a panel method aerodynamic solver and a shell analysis structural solver to predict the wing's response under varying conditions.","Additionally, this paper presents a comparative evaluation of commonly used non-intrusive UQ methods, including non-intrusive polynomial chaos, kriging, Monte Carlo, univariate dimension reduction, and gradient-enhanced univariate dimension reduction.","These methods are assessed based on their effectiveness in estimating various risk measures-mean, standard deviation, and 95th percentile-of critical structural response outputs such as maximum tip displacement and average strain energy.","The numerical results reveal significant variability in the structural response outputs, even under relatively small ranges of uncertain inputs.","This highlights the sensitivity of the system to uncertainties in gust and flight conditions.","Furthermore, the performance of the implemented UQ methods varies significantly depending on the specific risk measures and the quantity of interest being analyzed."],"url":"http://arxiv.org/abs/2501.03964v1"}
{"created":"2025-01-07 17:49:36","title":"Channel Coding based on Skew Polynomials and Multivariate Polynomials","abstract":"This dissertation considers new constructions and decoding approaches for error-correcting codes based on non-conventional polynomials, with the objective of providing new coding solutions to the applications mentioned above. With skew polynomials, we construct codes that are dual-containing, which is a desired property of quantum error-correcting codes. By considering evaluation codes based on skew polynomials, a condition on the existence of optimal support-constrained codes is derived and an application of such codes in the distributed multi-source networks is proposed. For a class of multicast networks, the advantage of vector network coding compared to scalar network coding is investigated. Multivariate polynomials have been attracting increasing interest in constructing codes with repair capabilities by accessing only a small amount of available symbols, which is required to build failure-resistant distributed storage systems. A new class of bivariate evaluation codes and their local recovery capability are studied. Interestingly, the well-known Reed-Solomon codes are used in a class of locally recoverable codes with availability (multiple disjoint recovery sets) via subspace design. Aside from new constructions, decoding approaches are considered in order to increase the error correction capability in the case where the code is fixed. In particular, new lower and upper bounds on the success probability of joint decoding interleaved alternant codes by a syndrome-based decoder are derived, where alternant codes are an important class of algebraic codes containing Goppa codes, BCH codes, and Reed-Muller codes as sub-classes.","sentences":["This dissertation considers new constructions and decoding approaches for error-correcting codes based on non-conventional polynomials, with the objective of providing new coding solutions to the applications mentioned above.","With skew polynomials, we construct codes that are dual-containing, which is a desired property of quantum error-correcting codes.","By considering evaluation codes based on skew polynomials, a condition on the existence of optimal support-constrained codes is derived and an application of such codes in the distributed multi-source networks is proposed.","For a class of multicast networks, the advantage of vector network coding compared to scalar network coding is investigated.","Multivariate polynomials have been attracting increasing interest in constructing codes with repair capabilities by accessing only a small amount of available symbols, which is required to build failure-resistant distributed storage systems.","A new class of bivariate evaluation codes and their local recovery capability are studied.","Interestingly, the well-known Reed-Solomon codes are used in a class of locally recoverable codes with availability (multiple disjoint recovery sets) via subspace design.","Aside from new constructions, decoding approaches are considered in order to increase the error correction capability in the case where the code is fixed.","In particular, new lower and upper bounds on the success probability of joint decoding interleaved alternant codes by a syndrome-based decoder are derived, where alternant codes are an important class of algebraic codes containing Goppa codes, BCH codes, and Reed-Muller codes as sub-classes."],"url":"http://arxiv.org/abs/2501.03961v1"}
{"created":"2025-01-07 17:37:57","title":"Vision Language Models as Values Detectors","abstract":"Large Language Models integrating textual and visual inputs have introduced new possibilities for interpreting complex data. Despite their remarkable ability to generate coherent and contextually relevant text based on visual stimuli, the alignment of these models with human perception in identifying relevant elements in images requires further exploration. This paper investigates the alignment between state-of-the-art LLMs and human annotators in detecting elements of relevance within home environment scenarios. We created a set of twelve images depicting various domestic scenarios and enlisted fourteen annotators to identify the key element in each image. We then compared these human responses with outputs from five different LLMs, including GPT-4o and four LLaVA variants. Our findings reveal a varied degree of alignment, with LLaVA 34B showing the highest performance but still scoring low. However, an analysis of the results highlights the models' potential to detect value-laden elements in images, suggesting that with improved training and refined prompts, LLMs could enhance applications in social robotics, assistive technologies, and human-computer interaction by providing deeper insights and more contextually relevant responses.","sentences":["Large Language Models integrating textual and visual inputs have introduced new possibilities for interpreting complex data.","Despite their remarkable ability to generate coherent and contextually relevant text based on visual stimuli, the alignment of these models with human perception in identifying relevant elements in images requires further exploration.","This paper investigates the alignment between state-of-the-art LLMs and human annotators in detecting elements of relevance within home environment scenarios.","We created a set of twelve images depicting various domestic scenarios and enlisted fourteen annotators to identify the key element in each image.","We then compared these human responses with outputs from five different LLMs, including GPT-4o and four LLaVA variants.","Our findings reveal a varied degree of alignment, with LLaVA 34B showing the highest performance but still scoring low.","However, an analysis of the results highlights the models' potential to detect value-laden elements in images, suggesting that with improved training and refined prompts, LLMs could enhance applications in social robotics, assistive technologies, and human-computer interaction by providing deeper insights and more contextually relevant responses."],"url":"http://arxiv.org/abs/2501.03957v1"}
{"created":"2025-01-07 17:24:17","title":"Localizing AI: Evaluating Open-Weight Language Models for Languages of Baltic States","abstract":"Although large language models (LLMs) have transformed our expectations of modern language technologies, concerns over data privacy often restrict the use of commercially available LLMs hosted outside of EU jurisdictions. This limits their application in governmental, defence, and other data-sensitive sectors. In this work, we evaluate the extent to which locally deployable open-weight LLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian. We examine various size and precision variants of the top-performing multilingual open-weight models, Llama~3, Gemma~2, Phi, and NeMo, on machine translation, multiple-choice question answering, and free-form text generation. The results indicate that while certain models like Gemma~2 perform close to the top commercially available models, many LLMs struggle with these languages. Most surprisingly, however, we find that these models, while showing close to state-of-the-art translation performance, are still prone to lexical hallucinations with errors in at least 1 in 20 words for all open-weight multilingual LLMs.","sentences":["Although large language models (LLMs) have transformed our expectations of modern language technologies, concerns over data privacy often restrict the use of commercially available LLMs hosted outside of EU jurisdictions.","This limits their application in governmental, defence, and other data-sensitive sectors.","In this work, we evaluate the extent to which locally deployable open-weight LLMs support lesser-spoken languages such as Lithuanian, Latvian, and Estonian.","We examine various size and precision variants of the top-performing multilingual open-weight models, Llama~3, Gemma~2, Phi, and NeMo, on machine translation, multiple-choice question answering, and free-form text generation.","The results indicate that while certain models like Gemma~2 perform close to the top commercially available models, many LLMs struggle with these languages.","Most surprisingly, however, we find that these models, while showing close to state-of-the-art translation performance, are still prone to lexical hallucinations with errors in at least 1 in 20 words for all open-weight multilingual LLMs."],"url":"http://arxiv.org/abs/2501.03952v1"}
{"created":"2025-01-07 17:13:24","title":"Reducing Proxy Discrimination","abstract":"Today, there is no clear legal test for regulating the use of variables that proxy for race and other protected classes and classifications. This Article develops such a test. Decision tools that use proxies are narrowly tailored when they exhibit the weakest total proxy power. The test is necessarily comparative. Thus, if two algorithms predict loan repayment or university academic performance with identical accuracy rates, but one uses zip code and the other does not, then the second algorithm can be said to have deployed a more equitable means for achieving the same result as the first algorithm. Scenarios in which two algorithms produce comparable and non-identical results present a greater challenge. This Article suggests that lawmakers can develop caps to permissible proxy power over time, as courts and algorithm builders learn more about the power of variables. Finally, the Article considers who should bear the burden of producing less discriminatory alternatives and suggests plaintiffs remain in the best position to keep defendants honest - so long as testing data is made available.","sentences":["Today, there is no clear legal test for regulating the use of variables that proxy for race and other protected classes and classifications.","This Article develops such a test.","Decision tools that use proxies are narrowly tailored when they exhibit the weakest total proxy power.","The test is necessarily comparative.","Thus, if two algorithms predict loan repayment or university academic performance with identical accuracy rates, but one uses zip code and the other does not, then the second algorithm can be said to have deployed a more equitable means for achieving the same result as the first algorithm.","Scenarios in which two algorithms produce comparable and non-identical results present a greater challenge.","This Article suggests that lawmakers can develop caps to permissible proxy power over time, as courts and algorithm builders learn more about the power of variables.","Finally, the Article considers who should bear the burden of producing less discriminatory alternatives and suggests plaintiffs remain in the best position to keep defendants honest - so long as testing data is made available."],"url":"http://arxiv.org/abs/2501.03946v1"}
{"created":"2025-01-07 17:09:07","title":"A GPU Implementation of Multi-Guiding Spark Fireworks Algorithm for Efficient Black-Box Neural Network Optimization","abstract":"Swarm intelligence optimization algorithms have gained significant attention due to their ability to solve complex optimization problems. However, the efficiency of optimization in large-scale problems limits the use of related methods. This paper presents a GPU-accelerated version of the Multi-Guiding Spark Fireworks Algorithm (MGFWA), which significantly improves the computational efficiency compared to its traditional CPU-based counterpart. We benchmark the GPU-MGFWA on several neural network black-box optimization problems and demonstrate its superior performance in terms of both speed and solution quality. By leveraging the parallel processing power of modern GPUs, the proposed GPU-MGFWA results in faster convergence and reduced computation time for large-scale optimization tasks. The proposed implementation offers a promising approach to accelerate swarm intelligence algorithms, making them more suitable for real-time applications and large-scale industrial problems. Source code is released at https://github.com/mxxxr/MGFWA.","sentences":["Swarm intelligence optimization algorithms have gained significant attention due to their ability to solve complex optimization problems.","However, the efficiency of optimization in large-scale problems limits the use of related methods.","This paper presents a GPU-accelerated version of the Multi-Guiding Spark Fireworks Algorithm (MGFWA), which significantly improves the computational efficiency compared to its traditional CPU-based counterpart.","We benchmark the GPU-MGFWA on several neural network black-box optimization problems and demonstrate its superior performance in terms of both speed and solution quality.","By leveraging the parallel processing power of modern GPUs, the proposed GPU-MGFWA results in faster convergence and reduced computation time for large-scale optimization tasks.","The proposed implementation offers a promising approach to accelerate swarm intelligence algorithms, making them more suitable for real-time applications and large-scale industrial problems.","Source code is released at https://github.com/mxxxr/MGFWA."],"url":"http://arxiv.org/abs/2501.03944v1"}
{"created":"2025-01-07 17:02:33","title":"Synthetic Data Privacy Metrics","abstract":"Recent advancements in generative AI have made it possible to create synthetic datasets that can be as accurate as real-world data for training AI models, powering statistical insights, and fostering collaboration with sensitive datasets while offering strong privacy guarantees. Effectively measuring the empirical privacy of synthetic data is an important step in the process. However, while there is a multitude of new privacy metrics being published every day, there currently is no standardization. In this paper, we review the pros and cons of popular metrics that include simulations of adversarial attacks. We also review current best practices for amending generative models to enhance the privacy of the data they create (e.g. differential privacy).","sentences":["Recent advancements in generative AI have made it possible to create synthetic datasets that can be as accurate as real-world data for training AI models, powering statistical insights, and fostering collaboration with sensitive datasets while offering strong privacy guarantees.","Effectively measuring the empirical privacy of synthetic data is an important step in the process.","However, while there is a multitude of new privacy metrics being published every day, there currently is no standardization.","In this paper, we review the pros and cons of popular metrics that include simulations of adversarial attacks.","We also review current best practices for amending generative models to enhance the privacy of the data they create (e.g. differential privacy)."],"url":"http://arxiv.org/abs/2501.03941v1"}
{"created":"2025-01-07 17:00:49","title":"Not all tokens are created equal: Perplexity Attention Weighted Networks for AI generated text detection","abstract":"The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it. However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora. Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success. We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently. Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length. Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements. PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters. Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts. It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages.","sentences":["The rapid advancement in large language models (LLMs) has significantly enhanced their ability to generate coherent and contextually relevant text, raising concerns about the misuse of AI-generated content and making it critical to detect it.","However, the task remains challenging, particularly in unseen domains or with unfamiliar LLMs.","Leveraging LLM next-token distribution outputs offers a theoretically appealing approach for detection, as they encapsulate insights from the models' extensive pre-training on diverse corpora.","Despite its promise, zero-shot methods that attempt to operationalize these outputs have met with limited success.","We hypothesize that one of the problems is that they use the mean to aggregate next-token distribution metrics across tokens, when some tokens are naturally easier or harder to predict and should be weighted differently.","Based on this idea, we propose the Perplexity Attention Weighted Network (PAWN), which uses the last hidden states of the LLM and positions to weight the sum of a series of features based on metrics from the next-token distribution across the sequence length.","Although not zero-shot, our method allows us to cache the last hidden states and next-token distribution metrics on disk, greatly reducing the training resource requirements.","PAWN shows competitive and even better performance in-distribution than the strongest baselines (fine-tuned LMs) with a fraction of their trainable parameters.","Our model also generalizes better to unseen domains and source models, with smaller variability in the decision boundary across distribution shifts.","It is also more robust to adversarial attacks, and if the backbone has multilingual capabilities, it presents decent generalization to languages not seen during supervised training, with LLaMA3-1B reaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine languages."],"url":"http://arxiv.org/abs/2501.03940v1"}
{"created":"2025-01-07 17:00:35","title":"Visual question answering: from early developments to recent advances -- a survey","abstract":"Visual Question Answering (VQA) is an evolving research field aimed at enabling machines to answer questions about visual content by integrating image and language processing techniques such as feature extraction, object detection, text embedding, natural language understanding, and language generation. With the growth of multimodal data research, VQA has gained significant attention due to its broad applications, including interactive educational tools, medical image diagnosis, customer service, entertainment, and social media captioning. Additionally, VQA plays a vital role in assisting visually impaired individuals by generating descriptive content from images. This survey introduces a taxonomy of VQA architectures, categorizing them based on design choices and key components to facilitate comparative analysis and evaluation. We review major VQA approaches, focusing on deep learning-based methods, and explore the emerging field of Large Visual Language Models (LVLMs) that have demonstrated success in multimodal tasks like VQA. The paper further examines available datasets and evaluation metrics essential for measuring VQA system performance, followed by an exploration of real-world VQA applications. Finally, we highlight ongoing challenges and future directions in VQA research, presenting open questions and potential areas for further development. This survey serves as a comprehensive resource for researchers and practitioners interested in the latest advancements and future","sentences":["Visual Question Answering (VQA) is an evolving research field aimed at enabling machines to answer questions about visual content by integrating image and language processing techniques such as feature extraction, object detection, text embedding, natural language understanding, and language generation.","With the growth of multimodal data research, VQA has gained significant attention due to its broad applications, including interactive educational tools, medical image diagnosis, customer service, entertainment, and social media captioning.","Additionally, VQA plays a vital role in assisting visually impaired individuals by generating descriptive content from images.","This survey introduces a taxonomy of VQA architectures, categorizing them based on design choices and key components to facilitate comparative analysis and evaluation.","We review major VQA approaches, focusing on deep learning-based methods, and explore the emerging field of Large Visual Language Models (LVLMs) that have demonstrated success in multimodal tasks like VQA.","The paper further examines available datasets and evaluation metrics essential for measuring VQA system performance, followed by an exploration of real-world VQA applications.","Finally, we highlight ongoing challenges and future directions in VQA research, presenting open questions and potential areas for further development.","This survey serves as a comprehensive resource for researchers and practitioners interested in the latest advancements and future"],"url":"http://arxiv.org/abs/2501.03939v1"}
{"created":"2025-01-07 16:56:40","title":"A precise asymptotic analysis of learning diffusion models: theory and insights","abstract":"In this manuscript, we consider the problem of learning a flow or diffusion-based generative model parametrized by a two-layer auto-encoder, trained with online stochastic gradient descent, on a high-dimensional target density with an underlying low-dimensional manifold structure. We derive a tight asymptotic characterization of low-dimensional projections of the distribution of samples generated by the learned model, ascertaining in particular its dependence on the number of training samples. Building on this analysis, we discuss how mode collapse can arise, and lead to model collapse when the generative model is re-trained on generated synthetic data.","sentences":["In this manuscript, we consider the problem of learning a flow or diffusion-based generative model parametrized by a two-layer auto-encoder, trained with online stochastic gradient descent, on a high-dimensional target density with an underlying low-dimensional manifold structure.","We derive a tight asymptotic characterization of low-dimensional projections of the distribution of samples generated by the learned model, ascertaining in particular its dependence on the number of training samples.","Building on this analysis, we discuss how mode collapse can arise, and lead to model collapse when the generative model is re-trained on generated synthetic data."],"url":"http://arxiv.org/abs/2501.03937v1"}
{"created":"2025-01-07 16:53:01","title":"PPTAgent: Generating and Evaluating Presentations Beyond Text-to-Slides","abstract":"Automatically generating presentations from documents is a challenging task that requires balancing content quality, visual design, and structural coherence. Existing methods primarily focus on improving and evaluating the content quality in isolation, often overlooking visual design and structural coherence, which limits their practical applicability. To address these limitations, we propose PPTAgent, which comprehensively improves presentation generation through a two-stage, edit-based approach inspired by human workflows. PPTAgent first analyzes reference presentations to understand their structural patterns and content schemas, then drafts outlines and generates slides through code actions to ensure consistency and alignment. To comprehensively evaluate the quality of generated presentations, we further introduce PPTEval, an evaluation framework that assesses presentations across three dimensions: Content, Design, and Coherence. Experiments show that PPTAgent significantly outperforms traditional automatic presentation generation methods across all three dimensions. The code and data are available at https://github.com/icip-cas/PPTAgent.","sentences":["Automatically generating presentations from documents is a challenging task that requires balancing content quality, visual design, and structural coherence.","Existing methods primarily focus on improving and evaluating the content quality in isolation, often overlooking visual design and structural coherence, which limits their practical applicability.","To address these limitations, we propose PPTAgent, which comprehensively improves presentation generation through a two-stage, edit-based approach inspired by human workflows.","PPTAgent first analyzes reference presentations to understand their structural patterns and content schemas, then drafts outlines and generates slides through code actions to ensure consistency and alignment.","To comprehensively evaluate the quality of generated presentations, we further introduce PPTEval, an evaluation framework that assesses presentations across three dimensions: Content, Design, and Coherence.","Experiments show that PPTAgent significantly outperforms traditional automatic presentation generation methods across all three dimensions.","The code and data are available at https://github.com/icip-cas/PPTAgent."],"url":"http://arxiv.org/abs/2501.03936v1"}
{"created":"2025-01-07 16:48:47","title":"CoStruction: Conjoint radiance field optimization for urban scene reconStruction with limited image overlap","abstract":"Reconstructing the surrounding surface geometry from recorded driving sequences poses a significant challenge due to the limited image overlap and complex topology of urban environments. SoTA neural implicit surface reconstruction methods often struggle in such setting, either failing due to small vision overlap or exhibiting suboptimal performance in accurately reconstructing both the surface and fine structures. To address these limitations, we introduce CoStruction, a novel hybrid implicit surface reconstruction method tailored for large driving sequences with limited camera overlap. CoStruction leverages cross-representation uncertainty estimation to filter out ambiguous geometry caused by limited observations. Our method performs joint optimization of both radiance fields in addition to guided sampling achieving accurate reconstruction of large areas along with fine structures in complex urban scenarios. Extensive evaluation on major driving datasets demonstrates the superiority of our approach in reconstructing large driving sequences with limited image overlap, outperforming concurrent SoTA methods.","sentences":["Reconstructing the surrounding surface geometry from recorded driving sequences poses a significant challenge due to the limited image overlap and complex topology of urban environments.","SoTA neural implicit surface reconstruction methods often struggle in such setting, either failing due to small vision overlap or exhibiting suboptimal performance in accurately reconstructing both the surface and fine structures.","To address these limitations, we introduce CoStruction, a novel hybrid implicit surface reconstruction method tailored for large driving sequences with limited camera overlap.","CoStruction leverages cross-representation uncertainty estimation to filter out ambiguous geometry caused by limited observations.","Our method performs joint optimization of both radiance fields in addition to guided sampling achieving accurate reconstruction of large areas along with fine structures in complex urban scenarios.","Extensive evaluation on major driving datasets demonstrates the superiority of our approach in reconstructing large driving sequences with limited image overlap, outperforming concurrent SoTA methods."],"url":"http://arxiv.org/abs/2501.03932v1"}
{"created":"2025-01-07 16:48:31","title":"Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers","abstract":"We present Magic Mirror, a framework for generating identity-preserved videos with cinematic-level quality and dynamic motion. While recent advances in video diffusion models have shown impressive capabilities in text-to-video generation, maintaining consistent identity while producing natural motion remains challenging. Previous methods either require person-specific fine-tuning or struggle to balance identity preservation with motion diversity. Built upon Video Diffusion Transformers, our method introduces three key components: (1) a dual-branch facial feature extractor that captures both identity and structural features, (2) a lightweight cross-modal adapter with Conditioned Adaptive Normalization for efficient identity integration, and (3) a two-stage training strategy combining synthetic identity pairs with video data. Extensive experiments demonstrate that Magic Mirror effectively balances identity consistency with natural motion, outperforming existing methods across multiple metrics while requiring minimal parameters added. The code and model will be made publicly available at: https://github.com/dvlab-research/MagicMirror/","sentences":["We present Magic Mirror, a framework for generating identity-preserved videos with cinematic-level quality and dynamic motion.","While recent advances in video diffusion models have shown impressive capabilities in text-to-video generation, maintaining consistent identity while producing natural motion remains challenging.","Previous methods either require person-specific fine-tuning or struggle to balance identity preservation with motion diversity.","Built upon Video Diffusion Transformers, our method introduces three key components: (1) a dual-branch facial feature extractor that captures both identity and structural features, (2) a lightweight cross-modal adapter with Conditioned Adaptive Normalization for efficient identity integration, and (3) a two-stage training strategy combining synthetic identity pairs with video data.","Extensive experiments demonstrate that Magic Mirror effectively balances identity consistency with natural motion, outperforming existing methods across multiple metrics while requiring minimal parameters added.","The code and model will be made publicly available at: https://github.com/dvlab-research/MagicMirror/"],"url":"http://arxiv.org/abs/2501.03931v1"}
{"created":"2025-01-07 16:48:21","title":"Towards Reliable Testing for Multiple Information Retrieval System Comparisons","abstract":"Null Hypothesis Significance Testing is the \\textit{de facto} tool for assessing effectiveness differences between Information Retrieval systems. Researchers use statistical tests to check whether those differences will generalise to online settings or are just due to the samples observed in the laboratory. Much work has been devoted to studying which test is the most reliable when comparing a pair of systems, but most of the IR real-world experiments involve more than two. In the multiple comparisons scenario, testing several systems simultaneously may inflate the errors committed by the tests. In this paper, we use a new approach to assess the reliability of multiple comparison procedures using simulated and real TREC data. Experiments show that Wilcoxon plus the Benjamini-Hochberg correction yields Type I error rates according to the significance level for typical sample sizes while being the best test in terms of statistical power.","sentences":["Null Hypothesis Significance Testing is the \\textit{de facto} tool for assessing effectiveness differences between Information Retrieval systems.","Researchers use statistical tests to check whether those differences will generalise to online settings or are just due to the samples observed in the laboratory.","Much work has been devoted to studying which test is the most reliable when comparing a pair of systems, but most of the IR real-world experiments involve more than two.","In the multiple comparisons scenario, testing several systems simultaneously may inflate the errors committed by the tests.","In this paper, we use a new approach to assess the reliability of multiple comparison procedures using simulated and real TREC data.","Experiments show that Wilcoxon plus the Benjamini-Hochberg correction yields Type I error rates according to the significance level for typical sample sizes while being the best test in terms of statistical power."],"url":"http://arxiv.org/abs/2501.03930v1"}
{"created":"2025-01-07 16:45:37","title":"From Newswire to Nexus: Using text-based actor embeddings and transformer networks to forecast conflict dynamics","abstract":"This study advances the field of conflict forecasting by using text-based actor embeddings with transformer models to predict dynamic changes in violent conflict patterns at the actor level. More specifically, we combine newswire texts with structured conflict event data and leverage recent advances in Natural Language Processing (NLP) techniques to forecast escalations and de-escalations among conflicting actors, such as governments, militias, separatist movements, and terrorists. This new approach accurately and promptly captures the inherently volatile patterns of violent conflicts, which existing methods have not been able to achieve. To create this framework, we began by curating and annotating a vast international newswire corpus, leveraging hand-labeled event data from the Uppsala Conflict Data Program. By using this hybrid dataset, our models can incorporate the textual context of news sources along with the precision and detail of structured event data. This combination enables us to make both dynamic and granular predictions about conflict developments. We validate our approach through rigorous back-testing against historical events, demonstrating superior out-of-sample predictive power. We find that our approach is quite effective in identifying and predicting phases of conflict escalation and de-escalation, surpassing the capabilities of traditional models. By focusing on actor interactions, our explicit goal is to provide actionable insights to policymakers, humanitarian organizations, and peacekeeping operations in order to enable targeted and effective intervention strategies.","sentences":["This study advances the field of conflict forecasting by using text-based actor embeddings with transformer models to predict dynamic changes in violent conflict patterns at the actor level.","More specifically, we combine newswire texts with structured conflict event data and leverage recent advances in Natural Language Processing (NLP) techniques to forecast escalations and de-escalations among conflicting actors, such as governments, militias, separatist movements, and terrorists.","This new approach accurately and promptly captures the inherently volatile patterns of violent conflicts, which existing methods have not been able to achieve.","To create this framework, we began by curating and annotating a vast international newswire corpus, leveraging hand-labeled event data from the Uppsala Conflict Data Program.","By using this hybrid dataset, our models can incorporate the textual context of news sources along with the precision and detail of structured event data.","This combination enables us to make both dynamic and granular predictions about conflict developments.","We validate our approach through rigorous back-testing against historical events, demonstrating superior out-of-sample predictive power.","We find that our approach is quite effective in identifying and predicting phases of conflict escalation and de-escalation, surpassing the capabilities of traditional models.","By focusing on actor interactions, our explicit goal is to provide actionable insights to policymakers, humanitarian organizations, and peacekeeping operations in order to enable targeted and effective intervention strategies."],"url":"http://arxiv.org/abs/2501.03928v1"}
{"created":"2025-01-07 16:31:10","title":"Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback","abstract":"The scientific research paradigm is undergoing a profound transformation owing to the development of Artificial Intelligence (AI). Recent works demonstrate that various AI-assisted research methods can largely improve research efficiency by improving data analysis, accelerating computation, and fostering novel idea generation. To further move towards the ultimate goal (i.e., automatic scientific research), in this paper, we propose Dolphin, the first closed-loop open-ended auto-research framework to further build the entire process of human scientific research. Dolphin can generate research ideas, perform experiments, and get feedback from experimental results to generate higher-quality ideas. More specifically, Dolphin first generates novel ideas based on relevant papers which are ranked by the topic and task attributes. Then, the codes are automatically generated and debugged with the exception-traceback-guided local code structure. Finally, Dolphin automatically analyzes the results of each idea and feeds the results back to the next round of idea generation. Experiments are conducted on the benchmark datasets of different topics and results show that Dolphin can generate novel ideas continuously and complete the experiment in a loop. We highlight that Dolphin can automatically propose methods that are comparable to the state-of-the-art in some tasks such as 2D image classification and 3D point classification.","sentences":["The scientific research paradigm is undergoing a profound transformation owing to the development of Artificial Intelligence (AI).","Recent works demonstrate that various AI-assisted research methods can largely improve research efficiency by improving data analysis, accelerating computation, and fostering novel idea generation.","To further move towards the ultimate goal (i.e., automatic scientific research), in this paper, we propose Dolphin, the first closed-loop open-ended auto-research framework to further build the entire process of human scientific research.","Dolphin can generate research ideas, perform experiments, and get feedback from experimental results to generate higher-quality ideas.","More specifically, Dolphin first generates novel ideas based on relevant papers which are ranked by the topic and task attributes.","Then, the codes are automatically generated and debugged with the exception-traceback-guided local code structure.","Finally, Dolphin automatically analyzes the results of each idea and feeds the results back to the next round of idea generation.","Experiments are conducted on the benchmark datasets of different topics and results show that Dolphin can generate novel ideas continuously and complete the experiment in a loop.","We highlight that Dolphin can automatically propose methods that are comparable to the state-of-the-art in some tasks such as 2D image classification and 3D point classification."],"url":"http://arxiv.org/abs/2501.03916v1"}
{"created":"2025-01-07 16:28:49","title":"Active Learning Techniques for Pomset Recognizers","abstract":"Pomsets are a promising formalism for concurrent programs based on partially ordered sets. Among this class, series-parallel pomsets admit a convenient linear representation and can be recognized by simple algebraic structures known as pomset recognizers. Active learning consists in inferring a formal model of a recognizable language by asking membership and equivalence queries to a minimally adequate teacher (MAT). We improve existing learning algorithms for pomset recognizers by 1. introducing a new counter-example analysis procedure that is in the best case scenario exponentially more efficient than existing methods 2. adapting the state-of-the-art $L^{\\lambda}$ algorithm to minimize the impact of exceedingly verbose counter-examples and remove redundant queries 3. designing a suitable finite test suite that ensures general equivalence between two pomset recognizers by extending the well-known W-method.","sentences":["Pomsets are a promising formalism for concurrent programs based on partially ordered sets.","Among this class, series-parallel pomsets admit a convenient linear representation and can be recognized by simple algebraic structures known as pomset recognizers.","Active learning consists in inferring a formal model of a recognizable language by asking membership and equivalence queries to a minimally adequate teacher (MAT).","We improve existing learning algorithms for pomset recognizers by 1. introducing a new counter-example analysis procedure that is in the best case scenario exponentially more efficient than existing methods 2. adapting the state-of-the-art $L^{\\lambda}$ algorithm to minimize the impact of exceedingly verbose counter-examples and remove redundant queries 3. designing a suitable finite test suite that ensures general equivalence between two pomset recognizers by extending the well-known W-method."],"url":"http://arxiv.org/abs/2501.03914v1"}
{"created":"2025-01-07 16:24:43","title":"HYB-VITON: A Hybrid Approach to Virtual Try-On Combining Explicit and Implicit Warping","abstract":"Virtual try-on systems have significant potential in e-commerce, allowing customers to visualize garments on themselves. Existing image-based methods fall into two categories: those that directly warp garment-images onto person-images (explicit warping), and those using cross-attention to reconstruct given garments (implicit warping). Explicit warping preserves garment details but often produces unrealistic output, while implicit warping achieves natural reconstruction but struggles with fine details. We propose HYB-VITON, a novel approach that combines the advantages of each method and includes both a preprocessing pipeline for warped garments and a novel training option. These components allow us to utilize beneficial regions of explicitly warped garments while leveraging the natural reconstruction of implicit warping. A series of experiments demonstrates that HYB-VITON preserves garment details more faithfully than recent diffusion-based methods, while producing more realistic results than a state-of-the-art explicit warping method.","sentences":["Virtual try-on systems have significant potential in e-commerce, allowing customers to visualize garments on themselves.","Existing image-based methods fall into two categories: those that directly warp garment-images onto person-images (explicit warping), and those using cross-attention to reconstruct given garments (implicit warping).","Explicit warping preserves garment details but often produces unrealistic output, while implicit warping achieves natural reconstruction but struggles with fine details.","We propose HYB-VITON, a novel approach that combines the advantages of each method and includes both a preprocessing pipeline for warped garments and a novel training option.","These components allow us to utilize beneficial regions of explicitly warped garments while leveraging the natural reconstruction of implicit warping.","A series of experiments demonstrates that HYB-VITON preserves garment details more faithfully than recent diffusion-based methods, while producing more realistic results than a state-of-the-art explicit warping method."],"url":"http://arxiv.org/abs/2501.03910v1"}
{"created":"2025-01-07 16:22:12","title":"Implicit Coordination using Active Epistemic Inference","abstract":"A Multi-robot system (MRS) provides significant advantages for intricate tasks such as environmental monitoring, underwater inspections, and space missions. However, addressing potential communication failures or the lack of communication infrastructure in these fields remains a challenge. A significant portion of MRS research presumes that the system can maintain communication with proximity constraints, but this approach does not solve situations where communication is either non-existent, unreliable, or poses a security risk. Some approaches tackle this issue using predictions about other robots while not communicating, but these methods generally only permit agents to utilize first-order reasoning, which involves reasoning based purely on their own observations. In contrast, to deal with this problem, our proposed framework utilizes Theory of Mind (ToM), employing higher-order reasoning by shifting a robot's perspective to reason about a belief of others observations. Our approach has two main phases: i) an efficient runtime plan adaptation using active inference to signal intentions and reason about a robot's own belief and the beliefs of others in the system, and ii) a hierarchical epistemic planning framework to iteratively reason about the current MRS mission state. The proposed framework outperforms greedy and first-order reasoning approaches and is validated using simulations and experiments with heterogeneous robotic systems.","sentences":["A Multi-robot system (MRS) provides significant advantages for intricate tasks such as environmental monitoring, underwater inspections, and space missions.","However, addressing potential communication failures or the lack of communication infrastructure in these fields remains a challenge.","A significant portion of MRS research presumes that the system can maintain communication with proximity constraints, but this approach does not solve situations where communication is either non-existent, unreliable, or poses a security risk.","Some approaches tackle this issue using predictions about other robots while not communicating, but these methods generally only permit agents to utilize first-order reasoning, which involves reasoning based purely on their own observations.","In contrast, to deal with this problem, our proposed framework utilizes Theory of Mind (ToM), employing higher-order reasoning by shifting a robot's perspective to reason about a belief of others observations.","Our approach has two main phases: i) an efficient runtime plan adaptation using active inference to signal intentions and reason about a robot's own belief and the beliefs of others in the system, and ii) a hierarchical epistemic planning framework to iteratively reason about the current MRS mission state.","The proposed framework outperforms greedy and first-order reasoning approaches and is validated using simulations and experiments with heterogeneous robotic systems."],"url":"http://arxiv.org/abs/2501.03907v1"}
{"created":"2025-01-07 16:19:40","title":"mFabric: An Efficient and Scalable Fabric for Mixture-of-Experts Training","abstract":"Mixture-of-Expert (MoE) models outperform conventional models by selectively activating different subnets, named \\emph{experts}, on a per-token basis. This gated computation generates dynamic communications that cannot be determined beforehand, challenging the existing GPU interconnects that remain \\emph{static} during the distributed training process. In this paper, we advocate for a first-of-its-kind system, called mFabric, that unlocks topology reconfiguration \\emph{during} distributed MoE training. Towards this vision, we first perform a production measurement study and show that the MoE dynamic communication pattern has \\emph{strong locality}, alleviating the requirement of global reconfiguration. Based on this, we design and implement a \\emph{regionally reconfigurable high-bandwidth domain} on top of existing electrical interconnects using optical circuit switching (OCS), achieving scalability while maintaining rapid adaptability. We have built a fully functional mFabric prototype with commodity hardware and a customized collective communication runtime that trains state-of-the-art MoE models with \\emph{in-training} topology reconfiguration across 32 A100 GPUs. Large-scale packet-level simulations show that mFabric delivers comparable performance as the non-blocking fat-tree fabric while boosting the training cost efficiency (e.g., performance per dollar) of four representative MoE models by 1.2$\\times$--1.5$\\times$ and 1.9$\\times$--2.3$\\times$ at 100 Gbps and 400 Gbps link bandwidths, respectively.","sentences":["Mixture-of-Expert (MoE) models outperform conventional models by selectively activating different subnets, named \\emph{experts}, on a per-token basis.","This gated computation generates dynamic communications that cannot be determined beforehand, challenging the existing GPU interconnects that remain \\emph{static} during the distributed training process.","In this paper, we advocate for a first-of-its-kind system, called mFabric, that unlocks topology reconfiguration \\emph{during} distributed MoE training.","Towards this vision, we first perform a production measurement study and show that the MoE dynamic communication pattern has \\emph{strong locality}, alleviating the requirement of global reconfiguration.","Based on this, we design and implement a \\emph{regionally reconfigurable high-bandwidth domain} on top of existing electrical interconnects using optical circuit switching (OCS), achieving scalability while maintaining rapid adaptability.","We have built a fully functional mFabric prototype with commodity hardware and a customized collective communication runtime that trains state-of-the-art MoE models with \\emph{in-training} topology reconfiguration across 32 A100 GPUs.","Large-scale packet-level simulations show that mFabric delivers comparable performance as the non-blocking fat-tree fabric while boosting the training cost efficiency (e.g., performance per dollar) of four representative MoE models by 1.2$\\times$--1.5$\\times$ and 1.9$\\times$--2.3$\\times$ at 100 Gbps and 400 Gbps link bandwidths, respectively."],"url":"http://arxiv.org/abs/2501.03905v1"}
{"created":"2025-01-07 16:18:55","title":"Exploring the Potential of Large Language Models in Public Transportation: San Antonio Case Study","abstract":"The integration of large language models (LLMs) into public transit systems presents a transformative opportunity to enhance urban mobility. This study explores the potential of LLMs to revolutionize public transportation management within the context of San Antonio's transit system. Leveraging the capabilities of LLMs in natural language processing and data analysis, we investigate their capabilities to optimize route planning, reduce wait times, and provide personalized travel assistance. By utilizing the General Transit Feed Specification (GTFS) and other relevant data, this research aims to demonstrate how LLMs can potentially improve resource allocation, elevate passenger satisfaction, and inform data-driven decision-making in transit operations. A comparative analysis of different ChatGPT models was conducted to assess their ability to understand transportation information, retrieve relevant data, and provide comprehensive responses. Findings from this study suggest that while LLMs hold immense promise for public transit, careful engineering and fine-tuning are essential to realizing their full potential. San Antonio serves as a case study to inform the development of LLM-powered transit systems in other urban environments.","sentences":["The integration of large language models (LLMs) into public transit systems presents a transformative opportunity to enhance urban mobility.","This study explores the potential of LLMs to revolutionize public transportation management within the context of San Antonio's transit system.","Leveraging the capabilities of LLMs in natural language processing and data analysis, we investigate their capabilities to optimize route planning, reduce wait times, and provide personalized travel assistance.","By utilizing the General Transit Feed Specification (GTFS) and other relevant data, this research aims to demonstrate how LLMs can potentially improve resource allocation, elevate passenger satisfaction, and inform data-driven decision-making in transit operations.","A comparative analysis of different ChatGPT models was conducted to assess their ability to understand transportation information, retrieve relevant data, and provide comprehensive responses.","Findings from this study suggest that while LLMs hold immense promise for public transit, careful engineering and fine-tuning are essential to realizing their full potential.","San Antonio serves as a case study to inform the development of LLM-powered transit systems in other urban environments."],"url":"http://arxiv.org/abs/2501.03904v1"}
{"created":"2025-01-07 16:10:09","title":"Explainable Reinforcement Learning via Temporal Policy Decomposition","abstract":"We investigate the explainability of Reinforcement Learning (RL) policies from a temporal perspective, focusing on the sequence of future outcomes associated with individual actions. In RL, value functions compress information about rewards collected across multiple trajectories and over an infinite horizon, allowing a compact form of knowledge representation. However, this compression obscures the temporal details inherent in sequential decision-making, presenting a key challenge for interpretability. We present Temporal Policy Decomposition (TPD), a novel explainability approach that explains individual RL actions in terms of their Expected Future Outcome (EFO). These explanations decompose generalized value functions into a sequence of EFOs, one for each time step up to a prediction horizon of interest, revealing insights into when specific outcomes are expected to occur. We leverage fixed-horizon temporal difference learning to devise an off-policy method for learning EFOs for both optimal and suboptimal actions, enabling contrastive explanations consisting of EFOs for different state-action pairs. Our experiments demonstrate that TPD generates accurate explanations that (i) clarify the policy's future strategy and anticipated trajectory for a given action and (ii) improve understanding of the reward composition, facilitating fine-tuning of the reward function to align with human expectations.","sentences":["We investigate the explainability of Reinforcement Learning (RL) policies from a temporal perspective, focusing on the sequence of future outcomes associated with individual actions.","In RL, value functions compress information about rewards collected across multiple trajectories and over an infinite horizon, allowing a compact form of knowledge representation.","However, this compression obscures the temporal details inherent in sequential decision-making, presenting a key challenge for interpretability.","We present Temporal Policy Decomposition (TPD), a novel explainability approach that explains individual RL actions in terms of their Expected Future Outcome (EFO).","These explanations decompose generalized value functions into a sequence of EFOs, one for each time step up to a prediction horizon of interest, revealing insights into when specific outcomes are expected to occur.","We leverage fixed-horizon temporal difference learning to devise an off-policy method for learning EFOs for both optimal and suboptimal actions, enabling contrastive explanations consisting of EFOs for different state-action pairs.","Our experiments demonstrate that TPD generates accurate explanations that (i) clarify the policy's future strategy and anticipated trajectory for a given action and (ii) improve understanding of the reward composition, facilitating fine-tuning of the reward function to align with human expectations."],"url":"http://arxiv.org/abs/2501.03902v1"}
{"created":"2025-01-07 16:05:27","title":"SPECTRE: A Hybrid System for an Adaptative and Optimised Cyber Threats Detection, Response and Investigation in Volatile Memory","abstract":"The increasing sophistication of modern cyber threats, particularly file-less malware relying on living-off-the-land techniques, poses significant challenges to traditional detection mechanisms. Memory forensics has emerged as a crucial method for uncovering such threats by analysing dynamic changes in memory. This research introduces SPECTRE (Snapshot Processing, Emulation, Comparison, and Threat Reporting Engine), a modular Cyber Incident Response System designed to enhance threat detection, investigation, and visualization. By adopting Volatility JSON format as an intermediate output, SPECTRE ensures compatibility with widely used DFIR tools, minimizing manual data transformations and enabling seamless integration into established workflows. Its emulation capabilities safely replicate realistic attack scenarios, such as credential dumping and malicious process injections, for controlled experimentation and validation. The anomaly detection module addresses critical attack vectors, including RunDLL32 abuse and malicious IP detection, while the IP forensics module enhances threat intelligence by integrating tools like Virus Total and geolocation APIs. SPECTRE advanced visualization techniques transform raw memory data into actionable insights, aiding Red, Blue and Purple teams in refining strategies and responding effectively to threats. Bridging gaps between memory and network forensics, SPECTRE offers a scalable, robust platform for advancing threat detection, team training, and forensic research in combating sophisticated cyber threats.","sentences":["The increasing sophistication of modern cyber threats, particularly file-less malware relying on living-off-the-land techniques, poses significant challenges to traditional detection mechanisms.","Memory forensics has emerged as a crucial method for uncovering such threats by analysing dynamic changes in memory.","This research introduces SPECTRE (Snapshot Processing, Emulation, Comparison, and Threat Reporting Engine), a modular Cyber Incident Response System designed to enhance threat detection, investigation, and visualization.","By adopting Volatility JSON format as an intermediate output, SPECTRE ensures compatibility with widely used DFIR tools, minimizing manual data transformations and enabling seamless integration into established workflows.","Its emulation capabilities safely replicate realistic attack scenarios, such as credential dumping and malicious process injections, for controlled experimentation and validation.","The anomaly detection module addresses critical attack vectors, including RunDLL32 abuse and malicious IP detection, while the IP forensics module enhances threat intelligence by integrating tools like Virus Total and geolocation APIs.","SPECTRE advanced visualization techniques transform raw memory data into actionable insights, aiding Red, Blue and Purple teams in refining strategies and responding effectively to threats.","Bridging gaps between memory and network forensics, SPECTRE offers a scalable, robust platform for advancing threat detection, team training, and forensic research in combating sophisticated cyber threats."],"url":"http://arxiv.org/abs/2501.03898v1"}
{"created":"2025-01-07 16:03:14","title":"LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token","abstract":"The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs. LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead. Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens. To achieve a high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play a crucial role in the early layers of LLM backbone, where they mainly fuse visual information into text tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token. LLaVA-Mini is a unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner. Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory.","sentences":["The advent of real-time large multimodal models (LMMs) like GPT-4o has sparked considerable interest in efficient LMMs.","LMM frameworks typically encode visual inputs into vision tokens (continuous representations) and integrate them and textual instructions into the context of large language models (LLMs), where large-scale parameters and numerous context tokens (predominantly vision tokens) result in substantial computational overhead.","Previous efforts towards efficient LMMs always focus on replacing the LLM backbone with smaller models, while neglecting the crucial issue of token quantity.","In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal vision tokens.","To achieve a high compression ratio of vision tokens while preserving visual information, we first analyze how LMMs understand vision tokens and find that most vision tokens only play a crucial role in the early layers of LLM backbone, where they mainly fuse visual information into text tokens.","Building on this finding, LLaVA-Mini introduces modality pre-fusion to fuse visual information into text tokens in advance, thereby facilitating the extreme compression of vision tokens fed to LLM backbone into one token.","LLaVA-Mini is a unified large multimodal model that can support the understanding of images, high-resolution images, and videos in an efficient manner.","Experiments across 11 image-based and 7 video-based benchmarks demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token instead of 576.","Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by 77%, deliver low-latency responses within 40 milliseconds, and process over 10,000 frames of video on the GPU hardware with 24GB of memory."],"url":"http://arxiv.org/abs/2501.03895v1"}
{"created":"2025-01-07 16:00:40","title":"LEAP: LLM-powered End-to-end Automatic Library for Processing Social Science Queries on Unstructured Data","abstract":"Social scientists are increasingly interested in analyzing the semantic information (e.g., emotion) of unstructured data (e.g., Tweets), where the semantic information is not natively present. Performing this analysis in a cost-efficient manner requires using machine learning (ML) models to extract the semantic information and subsequently analyze the now structured data. However, this process remains challenging for domain experts.   To demonstrate the challenges in social science analytics, we collect a dataset, QUIET-ML, of 120 real-world social science queries in natural language and their ground truth answers. Existing systems struggle with these queries since (1) they require selecting and applying ML models, and (2) more than a quarter of these queries are vague, making standard tools like natural language to SQL systems unsuited. To address these issues, we develop LEAP, an end-to-end library that answers social science queries in natural language with ML. LEAP filters vague queries to ensure that the answers are deterministic and selects from internally supported and user-defined ML functions to extend the unstructured data to structured tables with necessary annotations. LEAP further generates and executes code to respond to these natural language queries. LEAP achieves a 100% pass @ 3 and 92% pass @ 1 on QUIET-ML, with a \\$1.06 average end-to-end cost, of which code generation costs \\$0.02.","sentences":["Social scientists are increasingly interested in analyzing the semantic information (e.g., emotion) of unstructured data (e.g., Tweets), where the semantic information is not natively present.","Performing this analysis in a cost-efficient manner requires using machine learning (ML) models to extract the semantic information and subsequently analyze the now structured data.","However, this process remains challenging for domain experts.   ","To demonstrate the challenges in social science analytics, we collect a dataset, QUIET-ML, of 120 real-world social science queries in natural language and their ground truth answers.","Existing systems struggle with these queries since (1) they require selecting and applying ML models, and (2) more than a quarter of these queries are vague, making standard tools like natural language to SQL systems unsuited.","To address these issues, we develop LEAP, an end-to-end library that answers social science queries in natural language with ML.","LEAP filters vague queries to ensure that the answers are deterministic and selects from internally supported and user-defined ML functions to extend the unstructured data to structured tables with necessary annotations.","LEAP further generates and executes code to respond to these natural language queries.","LEAP achieves a 100% pass @","3 and 92% pass @ 1 on QUIET-ML, with a \\$1.06 average end-to-end cost, of which code generation costs \\$0.02."],"url":"http://arxiv.org/abs/2501.03892v1"}
{"created":"2025-01-07 15:54:03","title":"Superpixel Boundary Correction for Weakly-Supervised Semantic Segmentation on Histopathology Images","abstract":"With the rapid advancement of deep learning, computational pathology has made significant progress in cancer diagnosis and subtyping. Tissue segmentation is a core challenge, essential for prognosis and treatment decisions. Weakly supervised semantic segmentation (WSSS) reduces the annotation requirement by using image-level labels instead of pixel-level ones. However, Class Activation Map (CAM)-based methods still suffer from low spatial resolution and unclear boundaries. To address these issues, we propose a multi-level superpixel correction algorithm that refines CAM boundaries using superpixel clustering and floodfill. Experimental results show that our method achieves great performance on breast cancer segmentation dataset with mIoU of 71.08%, significantly improving tumor microenvironment boundary delineation.","sentences":["With the rapid advancement of deep learning, computational pathology has made significant progress in cancer diagnosis and subtyping.","Tissue segmentation is a core challenge, essential for prognosis and treatment decisions.","Weakly supervised semantic segmentation (WSSS) reduces the annotation requirement by using image-level labels instead of pixel-level ones.","However, Class Activation Map (CAM)-based methods still suffer from low spatial resolution and unclear boundaries.","To address these issues, we propose a multi-level superpixel correction algorithm that refines CAM boundaries using superpixel clustering and floodfill.","Experimental results show that our method achieves great performance on breast cancer segmentation dataset with mIoU of 71.08%, significantly improving tumor microenvironment boundary delineation."],"url":"http://arxiv.org/abs/2501.03891v1"}
{"created":"2025-01-07 15:51:49","title":"Neural DNF-MT: A Neuro-symbolic Approach for Learning Interpretable and Editable Policies","abstract":"Although deep reinforcement learning has been shown to be effective, the model's black-box nature presents barriers to direct policy interpretation. To address this problem, we propose a neuro-symbolic approach called neural DNF-MT for end-to-end policy learning. The differentiable nature of the neural DNF-MT model enables the use of deep actor-critic algorithms for training. At the same time, its architecture is designed so that trained models can be directly translated into interpretable policies expressed as standard (bivalent or probabilistic) logic programs. Moreover, additional layers can be included to extract abstract features from complex observations, acting as a form of predicate invention. The logic representations are highly interpretable, and we show how the bivalent representations of deterministic policies can be edited and incorporated back into a neural model, facilitating manual intervention and adaptation of learned policies. We evaluate our approach on a range of tasks requiring learning deterministic or stochastic behaviours from various forms of observations. Our empirical results show that our neural DNF-MT model performs at the level of competing black-box methods whilst providing interpretable policies.","sentences":["Although deep reinforcement learning has been shown to be effective, the model's black-box nature presents barriers to direct policy interpretation.","To address this problem, we propose a neuro-symbolic approach called neural DNF-MT for end-to-end policy learning.","The differentiable nature of the neural DNF-MT model enables the use of deep actor-critic algorithms for training.","At the same time, its architecture is designed so that trained models can be directly translated into interpretable policies expressed as standard (bivalent or probabilistic) logic programs.","Moreover, additional layers can be included to extract abstract features from complex observations, acting as a form of predicate invention.","The logic representations are highly interpretable, and we show how the bivalent representations of deterministic policies can be edited and incorporated back into a neural model, facilitating manual intervention and adaptation of learned policies.","We evaluate our approach on a range of tasks requiring learning deterministic or stochastic behaviours from various forms of observations.","Our empirical results show that our neural DNF-MT model performs at the level of competing black-box methods whilst providing interpretable policies."],"url":"http://arxiv.org/abs/2501.03888v1"}
{"created":"2025-01-07 15:46:42","title":"AlphaPO -- Reward shape matters for LLM alignment","abstract":"Reinforcement Learning with Human Feedback (RLHF) and its variants have made huge strides toward the effective alignment of large language models (LLMs) to follow instructions and reflect human values. More recently, Direct Alignment Algorithms (DAAs) have emerged in which the reward modeling stage of RLHF is skipped by characterizing the reward directly as a function of the policy being learned. Examples include Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO). These methods often suffer from likelihood displacement, a phenomenon by which the probabilities of preferred responses are often reduced undesirably.   In this paper, we argue that, for DAAs the reward (function) shape matters. We introduce AlphaPO, a new DAA method that leverages an $\\alpha$-parameter to help change the shape of the reward function beyond the standard log reward. AlphaPO helps maintain fine-grained control over likelihood displacement and over-optimization. Compared to SimPO, one of the best performing DAAs, AlphaPO leads to about 7\\% to 10\\% relative improvement in alignment performance for the instruct versions of Mistral-7B and Llama3-8B. The analysis and results presented highlight the importance of the reward shape, and how one can systematically change it to affect training dynamics, as well as improve alignment performance.","sentences":["Reinforcement Learning with Human Feedback (RLHF) and its variants have made huge strides toward the effective alignment of large language models (LLMs) to follow instructions and reflect human values.","More recently, Direct Alignment Algorithms (DAAs) have emerged in which the reward modeling stage of RLHF is skipped by characterizing the reward directly as a function of the policy being learned.","Examples include Direct Preference Optimization (DPO) and Simple Preference Optimization (SimPO).","These methods often suffer from likelihood displacement, a phenomenon by which the probabilities of preferred responses are often reduced undesirably.   ","In this paper, we argue that, for DAAs the reward (function) shape matters.","We introduce AlphaPO, a new DAA method that leverages an $\\alpha$-parameter to help change the shape of the reward function beyond the standard log reward.","AlphaPO helps maintain fine-grained control over likelihood displacement and over-optimization.","Compared to SimPO, one of the best performing DAAs, AlphaPO leads to about 7\\% to 10\\% relative improvement in alignment performance for the instruct versions of Mistral-7B and Llama3-8B.","The analysis and results presented highlight the importance of the reward shape, and how one can systematically change it to affect training dynamics, as well as improve alignment performance."],"url":"http://arxiv.org/abs/2501.03884v1"}
{"created":"2025-01-07 15:44:06","title":"An LSTM-based Test Selection Method for Self-Driving Cars","abstract":"Self-driving cars require extensive testing, which can be costly in terms of time. To optimize this process, simple and straightforward tests should be excluded, focusing on challenging tests instead. This study addresses the test selection problem for lane-keeping systems for self-driving cars. Road segment features, such as angles and lengths, were extracted and treated as sequences, enabling classification of the test cases as \"safe\" or \"unsafe\" using a long short-term memory (LSTM) model. The proposed model is compared against machine learning-based test selectors. Results demonstrated that the LSTM-based method outperformed machine learning-based methods in accuracy and precision metrics while exhibiting comparable performance in recall and F1 scores. This work introduces a novel deep learning-based approach to the road classification problem, providing an effective solution for self-driving car test selection using a simulation environment.","sentences":["Self-driving cars require extensive testing, which can be costly in terms of time.","To optimize this process, simple and straightforward tests should be excluded, focusing on challenging tests instead.","This study addresses the test selection problem for lane-keeping systems for self-driving cars.","Road segment features, such as angles and lengths, were extracted and treated as sequences, enabling classification of the test cases as \"safe\" or \"unsafe\" using a long short-term memory (LSTM) model.","The proposed model is compared against machine learning-based test selectors.","Results demonstrated that the LSTM-based method outperformed machine learning-based methods in accuracy and precision metrics while exhibiting comparable performance in recall and F1 scores.","This work introduces a novel deep learning-based approach to the road classification problem, providing an effective solution for self-driving car test selection using a simulation environment."],"url":"http://arxiv.org/abs/2501.03881v1"}
{"created":"2025-01-07 15:42:32","title":"CL3DOR: Contrastive Learning for 3D Large Multimodal Models via Odds Ratio on High-Resolution Point Clouds","abstract":"Recent research has demonstrated that Large Language Models (LLMs) are not limited to text-only tasks but can also function as multimodal models across various modalities, including audio, images, and videos. In particular, research on 3D Large Multimodal Models (3D LMMs) is making notable strides, driven by the potential of processing higher-dimensional data like point clouds. However, upon closer examination, we find that the visual and textual content within each sample of existing training datasets lacks both high informational granularity and clarity, which serve as a bottleneck for precise cross-modal understanding. To address these issues, we propose CL3DOR, Contrastive Learning for 3D large multimodal models via Odds ratio on high-Resolution point clouds, designed to ensure greater specificity and clarity in both visual and textual content. Specifically, we increase the density of point clouds per object and construct informative hard negative responses in the training dataset to penalize unwanted responses. To leverage hard negative responses, we incorporate the odds ratio as an auxiliary term for contrastive learning into the conventional language modeling loss. CL3DOR achieves state-of-the-art performance in 3D scene understanding and reasoning benchmarks. Additionally, we demonstrate the effectiveness of CL3DOR's key components through extensive experiments.","sentences":["Recent research has demonstrated that Large Language Models (LLMs) are not limited to text-only tasks but can also function as multimodal models across various modalities, including audio, images, and videos.","In particular, research on 3D Large Multimodal Models (3D LMMs) is making notable strides, driven by the potential of processing higher-dimensional data like point clouds.","However, upon closer examination, we find that the visual and textual content within each sample of existing training datasets lacks both high informational granularity and clarity, which serve as a bottleneck for precise cross-modal understanding.","To address these issues, we propose CL3DOR, Contrastive Learning for 3D large multimodal models via Odds ratio on high-Resolution point clouds, designed to ensure greater specificity and clarity in both visual and textual content.","Specifically, we increase the density of point clouds per object and construct informative hard negative responses in the training dataset to penalize unwanted responses.","To leverage hard negative responses, we incorporate the odds ratio as an auxiliary term for contrastive learning into the conventional language modeling loss.","CL3DOR achieves state-of-the-art performance in 3D scene understanding and reasoning benchmarks.","Additionally, we demonstrate the effectiveness of CL3DOR's key components through extensive experiments."],"url":"http://arxiv.org/abs/2501.03879v1"}
{"created":"2025-01-07 15:40:22","title":"Stochastically Constrained Best Arm Identification with Thompson Sampling","abstract":"We consider the problem of the best arm identification in the presence of stochastic constraints, where there is a finite number of arms associated with multiple performance measures. The goal is to identify the arm that optimizes the objective measure subject to constraints on the remaining measures. We will explore the popular idea of Thompson sampling (TS) as a means to solve it. To the best of our knowledge, it is the first attempt to extend TS to this problem. We will design a TS-based sampling algorithm, establish its asymptotic optimality in the rate of posterior convergence, and demonstrate its superior performance using numerical examples.","sentences":["We consider the problem of the best arm identification in the presence of stochastic constraints, where there is a finite number of arms associated with multiple performance measures.","The goal is to identify the arm that optimizes the objective measure subject to constraints on the remaining measures.","We will explore the popular idea of Thompson sampling (TS) as a means to solve it.","To the best of our knowledge, it is the first attempt to extend TS to this problem.","We will design a TS-based sampling algorithm, establish its asymptotic optimality in the rate of posterior convergence, and demonstrate its superior performance using numerical examples."],"url":"http://arxiv.org/abs/2501.03877v1"}
{"created":"2025-01-07 15:39:02","title":"ZDySS -- Zero-Shot Dynamic Scene Stylization using Gaussian Splatting","abstract":"Stylizing a dynamic scene based on an exemplar image is critical for various real-world applications, including gaming, filmmaking, and augmented and virtual reality. However, achieving consistent stylization across both spatial and temporal dimensions remains a significant challenge. Most existing methods are designed for static scenes and often require an optimization process for each style image, limiting their adaptability. We introduce ZDySS, a zero-shot stylization framework for dynamic scenes, allowing our model to generalize to previously unseen style images at inference. Our approach employs Gaussian splatting for scene representation, linking each Gaussian to a learned feature vector that renders a feature map for any given view and timestamp. By applying style transfer on the learned feature vectors instead of the rendered feature map, we enhance spatio-temporal consistency across frames. Our method demonstrates superior performance and coherence over state-of-the-art baselines in tests on real-world dynamic scenes, making it a robust solution for practical applications.","sentences":["Stylizing a dynamic scene based on an exemplar image is critical for various real-world applications, including gaming, filmmaking, and augmented and virtual reality.","However, achieving consistent stylization across both spatial and temporal dimensions remains a significant challenge.","Most existing methods are designed for static scenes and often require an optimization process for each style image, limiting their adaptability.","We introduce ZDySS, a zero-shot stylization framework for dynamic scenes, allowing our model to generalize to previously unseen style images at inference.","Our approach employs Gaussian splatting for scene representation, linking each Gaussian to a learned feature vector that renders a feature map for any given view and timestamp.","By applying style transfer on the learned feature vectors instead of the rendered feature map, we enhance spatio-temporal consistency across frames.","Our method demonstrates superior performance and coherence over state-of-the-art baselines in tests on real-world dynamic scenes, making it a robust solution for practical applications."],"url":"http://arxiv.org/abs/2501.03875v1"}
{"created":"2025-01-07 15:38:13","title":"Neuromorphic Optical Tracking and Imaging of Randomly Moving Targets through Strongly Scattering Media","abstract":"Tracking and acquiring simultaneous optical images of randomly moving targets obscured by scattering media remains a challenging problem of importance to many applications that require precise object localization and identification. In this work we develop an end-to-end neuromorphic optical engineering and computational approach to demonstrate how to track and image normally invisible objects by combining an event detecting camera with a multistage neuromorphic deep learning strategy. Photons emerging from dense scattering media are detected by the event camera and converted to pixel-wise asynchronized spike trains - a first step in isolating object-specific information from the dominant uninformative background. Spiking data is fed into a deep spiking neural network (SNN) engine where object tracking and image reconstruction are performed by two separate yet interconnected modules running in parallel in discrete time steps over the event duration. Through benchtop experiments we demonstrate tracking and imaging randomly moving objects in dense turbid media as well as image reconstruction of spatially stationary but optically dynamic objects. Standardized character sets serve as representative proxies for geometrically complex objects, underscoring the method's generality. The results highlight the advantages of a fully neuromorphic approach in meeting a major imaging technology with high computational efficiency and low power consumption.","sentences":["Tracking and acquiring simultaneous optical images of randomly moving targets obscured by scattering media remains a challenging problem of importance to many applications that require precise object localization and identification.","In this work we develop an end-to-end neuromorphic optical engineering and computational approach to demonstrate how to track and image normally invisible objects by combining an event detecting camera with a multistage neuromorphic deep learning strategy.","Photons emerging from dense scattering media are detected by the event camera and converted to pixel-wise asynchronized spike trains - a first step in isolating object-specific information from the dominant uninformative background.","Spiking data is fed into a deep spiking neural network (SNN) engine where object tracking and image reconstruction are performed by two separate yet interconnected modules running in parallel in discrete time steps over the event duration.","Through benchtop experiments we demonstrate tracking and imaging randomly moving objects in dense turbid media as well as image reconstruction of spatially stationary but optically dynamic objects.","Standardized character sets serve as representative proxies for geometrically complex objects, underscoring the method's generality.","The results highlight the advantages of a fully neuromorphic approach in meeting a major imaging technology with high computational efficiency and low power consumption."],"url":"http://arxiv.org/abs/2501.03874v1"}
{"created":"2025-01-07 15:36:44","title":"Parameterized Complexity of Segment Routing","abstract":"Segment Routing is a recent network technology that helps optimizing network throughput by providing finer control over the routing paths. Instead of routing directly from a source to a target, packets are routed via intermediate waypoints. Between consecutive waypoints, the packets are routed according to traditional shortest path routing protocols. Bottlenecks in the network can be avoided by such rerouting, preventing overloading parts of the network. The associated NP-hard computational problem is Segment Routing: Given a network on $n$ vertices, $d$ traffic demands (vertex pairs), and a (small) number $k$, the task is to find for each demand pair at most $k$ waypoints such that with shortest path routing along these waypoints, all demands are fulfilled without exceeding the capacities of the network. We investigate if special structures of real-world communication networks could be exploited algorithmically. Our results comprise NP-hardness on graphs with constant treewidth even if only one waypoint per demand is allowed. We further exclude (under standard complexity assumptions) algorithms with running time $f(d) n^{g(k)}$ for any functions $f$ and $g$. We complement these lower bounds with polynomial-time solvable special cases.","sentences":["Segment Routing is a recent network technology that helps optimizing network throughput by providing finer control over the routing paths.","Instead of routing directly from a source to a target, packets are routed via intermediate waypoints.","Between consecutive waypoints, the packets are routed according to traditional shortest path routing protocols.","Bottlenecks in the network can be avoided by such rerouting, preventing overloading parts of the network.","The associated NP-hard computational problem is Segment Routing: Given a network on $n$ vertices, $d$ traffic demands (vertex pairs), and a (small) number $k$, the task is to find for each demand pair at most $k$ waypoints such that with shortest path routing along these waypoints, all demands are fulfilled without exceeding the capacities of the network.","We investigate if special structures of real-world communication networks could be exploited algorithmically.","Our results comprise NP-hardness on graphs with constant treewidth even if only one waypoint per demand is allowed.","We further exclude (under standard complexity assumptions) algorithms with running time $f(d) n^{g(k)}$ for any functions $f$ and $g$. We complement these lower bounds with polynomial-time solvable special cases."],"url":"http://arxiv.org/abs/2501.03871v1"}
{"created":"2025-01-07 15:36:35","title":"Add Noise, Tasks, or Layers? MaiNLP at the VarDial 2025 Shared Task on Norwegian Dialectal Slot and Intent Detection","abstract":"Slot and intent detection (SID) is a classic natural language understanding task. Despite this, research has only more recently begun focusing on SID for dialectal and colloquial varieties. Many approaches for low-resource scenarios have not yet been applied to dialectal SID data, or compared to each other on the same datasets. We participate in the VarDial 2025 shared task on slot and intent detection in Norwegian varieties, and compare multiple set-ups: varying the training data (English, Norwegian, or dialectal Norwegian), injecting character-level noise, training on auxiliary tasks, and applying Layer Swapping, a technique in which layers of models fine-tuned on different datasets are assembled into a model. We find noise injection to be beneficial while the effects of auxiliary tasks are mixed. Though some experimentation was required to successfully assemble a model from layers, it worked surprisingly well; a combination of models trained on English and small amounts of dialectal data produced the most robust slot predictions. Our best models achieve 97.6% intent accuracy and 85.6% slot F1 in the shared task.","sentences":["Slot and intent detection (SID) is a classic natural language understanding task.","Despite this, research has only more recently begun focusing on SID for dialectal and colloquial varieties.","Many approaches for low-resource scenarios have not yet been applied to dialectal SID data, or compared to each other on the same datasets.","We participate in the VarDial 2025 shared task on slot and intent detection in Norwegian varieties, and compare multiple set-ups: varying the training data (English, Norwegian, or dialectal Norwegian), injecting character-level noise, training on auxiliary tasks, and applying Layer Swapping, a technique in which layers of models fine-tuned on different datasets are assembled into a model.","We find noise injection to be beneficial while the effects of auxiliary tasks are mixed.","Though some experimentation was required to successfully assemble a model from layers, it worked surprisingly well; a combination of models trained on English and small amounts of dialectal data produced the most robust slot predictions.","Our best models achieve 97.6% intent accuracy and 85.6% slot F1 in the shared task."],"url":"http://arxiv.org/abs/2501.03870v1"}
{"created":"2025-01-07 15:24:53","title":"Truthful mechanisms for linear bandit games with private contexts","abstract":"The contextual bandit problem, where agents arrive sequentially with personal contexts and the system adapts its arm allocation decisions accordingly, has recently garnered increasing attention for enabling more personalized outcomes. However, in many healthcare and recommendation applications, agents have private profiles and may misreport their contexts to gain from the system. For example, in adaptive clinical trials, where hospitals sequentially recruit volunteers to test multiple new treatments and adjust plans based on volunteers' reported profiles such as symptoms and interim data, participants may misreport severe side effects like allergy and nausea to avoid perceived suboptimal treatments. We are the first to study this issue of private context misreporting in a stochastic contextual bandit game between the system and non-repeated agents. We show that traditional low-regret algorithms, such as UCB family algorithms and Thompson sampling, fail to ensure truthful reporting and can result in linear regret in the worst case, while traditional truthful algorithms like explore-then-commit (ETC) and $\\epsilon$-greedy algorithm incur sublinear but high regret. We propose a mechanism that uses a linear program to ensure truthfulness while minimizing deviation from Thompson sampling, yielding an $O(\\ln T)$ frequentist regret. Our numerical experiments further demonstrate strong performance in multiple contexts and across other distribution families.","sentences":["The contextual bandit problem, where agents arrive sequentially with personal contexts and the system adapts its arm allocation decisions accordingly, has recently garnered increasing attention for enabling more personalized outcomes.","However, in many healthcare and recommendation applications, agents have private profiles and may misreport their contexts to gain from the system.","For example, in adaptive clinical trials, where hospitals sequentially recruit volunteers to test multiple new treatments and adjust plans based on volunteers' reported profiles such as symptoms and interim data, participants may misreport severe side effects like allergy and nausea to avoid perceived suboptimal treatments.","We are the first to study this issue of private context misreporting in a stochastic contextual bandit game between the system and non-repeated agents.","We show that traditional low-regret algorithms, such as UCB family algorithms and Thompson sampling, fail to ensure truthful reporting and can result in linear regret in the worst case, while traditional truthful algorithms like explore-then-commit (ETC) and $\\epsilon$-greedy algorithm incur sublinear but high regret.","We propose a mechanism that uses a linear program to ensure truthfulness while minimizing deviation from Thompson sampling, yielding an $O(\\ln T)$ frequentist regret.","Our numerical experiments further demonstrate strong performance in multiple contexts and across other distribution families."],"url":"http://arxiv.org/abs/2501.03865v1"}
{"created":"2025-01-07 15:21:07","title":"Improving Dialectal Slot and Intent Detection with Auxiliary Tasks: A Multi-Dialectal Bavarian Case Study","abstract":"Reliable slot and intent detection (SID) is crucial in natural language understanding for applications like digital assistants. Encoder-only transformer models fine-tuned on high-resource languages generally perform well on SID. However, they struggle with dialectal data, where no standardized form exists and training data is scarce and costly to produce. We explore zero-shot transfer learning for SID, focusing on multiple Bavarian dialects, for which we release a new dataset for the Munich dialect. We evaluate models trained on auxiliary tasks in Bavarian, and compare joint multi-task learning with intermediate-task training. We also compare three types of auxiliary tasks: token-level syntactic tasks, named entity recognition (NER), and language modelling. We find that the included auxiliary tasks have a more positive effect on slot filling than intent classification (with NER having the most positive effect), and that intermediate-task training yields more consistent performance gains. Our best-performing approach improves intent classification performance on Bavarian dialects by 5.1 and slot filling F1 by 8.4 percentage points.","sentences":["Reliable slot and intent detection (SID) is crucial in natural language understanding for applications like digital assistants.","Encoder-only transformer models fine-tuned on high-resource languages generally perform well on SID.","However, they struggle with dialectal data, where no standardized form exists and training data is scarce and costly to produce.","We explore zero-shot transfer learning for SID, focusing on multiple Bavarian dialects, for which we release a new dataset for the Munich dialect.","We evaluate models trained on auxiliary tasks in Bavarian, and compare joint multi-task learning with intermediate-task training.","We also compare three types of auxiliary tasks: token-level syntactic tasks, named entity recognition (NER), and language modelling.","We find that the included auxiliary tasks have a more positive effect on slot filling than intent classification (with NER having the most positive effect), and that intermediate-task training yields more consistent performance gains.","Our best-performing approach improves intent classification performance on Bavarian dialects by 5.1 and slot filling F1 by 8.4 percentage points."],"url":"http://arxiv.org/abs/2501.03863v1"}
{"created":"2025-01-07 15:19:17","title":"Rendezfood: A Design Case Study of a Conversational Location-based Approach in Restaurants","abstract":"The restaurant industry is currently facing a challenging socio-economic situation caused by the rise of delivery services, inflation, and typically low margins. Often, technological opportunities for process optimization or customer retention are not fully utilized. In our design case study, we investigate which technologies are already being used to improve the customer experience in restaurants and explore a novel new approach to this issue. We designed, implemented, and evaluated a platform with customers and restaurateurs to increase visibility and emotional connection to nearby restaurants through their dishes. Some of our key findings include the enormous potential of combining location-based systems and conversational agents, but also the difficulties in creating content for such platforms. We contribute to the field of Human-Food Interaction by (1) identifying promising design spaces as well as customer and restaurateur requirements for technology in this domain, (2) presenting an innovative design case study to improve the user experience, and (3) exploring the broader implications of our design case study findings for approaching a real-world metaverse.","sentences":["The restaurant industry is currently facing a challenging socio-economic situation caused by the rise of delivery services, inflation, and typically low margins.","Often, technological opportunities for process optimization or customer retention are not fully utilized.","In our design case study, we investigate which technologies are already being used to improve the customer experience in restaurants and explore a novel new approach to this issue.","We designed, implemented, and evaluated a platform with customers and restaurateurs to increase visibility and emotional connection to nearby restaurants through their dishes.","Some of our key findings include the enormous potential of combining location-based systems and conversational agents, but also the difficulties in creating content for such platforms.","We contribute to the field of Human-Food Interaction by (1) identifying promising design spaces as well as customer and restaurateur requirements for technology in this domain, (2) presenting an innovative design case study to improve the user experience, and (3) exploring the broader implications of our design case study findings for approaching a real-world metaverse."],"url":"http://arxiv.org/abs/2501.03862v1"}
{"created":"2025-01-07 15:16:16","title":"A Synergistic Framework for Learning Shape Estimation and Shape-Aware Whole-Body Control Policy for Continuum Robots","abstract":"In this paper, we present a novel synergistic framework for learning shape estimation and a shape-aware whole-body control policy for tendon-driven continuum robots. Our approach leverages the interaction between two Augmented Neural Ordinary Differential Equations (ANODEs) -- the Shape-NODE and Control-NODE -- to achieve continuous shape estimation and shape-aware control. The Shape-NODE integrates prior knowledge from Cosserat rod theory, allowing it to adapt and account for model mismatches, while the Control-NODE uses this shape information to optimize a whole-body control policy, trained in a Model Predictive Control (MPC) fashion. This unified framework effectively overcomes limitations of existing data-driven methods, such as poor shape awareness and challenges in capturing complex nonlinear dynamics. Extensive evaluations in both simulation and real-world environments demonstrate the framework's robust performance in shape estimation, trajectory tracking, and obstacle avoidance. The proposed method consistently outperforms state-of-the-art end-to-end, Neural-ODE, and Recurrent Neural Network (RNN) models, particularly in terms of tracking accuracy and generalization capabilities.","sentences":["In this paper, we present a novel synergistic framework for learning shape estimation and a shape-aware whole-body control policy for tendon-driven continuum robots.","Our approach leverages the interaction between two Augmented Neural Ordinary Differential Equations (ANODEs) -- the Shape-NODE and Control-NODE -- to achieve continuous shape estimation and shape-aware control.","The Shape-NODE integrates prior knowledge from Cosserat rod theory, allowing it to adapt and account for model mismatches, while the Control-NODE uses this shape information to optimize a whole-body control policy, trained in a Model Predictive Control (MPC) fashion.","This unified framework effectively overcomes limitations of existing data-driven methods, such as poor shape awareness and challenges in capturing complex nonlinear dynamics.","Extensive evaluations in both simulation and real-world environments demonstrate the framework's robust performance in shape estimation, trajectory tracking, and obstacle avoidance.","The proposed method consistently outperforms state-of-the-art end-to-end, Neural-ODE, and Recurrent Neural Network (RNN) models, particularly in terms of tracking accuracy and generalization capabilities."],"url":"http://arxiv.org/abs/2501.03859v1"}
{"created":"2025-01-07 15:14:58","title":"Symmetry and Generalisation in Machine Learning","abstract":"This work is about understanding the impact of invariance and equivariance on generalisation in supervised learning. We use the perspective afforded by an averaging operator to show that for any predictor that is not equivariant, there is an equivariant predictor with strictly lower test risk on all regression problems where the equivariance is correctly specified. This constitutes a rigorous proof that symmetry, in the form of invariance or equivariance, is a useful inductive bias.   We apply these ideas to equivariance and invariance in random design least squares and kernel ridge regression respectively. This allows us to specify the reduction in expected test risk in more concrete settings and express it in terms of properties of the group, the model and the data.   Along the way, we give examples and additional results to demonstrate the utility of the averaging operator approach in analysing equivariant predictors. In addition, we adopt an alternative perspective and formalise the common intuition that learning with invariant models reduces to a problem in terms of orbit representatives. The formalism extends naturally to a similar intuition for equivariant models. We conclude by connecting the two perspectives and giving some ideas for future work.","sentences":["This work is about understanding the impact of invariance and equivariance on generalisation in supervised learning.","We use the perspective afforded by an averaging operator to show that for any predictor that is not equivariant, there is an equivariant predictor with strictly lower test risk on all regression problems where the equivariance is correctly specified.","This constitutes a rigorous proof that symmetry, in the form of invariance or equivariance, is a useful inductive bias.   ","We apply these ideas to equivariance and invariance in random design least squares and kernel ridge regression respectively.","This allows us to specify the reduction in expected test risk in more concrete settings and express it in terms of properties of the group, the model and the data.   ","Along the way, we give examples and additional results to demonstrate the utility of the averaging operator approach in analysing equivariant predictors.","In addition, we adopt an alternative perspective and formalise the common intuition that learning with invariant models reduces to a problem in terms of orbit representatives.","The formalism extends naturally to a similar intuition for equivariant models.","We conclude by connecting the two perspectives and giving some ideas for future work."],"url":"http://arxiv.org/abs/2501.03858v1"}
{"created":"2025-01-07 15:14:37","title":"Progressive Document-level Text Simplification via Large Language Models","abstract":"Research on text simplification has primarily focused on lexical and sentence-level changes. Long document-level simplification (DS) is still relatively unexplored. Large Language Models (LLMs), like ChatGPT, have excelled in many natural language processing tasks. However, their performance on DS tasks is unsatisfactory, as they often treat DS as merely document summarization. For the DS task, the generated long sequences not only must maintain consistency with the original document throughout, but complete moderate simplification operations encompassing discourses, sentences, and word-level simplifications. Human editors employ a hierarchical complexity simplification strategy to simplify documents. This study delves into simulating this strategy through the utilization of a multi-stage collaboration using LLMs. We propose a progressive simplification method (ProgDS) by hierarchically decomposing the task, including the discourse-level, topic-level, and lexical-level simplification. Experimental results demonstrate that ProgDS significantly outperforms existing smaller models or direct prompting with LLMs, advancing the state-of-the-art in the document simplification task.","sentences":["Research on text simplification has primarily focused on lexical and sentence-level changes.","Long document-level simplification (DS) is still relatively unexplored.","Large Language Models (LLMs), like ChatGPT, have excelled in many natural language processing tasks.","However, their performance on DS tasks is unsatisfactory, as they often treat DS as merely document summarization.","For the DS task, the generated long sequences not only must maintain consistency with the original document throughout, but complete moderate simplification operations encompassing discourses, sentences, and word-level simplifications.","Human editors employ a hierarchical complexity simplification strategy to simplify documents.","This study delves into simulating this strategy through the utilization of a multi-stage collaboration using LLMs.","We propose a progressive simplification method (ProgDS) by hierarchically decomposing the task, including the discourse-level, topic-level, and lexical-level simplification.","Experimental results demonstrate that ProgDS significantly outperforms existing smaller models or direct prompting with LLMs, advancing the state-of-the-art in the document simplification task."],"url":"http://arxiv.org/abs/2501.03857v1"}
{"created":"2025-01-07 15:13:45","title":"BabyLMs for isiXhosa: Data-Efficient Language Modelling in a Low-Resource Context","abstract":"The BabyLM challenge called on participants to develop sample-efficient language models. Submissions were pretrained on a fixed English corpus, limited to the amount of words children are exposed to in development (<100m). The challenge produced new architectures for data-efficient language modelling, which outperformed models trained on trillions of words. This is promising for low-resource languages, where available corpora are limited to much less than 100m words. In this paper, we explore the potential of BabyLMs for low-resource languages, using the isiXhosa language as a case study. We pretrain two BabyLM architectures, ELC-BERT and MLSM, on an isiXhosa corpus. They outperform a vanilla pretrained model on POS tagging and NER, achieving notable gains (+3.2 F1) for the latter. In some instances, the BabyLMs even outperform XLM-R. Our findings show that data-efficient models are viable for low-resource languages, but highlight the continued importance, and lack of, high-quality pretraining data. Finally, we visually analyse how BabyLM architectures encode isiXhosa.","sentences":["The BabyLM challenge called on participants to develop sample-efficient language models.","Submissions were pretrained on a fixed English corpus, limited to the amount of words children are exposed to in development (<100m).","The challenge produced new architectures for data-efficient language modelling, which outperformed models trained on trillions of words.","This is promising for low-resource languages, where available corpora are limited to much less than 100m words.","In this paper, we explore the potential of BabyLMs for low-resource languages, using the isiXhosa language as a case study.","We pretrain two BabyLM architectures, ELC-BERT and MLSM, on an isiXhosa corpus.","They outperform a vanilla pretrained model on POS tagging and NER, achieving notable gains (+3.2 F1) for the latter.","In some instances, the BabyLMs even outperform XLM-R. Our findings show that data-efficient models are viable for low-resource languages, but highlight the continued importance, and lack of, high-quality pretraining data.","Finally, we visually analyse how BabyLM architectures encode isiXhosa."],"url":"http://arxiv.org/abs/2501.03855v1"}
{"created":"2025-01-07 15:11:19","title":"Comparison of Integration Methods for Cut Elements","abstract":"Using an interface inserted in a background mesh is an alternative way of constructing a complex geometrical shape with a relative low meshing efforts. However, this process may require special treatment of elements cut by the interface. Our study focuses on comparing the integration of cut elements defined by implicit and parametric curves. We investigate the efficiency and robustness of open-source tools such as Algoim [5](a library for quadrature on implicitly defined geometries) and Ginkgo [2](a library for isogeometric analysis on Boolean operations with a parametric description) with numerical examples computing the area defined by the interface and benchmarks for 2D elasticity problem using the open-source code GeoPDEs [7]. It is concluded that none of the two interface descriptions is preferable with respect to the quality of the integration. Thus, the choice of the interface type depends only on the studied problem and the available curve description, but not on the numerical aspects of the integration.","sentences":["Using an interface inserted in a background mesh is an alternative way of constructing a complex geometrical shape with a relative low meshing efforts.","However, this process may require special treatment of elements cut by the interface.","Our study focuses on comparing the integration of cut elements defined by implicit and parametric curves.","We investigate the efficiency and robustness of open-source tools such as Algoim [5](a library for quadrature on implicitly defined geometries) and Ginkgo [2](a library for isogeometric analysis on Boolean operations with a parametric description) with numerical examples computing the area defined by the interface and benchmarks for 2D elasticity problem using the open-source code GeoPDEs","[7].","It is concluded that none of the two interface descriptions is preferable with respect to the quality of the integration.","Thus, the choice of the interface type depends only on the studied problem and the available curve description, but not on the numerical aspects of the integration."],"url":"http://arxiv.org/abs/2501.03854v1"}
{"created":"2025-01-07 15:07:07","title":"Partitioning Strategies for Parallel Computation of Flexible Skylines","abstract":"While classical skyline queries identify interesting data within large datasets, flexible skylines introduce preferences through constraints on attribute weights, and further reduce the data returned. However, computing these queries can be time-consuming for large datasets. We propose and implement a parallel computation scheme consisting of a parallel phase followed by a sequential phase, and apply it to flexible skylines. We assess the additional effect of an initial filtering phase to reduce dataset size before parallel processing, and the elimination of the sequential part (the most time-consuming) altogether. All our experiments are executed in the PySpark framework for a number of different datasets of varying sizes and dimensions.","sentences":["While classical skyline queries identify interesting data within large datasets, flexible skylines introduce preferences through constraints on attribute weights, and further reduce the data returned.","However, computing these queries can be time-consuming for large datasets.","We propose and implement a parallel computation scheme consisting of a parallel phase followed by a sequential phase, and apply it to flexible skylines.","We assess the additional effect of an initial filtering phase to reduce dataset size before parallel processing, and the elimination of the sequential part (the most time-consuming) altogether.","All our experiments are executed in the PySpark framework for a number of different datasets of varying sizes and dimensions."],"url":"http://arxiv.org/abs/2501.03850v1"}
{"created":"2025-01-07 15:01:58","title":"Diffusion as Shader: 3D-aware Video Diffusion for Versatile Video Generation Control","abstract":"Diffusion models have demonstrated impressive performance in generating high-quality videos from text prompts or images. However, precise control over the video generation process, such as camera manipulation or content editing, remains a significant challenge. Existing methods for controlled video generation are typically limited to a single control type, lacking the flexibility to handle diverse control demands. In this paper, we introduce Diffusion as Shader (DaS), a novel approach that supports multiple video control tasks within a unified architecture. Our key insight is that achieving versatile video control necessitates leveraging 3D control signals, as videos are fundamentally 2D renderings of dynamic 3D content. Unlike prior methods limited to 2D control signals, DaS leverages 3D tracking videos as control inputs, making the video diffusion process inherently 3D-aware. This innovation allows DaS to achieve a wide range of video controls by simply manipulating the 3D tracking videos. A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos. With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities across diverse tasks, including mesh-to-video generation, camera control, motion transfer, and object manipulation.","sentences":["Diffusion models have demonstrated impressive performance in generating high-quality videos from text prompts or images.","However, precise control over the video generation process, such as camera manipulation or content editing, remains a significant challenge.","Existing methods for controlled video generation are typically limited to a single control type, lacking the flexibility to handle diverse control demands.","In this paper, we introduce Diffusion as Shader (DaS), a novel approach that supports multiple video control tasks within a unified architecture.","Our key insight is that achieving versatile video control necessitates leveraging 3D control signals, as videos are fundamentally 2D renderings of dynamic 3D content.","Unlike prior methods limited to 2D control signals, DaS leverages 3D tracking videos as control inputs, making the video diffusion process inherently 3D-aware.","This innovation allows DaS to achieve a wide range of video controls by simply manipulating the 3D tracking videos.","A further advantage of using 3D tracking videos is their ability to effectively link frames, significantly enhancing the temporal consistency of the generated videos.","With just 3 days of fine-tuning on 8 H800 GPUs using less than 10k videos, DaS demonstrates strong control capabilities across diverse tasks, including mesh-to-video generation, camera control, motion transfer, and object manipulation."],"url":"http://arxiv.org/abs/2501.03847v1"}
{"created":"2025-01-07 14:53:35","title":"BERTopic for Topic Modeling of Hindi Short Texts: A Comparative Study","abstract":"As short text data in native languages like Hindi increasingly appear in modern media, robust methods for topic modeling on such data have gained importance. This study investigates the performance of BERTopic in modeling Hindi short texts, an area that has been under-explored in existing research. Using contextual embeddings, BERTopic can capture semantic relationships in data, making it potentially more effective than traditional models, especially for short and diverse texts. We evaluate BERTopic using 6 different document embedding models and compare its performance against 8 established topic modeling techniques, such as Latent Dirichlet Allocation (LDA), Non-negative Matrix Factorization (NMF), Latent Semantic Indexing (LSI), Additive Regularization of Topic Models (ARTM), Probabilistic Latent Semantic Analysis (PLSA), Embedded Topic Model (ETM), Combined Topic Model (CTM), and Top2Vec. The models are assessed using coherence scores across a range of topic counts. Our results reveal that BERTopic consistently outperforms other models in capturing coherent topics from short Hindi texts.","sentences":["As short text data in native languages like Hindi increasingly appear in modern media, robust methods for topic modeling on such data have gained importance.","This study investigates the performance of BERTopic in modeling Hindi short texts, an area that has been under-explored in existing research.","Using contextual embeddings, BERTopic can capture semantic relationships in data, making it potentially more effective than traditional models, especially for short and diverse texts.","We evaluate BERTopic using 6 different document embedding models and compare its performance against 8 established topic modeling techniques, such as Latent Dirichlet Allocation (LDA), Non-negative Matrix Factorization (NMF), Latent Semantic Indexing (LSI), Additive Regularization of Topic Models (ARTM), Probabilistic Latent Semantic Analysis (PLSA), Embedded Topic Model (ETM), Combined Topic Model (CTM), and Top2Vec.","The models are assessed using coherence scores across a range of topic counts.","Our results reveal that BERTopic consistently outperforms other models in capturing coherent topics from short Hindi texts."],"url":"http://arxiv.org/abs/2501.03843v1"}
{"created":"2025-01-07 14:50:33","title":"OmniManip: Towards General Robotic Manipulation via Object-Centric Interaction Primitives as Spatial Constraints","abstract":"The development of general robotic systems capable of manipulating in unstructured environments is a significant challenge. While Vision-Language Models(VLM) excel in high-level commonsense reasoning, they lack the fine-grained 3D spatial understanding required for precise manipulation tasks. Fine-tuning VLM on robotic datasets to create Vision-Language-Action Models(VLA) is a potential solution, but it is hindered by high data collection costs and generalization issues. To address these challenges, we propose a novel object-centric representation that bridges the gap between VLM's high-level reasoning and the low-level precision required for manipulation. Our key insight is that an object's canonical space, defined by its functional affordances, provides a structured and semantically meaningful way to describe interaction primitives, such as points and directions. These primitives act as a bridge, translating VLM's commonsense reasoning into actionable 3D spatial constraints. In this context, we introduce a dual closed-loop, open-vocabulary robotic manipulation system: one loop for high-level planning through primitive resampling, interaction rendering and VLM checking, and another for low-level execution via 6D pose tracking. This design ensures robust, real-time control without requiring VLM fine-tuning. Extensive experiments demonstrate strong zero-shot generalization across diverse robotic manipulation tasks, highlighting the potential of this approach for automating large-scale simulation data generation.","sentences":["The development of general robotic systems capable of manipulating in unstructured environments is a significant challenge.","While Vision-Language Models(VLM) excel in high-level commonsense reasoning, they lack the fine-grained 3D spatial understanding required for precise manipulation tasks.","Fine-tuning VLM on robotic datasets to create Vision-Language-Action Models(VLA) is a potential solution, but it is hindered by high data collection costs and generalization issues.","To address these challenges, we propose a novel object-centric representation that bridges the gap between VLM's high-level reasoning and the low-level precision required for manipulation.","Our key insight is that an object's canonical space, defined by its functional affordances, provides a structured and semantically meaningful way to describe interaction primitives, such as points and directions.","These primitives act as a bridge, translating VLM's commonsense reasoning into actionable 3D spatial constraints.","In this context, we introduce a dual closed-loop, open-vocabulary robotic manipulation system: one loop for high-level planning through primitive resampling, interaction rendering and VLM checking, and another for low-level execution via 6D pose tracking.","This design ensures robust, real-time control without requiring VLM fine-tuning.","Extensive experiments demonstrate strong zero-shot generalization across diverse robotic manipulation tasks, highlighting the potential of this approach for automating large-scale simulation data generation."],"url":"http://arxiv.org/abs/2501.03841v1"}
{"created":"2025-01-07 14:50:05","title":"Machine learning applications in archaeological practices: a review","abstract":"Artificial intelligence and machine learning applications in archaeology have increased significantly in recent years, and these now span all subfields, geographical regions, and time periods. The prevalence and success of these applications have remained largely unexamined, as recent reviews on the use of machine learning in archaeology have only focused only on specific subfields of archaeology. Our review examined an exhaustive corpus of 135 articles published between 1997 and 2022. We observed a significant increase in the number of relevant publications from 2019 onwards. Automatic structure detection and artefact classification were the most represented tasks in the articles reviewed, followed by taphonomy, and archaeological predictive modelling. From the review, clustering and unsupervised methods were underrepresented compared to supervised models. Artificial neural networks and ensemble learning account for two thirds of the total number of models used. However, if machine learning is gaining in popularity it remains subject to misunderstanding. We observed, in some cases, poorly defined requirements and caveats of the machine learning methods used. Furthermore, the goals and the needs of machine learning applications for archaeological purposes are in some cases unclear or poorly expressed. To address this, we proposed a workflow guide for archaeologists to develop coherent and consistent methodologies adapted to their research questions, project scale and data. As in many other areas, machine learning is rapidly becoming an important tool in archaeological research and practice, useful for the analyses of large and multivariate data, although not without limitations. This review highlights the importance of well-defined and well-reported structured methodologies and collaborative practices to maximise the potential of applications of machine learning methods in archaeology.","sentences":["Artificial intelligence and machine learning applications in archaeology have increased significantly in recent years, and these now span all subfields, geographical regions, and time periods.","The prevalence and success of these applications have remained largely unexamined, as recent reviews on the use of machine learning in archaeology have only focused only on specific subfields of archaeology.","Our review examined an exhaustive corpus of 135 articles published between 1997 and 2022.","We observed a significant increase in the number of relevant publications from 2019 onwards.","Automatic structure detection and artefact classification were the most represented tasks in the articles reviewed, followed by taphonomy, and archaeological predictive modelling.","From the review, clustering and unsupervised methods were underrepresented compared to supervised models.","Artificial neural networks and ensemble learning account for two thirds of the total number of models used.","However, if machine learning is gaining in popularity it remains subject to misunderstanding.","We observed, in some cases, poorly defined requirements and caveats of the machine learning methods used.","Furthermore, the goals and the needs of machine learning applications for archaeological purposes are in some cases unclear or poorly expressed.","To address this, we proposed a workflow guide for archaeologists to develop coherent and consistent methodologies adapted to their research questions, project scale and data.","As in many other areas, machine learning is rapidly becoming an important tool in archaeological research and practice, useful for the analyses of large and multivariate data, although not without limitations.","This review highlights the importance of well-defined and well-reported structured methodologies and collaborative practices to maximise the potential of applications of machine learning methods in archaeology."],"url":"http://arxiv.org/abs/2501.03840v1"}
{"created":"2025-01-07 14:47:15","title":"LM-Net: A Light-weight and Multi-scale Network for Medical Image Segmentation","abstract":"Current medical image segmentation approaches have limitations in deeply exploring multi-scale information and effectively combining local detail textures with global contextual semantic information. This results in over-segmentation, under-segmentation, and blurred segmentation boundaries. To tackle these challenges, we explore multi-scale feature representations from different perspectives, proposing a novel, lightweight, and multi-scale architecture (LM-Net) that integrates advantages of both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to enhance segmentation accuracy. LM-Net employs a lightweight multi-branch module to capture multi-scale features at the same level. Furthermore, we introduce two modules to concurrently capture local detail textures and global semantics with multi-scale features at different levels: the Local Feature Transformer (LFT) and Global Feature Transformer (GFT). The LFT integrates local window self-attention to capture local detail textures, while the GFT leverages global self-attention to capture global contextual semantics. By combining these modules, our model achieves complementarity between local and global representations, alleviating the problem of blurred segmentation boundaries in medical image segmentation. To evaluate the feasibility of LM-Net, extensive experiments have been conducted on three publicly available datasets with different modalities. Our proposed model achieves state-of-the-art results, surpassing previous methods, while only requiring 4.66G FLOPs and 5.4M parameters. These state-of-the-art results on three datasets with different modalities demonstrate the effectiveness and adaptability of our proposed LM-Net for various medical image segmentation tasks.","sentences":["Current medical image segmentation approaches have limitations in deeply exploring multi-scale information and effectively combining local detail textures with global contextual semantic information.","This results in over-segmentation, under-segmentation, and blurred segmentation boundaries.","To tackle these challenges, we explore multi-scale feature representations from different perspectives, proposing a novel, lightweight, and multi-scale architecture (LM-Net) that integrates advantages of both Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) to enhance segmentation accuracy.","LM-Net employs a lightweight multi-branch module to capture multi-scale features at the same level.","Furthermore, we introduce two modules to concurrently capture local detail textures and global semantics with multi-scale features at different levels: the Local Feature Transformer (LFT) and Global Feature Transformer (GFT).","The LFT integrates local window self-attention to capture local detail textures, while the GFT leverages global self-attention to capture global contextual semantics.","By combining these modules, our model achieves complementarity between local and global representations, alleviating the problem of blurred segmentation boundaries in medical image segmentation.","To evaluate the feasibility of LM-Net, extensive experiments have been conducted on three publicly available datasets with different modalities.","Our proposed model achieves state-of-the-art results, surpassing previous methods, while only requiring 4.66G FLOPs and 5.4M parameters.","These state-of-the-art results on three datasets with different modalities demonstrate the effectiveness and adaptability of our proposed LM-Net for various medical image segmentation tasks."],"url":"http://arxiv.org/abs/2501.03838v1"}
{"created":"2025-01-07 14:47:02","title":"A Unification of Zeilberger's Algorithm and Its q-Analogue","abstract":"We adapt the theory of normal and special polynomials from symbolic integration to the summation setting, and then built up a general framework embracing both the usual shift case and the q-shift case. In the context of this general framework, we develop a unified reduction algorithm, and subsequently a creative telescoping algorithm, applicable to both hypergeometric terms and their q-analogues. Our algorithms allow to split up the usual shift case and the q-shift case only when it is really necessary, and thus instantly reveal the intrinsic differences between these two cases. Computational experiments are also provided.","sentences":["We adapt the theory of normal and special polynomials from symbolic integration to the summation setting, and then built up a general framework embracing both the usual shift case and the q-shift case.","In the context of this general framework, we develop a unified reduction algorithm, and subsequently a creative telescoping algorithm, applicable to both hypergeometric terms and their q-analogues.","Our algorithms allow to split up the usual shift case and the q-shift case only when it is really necessary, and thus instantly reveal the intrinsic differences between these two cases.","Computational experiments are also provided."],"url":"http://arxiv.org/abs/2501.03837v1"}
{"created":"2025-01-07 14:45:30","title":"TACLR: A Scalable and Efficient Retrieval-based Method for Industrial Product Attribute Value Identification","abstract":"Product Attribute Value Identification (PAVI) involves identifying attribute values from product profiles, a key task for improving product search, recommendations, and business analytics on e-commerce platforms. However, existing PAVI methods face critical challenges, such as inferring implicit values, handling out-of-distribution (OOD) values, and producing normalized outputs. To address these limitations, we introduce Taxonomy-Aware Contrastive Learning Retrieval (TACLR), the first retrieval-based method for PAVI. TACLR formulates PAVI as an information retrieval task by encoding product profiles and candidate values into embeddings and retrieving values based on their similarity to the item embedding. It leverages contrastive training with taxonomy-aware hard negative sampling and employs adaptive inference with dynamic thresholds. TACLR offers three key advantages: (1) it effectively handles implicit and OOD values while producing normalized outputs; (2) it scales to thousands of categories, tens of thousands of attributes, and millions of values; and (3) it supports efficient inference for high-load industrial scenarios. Extensive experiments on proprietary and public datasets validate the effectiveness and efficiency of TACLR. Moreover, it has been successfully deployed in a real-world e-commerce platform, processing millions of product listings daily while supporting dynamic, large-scale attribute taxonomies.","sentences":["Product Attribute Value Identification (PAVI) involves identifying attribute values from product profiles, a key task for improving product search, recommendations, and business analytics on e-commerce platforms.","However, existing PAVI methods face critical challenges, such as inferring implicit values, handling out-of-distribution (OOD) values, and producing normalized outputs.","To address these limitations, we introduce Taxonomy-Aware Contrastive Learning Retrieval (TACLR), the first retrieval-based method for PAVI.","TACLR formulates PAVI as an information retrieval task by encoding product profiles and candidate values into embeddings and retrieving values based on their similarity to the item embedding.","It leverages contrastive training with taxonomy-aware hard negative sampling and employs adaptive inference with dynamic thresholds.","TACLR offers three key advantages: (1) it effectively handles implicit and OOD values while producing normalized outputs; (2) it scales to thousands of categories, tens of thousands of attributes, and millions of values; and (3) it supports efficient inference for high-load industrial scenarios.","Extensive experiments on proprietary and public datasets validate the effectiveness and efficiency of TACLR.","Moreover, it has been successfully deployed in a real-world e-commerce platform, processing millions of product listings daily while supporting dynamic, large-scale attribute taxonomies."],"url":"http://arxiv.org/abs/2501.03835v1"}
{"created":"2025-01-07 14:43:47","title":"The Geodesic Fr\u00e9chet Distance Between Two Curves Bounding a Simple Polygon","abstract":"The Fr\\'echet distance is a popular similarity measure that is well-understood for polygonal curves in $\\mathbb{R}^d$: near-quadratic time algorithms exist, and conditional lower bounds suggest that these results cannot be improved significantly, even in one dimension and when approximating with a factor less than three. We consider the special case where the curves bound a simple polygon and distances are measured via geodesics inside this simple polygon. Here the conditional lower bounds do not apply; Efrat $et$ $al.$ (2002) were able to give a near-linear time $2$-approximation algorithm.   In this paper, we significantly improve upon their result: we present a $(1+\\varepsilon)$-approximation algorithm, for any $\\varepsilon > 0$, that runs in $\\mathcal{O}(\\frac{1}{\\varepsilon} (n+m \\log n) \\log nm \\log \\frac{1}{\\varepsilon})$ time for a simple polygon bounded by two curves with $n$ and $m$ vertices, respectively. To do so, we show how to compute the reachability of specific groups of points in the free space at once and in near-linear time, by interpreting their free space as one between separated one-dimensional curves. Bringmann and K\\\"unnemann (2015) previously solved the decision version of the Fr\\'echet distance in this setting in $\\mathcal{O}((n+m) \\log nm)$ time. We strengthen their result and compute the Fr\\'echet distance between two separated one-dimensional curves in linear time. Finally, we give a linear time exact algorithm if the two curves bound a convex polygon.","sentences":["The Fr\\'echet distance is a popular similarity measure that is well-understood for polygonal curves in $\\mathbb{R}^d$: near-quadratic time algorithms exist, and conditional lower bounds suggest that these results cannot be improved significantly, even in one dimension and when approximating with a factor less than three.","We consider the special case where the curves bound a simple polygon and distances are measured via geodesics inside this simple polygon.","Here the conditional lower bounds do not apply; Efrat $et$ $al.$ (2002) were able to give a near-linear time $2$-approximation algorithm.   ","In this paper, we significantly improve upon their result: we present a $(1+\\varepsilon)$-approximation algorithm, for any $\\varepsilon > 0$, that runs in $\\mathcal{O}(\\frac{1}{\\varepsilon} (n+m \\log n) \\log nm \\log \\frac{1}{\\varepsilon})$ time for a simple polygon bounded by two curves with $n$ and $m$ vertices, respectively.","To do so, we show how to compute the reachability of specific groups of points in the free space at once and in near-linear time, by interpreting their free space as one between separated one-dimensional curves.","Bringmann and K\\\"unnemann (2015) previously solved the decision version of the Fr\\'echet distance in this setting in $\\mathcal{O}((n+m) \\log nm)$ time.","We strengthen their result and compute the Fr\\'echet distance between two separated one-dimensional curves in linear time.","Finally, we give a linear time exact algorithm if the two curves bound a convex polygon."],"url":"http://arxiv.org/abs/2501.03834v1"}
{"created":"2025-01-07 14:43:15","title":"Sequence Reconstruction for Single-Deletion Single-Substitution Channel","abstract":"The central problem in sequence reconstruction is to find the minimum number of distinct channel outputs required to uniquely reconstruct the transmitted sequence. According to Levenshtein's work in 2001, this number is determined by the maximum size of the intersection between the error balls of any two distinct input sequences of the channel. In this work, we study the sequence reconstruction problem for single-deletion single-substitution channel, assuming that the transmitted sequence belongs to a $q$-ary code with minimum Hamming distance at least $2$, where $q\\geq 2$ is any fixed integer. Specifically, we prove that for any two $q$-ary sequences with Hamming distance $d\\geq 2$, the size of the intersection of their error balls is upper bounded by $2qn-3q-2-\\delta_{q,2}$, where $\\delta_{i,j}$ is the Kronecker delta. We also prove the tightness of this bound by constructing two sequences the intersection size of whose error balls achieves this bound.","sentences":["The central problem in sequence reconstruction is to find the minimum number of distinct channel outputs required to uniquely reconstruct the transmitted sequence.","According to Levenshtein's work in 2001, this number is determined by the maximum size of the intersection between the error balls of any two distinct input sequences of the channel.","In this work, we study the sequence reconstruction problem for single-deletion single-substitution channel, assuming that the transmitted sequence belongs to a $q$-ary code with minimum Hamming distance at least $2$, where $q\\geq 2$ is any fixed integer.","Specifically, we prove that for any two $q$-ary sequences with Hamming distance $d\\geq 2$, the size of the intersection of their error balls is upper bounded by $2qn-3q-2-\\delta_{q,2}$, where $\\delta_{i,j}$ is the Kronecker delta.","We also prove the tightness of this bound by constructing two sequences the intersection size of whose error balls achieves this bound."],"url":"http://arxiv.org/abs/2501.03833v1"}
{"created":"2025-01-07 14:42:38","title":"Three-dimensional attention Transformer for state evaluation in real-time strategy games","abstract":"Situation assessment in Real-Time Strategy (RTS) games is crucial for understanding decision-making in complex adversarial environments. However, existing methods remain limited in processing multi-dimensional feature information and temporal dependencies. Here we propose a tri-dimensional Space-Time-Feature Transformer (TSTF Transformer) architecture, which efficiently models battlefield situations through three independent but cascaded modules: spatial attention, temporal attention, and feature attention. On a dataset comprising 3,150 adversarial experiments, the 8-layer TSTF Transformer demonstrates superior performance: achieving 58.7% accuracy in the early game (~4% progress), significantly outperforming the conventional Timesformer's 41.8%; reaching 97.6% accuracy in the mid-game (~40% progress) while maintaining low performance variation (standard deviation 0.114). Meanwhile, this architecture requires fewer parameters (4.75M) compared to the baseline model (5.54M). Our study not only provides new insights into situation assessment in RTS games but also presents an innovative paradigm for Transformer-based multi-dimensional temporal modeling.","sentences":["Situation assessment in Real-Time Strategy (RTS) games is crucial for understanding decision-making in complex adversarial environments.","However, existing methods remain limited in processing multi-dimensional feature information and temporal dependencies.","Here we propose a tri-dimensional Space-Time-Feature Transformer (TSTF Transformer) architecture, which efficiently models battlefield situations through three independent but cascaded modules: spatial attention, temporal attention, and feature attention.","On a dataset comprising 3,150 adversarial experiments, the 8-layer TSTF Transformer demonstrates superior performance: achieving 58.7% accuracy in the early game (~4% progress), significantly outperforming the conventional Timesformer's 41.8%; reaching 97.6% accuracy in the mid-game (~40% progress) while maintaining low performance variation (standard deviation 0.114).","Meanwhile, this architecture requires fewer parameters (4.75M) compared to the baseline model (5.54M).","Our study not only provides new insights into situation assessment in RTS games but also presents an innovative paradigm for Transformer-based multi-dimensional temporal modeling."],"url":"http://arxiv.org/abs/2501.03832v1"}
{"created":"2025-01-07 14:41:26","title":"MeshConv3D: Efficient convolution and pooling operators for triangular 3D meshes","abstract":"Convolutional neural networks (CNNs) have been pivotal in various 2D image analysis tasks, including computer vision, image indexing and retrieval or semantic classification. Extending CNNs to 3D data such as point clouds and 3D meshes raises significant challenges since the very basic convolution and pooling operators need to be completely re-visited and re-defined in an appropriate manner to tackle irregular connectivity issues. In this paper, we introduce MeshConv3D, a 3D mesh-dedicated methodology integrating specialized convolution and face collapse-based pooling operators. MeshConv3D operates directly on meshes of arbitrary topology, without any need of prior re-meshing/conversion techniques. In order to validate our approach, we have considered a semantic classification task. The experimental results obtained on three distinct benchmark datasets show that the proposed approach makes it possible to achieve equivalent or superior classification results, while minimizing the related memory footprint and computational load.","sentences":["Convolutional neural networks (CNNs) have been pivotal in various 2D image analysis tasks, including computer vision, image indexing and retrieval or semantic classification.","Extending CNNs to 3D data such as point clouds and 3D meshes raises significant challenges since the very basic convolution and pooling operators need to be completely re-visited and re-defined in an appropriate manner to tackle irregular connectivity issues.","In this paper, we introduce MeshConv3D, a 3D mesh-dedicated methodology integrating specialized convolution and face collapse-based pooling operators.","MeshConv3D operates directly on meshes of arbitrary topology, without any need of prior re-meshing/conversion techniques.","In order to validate our approach, we have considered a semantic classification task.","The experimental results obtained on three distinct benchmark datasets show that the proposed approach makes it possible to achieve equivalent or superior classification results, while minimizing the related memory footprint and computational load."],"url":"http://arxiv.org/abs/2501.03830v1"}
{"created":"2025-01-07 14:38:49","title":"Investigating the Impact of Data Selection Strategies on Language Model Performance","abstract":"Data selection is critical for enhancing the performance of language models, particularly when aligning training datasets with a desired target distribution. This study explores the effects of different data selection methods and feature types on model performance. We evaluate whether selecting data subsets can influence downstream tasks, whether n-gram features improve alignment with target distributions, and whether embedding-based neural features provide complementary benefits. Through comparative experiments using baseline random selection methods and distribution aligned approaches, we provide insights into the interplay between data selection strategies and model training efficacy. All code for this study can be found on \\href{https://github.com/jgu13/HIR-Hybrid-Importance-Resampling-for-Language-Models}{github repository}.","sentences":["Data selection is critical for enhancing the performance of language models, particularly when aligning training datasets with a desired target distribution.","This study explores the effects of different data selection methods and feature types on model performance.","We evaluate whether selecting data subsets can influence downstream tasks, whether n-gram features improve alignment with target distributions, and whether embedding-based neural features provide complementary benefits.","Through comparative experiments using baseline random selection methods and distribution aligned approaches, we provide insights into the interplay between data selection strategies and model training efficacy.","All code for this study can be found on \\href{https://github.com/jgu13/HIR-Hybrid-Importance-Resampling-for-Language-Models}{github repository}."],"url":"http://arxiv.org/abs/2501.03826v1"}
{"created":"2025-01-07 14:36:33","title":"Online Reinforcement Learning-Based Dynamic Adaptive Evaluation Function for Real-Time Strategy Tasks","abstract":"Effective evaluation of real-time strategy tasks requires adaptive mechanisms to cope with dynamic and unpredictable environments. This study proposes a method to improve evaluation functions for real-time responsiveness to battle-field situation changes, utilizing an online reinforcement learning-based dynam-ic weight adjustment mechanism within the real-time strategy game. Building on traditional static evaluation functions, the method employs gradient descent in online reinforcement learning to update weights dynamically, incorporating weight decay techniques to ensure stability. Additionally, the AdamW optimizer is integrated to adjust the learning rate and decay rate of online reinforcement learning in real time, further reducing the dependency on manual parameter tun-ing. Round-robin competition experiments demonstrate that this method signifi-cantly enhances the application effectiveness of the Lanchester combat model evaluation function, Simple evaluation function, and Simple Sqrt evaluation function in planning algorithms including IDABCD, IDRTMinimax, and Port-folio AI. The method achieves a notable improvement in scores, with the en-hancement becoming more pronounced as the map size increases. Furthermore, the increase in evaluation function computation time induced by this method is kept below 6% for all evaluation functions and planning algorithms. The pro-posed dynamic adaptive evaluation function demonstrates a promising approach for real-time strategy task evaluation.","sentences":["Effective evaluation of real-time strategy tasks requires adaptive mechanisms to cope with dynamic and unpredictable environments.","This study proposes a method to improve evaluation functions for real-time responsiveness to battle-field situation changes, utilizing an online reinforcement learning-based dynam-ic weight adjustment mechanism within the real-time strategy game.","Building on traditional static evaluation functions, the method employs gradient descent in online reinforcement learning to update weights dynamically, incorporating weight decay techniques to ensure stability.","Additionally, the AdamW optimizer is integrated to adjust the learning rate and decay rate of online reinforcement learning in real time, further reducing the dependency on manual parameter tun-ing.","Round-robin competition experiments demonstrate that this method signifi-cantly enhances the application effectiveness of the Lanchester combat model evaluation function, Simple evaluation function, and Simple Sqrt evaluation function in planning algorithms including IDABCD, IDRTMinimax, and Port-folio AI.","The method achieves a notable improvement in scores, with the en-hancement becoming more pronounced as the map size increases.","Furthermore, the increase in evaluation function computation time induced by this method is kept below 6% for all evaluation functions and planning algorithms.","The pro-posed dynamic adaptive evaluation function demonstrates a promising approach for real-time strategy task evaluation."],"url":"http://arxiv.org/abs/2501.03824v1"}
{"created":"2025-01-07 14:32:36","title":"An innovative mixed reality approach for Robotics Surgery","abstract":"Robotic-assisted procedures offer numerous advantages over traditional approaches, including improved dexterity, reduced fatigue, minimized trauma, and superior outcomes. However, the main challenge of these systems remains the poor visualization and perception of the surgical field. The goal of this paper is to provide an innovative approach concerning an application able to improve the surgical procedures offering assistance in both preplanning and intraoperative steps of the surgery. The system has been designed to offer a better understanding of the patient through techniques that provide medical images visualization, 3D anatomical structures perception and robotic planning. The application was designed to be intuitive and user friendly, providing an augmented reality experience through the Hololens 2 device. It was tested in laboratory conditions, yielding positive results.","sentences":["Robotic-assisted procedures offer numerous advantages over traditional approaches, including improved dexterity, reduced fatigue, minimized trauma, and superior outcomes.","However, the main challenge of these systems remains the poor visualization and perception of the surgical field.","The goal of this paper is to provide an innovative approach concerning an application able to improve the surgical procedures offering assistance in both preplanning and intraoperative steps of the surgery.","The system has been designed to offer a better understanding of the patient through techniques that provide medical images visualization, 3D anatomical structures perception and robotic planning.","The application was designed to be intuitive and user friendly, providing an augmented reality experience through the Hololens 2 device.","It was tested in laboratory conditions, yielding positive results."],"url":"http://arxiv.org/abs/2501.03819v1"}
{"created":"2025-01-07 14:24:49","title":"Extending ChatGPT with a Browserless System for Web Product Price Extraction","abstract":"With the advenement of ChatGPT, we can find very clean, precise answers to a varied amount of questions. However, for questions such as 'find the price of the lemon cake at zingerman's', the answer looks like 'I can't browse the web right now'. In this paper, we propose a system, called Wextractor, which extends ChatGPT to answer questions as the one mentioned before. Obviously, our system cannot be labeled as `artificial intelligence'. Simply, it offers to cover a kind of transactional search that is not included in the current version of ChatGPT. Moreover, Wextractor includes two improvements with respect to the initial version: social extraction and pointing pattern extraction to improve the answer speed.","sentences":["With the advenement of ChatGPT, we can find very clean, precise answers to a varied amount of questions.","However, for questions such as 'find the price of the lemon cake at zingerman's', the answer looks like 'I can't browse the web right now'.","In this paper, we propose a system, called Wextractor, which extends ChatGPT to answer questions as the one mentioned before.","Obviously, our system cannot be labeled as `artificial intelligence'.","Simply, it offers to cover a kind of transactional search that is not included in the current version of ChatGPT.","Moreover, Wextractor includes two improvements with respect to the initial version: social extraction and pointing pattern extraction to improve the answer speed."],"url":"http://arxiv.org/abs/2501.03811v1"}
{"created":"2025-01-07 14:21:24","title":"Private, Auditable, and Distributed Ledger for Financial Institutes","abstract":"Distributed ledger technology offers several advantages for banking and finance industry, including efficient transaction processing and cross-party transaction reconciliation. The key challenges for adoption of this technology in financial institutes are (a) the building of a privacy-preserving ledger, (b) supporting auditing and regulatory requirements, and (c) flexibility to adapt to complex use-cases with multiple digital assets and actors. This paper proposes a framework for a private, audit-able, and distributed ledger (PADL) that adapts easily to fundamental use-cases within financial institutes. PADL employs widely-used cryptography schemes combined with zero-knowledge proofs to propose a transaction scheme for a `table' like ledger. It enables fast confidential peer-to-peer multi-asset transactions, and transaction graph anonymity, in a no-trust setup, but with customized privacy. We prove that integrity and anonymity of PADL is secured against a strong threat model. Furthermore, we showcase three fundamental real-life use-cases, namely, an assets exchange ledger, a settlement ledger, and a bond market ledger. Based on these use-cases we show that PADL supports smooth-lined inter-assets auditing while preserving privacy of the participants. For example, we show how a bank can be audited for its liquidity or credit risk without violation of privacy of itself or any other party, or how can PADL ensures honest coupon rate payment in bond market without sharing investors values. Finally, our evaluation shows PADL's advantage in performance against previous relevant schemes.","sentences":["Distributed ledger technology offers several advantages for banking and finance industry, including efficient transaction processing and cross-party transaction reconciliation.","The key challenges for adoption of this technology in financial institutes are (a) the building of a privacy-preserving ledger, (b) supporting auditing and regulatory requirements, and (c) flexibility to adapt to complex use-cases with multiple digital assets and actors.","This paper proposes a framework for a private, audit-able, and distributed ledger (PADL) that adapts easily to fundamental use-cases within financial institutes.","PADL employs widely-used cryptography schemes combined with zero-knowledge proofs to propose a transaction scheme for a `table' like ledger.","It enables fast confidential peer-to-peer multi-asset transactions, and transaction graph anonymity, in a no-trust setup, but with customized privacy.","We prove that integrity and anonymity of PADL is secured against a strong threat model.","Furthermore, we showcase three fundamental real-life use-cases, namely, an assets exchange ledger, a settlement ledger, and a bond market ledger.","Based on these use-cases we show that PADL supports smooth-lined inter-assets auditing while preserving privacy of the participants.","For example, we show how a bank can be audited for its liquidity or credit risk without violation of privacy of itself or any other party, or how can PADL ensures honest coupon rate payment in bond market without sharing investors values.","Finally, our evaluation shows PADL's advantage in performance against previous relevant schemes."],"url":"http://arxiv.org/abs/2501.03808v1"}
{"created":"2025-01-07 14:17:47","title":"Detecting the Undetectable: Assessing the Efficacy of Current Spoof Detection Methods Against Seamless Speech Edits","abstract":"Neural speech editing advancements have raised concerns about their misuse in spoofing attacks. Traditional partially edited speech corpora primarily focus on cut-and-paste edits, which, while maintaining speaker consistency, often introduce detectable discontinuities. Recent methods, like A\\textsuperscript{3}T and Voicebox, improve transitions by leveraging contextual information. To foster spoofing detection research, we introduce the Speech INfilling Edit (SINE) dataset, created with Voicebox. We detailed the process of re-implementing Voicebox training and dataset creation. Subjective evaluations confirm that speech edited using this novel technique is more challenging to detect than conventional cut-and-paste methods. Despite human difficulty, experimental results demonstrate that self-supervised-based detectors can achieve remarkable performance in detection, localization, and generalization across different edit methods. The dataset and related models will be made publicly available.","sentences":["Neural speech editing advancements have raised concerns about their misuse in spoofing attacks.","Traditional partially edited speech corpora primarily focus on cut-and-paste edits, which, while maintaining speaker consistency, often introduce detectable discontinuities.","Recent methods, like A\\textsuperscript{3}T and Voicebox, improve transitions by leveraging contextual information.","To foster spoofing detection research, we introduce the Speech INfilling Edit (SINE) dataset, created with Voicebox.","We detailed the process of re-implementing Voicebox training and dataset creation.","Subjective evaluations confirm that speech edited using this novel technique is more challenging to detect than conventional cut-and-paste methods.","Despite human difficulty, experimental results demonstrate that self-supervised-based detectors can achieve remarkable performance in detection, localization, and generalization across different edit methods.","The dataset and related models will be made publicly available."],"url":"http://arxiv.org/abs/2501.03805v1"}
