{"created":"2024-09-16 17:59:52","title":"RetrievalAttention: Accelerating Long-Context LLM Inference via Vector Retrieval","abstract":"Transformer-based large Language Models (LLMs) become increasingly important in various domains. However, the quadratic time complexity of attention operation poses a significant challenge for scaling to longer contexts due to the extremely high inference latency and GPU memory consumption for caching key-value (KV) vectors. This paper proposes RetrievalAttention, a training-free approach to accelerate attention computation. To leverage the dynamic sparse property of attention, RetrievalAttention builds approximate nearest neighbor search (ANNS) indexes upon KV vectors in CPU memory and retrieves the most relevant ones via vector search during generation. Due to the out-of-distribution (OOD) between query vectors and key vectors, off-the-shelf ANNS indexes still need to scan O(N) (usually 30% of all keys) data for accurate retrieval, which fails to exploit the high sparsity. RetrievalAttention first identifies the OOD challenge of ANNS-based attention, and addresses it via an attention-aware vector search algorithm that can adapt to queries and only access 1--3% of data, thus achieving a sub-linear time complexity. RetrievalAttention greatly reduces the inference cost of long-context LLM with much lower GPU memory requirements while maintaining the model accuracy. Especially, RetrievalAttention only needs 16GB GPU memory for serving 128K tokens in LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds on a single NVIDIA RTX4090 (24GB).","sentences":["Transformer-based large Language Models (LLMs) become increasingly important in various domains.","However, the quadratic time complexity of attention operation poses a significant challenge for scaling to longer contexts due to the extremely high inference latency and GPU memory consumption for caching key-value (KV) vectors.","This paper proposes RetrievalAttention, a training-free approach to accelerate attention computation.","To leverage the dynamic sparse property of attention, RetrievalAttention builds approximate nearest neighbor search (ANNS) indexes upon KV vectors in CPU memory and retrieves the most relevant ones via vector search during generation.","Due to the out-of-distribution (OOD) between query vectors and key vectors, off-the-shelf ANNS indexes still need to scan O(N) (usually 30% of all keys) data for accurate retrieval, which fails to exploit the high sparsity.","RetrievalAttention first identifies the OOD challenge of ANNS-based attention, and addresses it via an attention-aware vector search algorithm that can adapt to queries and only access 1--3% of data, thus achieving a sub-linear time complexity.","RetrievalAttention greatly reduces the inference cost of long-context LLM with much lower GPU memory requirements while maintaining the model accuracy.","Especially, RetrievalAttention only needs 16GB GPU memory for serving 128K tokens in LLMs with 8B parameters, which is capable of generating one token in 0.188 seconds on a single NVIDIA RTX4090 (24GB)."],"url":"http://arxiv.org/abs/2409.10516v1"}
{"created":"2024-09-16 17:57:59","title":"Enhancing Video Transmission with Machine Learning based Routing in Software-Defined Networks","abstract":"Our study uses the centralized, flexible, dynamic, and programmable structure of Software-Defined networks (SDN) to overcome the problems. Although SDN effectively addresses the challenges present in traditional networks, it still requires further enhancements to achieve a more optimized network architecture. The Floodlight controller utilized in this study employs metrics such as hop count, which provides limited information for routing. In scenarios such as video transmission, this situation is insufficient and the need for optimization arises. For this purpose, an artificial intelligence (AI) based routing algorithm is proposed between the server and the client in the scenario based on NSFNET topology. The topology designed with the Floodlight controller in the Mininet simulation environment includes a client, a server, and 14 switches. A realistic network environment is provided by adding different receivers and creating TCP traffic between these receivers using the iperf3 tool. In three scenarios, video streaming is performed using the FFmpeg tool, and 49 path metrics such as RTT, throughput, and loss are recorded. In these scenarios, PSNR and SSIM calculations are made to observe the differences between the transmitted and the original video in congested and uncongested environments. Due to the lack of a dataset suitable for the proposed network environment in the literature, a new dataset consisting of 876 records is created using continuously transmitted video traffic. Low and high traffic levels are created within the dataset, and different machine learning techniques such as KNN, Random Forest, SVM, AdaBoost, Logistic Regression and XGBoost are applied using the features that affect the traffic levels.","sentences":["Our study uses the centralized, flexible, dynamic, and programmable structure of Software-Defined networks (SDN) to overcome the problems.","Although SDN effectively addresses the challenges present in traditional networks, it still requires further enhancements to achieve a more optimized network architecture.","The Floodlight controller utilized in this study employs metrics such as hop count, which provides limited information for routing.","In scenarios such as video transmission, this situation is insufficient and the need for optimization arises.","For this purpose, an artificial intelligence (AI) based routing algorithm is proposed between the server and the client in the scenario based on NSFNET topology.","The topology designed with the Floodlight controller in the Mininet simulation environment includes a client, a server, and 14 switches.","A realistic network environment is provided by adding different receivers and creating TCP traffic between these receivers using the iperf3 tool.","In three scenarios, video streaming is performed using the FFmpeg tool, and 49 path metrics such as RTT, throughput, and loss are recorded.","In these scenarios, PSNR and SSIM calculations are made to observe the differences between the transmitted and the original video in congested and uncongested environments.","Due to the lack of a dataset suitable for the proposed network environment in the literature, a new dataset consisting of 876 records is created using continuously transmitted video traffic.","Low and high traffic levels are created within the dataset, and different machine learning techniques such as KNN, Random Forest, SVM, AdaBoost, Logistic Regression and XGBoost are applied using the features that affect the traffic levels."],"url":"http://arxiv.org/abs/2409.10512v1"}
{"created":"2024-09-16 17:56:57","title":"Weak Superimposed Codes of Improved Asymptotic Rate and Their Randomized Construction","abstract":"Weak superimposed codes are combinatorial structures related closely to generalized cover-free families, superimposed codes, and disjunct matrices in that they are only required to satisfy similar but less stringent conditions. This class of codes may also be seen as a stricter variant of what are known as locally thin families in combinatorics. Originally, weak superimposed codes were introduced in the context of multimedia content protection against illegal distribution of copies under the assumption that a coalition of malicious users may employ the averaging attack with adversarial noise. As in many other kinds of codes in information theory, it is of interest and importance in the study of weak superimposed codes to find the highest achievable rate in the asymptotic regime and give an efficient construction that produces an infinite sequence of codes that achieve it. Here, we prove a tighter lower bound than the sharpest known one on the rate of optimal weak superimposed codes and give a polynomial-time randomized construction algorithm for codes that asymptotically attain our improved bound with high probability. Our probabilistic approach is versatile and applicable to many other related codes and arrays.","sentences":["Weak superimposed codes are combinatorial structures related closely to generalized cover-free families, superimposed codes, and disjunct matrices in that they are only required to satisfy similar but less stringent conditions.","This class of codes may also be seen as a stricter variant of what are known as locally thin families in combinatorics.","Originally, weak superimposed codes were introduced in the context of multimedia content protection against illegal distribution of copies under the assumption that a coalition of malicious users may employ the averaging attack with adversarial noise.","As in many other kinds of codes in information theory, it is of interest and importance in the study of weak superimposed codes to find the highest achievable rate in the asymptotic regime and give an efficient construction that produces an infinite sequence of codes that achieve it.","Here, we prove a tighter lower bound than the sharpest known one on the rate of optimal weak superimposed codes and give a polynomial-time randomized construction algorithm for codes that asymptotically attain our improved bound with high probability.","Our probabilistic approach is versatile and applicable to many other related codes and arrays."],"url":"http://arxiv.org/abs/2409.10511v1"}
{"created":"2024-09-16 17:55:58","title":"Pennsieve - A Collaborative Platform for Translational Neuroscience and Beyond","abstract":"The exponential growth of neuroscientific data necessitates platforms that facilitate data management and multidisciplinary collaboration. In this paper, we introduce Pennsieve - an open-source, cloud-based scientific data management platform built to meet these needs. Pennsieve supports complex multimodal datasets and provides tools for data visualization and analyses. It takes a comprehensive approach to data integration, enabling researchers to define custom metadata schemas and utilize advanced tools to filter and query their data. Pennsieve's modular architecture allows external applications to extend its capabilities, and collaborative workspaces with peer-reviewed data publishing mechanisms promote high-quality datasets optimized for downstream analysis, both in the cloud and on-premises.   Pennsieve forms the core for major neuroscience research programs including the NIH SPARC Initiative, NIH HEAL Initiative's PRECISION Human Pain Network, and NIH HEAL RE-JOIN Initiative. It serves more than 80 research groups worldwide, along with several large-scale, inter-institutional projects at clinical sites through the University of Pennsylvania. Underpinning the SPARC.Science, Epilepsy.Science, and Pennsieve Discover portals, Pennsieve stores over 125 TB of scientific data, with 35 TB of data publicly available across more than 350 high-impact datasets. It adheres to the findable, accessible, interoperable, and reusable (FAIR) principles of data sharing and is recognized as one of the NIH-approved Data Repositories. By facilitating scientific data management, discovery, and analysis, Pennsieve fosters a robust and collaborative research ecosystem for neuroscience and beyond.","sentences":["The exponential growth of neuroscientific data necessitates platforms that facilitate data management and multidisciplinary collaboration.","In this paper, we introduce Pennsieve - an open-source, cloud-based scientific data management platform built to meet these needs.","Pennsieve supports complex multimodal datasets and provides tools for data visualization and analyses.","It takes a comprehensive approach to data integration, enabling researchers to define custom metadata schemas and utilize advanced tools to filter and query their data.","Pennsieve's modular architecture allows external applications to extend its capabilities, and collaborative workspaces with peer-reviewed data publishing mechanisms promote high-quality datasets optimized for downstream analysis, both in the cloud and on-premises.   ","Pennsieve forms the core for major neuroscience research programs including the NIH SPARC Initiative, NIH HEAL Initiative's PRECISION Human Pain Network, and NIH HEAL RE-JOIN Initiative.","It serves more than 80 research groups worldwide, along with several large-scale, inter-institutional projects at clinical sites through the University of Pennsylvania.","Underpinning the SPARC.Science, Epilepsy.","Science, and Pennsieve Discover portals, Pennsieve stores over 125 TB of scientific data, with 35 TB of data publicly available across more than 350 high-impact datasets.","It adheres to the findable, accessible, interoperable, and reusable (FAIR) principles of data sharing and is recognized as one of the NIH-approved Data Repositories.","By facilitating scientific data management, discovery, and analysis, Pennsieve fosters a robust and collaborative research ecosystem for neuroscience and beyond."],"url":"http://arxiv.org/abs/2409.10509v1"}
{"created":"2024-09-16 17:52:36","title":"Context-aware Code Segmentation for C-to-Rust Translation using Large Language Models","abstract":"There is strong motivation to translate C code into Rust code due to the continuing threat of memory safety vulnerabilities in existing C programs and the significant attention paid to Rust as an alternative to the C language. While large language models (LLMs) show promise for automating this translation by generating more natural and safer code than rule-based methods, previous studies have shown that LLM-generated Rust code often fails to compile, even for relatively small C programs, due to significant differences between the two languages and context window limitations. We propose an LLM-based translation scheme that improves the success rate of translating large-scale C code into compilable Rust code. Our approach involves three key techniques: (1) pre-processing the C code to better align its structure and expressions with Rust, (2) segmenting the code into optimally sized translation units to avoid exceeding the LLM's context window limits, and (3) iteratively compiling and repairing errors while maintaining consistency between translation units using context-supplementing prompts. Compilation success is an essential first step in achieving functional equivalence, as only compilable code can be further tested. In experiments with 20 benchmark C programs, including those exceeding 4 kilo lines of code, we successfully translated all programs into compilable Rust code without losing corresponding parts of the original code.","sentences":["There is strong motivation to translate C code into Rust code due to the continuing threat of memory safety vulnerabilities in existing C programs and the significant attention paid to Rust as an alternative to the C language.","While large language models (LLMs) show promise for automating this translation by generating more natural and safer code than rule-based methods, previous studies have shown that LLM-generated Rust code often fails to compile, even for relatively small C programs, due to significant differences between the two languages and context window limitations.","We propose an LLM-based translation scheme that improves the success rate of translating large-scale C code into compilable Rust code.","Our approach involves three key techniques: (1) pre-processing the C code to better align its structure and expressions with Rust, (2) segmenting the code into optimally sized translation units to avoid exceeding the LLM's context window limits, and (3) iteratively compiling and repairing errors while maintaining consistency between translation units using context-supplementing prompts.","Compilation success is an essential first step in achieving functional equivalence, as only compilable code can be further tested.","In experiments with 20 benchmark C programs, including those exceeding 4 kilo lines of code, we successfully translated all programs into compilable Rust code without losing corresponding parts of the original code."],"url":"http://arxiv.org/abs/2409.10506v1"}
{"created":"2024-09-16 17:45:40","title":"DILA: Dictionary Label Attention for Mechanistic Interpretability in High-dimensional Multi-label Medical Coding Prediction","abstract":"Predicting high-dimensional or extreme multilabels, such as in medical coding, requires both accuracy and interpretability. Existing works often rely on local interpretability methods, failing to provide comprehensive explanations of the overall mechanism behind each label prediction within a multilabel set. We propose a mechanistic interpretability module called DIctionary Label Attention (\\method) that disentangles uninterpretable dense embeddings into a sparse embedding space, where each nonzero element (a dictionary feature) represents a globally learned medical concept. Through human evaluations, we show that our sparse embeddings are more human understandable than its dense counterparts by at least 50 percent. Our automated dictionary feature identification pipeline, leveraging large language models (LLMs), uncovers thousands of learned medical concepts by examining and summarizing the highest activating tokens for each dictionary feature. We represent the relationships between dictionary features and medical codes through a sparse interpretable matrix, enhancing the mechanistic and global understanding of the model's predictions while maintaining competitive performance and scalability without extensive human annotation.","sentences":["Predicting high-dimensional or extreme multilabels, such as in medical coding, requires both accuracy and interpretability.","Existing works often rely on local interpretability methods, failing to provide comprehensive explanations of the overall mechanism behind each label prediction within a multilabel set.","We propose a mechanistic interpretability module called DIctionary Label Attention (\\method) that disentangles uninterpretable dense embeddings into a sparse embedding space, where each nonzero element (a dictionary feature) represents a globally learned medical concept.","Through human evaluations, we show that our sparse embeddings are more human understandable than its dense counterparts by at least 50 percent.","Our automated dictionary feature identification pipeline, leveraging large language models (LLMs), uncovers thousands of learned medical concepts by examining and summarizing the highest activating tokens for each dictionary feature.","We represent the relationships between dictionary features and medical codes through a sparse interpretable matrix, enhancing the mechanistic and global understanding of the model's predictions while maintaining competitive performance and scalability without extensive human annotation."],"url":"http://arxiv.org/abs/2409.10504v1"}
{"created":"2024-09-16 17:42:15","title":"Causal Language Modeling Can Elicit Search and Reasoning Capabilities on Logic Puzzles","abstract":"Causal language modeling using the Transformer architecture has yielded remarkable capabilities in Large Language Models (LLMs) over the last few years. However, the extent to which fundamental search and reasoning capabilities emerged within LLMs remains a topic of ongoing debate. In this work, we study if causal language modeling can learn a complex task such as solving Sudoku puzzles. To solve a Sudoku, the model is first required to search over all empty cells of the puzzle to decide on a cell to fill and then apply an appropriate strategy to fill the decided cell. Sometimes, the application of a strategy only results in thinning down the possible values in a cell rather than concluding the exact value of the cell. In such cases, multiple strategies are applied one after the other to fill a single cell. We observe that Transformer models trained on this synthetic task can indeed learn to solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly) when trained on a logical sequence of steps taken by a solver. We find that training Transformers with the logical sequence of steps is necessary and without such training, they fail to learn Sudoku. We also extend our analysis to Zebra puzzles (known as Einstein puzzles) and show that the model solves $92.04 \\%$ of the puzzles fully correctly. In addition, we study the internal representations of the trained Transformer and find that through linear probing, we can decode information about the set of possible values in any given cell from them, pointing to the presence of a strong reasoning engine implicit in the Transformer weights.","sentences":["Causal language modeling using the Transformer architecture has yielded remarkable capabilities in Large Language Models (LLMs) over the last few years.","However, the extent to which fundamental search and reasoning capabilities emerged within LLMs remains a topic of ongoing debate.","In this work, we study if causal language modeling can learn a complex task such as solving Sudoku puzzles.","To solve a Sudoku, the model is first required to search over all empty cells of the puzzle to decide on a cell to fill and then apply an appropriate strategy to fill the decided cell.","Sometimes, the application of a strategy only results in thinning down the possible values in a cell rather than concluding the exact value of the cell.","In such cases, multiple strategies are applied one after the other to fill a single cell.","We observe that Transformer models trained on this synthetic task can indeed learn to solve Sudokus (our model solves $94.21\\%$ of the puzzles fully correctly) when trained on a logical sequence of steps taken by a solver.","We find that training Transformers with the logical sequence of steps is necessary and without such training, they fail to learn Sudoku.","We also extend our analysis to Zebra puzzles (known as Einstein puzzles) and show that the model solves $92.04 \\%$ of the puzzles fully correctly.","In addition, we study the internal representations of the trained Transformer and find that through linear probing, we can decode information about the set of possible values in any given cell from them, pointing to the presence of a strong reasoning engine implicit in the Transformer weights."],"url":"http://arxiv.org/abs/2409.10502v1"}
{"created":"2024-09-16 17:41:45","title":"Partial Distribution Matching via Partial Wasserstein Adversarial Networks","abstract":"This paper studies the problem of distribution matching (DM), which is a fundamental machine learning problem seeking to robustly align two probability distributions. Our approach is established on a relaxed formulation, called partial distribution matching (PDM), which seeks to match a fraction of the distributions instead of matching them completely. We theoretically derive the Kantorovich-Rubinstein duality for the partial Wasserstain-1 (PW) discrepancy, and develop a partial Wasserstein adversarial network (PWAN) that efficiently approximates the PW discrepancy based on this dual form. Partial matching can then be achieved by optimizing the network using gradient descent. Two practical tasks, point set registration and partial domain adaptation are investigated, where the goals are to partially match distributions in 3D space and high-dimensional feature space respectively. The experiment results confirm that the proposed PWAN effectively produces highly robust matching results, performing better or on par with the state-of-the-art methods.","sentences":["This paper studies the problem of distribution matching (DM), which is a fundamental machine learning problem seeking to robustly align two probability distributions.","Our approach is established on a relaxed formulation, called partial distribution matching (PDM), which seeks to match a fraction of the distributions instead of matching them completely.","We theoretically derive the Kantorovich-Rubinstein duality for the partial Wasserstain-1 (PW) discrepancy, and develop a partial Wasserstein adversarial network (PWAN) that efficiently approximates the PW discrepancy based on this dual form.","Partial matching can then be achieved by optimizing the network using gradient descent.","Two practical tasks, point set registration and partial domain adaptation are investigated, where the goals are to partially match distributions in 3D space and high-dimensional feature space respectively.","The experiment results confirm that the proposed PWAN effectively produces highly robust matching results, performing better or on par with the state-of-the-art methods."],"url":"http://arxiv.org/abs/2409.10499v1"}
{"created":"2024-09-16 17:28:21","title":"MusicLIME: Explainable Multimodal Music Understanding","abstract":"Multimodal models are critical for music understanding tasks, as they capture the complex interplay between audio and lyrics. However, as these models become more prevalent, the need for explainability grows-understanding how these systems make decisions is vital for ensuring fairness, reducing bias, and fostering trust. In this paper, we introduce MusicLIME, a model-agnostic feature importance explanation method designed for multimodal music models. Unlike traditional unimodal methods, which analyze each modality separately without considering the interaction between them, often leading to incomplete or misleading explanations, MusicLIME reveals how audio and lyrical features interact and contribute to predictions, providing a holistic view of the model's decision-making. Additionally, we enhance local explanations by aggregating them into global explanations, giving users a broader perspective of model behavior. Through this work, we contribute to improving the interpretability of multimodal music models, empowering users to make informed choices, and fostering more equitable, fair, and transparent music understanding systems.","sentences":["Multimodal models are critical for music understanding tasks, as they capture the complex interplay between audio and lyrics.","However, as these models become more prevalent, the need for explainability grows-understanding how these systems make decisions is vital for ensuring fairness, reducing bias, and fostering trust.","In this paper, we introduce MusicLIME, a model-agnostic feature importance explanation method designed for multimodal music models.","Unlike traditional unimodal methods, which analyze each modality separately without considering the interaction between them, often leading to incomplete or misleading explanations, MusicLIME reveals how audio and lyrical features interact and contribute to predictions, providing a holistic view of the model's decision-making.","Additionally, we enhance local explanations by aggregating them into global explanations, giving users a broader perspective of model behavior.","Through this work, we contribute to improving the interpretability of multimodal music models, empowering users to make informed choices, and fostering more equitable, fair, and transparent music understanding systems."],"url":"http://arxiv.org/abs/2409.10496v1"}
{"created":"2024-09-16 17:27:27","title":"Incorporating Classifier-Free Guidance in Diffusion Model-Based Recommendation","abstract":"This paper presents a diffusion-based recommender system that incorporates classifier-free guidance. Most current recommender systems provide recommendations using conventional methods such as collaborative or content-based filtering. Diffusion is a new approach to generative AI that improves on previous generative AI approaches such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs). We incorporate diffusion in a recommender system that mirrors the sequence users take when browsing and rating items. Although a few current recommender systems incorporate diffusion, they do not incorporate classifier-free guidance, a new innovation in diffusion models as a whole. In this paper, we present a diffusion recommender system that augments the underlying recommender system model for improved performance and also incorporates classifier-free guidance. Our findings show improvements over state-of-the-art recommender systems for most metrics for several recommendation tasks on a variety of datasets. In particular, our approach demonstrates the potential to provide better recommendations when data is sparse.","sentences":["This paper presents a diffusion-based recommender system that incorporates classifier-free guidance.","Most current recommender systems provide recommendations using conventional methods such as collaborative or content-based filtering.","Diffusion is a new approach to generative AI that improves on previous generative AI approaches such as Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs).","We incorporate diffusion in a recommender system that mirrors the sequence users take when browsing and rating items.","Although a few current recommender systems incorporate diffusion, they do not incorporate classifier-free guidance, a new innovation in diffusion models as a whole.","In this paper, we present a diffusion recommender system that augments the underlying recommender system model for improved performance and also incorporates classifier-free guidance.","Our findings show improvements over state-of-the-art recommender systems for most metrics for several recommendation tasks on a variety of datasets.","In particular, our approach demonstrates the potential to provide better recommendations when data is sparse."],"url":"http://arxiv.org/abs/2409.10494v1"}
{"created":"2024-09-16 17:24:29","title":"Radar Teach and Repeat: Architecture and Initial Field Testing","abstract":"Frequency-modulated continuous-wave (FMCW) scanning radar has emerged as an alternative to spinning LiDAR for state estimation on mobile robots. Radar's longer wavelength is less affected by small particulates, providing operational advantages in challenging environments such as dust, smoke, and fog. This paper presents Radar Teach and Repeat (RT&R): a full-stack radar system for long-term off-road robot autonomy. RT&R can drive routes reliably in off-road cluttered areas without any GPS. We benchmark the radar system's closed-loop path-tracking performance and compare it to its 3D LiDAR counterpart. 11.8 km of autonomous driving was completed without interventions using only radar and gyro for navigation. RT&R was evaluated on different routes with progressively less structured scene geometry. RT&R achieved lateral path-tracking root mean squared errors (RMSE) of 5.6 cm, 7.5 cm, and 12.1 cm as the routes became more challenging. On the robot we used for testing, these RMSE values are less than half of the width of one tire (24 cm). These same routes have worst-case errors of 21.7 cm, 24.0 cm, and 43.8 cm. We conclude that radar is a viable alternative to LiDAR for long-term autonomy in challenging off-road scenarios. The implementation of RT&R is open-source and available at: https://github.com/utiasASRL/vtr3.","sentences":["Frequency-modulated continuous-wave (FMCW) scanning radar has emerged as an alternative to spinning LiDAR for state estimation on mobile robots.","Radar's longer wavelength is less affected by small particulates, providing operational advantages in challenging environments such as dust, smoke, and fog.","This paper presents Radar Teach and Repeat (RT&R): a full-stack radar system for long-term off-road robot autonomy.","RT&R can drive routes reliably in off-road cluttered areas without any GPS.","We benchmark the radar system's closed-loop path-tracking performance and compare it to its 3D LiDAR counterpart.","11.8 km of autonomous driving was completed without interventions using only radar and gyro for navigation.","RT&R was evaluated on different routes with progressively less structured scene geometry.","RT&R achieved lateral path-tracking root mean squared errors (RMSE) of 5.6 cm, 7.5 cm, and 12.1 cm as the routes became more challenging.","On the robot we used for testing, these RMSE values are less than half of the width of one tire (24 cm).","These same routes have worst-case errors of 21.7 cm, 24.0 cm, and 43.8 cm.","We conclude that radar is a viable alternative to LiDAR for long-term autonomy in challenging off-road scenarios.","The implementation of RT&R is open-source and available at: https://github.com/utiasASRL/vtr3."],"url":"http://arxiv.org/abs/2409.10491v1"}
{"created":"2024-09-16 17:23:00","title":"Code Vulnerability Detection: A Comparative Analysis of Emerging Large Language Models","abstract":"The growing trend of vulnerability issues in software development as a result of a large dependence on open-source projects has received considerable attention recently. This paper investigates the effectiveness of Large Language Models (LLMs) in identifying vulnerabilities within codebases, with a focus on the latest advancements in LLM technology. Through a comparative analysis, we assess the performance of emerging LLMs, specifically Llama, CodeLlama, Gemma, and CodeGemma, alongside established state-of-the-art models such as BERT, RoBERTa, and GPT-3. Our study aims to shed light on the capabilities of LLMs in vulnerability detection, contributing to the enhancement of software security practices across diverse open-source repositories. We observe that CodeGemma achieves the highest F1-score of 58\\ and a Recall of 87\\, amongst the recent additions of large language models to detect software security vulnerabilities.","sentences":["The growing trend of vulnerability issues in software development as a result of a large dependence on open-source projects has received considerable attention recently.","This paper investigates the effectiveness of Large Language Models (LLMs) in identifying vulnerabilities within codebases, with a focus on the latest advancements in LLM technology.","Through a comparative analysis, we assess the performance of emerging LLMs, specifically Llama, CodeLlama, Gemma, and CodeGemma, alongside established state-of-the-art models such as BERT, RoBERTa, and GPT-3.","Our study aims to shed light on the capabilities of LLMs in vulnerability detection, contributing to the enhancement of software security practices across diverse open-source repositories.","We observe that CodeGemma achieves the highest F1-score of 58\\ and a Recall of 87\\, amongst the recent additions of large language models to detect software security vulnerabilities."],"url":"http://arxiv.org/abs/2409.10490v1"}
{"created":"2024-09-16 17:22:34","title":"Flash STU: Fast Spectral Transform Units","abstract":"This paper describes an efficient, open source PyTorch implementation of the Spectral Transform Unit. We investigate sequence prediction tasks over several modalities including language, robotics, and simulated dynamical systems. We find that for the same parameter count, the STU and its variants outperform the Transformer as well as other leading state space models across various modalities.","sentences":["This paper describes an efficient, open source PyTorch implementation of the Spectral Transform Unit.","We investigate sequence prediction tasks over several modalities including language, robotics, and simulated dynamical systems.","We find that for the same parameter count, the STU and its variants outperform the Transformer as well as other leading state space models across various modalities."],"url":"http://arxiv.org/abs/2409.10489v1"}
{"created":"2024-09-16 17:22:18","title":"Do Pre-trained Vision-Language Models Encode Object States?","abstract":"For a vision-language model (VLM) to understand the physical world, such as cause and effect, a first step is to capture the temporal dynamics of the visual world, for example how the physical states of objects evolve over time (e.g. a whole apple into a sliced apple). Our paper aims to investigate if VLMs pre-trained on web-scale data learn to encode object states, which can be extracted with zero-shot text prompts. We curate an object state recognition dataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models trained with contrastive and generative objectives. We observe that while these state-of-the-art vision-language models can reliably perform object recognition, they consistently fail to accurately distinguish the objects' physical states. Through extensive experiments, we identify three areas for improvements for VLMs to better encode object states, namely the quality of object localization, the architecture to bind concepts to objects, and the objective to learn discriminative visual and language encoders on object states. Data and code are released.","sentences":["For a vision-language model (VLM) to understand the physical world, such as cause and effect, a first step is to capture the temporal dynamics of the visual world, for example how the physical states of objects evolve over time (e.g. a whole apple into a sliced apple).","Our paper aims to investigate if VLMs pre-trained on web-scale data learn to encode object states, which can be extracted with zero-shot text prompts.","We curate an object state recognition dataset ChangeIt-Frames, and evaluate nine open-source VLMs, including models trained with contrastive and generative objectives.","We observe that while these state-of-the-art vision-language models can reliably perform object recognition, they consistently fail to accurately distinguish the objects' physical states.","Through extensive experiments, we identify three areas for improvements for VLMs to better encode object states, namely the quality of object localization, the architecture to bind concepts to objects, and the objective to learn discriminative visual and language encoders on object states.","Data and code are released."],"url":"http://arxiv.org/abs/2409.10488v1"}
{"created":"2024-09-16 17:18:11","title":"Schrodinger's Memory: Large Language Models","abstract":"Memory is the foundation of LLMs' functionality, yet past research has lacked an in-depth exploration of their memory capabilities and underlying theory. In this paper, we apply UAT theory to explain the memory mechanism of LLMs and propose a new approach for evaluating LLM performance by comparing the memory capacities of different models. Through extensive experiments, we validate our theory and the memory abilities of LLMs. Finally, we compare the capabilities of the human brain and LLMs, highlighting both their similarities and differences in terms of working mechanisms.","sentences":["Memory is the foundation of LLMs' functionality, yet past research has lacked an in-depth exploration of their memory capabilities and underlying theory.","In this paper, we apply UAT theory to explain the memory mechanism of LLMs and propose a new approach for evaluating LLM performance by comparing the memory capacities of different models.","Through extensive experiments, we validate our theory and the memory abilities of LLMs.","Finally, we compare the capabilities of the human brain and LLMs, highlighting both their similarities and differences in terms of working mechanisms."],"url":"http://arxiv.org/abs/2409.10482v1"}
{"created":"2024-09-16 17:17:47","title":"Exploring 3D Face Reconstruction and Fusion Methods for Face Verification: A Case-Study in Video Surveillance","abstract":"3D face reconstruction (3DFR) algorithms are based on specific assumptions tailored to distinct application scenarios. These assumptions limit their use when acquisition conditions, such as the subject's distance from the camera or the camera's characteristics, are different than expected, as typically happens in video surveillance. Additionally, 3DFR algorithms follow various strategies to address the reconstruction of a 3D shape from 2D data, such as statistical model fitting, photometric stereo, or deep learning. In the present study, we explore the application of three 3DFR algorithms representative of the SOTA, employing each one as the template set generator for a face verification system. The scores provided by each system are combined by score-level fusion. We show that the complementarity induced by different 3DFR algorithms improves performance when tests are conducted at never-seen-before distances from the camera and camera characteristics (cross-distance and cross-camera settings), thus encouraging further investigations on multiple 3DFR-based approaches.","sentences":["3D face reconstruction (3DFR) algorithms are based on specific assumptions tailored to distinct application scenarios.","These assumptions limit their use when acquisition conditions, such as the subject's distance from the camera or the camera's characteristics, are different than expected, as typically happens in video surveillance.","Additionally, 3DFR algorithms follow various strategies to address the reconstruction of a 3D shape from 2D data, such as statistical model fitting, photometric stereo, or deep learning.","In the present study, we explore the application of three 3DFR algorithms representative of the SOTA, employing each one as the template set generator for a face verification system.","The scores provided by each system are combined by score-level fusion.","We show that the complementarity induced by different 3DFR algorithms improves performance when tests are conducted at never-seen-before distances from the camera and camera characteristics (cross-distance and cross-camera settings), thus encouraging further investigations on multiple 3DFR-based approaches."],"url":"http://arxiv.org/abs/2409.10481v1"}
{"created":"2024-09-16 17:10:50","title":"SimInversion: A Simple Framework for Inversion-Based Text-to-Image Editing","abstract":"Diffusion models demonstrate impressive image generation performance with text guidance. Inspired by the learning process of diffusion, existing images can be edited according to text by DDIM inversion. However, the vanilla DDIM inversion is not optimized for classifier-free guidance and the accumulated error will result in the undesired performance. While many algorithms are developed to improve the framework of DDIM inversion for editing, in this work, we investigate the approximation error in DDIM inversion and propose to disentangle the guidance scale for the source and target branches to reduce the error while keeping the original framework. Moreover, a better guidance scale (i.e., 0.5) than default settings can be derived theoretically. Experiments on PIE-Bench show that our proposal can improve the performance of DDIM inversion dramatically without sacrificing efficiency.","sentences":["Diffusion models demonstrate impressive image generation performance with text guidance.","Inspired by the learning process of diffusion, existing images can be edited according to text by DDIM inversion.","However, the vanilla DDIM inversion is not optimized for classifier-free guidance and the accumulated error will result in the undesired performance.","While many algorithms are developed to improve the framework of DDIM inversion for editing, in this work, we investigate the approximation error in DDIM inversion and propose to disentangle the guidance scale for the source and target branches to reduce the error while keeping the original framework.","Moreover, a better guidance scale (i.e., 0.5) than default settings can be derived theoretically.","Experiments on PIE-Bench show that our proposal can improve the performance of DDIM inversion dramatically without sacrificing efficiency."],"url":"http://arxiv.org/abs/2409.10476v1"}
{"created":"2024-09-16 17:06:10","title":"MacDiff: Unified Skeleton Modeling with Masked Conditional Diffusion","abstract":"Self-supervised learning has proved effective for skeleton-based human action understanding. However, previous works either rely on contrastive learning that suffers false negative problems or are based on reconstruction that learns too much unessential low-level clues, leading to limited representations for downstream tasks. Recently, great advances have been made in generative learning, which is naturally a challenging yet meaningful pretext task to model the general underlying data distributions. However, the representation learning capacity of generative models is under-explored, especially for the skeletons with spacial sparsity and temporal redundancy. To this end, we propose Masked Conditional Diffusion (MacDiff) as a unified framework for human skeleton modeling. For the first time, we leverage diffusion models as effective skeleton representation learners. Specifically, we train a diffusion decoder conditioned on the representations extracted by a semantic encoder. Random masking is applied to encoder inputs to introduce a information bottleneck and remove redundancy of skeletons. Furthermore, we theoretically demonstrate that our generative objective involves the contrastive learning objective which aligns the masked and noisy views. Meanwhile, it also enforces the representation to complement for the noisy view, leading to better generalization performance. MacDiff achieves state-of-the-art performance on representation learning benchmarks while maintaining the competence for generative tasks. Moreover, we leverage the diffusion model for data augmentation, significantly enhancing the fine-tuning performance in scenarios with scarce labeled data. Our project is available at https://lehongwu.github.io/ECCV24MacDiff/.","sentences":["Self-supervised learning has proved effective for skeleton-based human action understanding.","However, previous works either rely on contrastive learning that suffers false negative problems or are based on reconstruction that learns too much unessential low-level clues, leading to limited representations for downstream tasks.","Recently, great advances have been made in generative learning, which is naturally a challenging yet meaningful pretext task to model the general underlying data distributions.","However, the representation learning capacity of generative models is under-explored, especially for the skeletons with spacial sparsity and temporal redundancy.","To this end, we propose Masked Conditional Diffusion (MacDiff) as a unified framework for human skeleton modeling.","For the first time, we leverage diffusion models as effective skeleton representation learners.","Specifically, we train a diffusion decoder conditioned on the representations extracted by a semantic encoder.","Random masking is applied to encoder inputs to introduce a information bottleneck and remove redundancy of skeletons.","Furthermore, we theoretically demonstrate that our generative objective involves the contrastive learning objective which aligns the masked and noisy views.","Meanwhile, it also enforces the representation to complement for the noisy view, leading to better generalization performance.","MacDiff achieves state-of-the-art performance on representation learning benchmarks while maintaining the competence for generative tasks.","Moreover, we leverage the diffusion model for data augmentation, significantly enhancing the fine-tuning performance in scenarios with scarce labeled data.","Our project is available at https://lehongwu.github.io/ECCV24MacDiff/."],"url":"http://arxiv.org/abs/2409.10473v1"}
{"created":"2024-09-16 17:05:46","title":"Towards Semantic Versioning of Open Pre-trained Language Model Releases on Hugging Face","abstract":"The proliferation of open Pre-trained Language Models (PTLMs) on model registry platforms like Hugging Face (HF) presents both opportunities and challenges for companies building products around them. Similar to traditional software dependencies, PTLMs continue to evolve after a release. However, the current state of release practices of PTLMs on model registry platforms are plagued by a variety of inconsistencies, such as ambiguous naming conventions and inaccessible model training documentation. Given the knowledge gap on current PTLM release practices, our empirical study uses a mixed-methods approach to analyze the releases of 52,227 PTLMs on the most well-known model registry, HF. Our results reveal 148 different naming practices for PTLM releases, with 40.87% of changes to model weight files not represented in the adopted name-based versioning practice or their documentation. In addition, we identified that the 52,227 PTLMs are derived from only 299 different base models (the modified original models used to create 52,227 PTLMs), with Fine-tuning and Quantization being the most prevalent modification methods applied to these base models. Significant gaps in release transparency, in terms of training dataset specifications and model card availability, still exist, highlighting the need for standardized documentation. While we identified a model naming practice explicitly differentiating between major and minor PTLM releases, we did not find any significant difference in the types of changes that went into either type of releases, suggesting that major/minor version numbers for PTLMs often are chosen arbitrarily. Our findings provide valuable insights to improve PTLM release practices, nudging the field towards more formal semantic versioning practices.","sentences":["The proliferation of open Pre-trained Language Models (PTLMs) on model registry platforms like Hugging Face (HF) presents both opportunities and challenges for companies building products around them.","Similar to traditional software dependencies, PTLMs continue to evolve after a release.","However, the current state of release practices of PTLMs on model registry platforms are plagued by a variety of inconsistencies, such as ambiguous naming conventions and inaccessible model training documentation.","Given the knowledge gap on current PTLM release practices, our empirical study uses a mixed-methods approach to analyze the releases of 52,227 PTLMs on the most well-known model registry, HF.","Our results reveal 148 different naming practices for PTLM releases, with 40.87% of changes to model weight files not represented in the adopted name-based versioning practice or their documentation.","In addition, we identified that the 52,227 PTLMs are derived from only 299 different base models (the modified original models used to create 52,227 PTLMs), with Fine-tuning and Quantization being the most prevalent modification methods applied to these base models.","Significant gaps in release transparency, in terms of training dataset specifications and model card availability, still exist, highlighting the need for standardized documentation.","While we identified a model naming practice explicitly differentiating between major and minor PTLM releases, we did not find any significant difference in the types of changes that went into either type of releases, suggesting that major/minor version numbers for PTLMs often are chosen arbitrarily.","Our findings provide valuable insights to improve PTLM release practices, nudging the field towards more formal semantic versioning practices."],"url":"http://arxiv.org/abs/2409.10472v1"}
{"created":"2024-09-16 17:01:10","title":"Real-Time Whole-Body Control of Legged Robots with Model-Predictive Path Integral Control","abstract":"This paper presents a system for enabling real-time synthesis of whole-body locomotion and manipulation policies for real-world legged robots. Motivated by recent advancements in robot simulation, we leverage the efficient parallelization capabilities of the MuJoCo simulator to achieve fast sampling over the robot state and action trajectories. Our results show surprisingly effective real-world locomotion and manipulation capabilities with a very simple control strategy. We demonstrate our approach on several hardware and simulation experiments: robust locomotion over flat and uneven terrains, climbing over a box whose height is comparable to the robot, and pushing a box to a goal position. To our knowledge, this is the first successful deployment of whole-body sampling-based MPC on real-world legged robot hardware. Experiment videos and code can be found at: https://whole-body-mppi.github.io/","sentences":["This paper presents a system for enabling real-time synthesis of whole-body locomotion and manipulation policies for real-world legged robots.","Motivated by recent advancements in robot simulation, we leverage the efficient parallelization capabilities of the MuJoCo simulator to achieve fast sampling over the robot state and action trajectories.","Our results show surprisingly effective real-world locomotion and manipulation capabilities with a very simple control strategy.","We demonstrate our approach on several hardware and simulation experiments: robust locomotion over flat and uneven terrains, climbing over a box whose height is comparable to the robot, and pushing a box to a goal position.","To our knowledge, this is the first successful deployment of whole-body sampling-based MPC on real-world legged robot hardware.","Experiment videos and code can be found at: https://whole-body-mppi.github.io/"],"url":"http://arxiv.org/abs/2409.10469v1"}
{"created":"2024-09-16 16:56:28","title":"New Direct Sum Tests","abstract":"A function $f:[n]^{d} \\to \\mathbb{F}_2$ is a \\defn{direct sum} if there are functions $L_i:[n]\\to \\mathbb{F}_2$ such that ${f(x) = \\sum_{i}L_i(x_i)}$. In this work we give multiple results related to the property testing of direct sums.   Our first result concerns a test proposed by Dinur and Golubev in 2019. We call their test the Diamond test and show that it is indeed a direct sum tester. More specifically, we show that if a function $f$ is $\\epsilon$-far from being a direct sum function, then the Diamond test rejects $f$ with probability at least $\\Omega_{n,\\epsilon}(1)$. Even in the case of $n = 2$, the Diamond test is, to the best of our knowledge, novel and yields a new tester for the classic property of affinity.   Apart from the Diamond test, we also analyze a broad family of direct sum tests, which at a high level, run an arbitrary affinity test on the restriction of $f$ to a random hypercube inside of $[n]^d$. This family of tests includes the direct sum test analyzed in \\cite{di19}, but does not include the Diamond test. As an application of our result, we obtain a direct sum test which works in the online adversary model of \\cite{KRV}.   Finally, we also discuss a Fourier analytic interpretation of the diamond tester in the $n=2$ case, as well as prove local correction results for direct sum as conjectured by Dinur and Golubev.","sentences":["A function $f:[n]^{d} \\to \\mathbb{F}_2$ is a \\defn{direct sum} if there are functions $L_i:[n]\\to \\mathbb{F}_2$ such that ${f(x) = \\sum_{i}L_i(x_i)}$. In this work we give multiple results related to the property testing of direct sums.   ","Our first result concerns a test proposed by Dinur and Golubev in 2019.","We call their test the Diamond test and show that it is indeed a direct sum tester.","More specifically, we show that if a function $f$ is $\\epsilon$-far from being a direct sum function, then the Diamond test rejects $f$ with probability at least $\\Omega_{n,\\epsilon}(1)$. Even in the case of $n = 2$, the Diamond test is, to the best of our knowledge, novel and yields a new tester for the classic property of affinity.   ","Apart from the Diamond test, we also analyze a broad family of direct sum tests, which at a high level, run an arbitrary affinity test on the restriction of $f$ to a random hypercube inside of $[n]^d$. This family of tests includes the direct sum test analyzed in \\cite{di19}, but does not include the Diamond test.","As an application of our result, we obtain a direct sum test which works in the online adversary model of \\cite{KRV}.   ","Finally, we also discuss a Fourier analytic interpretation of the diamond tester in the $n=2$ case, as well as prove local correction results for direct sum as conjectured by Dinur and Golubev."],"url":"http://arxiv.org/abs/2409.10464v1"}
{"created":"2024-09-16 16:56:08","title":"Kolmogorov-Arnold Networks in Low-Data Regimes: A Comparative Study with Multilayer Perceptrons","abstract":"Multilayer Perceptrons (MLPs) have long been a cornerstone in deep learning, known for their capacity to model complex relationships. Recently, Kolmogorov-Arnold Networks (KANs) have emerged as a compelling alternative, utilizing highly flexible learnable activation functions directly on network edges, a departure from the neuron-centric approach of MLPs. However, KANs significantly increase the number of learnable parameters, raising concerns about their effectiveness in data-scarce environments. This paper presents a comprehensive comparative study of MLPs and KANs from both algorithmic and experimental perspectives, with a focus on low-data regimes. We introduce an effective technique for designing MLPs with unique, parameterized activation functions for each neuron, enabling a more balanced comparison with KANs. Using empirical evaluations on simulated data and two real-world data sets from medicine and engineering, we explore the trade-offs between model complexity and accuracy, with particular attention to the role of network depth. Our findings show that MLPs with individualized activation functions achieve significantly higher predictive accuracy with only a modest increase in parameters, especially when the sample size is limited to around one hundred. For example, in a three-class classification problem within additive manufacturing, MLPs achieve a median accuracy of 0.91, significantly outperforming KANs, which only reach a median accuracy of 0.53 with default hyperparameters. These results offer valuable insights into the impact of activation function selection in neural networks.","sentences":["Multilayer Perceptrons (MLPs) have long been a cornerstone in deep learning, known for their capacity to model complex relationships.","Recently, Kolmogorov-Arnold Networks (KANs) have emerged as a compelling alternative, utilizing highly flexible learnable activation functions directly on network edges, a departure from the neuron-centric approach of MLPs.","However, KANs significantly increase the number of learnable parameters, raising concerns about their effectiveness in data-scarce environments.","This paper presents a comprehensive comparative study of MLPs and KANs from both algorithmic and experimental perspectives, with a focus on low-data regimes.","We introduce an effective technique for designing MLPs with unique, parameterized activation functions for each neuron, enabling a more balanced comparison with KANs.","Using empirical evaluations on simulated data and two real-world data sets from medicine and engineering, we explore the trade-offs between model complexity and accuracy, with particular attention to the role of network depth.","Our findings show that MLPs with individualized activation functions achieve significantly higher predictive accuracy with only a modest increase in parameters, especially when the sample size is limited to around one hundred.","For example, in a three-class classification problem within additive manufacturing, MLPs achieve a median accuracy of 0.91, significantly outperforming KANs, which only reach a median accuracy of 0.53 with default hyperparameters.","These results offer valuable insights into the impact of activation function selection in neural networks."],"url":"http://arxiv.org/abs/2409.10463v1"}
{"created":"2024-09-16 16:49:59","title":"Efficiently Crowdsourcing Visual Importance with Punch-Hole Annotation","abstract":"We introduce a novel crowdsourcing method for identifying important areas in graphical images through punch-hole labeling. Traditional methods, such as gaze trackers and mouse-based annotations, which generate continuous data, can be impractical in crowdsourcing scenarios. They require many participants, and the outcome data can be noisy. In contrast, our method first segments the graphical image with a grid and drops a portion of the patches (punch holes). Then, we iteratively ask the labeler to validate each annotation with holes, narrowing down the annotation only having the most important area. This approach aims to reduce annotation noise in crowdsourcing by standardizing the annotations while enhancing labeling efficiency and reliability. Preliminary findings from fundamental charts demonstrate that punch-hole labeling can effectively pinpoint critical regions. This also highlights its potential for broader application in visualization research, particularly in studying large-scale users' graphical perception. Our future work aims to enhance the algorithm to achieve faster labeling speed and prove its utility through large-scale experiments.","sentences":["We introduce a novel crowdsourcing method for identifying important areas in graphical images through punch-hole labeling.","Traditional methods, such as gaze trackers and mouse-based annotations, which generate continuous data, can be impractical in crowdsourcing scenarios.","They require many participants, and the outcome data can be noisy.","In contrast, our method first segments the graphical image with a grid and drops a portion of the patches (punch holes).","Then, we iteratively ask the labeler to validate each annotation with holes, narrowing down the annotation only having the most important area.","This approach aims to reduce annotation noise in crowdsourcing by standardizing the annotations while enhancing labeling efficiency and reliability.","Preliminary findings from fundamental charts demonstrate that punch-hole labeling can effectively pinpoint critical regions.","This also highlights its potential for broader application in visualization research, particularly in studying large-scale users' graphical perception.","Our future work aims to enhance the algorithm to achieve faster labeling speed and prove its utility through large-scale experiments."],"url":"http://arxiv.org/abs/2409.10459v1"}
{"created":"2024-09-16 16:40:40","title":"Signed Graph Autoencoder for Explainable and Polarization-Aware Network Embeddings","abstract":"Autoencoders based on Graph Neural Networks (GNNs) have garnered significant attention in recent years for their ability to extract informative latent representations, characterizing the structure of complex topologies, such as graphs. Despite the prevalence of Graph Autoencoders, there has been limited focus on developing and evaluating explainable neural-based graph generative models specifically designed for signed networks. To address this gap, we propose the Signed Graph Archetypal Autoencoder (SGAAE) framework. SGAAE extracts node-level representations that express node memberships over distinct extreme profiles, referred to as archetypes, within the network. This is achieved by projecting the graph onto a learned polytope, which governs its polarization. The framework employs a recently proposed likelihood for analyzing signed networks based on the Skellam distribution, combined with relational archetypal analysis and GNNs. Our experimental evaluation demonstrates the SGAAEs' capability to successfully infer node memberships over the different underlying latent structures while extracting competing communities formed through the participation of the opposing views in the network. Additionally, we introduce the 2-level network polarization problem and show how SGAAE is able to characterize such a setting. The proposed model achieves high performance in different tasks of signed link prediction across four real-world datasets, outperforming several baseline models.","sentences":["Autoencoders based on Graph Neural Networks (GNNs) have garnered significant attention in recent years for their ability to extract informative latent representations, characterizing the structure of complex topologies, such as graphs.","Despite the prevalence of Graph Autoencoders, there has been limited focus on developing and evaluating explainable neural-based graph generative models specifically designed for signed networks.","To address this gap, we propose the Signed Graph Archetypal Autoencoder (SGAAE) framework.","SGAAE extracts node-level representations that express node memberships over distinct extreme profiles, referred to as archetypes, within the network.","This is achieved by projecting the graph onto a learned polytope, which governs its polarization.","The framework employs a recently proposed likelihood for analyzing signed networks based on the Skellam distribution, combined with relational archetypal analysis and GNNs.","Our experimental evaluation demonstrates the SGAAEs' capability to successfully infer node memberships over the different underlying latent structures while extracting competing communities formed through the participation of the opposing views in the network.","Additionally, we introduce the 2-level network polarization problem and show how SGAAE is able to characterize such a setting.","The proposed model achieves high performance in different tasks of signed link prediction across four real-world datasets, outperforming several baseline models."],"url":"http://arxiv.org/abs/2409.10452v1"}
{"created":"2024-09-16 16:37:21","title":"Charting EDA: Characterizing Interactive Visualization Use in Computational Notebooks with a Mixed-Methods Formalism","abstract":"Interactive visualizations are powerful tools for Exploratory Data Analysis (EDA), but how do they affect the observations analysts make about their data? We conducted a qualitative experiment with 13 professional data scientists analyzing two datasets with Jupyter notebooks, collecting a rich dataset of interaction traces and think-aloud utterances. By qualitatively coding participant utterances, we introduce a formalism that describes EDA as a sequence of analysis states, where each state is comprised of either a representation an analyst constructs (e.g., the output of a data frame, an interactive visualization, etc.) or an observation the analyst makes (e.g., about missing data, the relationship between variables, etc.). By applying our formalism to our dataset, we identify that interactive visualizations, on average, lead to earlier and more complex insights about relationships between dataset attributes compared to static visualizations. Moreover, by calculating metrics such as revisit count and representational diversity, we uncover that some representations serve more as \"planning aids\" during EDA rather than tools strictly for hypothesis-answering. We show how these measures help identify other patterns of analysis behavior, such as the \"80-20 rule\", where a small subset of representations drove the majority of observations. Based on these findings, we offer design guidelines for interactive exploratory analysis tooling and reflect on future directions for studying the role that visualizations play in EDA.","sentences":["Interactive visualizations are powerful tools for Exploratory Data Analysis (EDA), but how do they affect the observations analysts make about their data?","We conducted a qualitative experiment with 13 professional data scientists analyzing two datasets with Jupyter notebooks, collecting a rich dataset of interaction traces and think-aloud utterances.","By qualitatively coding participant utterances, we introduce a formalism that describes EDA as a sequence of analysis states, where each state is comprised of either a representation an analyst constructs (e.g., the output of a data frame, an interactive visualization, etc.) or an observation the analyst makes (e.g., about missing data, the relationship between variables, etc.).","By applying our formalism to our dataset, we identify that interactive visualizations, on average, lead to earlier and more complex insights about relationships between dataset attributes compared to static visualizations.","Moreover, by calculating metrics such as revisit count and representational diversity, we uncover that some representations serve more as \"planning aids\" during EDA rather than tools strictly for hypothesis-answering.","We show how these measures help identify other patterns of analysis behavior, such as the \"80-20 rule\", where a small subset of representations drove the majority of observations.","Based on these findings, we offer design guidelines for interactive exploratory analysis tooling and reflect on future directions for studying the role that visualizations play in EDA."],"url":"http://arxiv.org/abs/2409.10450v1"}
{"created":"2024-09-16 16:35:50","title":"KoroT-3E: A Personalized Musical Mnemonics Tool for Enhancing Memory Retention of Complex Computer Science Concepts","abstract":"As the demand for computer science (CS) skills grows, mastering foundational concepts is crucial yet challenging for novice learners. To address this challenge, we present KoroT-3E, an AI-based system that creates personalized musical mnemonics to enhance both memory retention and understanding of concepts in CS. KoroT-3E enables users to transform complex concepts into memorable lyrics and compose melodies that suit their musical preferences. We conducted semi-structured interviews (n=12) to investigate why novice learners find it challenging to memorize and understand CS concepts. The findings, combined with constructivist learning theory, established our initial design, which was then refined following consultations with CS education experts. An empirical experiment(n=36) showed that those using KoroT-3E (n=18) significantly outperformed the control group (n=18), with improved memory efficiency, increased motivation, and a positive learning experience. These findings demonstrate the effectiveness of integrating multimodal generative AI into CS education to create personalized and interactive learning experiences.","sentences":["As the demand for computer science (CS) skills grows, mastering foundational concepts is crucial yet challenging for novice learners.","To address this challenge, we present KoroT-3E, an AI-based system that creates personalized musical mnemonics to enhance both memory retention and understanding of concepts in CS.","KoroT-3E enables users to transform complex concepts into memorable lyrics and compose melodies that suit their musical preferences.","We conducted semi-structured interviews (n=12) to investigate why novice learners find it challenging to memorize and understand CS concepts.","The findings, combined with constructivist learning theory, established our initial design, which was then refined following consultations with CS education experts.","An empirical experiment(n=36) showed that those using KoroT-3E (n=18) significantly outperformed the control group (n=18), with improved memory efficiency, increased motivation, and a positive learning experience.","These findings demonstrate the effectiveness of integrating multimodal generative AI into CS education to create personalized and interactive learning experiences."],"url":"http://arxiv.org/abs/2409.10446v1"}
{"created":"2024-09-16 16:29:41","title":"Deep-Wide Learning Assistance for Insect Pest Classification","abstract":"Accurate insect pest recognition plays a critical role in agriculture. It is a challenging problem due to the intricate characteristics of insects. In this paper, we present DeWi, novel learning assistance for insect pest classification. With a one-stage and alternating training strategy, DeWi simultaneously improves several Convolutional Neural Networks in two perspectives: discrimination (by optimizing a triplet margin loss in a supervised training manner) and generalization (via data augmentation). From that, DeWi can learn discriminative and in-depth features of insect pests (deep) yet still generalize well to a large number of insect categories (wide). Experimental results show that DeWi achieves the highest performances on two insect pest classification benchmarks (76.44\\% accuracy on the IP102 dataset and 99.79\\% accuracy on the D0 dataset, respectively). In addition, extensive evaluations and ablation studies are conducted to thoroughly investigate our DeWi and demonstrate its superiority. Our source code is available at https://github.com/toannguyen1904/DeWi.","sentences":["Accurate insect pest recognition plays a critical role in agriculture.","It is a challenging problem due to the intricate characteristics of insects.","In this paper, we present DeWi, novel learning assistance for insect pest classification.","With a one-stage and alternating training strategy, DeWi simultaneously improves several Convolutional Neural Networks in two perspectives: discrimination (by optimizing a triplet margin loss in a supervised training manner) and generalization (via data augmentation).","From that, DeWi can learn discriminative and in-depth features of insect pests (deep) yet still generalize well to a large number of insect categories (wide).","Experimental results show that DeWi achieves the highest performances on two insect pest classification benchmarks (76.44\\% accuracy on the IP102 dataset and 99.79\\% accuracy on the D0 dataset, respectively).","In addition, extensive evaluations and ablation studies are conducted to thoroughly investigate our DeWi and demonstrate its superiority.","Our source code is available at https://github.com/toannguyen1904/DeWi."],"url":"http://arxiv.org/abs/2409.10445v1"}
{"created":"2024-09-16 16:28:34","title":"LLM as BT-Planner: Leveraging LLMs for Behavior Tree Generation in Robot Task Planning","abstract":"Robotic assembly tasks are open challenges due to the long task horizon and complex part relations. Behavior trees (BTs) are increasingly used in robot task planning for their modularity and flexibility, but manually designing them can be effort-intensive. Large language models (LLMs) have recently been applied in robotic task planning for generating action sequences, but their ability to generate BTs has not been fully investigated. To this end, We propose LLM as BT-planner, a novel framework to leverage LLMs for BT generation in robotic assembly task planning and execution. Four in-context learning methods are introduced to utilize the natural language processing and inference capabilities of LLMs to produce task plans in BT format, reducing manual effort and ensuring robustness and comprehensibility. We also evaluate the performance of fine-tuned, fewer-parameter LLMs on the same tasks. Experiments in simulated and real-world settings show that our framework enhances LLMs' performance in BT generation, improving success rates in BT generation through in-context learning and supervised fine-tuning.","sentences":["Robotic assembly tasks are open challenges due to the long task horizon and complex part relations.","Behavior trees (BTs) are increasingly used in robot task planning for their modularity and flexibility, but manually designing them can be effort-intensive.","Large language models (LLMs) have recently been applied in robotic task planning for generating action sequences, but their ability to generate BTs has not been fully investigated.","To this end, We propose LLM as BT-planner, a novel framework to leverage LLMs for BT generation in robotic assembly task planning and execution.","Four in-context learning methods are introduced to utilize the natural language processing and inference capabilities of LLMs to produce task plans in BT format, reducing manual effort and ensuring robustness and comprehensibility.","We also evaluate the performance of fine-tuned, fewer-parameter LLMs on the same tasks.","Experiments in simulated and real-world settings show that our framework enhances LLMs' performance in BT generation, improving success rates in BT generation through in-context learning and supervised fine-tuning."],"url":"http://arxiv.org/abs/2409.10444v1"}
{"created":"2024-09-16 16:22:43","title":"CtRNet-X: Camera-to-Robot Pose Estimation in Real-world Conditions Using a Single Camera","abstract":"Camera-to-robot calibration is crucial for vision-based robot control and requires effort to make it accurate. Recent advancements in markerless pose estimation methods have eliminated the need for time-consuming physical setups for camera-to-robot calibration. While the existing markerless pose estimation methods have demonstrated impressive accuracy without the need for cumbersome setups, they rely on the assumption that all the robot joints are visible within the camera's field of view. However, in practice, robots usually move in and out of view, and some portion of the robot may stay out-of-frame during the whole manipulation task due to real-world constraints, leading to a lack of sufficient visual features and subsequent failure of these approaches. To address this challenge and enhance the applicability to vision-based robot control, we propose a novel framework capable of estimating the robot pose with partially visible robot manipulators. Our approach leverages the Vision-Language Models for fine-grained robot components detection, and integrates it into a keypoint-based pose estimation network, which enables more robust performance in varied operational conditions. The framework is evaluated on both public robot datasets and self-collected partial-view datasets to demonstrate our robustness and generalizability. As a result, this method is effective for robot pose estimation in a wider range of real-world manipulation scenarios.","sentences":["Camera-to-robot calibration is crucial for vision-based robot control and requires effort to make it accurate.","Recent advancements in markerless pose estimation methods have eliminated the need for time-consuming physical setups for camera-to-robot calibration.","While the existing markerless pose estimation methods have demonstrated impressive accuracy without the need for cumbersome setups, they rely on the assumption that all the robot joints are visible within the camera's field of view.","However, in practice, robots usually move in and out of view, and some portion of the robot may stay out-of-frame during the whole manipulation task due to real-world constraints, leading to a lack of sufficient visual features and subsequent failure of these approaches.","To address this challenge and enhance the applicability to vision-based robot control, we propose a novel framework capable of estimating the robot pose with partially visible robot manipulators.","Our approach leverages the Vision-Language Models for fine-grained robot components detection, and integrates it into a keypoint-based pose estimation network, which enables more robust performance in varied operational conditions.","The framework is evaluated on both public robot datasets and self-collected partial-view datasets to demonstrate our robustness and generalizability.","As a result, this method is effective for robot pose estimation in a wider range of real-world manipulation scenarios."],"url":"http://arxiv.org/abs/2409.10441v1"}
{"created":"2024-09-16 16:07:21","title":"Structure-preserving learning for multi-symplectic PDEs","abstract":"This paper presents an energy-preserving machine learning method for inferring reduced-order models (ROMs) by exploiting the multi-symplectic form of partial differential equations (PDEs). The vast majority of energy-preserving reduced-order methods use symplectic Galerkin projection to construct reduced-order Hamiltonian models by projecting the full models onto a symplectic subspace. However, symplectic projection requires the existence of fully discrete operators, and in many cases, such as black-box PDE solvers, these operators are inaccessible. In this work, we propose an energy-preserving machine learning method that can infer the dynamics of the given PDE using data only, so that the proposed framework does not depend on the fully discrete operators. In this context, the proposed method is non-intrusive. The proposed method is grey box in the sense that it requires only some basic knowledge of the multi-symplectic model at the partial differential equation level. We prove that the proposed method satisfies spatially discrete local energy conservation and preserves the multi-symplectic conservation laws. We test our method on the linear wave equation, the Korteweg-de Vries equation, and the Zakharov-Kuznetsov equation. We test the generalization of our learned models by testing them far outside the training time interval.","sentences":["This paper presents an energy-preserving machine learning method for inferring reduced-order models (ROMs) by exploiting the multi-symplectic form of partial differential equations (PDEs).","The vast majority of energy-preserving reduced-order methods use symplectic Galerkin projection to construct reduced-order Hamiltonian models by projecting the full models onto a symplectic subspace.","However, symplectic projection requires the existence of fully discrete operators, and in many cases, such as black-box PDE solvers, these operators are inaccessible.","In this work, we propose an energy-preserving machine learning method that can infer the dynamics of the given PDE using data only, so that the proposed framework does not depend on the fully discrete operators.","In this context, the proposed method is non-intrusive.","The proposed method is grey box in the sense that it requires only some basic knowledge of the multi-symplectic model at the partial differential equation level.","We prove that the proposed method satisfies spatially discrete local energy conservation and preserves the multi-symplectic conservation laws.","We test our method on the linear wave equation, the Korteweg-de Vries equation, and the Zakharov-Kuznetsov equation.","We test the generalization of our learned models by testing them far outside the training time interval."],"url":"http://arxiv.org/abs/2409.10432v1"}
{"created":"2024-09-16 15:52:41","title":"Learning Semi-Supervised Medical Image Segmentation from Spatial Registration","abstract":"Semi-supervised medical image segmentation has shown promise in training models with limited labeled data and abundant unlabeled data. However, state-of-the-art methods ignore a potentially valuable source of unsupervised semantic information -- spatial registration transforms between image volumes. To address this, we propose CCT-R, a contrastive cross-teaching framework incorporating registration information. To leverage the semantic information available in registrations between volume pairs, CCT-R incorporates two proposed modules: Registration Supervision Loss (RSL) and Registration-Enhanced Positive Sampling (REPS). The RSL leverages segmentation knowledge derived from transforms between labeled and unlabeled volume pairs, providing an additional source of pseudo-labels. REPS enhances contrastive learning by identifying anatomically-corresponding positives across volumes using registration transforms. Experimental results on two challenging medical segmentation benchmarks demonstrate the effectiveness and superiority of CCT-R across various semi-supervised settings, with as few as one labeled case. Our code is available at https://github.com/kathyliu579/ContrastiveCross-teachingWithRegistration.","sentences":["Semi-supervised medical image segmentation has shown promise in training models with limited labeled data and abundant unlabeled data.","However, state-of-the-art methods ignore a potentially valuable source of unsupervised semantic information -- spatial registration transforms between image volumes.","To address this, we propose CCT-R, a contrastive cross-teaching framework incorporating registration information.","To leverage the semantic information available in registrations between volume pairs, CCT-R incorporates two proposed modules: Registration Supervision Loss (RSL) and Registration-Enhanced Positive Sampling (REPS).","The RSL leverages segmentation knowledge derived from transforms between labeled and unlabeled volume pairs, providing an additional source of pseudo-labels.","REPS enhances contrastive learning by identifying anatomically-corresponding positives across volumes using registration transforms.","Experimental results on two challenging medical segmentation benchmarks demonstrate the effectiveness and superiority of CCT-R across various semi-supervised settings, with as few as one labeled case.","Our code is available at https://github.com/kathyliu579/ContrastiveCross-teachingWithRegistration."],"url":"http://arxiv.org/abs/2409.10422v1"}
{"created":"2024-09-16 15:50:43","title":"The Second Generalized Covering Radius of Binary Primitive Double-Error-Correcting BCH Codes","abstract":"We completely determine the second covering radius for binary primitive double-error-correcting BCH codes. As part of this process, we provide a lower bound on the second covering radius for binary primitive BCH codes correcting more than two errors.","sentences":["We completely determine the second covering radius for binary primitive double-error-correcting BCH codes.","As part of this process, we provide a lower bound on the second covering radius for binary primitive BCH codes correcting more than two errors."],"url":"http://arxiv.org/abs/2409.10420v1"}
{"created":"2024-09-16 15:50:39","title":"HiFi-CS: Towards Open Vocabulary Visual Grounding For Robotic Grasping Using Vision-Language Models","abstract":"Robots interacting with humans through natural language can unlock numerous applications such as Referring Grasp Synthesis (RGS). Given a text query, RGS determines a stable grasp pose to manipulate the referred object in the robot's workspace. RGS comprises two steps: visual grounding and grasp pose estimation. Recent studies leverage powerful Vision-Language Models (VLMs) for visually grounding free-flowing natural language in real-world robotic execution. However, comparisons in complex, cluttered environments with multiple instances of the same object are lacking. This paper introduces HiFi-CS, featuring hierarchical application of Featurewise Linear Modulation (FiLM) to fuse image and text embeddings, enhancing visual grounding for complex attribute rich text queries encountered in robotic grasping. Visual grounding associates an object in 2D/3D space with natural language input and is studied in two scenarios: Closed and Open Vocabulary. HiFi-CS features a lightweight decoder combined with a frozen VLM and outperforms competitive baselines in closed vocabulary settings while being 100x smaller in size. Our model can effectively guide open-set object detectors like GroundedSAM to enhance open-vocabulary performance. We validate our approach through real-world RGS experiments using a 7-DOF robotic arm, achieving 90.33\\% visual grounding accuracy in 15 tabletop scenes. We include our codebase in the supplementary material.","sentences":["Robots interacting with humans through natural language can unlock numerous applications such as Referring Grasp Synthesis (RGS).","Given a text query, RGS determines a stable grasp pose to manipulate the referred object in the robot's workspace.","RGS comprises two steps: visual grounding and grasp pose estimation.","Recent studies leverage powerful Vision-Language Models (VLMs) for visually grounding free-flowing natural language in real-world robotic execution.","However, comparisons in complex, cluttered environments with multiple instances of the same object are lacking.","This paper introduces HiFi-CS, featuring hierarchical application of Featurewise Linear Modulation (FiLM) to fuse image and text embeddings, enhancing visual grounding for complex attribute rich text queries encountered in robotic grasping.","Visual grounding associates an object in 2D/3D space with natural language input and is studied in two scenarios: Closed and Open Vocabulary.","HiFi-CS features a lightweight decoder combined with a frozen VLM and outperforms competitive baselines in closed vocabulary settings while being 100x smaller in size.","Our model can effectively guide open-set object detectors like GroundedSAM to enhance open-vocabulary performance.","We validate our approach through real-world RGS experiments using a 7-DOF robotic arm, achieving 90.33\\% visual grounding accuracy in 15 tabletop scenes.","We include our codebase in the supplementary material."],"url":"http://arxiv.org/abs/2409.10419v1"}
{"created":"2024-09-16 15:44:43","title":"A Large-Scale Privacy Assessment of Android Third-Party SDKs","abstract":"Third-party Software Development Kits (SDKs) are widely adopted in Android app development, to effortlessly accelerate development pipelines and enhance app functionality. However, this convenience raises substantial concerns about unauthorized access to users' privacy-sensitive information, which could be further abused for illegitimate purposes like user tracking or monetization. Our study offers a targeted analysis of user privacy protection among Android third-party SDKs, filling a critical gap in the Android software supply chain. It focuses on two aspects of their privacy practices, including data exfiltration and behavior-policy compliance (or privacy compliance), utilizing techniques of taint analysis and large language models. It covers 158 widely-used SDKs from two key SDK release platforms, the official one and a large alternative one. From them, we identified 338 instances of privacy data exfiltration. On the privacy compliance, our study reveals that more than 30% of the examined SDKs fail to provide a privacy policy to disclose their data handling practices. Among those that provide privacy policies, 37% of them over-collect user data, and 88% falsely claim access to sensitive data. We revisit the latest versions of the SDKs after 12 months. Our analysis demonstrates a persistent lack of improvement in these concerning trends. Based on our findings, we propose three actionable recommendations to mitigate the privacy leakage risks and enhance privacy protection for Android users. Our research not only serves as an urgent call for industry attention but also provides crucial insights for future regulatory interventions.","sentences":["Third-party Software Development Kits (SDKs) are widely adopted in Android app development, to effortlessly accelerate development pipelines and enhance app functionality.","However, this convenience raises substantial concerns about unauthorized access to users' privacy-sensitive information, which could be further abused for illegitimate purposes like user tracking or monetization.","Our study offers a targeted analysis of user privacy protection among Android third-party SDKs, filling a critical gap in the Android software supply chain.","It focuses on two aspects of their privacy practices, including data exfiltration and behavior-policy compliance (or privacy compliance), utilizing techniques of taint analysis and large language models.","It covers 158 widely-used SDKs from two key SDK release platforms, the official one and a large alternative one.","From them, we identified 338 instances of privacy data exfiltration.","On the privacy compliance, our study reveals that more than 30% of the examined SDKs fail to provide a privacy policy to disclose their data handling practices.","Among those that provide privacy policies, 37% of them over-collect user data, and 88% falsely claim access to sensitive data.","We revisit the latest versions of the SDKs after 12 months.","Our analysis demonstrates a persistent lack of improvement in these concerning trends.","Based on our findings, we propose three actionable recommendations to mitigate the privacy leakage risks and enhance privacy protection for Android users.","Our research not only serves as an urgent call for industry attention but also provides crucial insights for future regulatory interventions."],"url":"http://arxiv.org/abs/2409.10411v1"}
{"created":"2024-09-16 15:44:27","title":"Estimates for Optimal Multistage Group Partition Testing","abstract":"In multistage group testing, the tests within the same stage are considered nonadaptive, while those conducted across different stages are adaptive. Specifically, when the pools within the same stage are disjoint, meaning that the entire set is divided into several disjoint subgroups, it is referred to as a multistage group partition testing problem, denoted as the (n, d, s) problem, where n, d, and s represent the total number of items, defectives, and stages respectively. This paper presents exact solutions for the (n, 1, s) and (n, d, 2) problems for the first time. Additionally, a general dynamic programming approach is developed for the (n, d, s) problem. Significantly I give the sharp upper and lower bounds estimates. If the defective number in unknown but bounded, I can provide an algorithm with an optimal competitive ratio in the asymptotic sense. While assuming the prior distribution of the defective items, I also establish a well performing upper and lower bound estimate to the expectation of optimal strategy","sentences":["In multistage group testing, the tests within the same stage are considered nonadaptive, while those conducted across different stages are adaptive.","Specifically, when the pools within the same stage are disjoint, meaning that the entire set is divided into several disjoint subgroups, it is referred to as a multistage group partition testing problem, denoted as the (n, d, s) problem, where n, d, and s represent the total number of items, defectives, and stages respectively.","This paper presents exact solutions for the (n, 1, s) and (n, d, 2) problems for the first time.","Additionally, a general dynamic programming approach is developed for the (n, d, s) problem.","Significantly I give the sharp upper and lower bounds estimates.","If the defective number in unknown but bounded, I can provide an algorithm with an optimal competitive ratio in the asymptotic sense.","While assuming the prior distribution of the defective items, I also establish a well performing upper and lower bound estimate to the expectation of optimal strategy"],"url":"http://arxiv.org/abs/2409.10410v1"}
{"created":"2024-09-16 15:34:58","title":"A Knowledge-Enhanced Disease Diagnosis Method Based on Prompt Learning and BERT Integration","abstract":"This paper proposes a knowledge-enhanced disease diagnosis method based on a prompt learning framework. The method retrieves structured knowledge from external knowledge graphs related to clinical cases, encodes it, and injects it into the prompt templates to enhance the language model's understanding and reasoning capabilities for the task.We conducted experiments on three public datasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR. The results show that the proposed method significantly outperforms existing models across multiple evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC dataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset. Additionally,ablation studies confirmed the critical role of the knowledge injection module,as the removal of this module resulted in a significant drop in F1 score. The experimental results demonstrate that the proposed method not only effectively improves the accuracy of disease diagnosis but also enhances the interpretability of the predictions, providing more reliable support and evidence for clinical diagnosis.","sentences":["This paper proposes a knowledge-enhanced disease diagnosis method based on a prompt learning framework.","The method retrieves structured knowledge from external knowledge graphs related to clinical cases, encodes it, and injects it into the prompt templates to enhance the language model's understanding and reasoning capabilities for the task.","We conducted experiments on three public datasets: CHIP-CTC, IMCS-V2-NER, and KUAKE-QTR.","The results show that the proposed method significantly outperforms existing models across multiple evaluation metrics, with an F1 score improvement of 2.4% on the CHIP-CTC dataset, 3.1% on the IMCS-V2-NER dataset,and 4.2% on the KUAKE-QTR dataset.","Additionally,ablation studies confirmed the critical role of the knowledge injection module,as the removal of this module resulted in a significant drop in F1 score.","The experimental results demonstrate that the proposed method not only effectively improves the accuracy of disease diagnosis but also enhances the interpretability of the predictions, providing more reliable support and evidence for clinical diagnosis."],"url":"http://arxiv.org/abs/2409.10403v1"}
{"created":"2024-09-16 15:31:41","title":"Reducing Leximin Fairness to Utilitarian Optimization","abstract":"Two prominent objectives in social choice are utilitarian - maximizing the sum of agents' utilities, and leximin - maximizing the smallest agent's utility, then the second-smallest, etc. Utilitarianism is typically computationally easier to attain but is generally viewed as less fair. This paper presents a general reduction scheme that, given a utilitarian solver, produces a distribution over outcomes that is leximin in expectation. Importantly, the scheme is robust in the sense that, given an approximate utilitarian solver, it produces an outcome that is approximately-leximin (in expectation) - with the same approximation factor. We apply our scheme to several social choice problems: stochastic allocations of indivisible goods, giveaway lotteries, and fair lotteries for participatory budgeting.","sentences":["Two prominent objectives in social choice are utilitarian - maximizing the sum of agents' utilities, and leximin - maximizing the smallest agent's utility, then the second-smallest, etc.","Utilitarianism is typically computationally easier to attain but is generally viewed as less fair.","This paper presents a general reduction scheme that, given a utilitarian solver, produces a distribution over outcomes that is leximin in expectation.","Importantly, the scheme is robust in the sense that, given an approximate utilitarian solver, it produces an outcome that is approximately-leximin (in expectation) - with the same approximation factor.","We apply our scheme to several social choice problems: stochastic allocations of indivisible goods, giveaway lotteries, and fair lotteries for participatory budgeting."],"url":"http://arxiv.org/abs/2409.10395v1"}
{"created":"2024-09-16 15:27:35","title":"TPFL: Tsetlin-Personalized Federated Learning with Confidence-Based Clustering","abstract":"The world of Machine Learning (ML) has witnessed rapid changes in terms of new models and ways to process users data. The majority of work that has been done is focused on Deep Learning (DL) based approaches. However, with the emergence of new algorithms such as the Tsetlin Machine (TM) algorithm, there is growing interest in exploring alternative approaches that may offer unique advantages in certain domains or applications. One of these domains is Federated Learning (FL), in which users privacy is of utmost importance. Due to its novelty, FL has seen a surge in the incorporation of personalization techniques to enhance model accuracy while maintaining user privacy under personalized conditions. In this work, we propose a novel approach dubbed TPFL: Tsetlin-Personalized Federated Learning, in which models are grouped into clusters based on their confidence towards a specific class. In this way, clustering can benefit from two key advantages. Firstly, clients share only what they are confident about, resulting in the elimination of wrongful weight aggregation among clients whose data for a specific class may have not been enough during the training. This phenomenon is prevalent when the data are non-Independent and Identically Distributed (non-IID). Secondly, by sharing only weights towards a specific class, communication cost is substantially reduced, making TPLF efficient in terms of both accuracy and communication cost. The results of TPFL demonstrated the highest accuracy on three different datasets; namely MNIST, FashionMNIST and FEMNIST.","sentences":["The world of Machine Learning (ML) has witnessed rapid changes in terms of new models and ways to process users data.","The majority of work that has been done is focused on Deep Learning (DL) based approaches.","However, with the emergence of new algorithms such as the Tsetlin Machine (TM) algorithm, there is growing interest in exploring alternative approaches that may offer unique advantages in certain domains or applications.","One of these domains is Federated Learning (FL), in which users privacy is of utmost importance.","Due to its novelty, FL has seen a surge in the incorporation of personalization techniques to enhance model accuracy while maintaining user privacy under personalized conditions.","In this work, we propose a novel approach dubbed TPFL: Tsetlin-Personalized Federated Learning, in which models are grouped into clusters based on their confidence towards a specific class.","In this way, clustering can benefit from two key advantages.","Firstly, clients share only what they are confident about, resulting in the elimination of wrongful weight aggregation among clients whose data for a specific class may have not been enough during the training.","This phenomenon is prevalent when the data are non-Independent and Identically Distributed (non-IID).","Secondly, by sharing only weights towards a specific class, communication cost is substantially reduced, making TPLF efficient in terms of both accuracy and communication cost.","The results of TPFL demonstrated the highest accuracy on three different datasets; namely MNIST, FashionMNIST and FEMNIST."],"url":"http://arxiv.org/abs/2409.10392v1"}
{"created":"2024-09-16 15:24:26","title":"Prompt-and-Transfer: Dynamic Class-aware Enhancement for Few-shot Segmentation","abstract":"For more efficient generalization to unseen domains (classes), most Few-shot Segmentation (FSS) would directly exploit pre-trained encoders and only fine-tune the decoder, especially in the current era of large models. However, such fixed feature encoders tend to be class-agnostic, inevitably activating objects that are irrelevant to the target class. In contrast, humans can effortlessly focus on specific objects in the line of sight. This paper mimics the visual perception pattern of human beings and proposes a novel and powerful prompt-driven scheme, called ``Prompt and Transfer\" (PAT), which constructs a dynamic class-aware prompting paradigm to tune the encoder for focusing on the interested object (target class) in the current task. Three key points are elaborated to enhance the prompting: 1) Cross-modal linguistic information is introduced to initialize prompts for each task. 2) Semantic Prompt Transfer (SPT) that precisely transfers the class-specific semantics within the images to prompts. 3) Part Mask Generator (PMG) that works in conjunction with SPT to adaptively generate different but complementary part prompts for different individuals. Surprisingly, PAT achieves competitive performance on 4 different tasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remote sensing domains), Weak-label FSS, and Zero-shot Segmentation, setting new state-of-the-arts on 11 benchmarks.","sentences":["For more efficient generalization to unseen domains (classes), most Few-shot Segmentation (FSS) would directly exploit pre-trained encoders and only fine-tune the decoder, especially in the current era of large models.","However, such fixed feature encoders tend to be class-agnostic, inevitably activating objects that are irrelevant to the target class.","In contrast, humans can effortlessly focus on specific objects in the line of sight.","This paper mimics the visual perception pattern of human beings and proposes a novel and powerful prompt-driven scheme, called ``Prompt and Transfer\" (PAT), which constructs a dynamic class-aware prompting paradigm to tune the encoder for focusing on the interested object (target class) in the current task.","Three key points are elaborated to enhance the prompting: 1) Cross-modal linguistic information is introduced to initialize prompts for each task.","2) Semantic Prompt Transfer (SPT) that precisely transfers the class-specific semantics within the images to prompts.","3) Part Mask Generator (PMG) that works in conjunction with SPT to adaptively generate different but complementary part prompts for different individuals.","Surprisingly, PAT achieves competitive performance on 4 different tasks including standard FSS, Cross-domain FSS (e.g., CV, medical, and remote sensing domains), Weak-label FSS, and Zero-shot Segmentation, setting new state-of-the-arts on 11 benchmarks."],"url":"http://arxiv.org/abs/2409.10389v1"}
{"created":"2024-09-16 15:24:25","title":"Revising the Structure of Recurrent Neural Networks to Eliminate Numerical Derivatives in Forming Physics Informed Loss Terms with Respect to Time","abstract":"Solving unsteady partial differential equations (PDEs) using recurrent neural networks (RNNs) typically requires numerical derivatives between each block of the RNN to form the physics informed loss function. However, this introduces the complexities of numerical derivatives into the training process of these models. In this study, we propose modifying the structure of the traditional RNN to enable the prediction of each block over a time interval, making it possible to calculate the derivative of the output with respect to time using the backpropagation algorithm. To achieve this, the time intervals of these blocks are overlapped, defining a mutual loss function between them. Additionally, the employment of conditional hidden states enables us to achieve a unique solution for each block. The forget factor is utilized to control the influence of the conditional hidden state on the prediction of the subsequent block. This new model, termed the Mutual Interval RNN (MI-RNN), is applied to solve three different benchmarks: the Burgers equation, unsteady heat conduction in an irregular domain, and the Green vortex problem. Our results demonstrate that MI-RNN can find the exact solution more accurately compared to existing RNN models. For instance, in the second problem, MI-RNN achieved one order of magnitude less relative error compared to the RNN model with numerical derivatives.","sentences":["Solving unsteady partial differential equations (PDEs) using recurrent neural networks (RNNs) typically requires numerical derivatives between each block of the RNN to form the physics informed loss function.","However, this introduces the complexities of numerical derivatives into the training process of these models.","In this study, we propose modifying the structure of the traditional RNN to enable the prediction of each block over a time interval, making it possible to calculate the derivative of the output with respect to time using the backpropagation algorithm.","To achieve this, the time intervals of these blocks are overlapped, defining a mutual loss function between them.","Additionally, the employment of conditional hidden states enables us to achieve a unique solution for each block.","The forget factor is utilized to control the influence of the conditional hidden state on the prediction of the subsequent block.","This new model, termed the Mutual Interval RNN (MI-RNN), is applied to solve three different benchmarks: the Burgers equation, unsteady heat conduction in an irregular domain, and the Green vortex problem.","Our results demonstrate that MI-RNN can find the exact solution more accurately compared to existing RNN models.","For instance, in the second problem, MI-RNN achieved one order of magnitude less relative error compared to the RNN model with numerical derivatives."],"url":"http://arxiv.org/abs/2409.10388v1"}
{"created":"2024-09-16 15:20:48","title":"Mamba-ST: State Space Model for Efficient Style Transfer","abstract":"The goal of style transfer is, given a content image and a style source, generating a new image preserving the content but with the artistic representation of the style source. Most of the state-of-the-art architectures use transformers or diffusion-based models to perform this task, despite the heavy computational burden that they require. In particular, transformers use self- and cross-attention layers which have large memory footprint, while diffusion models require high inference time. To overcome the above, this paper explores a novel design of Mamba, an emergent State-Space Model (SSM), called Mamba-ST, to perform style transfer. To do so, we adapt Mamba linear equation to simulate the behavior of cross-attention layers, which are able to combine two separate embeddings into a single output, but drastically reducing memory usage and time complexity. We modified the Mamba's inner equations so to accept inputs from, and combine, two separate data streams. To the best of our knowledge, this is the first attempt to adapt the equations of SSMs to a vision task like style transfer without requiring any other module like cross-attention or custom normalization layers. An extensive set of experiments demonstrates the superiority and efficiency of our method in performing style transfer compared to transformers and diffusion models. Results show improved quality in terms of both ArtFID and FID metrics. Code is available at https://github.com/FilippoBotti/MambaST.","sentences":["The goal of style transfer is, given a content image and a style source, generating a new image preserving the content but with the artistic representation of the style source.","Most of the state-of-the-art architectures use transformers or diffusion-based models to perform this task, despite the heavy computational burden that they require.","In particular, transformers use self- and cross-attention layers which have large memory footprint, while diffusion models require high inference time.","To overcome the above, this paper explores a novel design of Mamba, an emergent State-Space Model (SSM), called Mamba-ST, to perform style transfer.","To do so, we adapt Mamba linear equation to simulate the behavior of cross-attention layers, which are able to combine two separate embeddings into a single output, but drastically reducing memory usage and time complexity.","We modified the Mamba's inner equations so to accept inputs from, and combine, two separate data streams.","To the best of our knowledge, this is the first attempt to adapt the equations of SSMs to a vision task like style transfer without requiring any other module like cross-attention or custom normalization layers.","An extensive set of experiments demonstrates the superiority and efficiency of our method in performing style transfer compared to transformers and diffusion models.","Results show improved quality in terms of both ArtFID and FID metrics.","Code is available at https://github.com/FilippoBotti/MambaST."],"url":"http://arxiv.org/abs/2409.10385v1"}
{"created":"2024-09-16 15:16:43","title":"Decentralized and Asymmetric Multi-Agent Learning in Construction Sites","abstract":"Multi-agent collaboration involves multiple participants working together in a shared environment to achieve a common goal. These agents share information, divide tasks, and synchronize their actions. Key aspects of multi agent collaboration include coordination, communication, task allocation, cooperation, adaptation, and decentralization. On construction sites, surface grading is the process of leveling sand piles to increase a specific area's height. In this scenario, a bulldozer grades while a dumper allocates sand piles. Our work aims to utilize a multi-agent approach to enable these vehicles to collaborate effectively. To this end, we propose a decentralized and asymmetric multi-agent learning approach for construction sites (DAMALCS). We formulate DAMALCS to reduce expected collisions for operating vehicles. Therefore, we develop two heuristic experts capable of achieving their joint goal optimally by applying an innovative prioritization method. In this approach, the bulldozer's movements take precedence over the dumper's operations, enabling the bulldozer to clear the path for the dumper and ensure continuous operation of both vehicles. Since heuristics alone are insufficient in real-world scenarios, we utilize them to train AI agents, which proves to be highly effective. We simultaneously train the bulldozer and dumper agents to operate within the same environment, aiming to avoid collisions and optimize performance in terms of time efficiency and sand volume handling. Our trained agents and heuristics are evaluated in both simulation and real-world lab experiments, testing them under various conditions, such as visual noise and localization errors. The results demonstrate that our approach significantly reduces collision rates for these vehicles.","sentences":["Multi-agent collaboration involves multiple participants working together in a shared environment to achieve a common goal.","These agents share information, divide tasks, and synchronize their actions.","Key aspects of multi agent collaboration include coordination, communication, task allocation, cooperation, adaptation, and decentralization.","On construction sites, surface grading is the process of leveling sand piles to increase a specific area's height.","In this scenario, a bulldozer grades while a dumper allocates sand piles.","Our work aims to utilize a multi-agent approach to enable these vehicles to collaborate effectively.","To this end, we propose a decentralized and asymmetric multi-agent learning approach for construction sites (DAMALCS).","We formulate DAMALCS to reduce expected collisions for operating vehicles.","Therefore, we develop two heuristic experts capable of achieving their joint goal optimally by applying an innovative prioritization method.","In this approach, the bulldozer's movements take precedence over the dumper's operations, enabling the bulldozer to clear the path for the dumper and ensure continuous operation of both vehicles.","Since heuristics alone are insufficient in real-world scenarios, we utilize them to train AI agents, which proves to be highly effective.","We simultaneously train the bulldozer and dumper agents to operate within the same environment, aiming to avoid collisions and optimize performance in terms of time efficiency and sand volume handling.","Our trained agents and heuristics are evaluated in both simulation and real-world lab experiments, testing them under various conditions, such as visual noise and localization errors.","The results demonstrate that our approach significantly reduces collision rates for these vehicles."],"url":"http://arxiv.org/abs/2409.10375v1"}
{"created":"2024-09-16 15:15:51","title":"Instigating Cooperation among LLM Agents Using Adaptive Information Modulation","abstract":"This paper introduces a novel framework combining LLM agents as proxies for human strategic behavior with reinforcement learning (RL) to engage these agents in evolving strategic interactions within team environments. Our approach extends traditional agent-based simulations by using strategic LLM agents (SLA) and introducing dynamic and adaptive governance through a pro-social promoting RL agent (PPA) that modulates information access across agents in a network, optimizing social welfare and promoting pro-social behavior. Through validation in iterative games, including the prisoner dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations. The PPA agent effectively learns to adjust information transparency, resulting in enhanced cooperation rates. This framework offers significant insights into AI-mediated social dynamics, contributing to the deployment of AI in real-world team settings.","sentences":["This paper introduces a novel framework combining LLM agents as proxies for human strategic behavior with reinforcement learning (RL) to engage these agents in evolving strategic interactions within team environments.","Our approach extends traditional agent-based simulations by using strategic LLM agents (SLA) and introducing dynamic and adaptive governance through a pro-social promoting RL agent (PPA) that modulates information access across agents in a network, optimizing social welfare and promoting pro-social behavior.","Through validation in iterative games, including the prisoner dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations.","The PPA agent effectively learns to adjust information transparency, resulting in enhanced cooperation rates.","This framework offers significant insights into AI-mediated social dynamics, contributing to the deployment of AI in real-world team settings."],"url":"http://arxiv.org/abs/2409.10372v1"}
{"created":"2024-09-16 15:14:53","title":"Learning Gentle Grasping from Human-Free Force Control Demonstration","abstract":"Humans can steadily and gently grasp unfamiliar objects based on tactile perception. Robots still face challenges in achieving similar performance due to the difficulty of learning accurate grasp-force predictions and force control strategies that can be generalized from limited data. In this article, we propose an approach for learning grasping from ideal force control demonstrations, to achieve similar performance of human hands with limited data size. Our approach utilizes objects with known contact characteristics to automatically generate reference force curves without human demonstrations. In addition, we design the dual convolutional neural networks (Dual-CNN) architecture which incorporating a physics-based mechanics module for learning target grasping force predictions from demonstrations. The described method can be effectively applied in vision-based tactile sensors and enables gentle and stable grasping of objects from the ground. The described prediction model and grasping strategy were validated in offline evaluations and online experiments, and the accuracy and generalizability were demonstrated.","sentences":["Humans can steadily and gently grasp unfamiliar objects based on tactile perception.","Robots still face challenges in achieving similar performance due to the difficulty of learning accurate grasp-force predictions and force control strategies that can be generalized from limited data.","In this article, we propose an approach for learning grasping from ideal force control demonstrations, to achieve similar performance of human hands with limited data size.","Our approach utilizes objects with known contact characteristics to automatically generate reference force curves without human demonstrations.","In addition, we design the dual convolutional neural networks (Dual-CNN) architecture which incorporating a physics-based mechanics module for learning target grasping force predictions from demonstrations.","The described method can be effectively applied in vision-based tactile sensors and enables gentle and stable grasping of objects from the ground.","The described prediction model and grasping strategy were validated in offline evaluations and online experiments, and the accuracy and generalizability were demonstrated."],"url":"http://arxiv.org/abs/2409.10371v1"}
{"created":"2024-09-16 15:13:39","title":"Uncovering the Mechanism of Hepatotoxiciy of PFAS Targeting L-FABP Using GCN and Computational Modeling","abstract":"Per- and polyfluoroalkyl substances (PFAS) are persistent environmental pollutants with known toxicity and bioaccumulation issues. Their widespread industrial use and resistance to degradation have led to global environmental contamination and significant health concerns. While a minority of PFAS have been extensively studied, the toxicity of many PFAS remains poorly understood due to limited direct toxicological data. This study advances the predictive modeling of PFAS toxicity by combining semi-supervised graph convolutional networks (GCNs) with molecular descriptors and fingerprints. We propose a novel approach to enhance the prediction of PFAS binding affinities by isolating molecular fingerprints to construct graphs where then descriptors are set as the node features. This approach specifically captures the structural, physicochemical, and topological features of PFAS without overfitting due to an abundance of features. Unsupervised clustering then identifies representative compounds for detailed binding studies. Our results provide a more accurate ability to estimate PFAS hepatotoxicity to provide guidance in chemical discovery of new PFAS and the development of new safety regulations.","sentences":["Per- and polyfluoroalkyl substances (PFAS) are persistent environmental pollutants with known toxicity and bioaccumulation issues.","Their widespread industrial use and resistance to degradation have led to global environmental contamination and significant health concerns.","While a minority of PFAS have been extensively studied, the toxicity of many PFAS remains poorly understood due to limited direct toxicological data.","This study advances the predictive modeling of PFAS toxicity by combining semi-supervised graph convolutional networks (GCNs) with molecular descriptors and fingerprints.","We propose a novel approach to enhance the prediction of PFAS binding affinities by isolating molecular fingerprints to construct graphs where then descriptors are set as the node features.","This approach specifically captures the structural, physicochemical, and topological features of PFAS without overfitting due to an abundance of features.","Unsupervised clustering then identifies representative compounds for detailed binding studies.","Our results provide a more accurate ability to estimate PFAS hepatotoxicity to provide guidance in chemical discovery of new PFAS and the development of new safety regulations."],"url":"http://arxiv.org/abs/2409.10370v1"}
{"created":"2024-09-16 15:11:10","title":"Global Uncertainty-Aware Planning for Magnetic Anomaly-Based Navigation","abstract":"Navigating and localizing in partially observable, stochastic environments with magnetic anomalies presents significant challenges, especially when balancing the accuracy of state estimation and the stability of localization. Traditional approaches often struggle to maintain performance due to limited localization updates and dynamic conditions. This paper introduces a multi-objective global path planner for magnetic anomaly navigation (MagNav), which leverages entropy maps to assess spatial frequency variations in magnetic fields and identify high-information areas. The system generates paths toward these regions by employing a potential field planner, enhancing active localization. Hardware experiments demonstrate that the proposed method significantly improves localization stability and accuracy compared to existing active localization techniques. The results underscore the effectiveness of this method in reducing localization uncertainty and highlight its adaptability to various gradient-based navigation maps, including topographical and underwater depth-based environments.","sentences":["Navigating and localizing in partially observable, stochastic environments with magnetic anomalies presents significant challenges, especially when balancing the accuracy of state estimation and the stability of localization.","Traditional approaches often struggle to maintain performance due to limited localization updates and dynamic conditions.","This paper introduces a multi-objective global path planner for magnetic anomaly navigation (MagNav), which leverages entropy maps to assess spatial frequency variations in magnetic fields and identify high-information areas.","The system generates paths toward these regions by employing a potential field planner, enhancing active localization.","Hardware experiments demonstrate that the proposed method significantly improves localization stability and accuracy compared to existing active localization techniques.","The results underscore the effectiveness of this method in reducing localization uncertainty and highlight its adaptability to various gradient-based navigation maps, including topographical and underwater depth-based environments."],"url":"http://arxiv.org/abs/2409.10366v1"}
{"created":"2024-09-16 15:11:00","title":"Robust image representations with counterfactual contrastive learning","abstract":"Contrastive pretraining can substantially increase model generalisation and downstream performance. However, the quality of the learned representations is highly dependent on the data augmentation strategy applied to generate positive pairs. Positive contrastive pairs should preserve semantic meaning while discarding unwanted variations related to the data acquisition domain. Traditional contrastive pipelines attempt to simulate domain shifts through pre-defined generic image transformations. However, these do not always mimic realistic and relevant domain variations for medical imaging such as scanner differences. To tackle this issue, we herein introduce counterfactual contrastive learning, a novel framework leveraging recent advances in causal image synthesis to create contrastive positive pairs that faithfully capture relevant domain variations. Our method, evaluated across five datasets encompassing both chest radiography and mammography data, for two established contrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive learning in terms of robustness to acquisition shift. Notably, counterfactual contrastive learning achieves superior downstream performance on both in-distribution and on external datasets, especially for images acquired with scanners under-represented in the training set. Further experiments show that the proposed framework extends beyond acquisition shifts, with models trained with counterfactual contrastive learning substantially improving subgroup performance across biological sex.","sentences":["Contrastive pretraining can substantially increase model generalisation and downstream performance.","However, the quality of the learned representations is highly dependent on the data augmentation strategy applied to generate positive pairs.","Positive contrastive pairs should preserve semantic meaning while discarding unwanted variations related to the data acquisition domain.","Traditional contrastive pipelines attempt to simulate domain shifts through pre-defined generic image transformations.","However, these do not always mimic realistic and relevant domain variations for medical imaging such as scanner differences.","To tackle this issue, we herein introduce counterfactual contrastive learning, a novel framework leveraging recent advances in causal image synthesis to create contrastive positive pairs that faithfully capture relevant domain variations.","Our method, evaluated across five datasets encompassing both chest radiography and mammography data, for two established contrastive objectives (SimCLR and DINO-v2), outperforms standard contrastive learning in terms of robustness to acquisition shift.","Notably, counterfactual contrastive learning achieves superior downstream performance on both in-distribution and on external datasets, especially for images acquired with scanners under-represented in the training set.","Further experiments show that the proposed framework extends beyond acquisition shifts, with models trained with counterfactual contrastive learning substantially improving subgroup performance across biological sex."],"url":"http://arxiv.org/abs/2409.10365v1"}
{"created":"2024-09-16 15:10:07","title":"Frequency-Guided Masking for Enhanced Vision Self-Supervised Learning","abstract":"We present a novel frequency-based Self-Supervised Learning (SSL) approach that significantly enhances its efficacy for pre-training. Prior work in this direction masks out pre-defined frequencies in the input image and employs a reconstruction loss to pre-train the model. While achieving promising results, such an implementation has two fundamental limitations as identified in our paper. First, using pre-defined frequencies overlooks the variability of image frequency responses. Second, pre-trained with frequency-filtered images, the resulting model needs relatively more data to adapt to naturally looking images during fine-tuning. To address these drawbacks, we propose FOurier transform compression with seLf-Knowledge distillation (FOLK), integrating two dedicated ideas. First, inspired by image compression, we adaptively select the masked-out frequencies based on image frequency responses, creating more suitable SSL tasks for pre-training. Second, we employ a two-branch framework empowered by knowledge distillation, enabling the model to take both the filtered and original images as input, largely reducing the burden of downstream tasks. Our experimental results demonstrate the effectiveness of FOLK in achieving competitive performance to many state-of-the-art SSL methods across various downstream tasks, including image classification, few-shot learning, and semantic segmentation.","sentences":["We present a novel frequency-based Self-Supervised Learning (SSL) approach that significantly enhances its efficacy for pre-training.","Prior work in this direction masks out pre-defined frequencies in the input image and employs a reconstruction loss to pre-train the model.","While achieving promising results, such an implementation has two fundamental limitations as identified in our paper.","First, using pre-defined frequencies overlooks the variability of image frequency responses.","Second, pre-trained with frequency-filtered images, the resulting model needs relatively more data to adapt to naturally looking images during fine-tuning.","To address these drawbacks, we propose FOurier transform compression with seLf-Knowledge distillation (FOLK), integrating two dedicated ideas.","First, inspired by image compression, we adaptively select the masked-out frequencies based on image frequency responses, creating more suitable SSL tasks for pre-training.","Second, we employ a two-branch framework empowered by knowledge distillation, enabling the model to take both the filtered and original images as input, largely reducing the burden of downstream tasks.","Our experimental results demonstrate the effectiveness of FOLK in achieving competitive performance to many state-of-the-art SSL methods across various downstream tasks, including image classification, few-shot learning, and semantic segmentation."],"url":"http://arxiv.org/abs/2409.10362v1"}
{"created":"2024-09-16 15:06:12","title":"2D or not 2D: How Does the Dimensionality of Gesture Representation Affect 3D Co-Speech Gesture Generation?","abstract":"Co-speech gestures are fundamental for communication. The advent of recent deep learning techniques has facilitated the creation of lifelike, synchronous co-speech gestures for Embodied Conversational Agents. \"In-the-wild\" datasets, aggregating video content from platforms like YouTube via human pose detection technologies, provide a feasible solution by offering 2D skeletal sequences aligned with speech. Concurrent developments in lifting models enable the conversion of these 2D sequences into 3D gesture databases. However, it is important to note that the 3D poses estimated from the 2D extracted poses are, in essence, approximations of the ground-truth, which remains in the 2D domain. This distinction raises questions about the impact of gesture representation dimensionality on the quality of generated motions - a topic that, to our knowledge, remains largely unexplored. Our study examines the effect of using either 2D or 3D joint coordinates as training data on the performance of speech-to-gesture deep generative models. We employ a lifting model for converting generated 2D pose sequences into 3D and assess how gestures created directly in 3D stack up against those initially generated in 2D and then converted to 3D. We perform an objective evaluation using widely used metrics in the gesture generation field as well as a user study to qualitatively evaluate the different approaches.","sentences":["Co-speech gestures are fundamental for communication.","The advent of recent deep learning techniques has facilitated the creation of lifelike, synchronous co-speech gestures for Embodied Conversational Agents.","\"In-the-wild\" datasets, aggregating video content from platforms like YouTube via human pose detection technologies, provide a feasible solution by offering 2D skeletal sequences aligned with speech.","Concurrent developments in lifting models enable the conversion of these 2D sequences into 3D gesture databases.","However, it is important to note that the 3D poses estimated from the 2D extracted poses are, in essence, approximations of the ground-truth, which remains in the 2D domain.","This distinction raises questions about the impact of gesture representation dimensionality on the quality of generated motions - a topic that, to our knowledge, remains largely unexplored.","Our study examines the effect of using either 2D or 3D joint coordinates as training data on the performance of speech-to-gesture deep generative models.","We employ a lifting model for converting generated 2D pose sequences into 3D and assess how gestures created directly in 3D stack up against those initially generated in 2D and then converted to 3D.","We perform an objective evaluation using widely used metrics in the gesture generation field as well as a user study to qualitatively evaluate the different approaches."],"url":"http://arxiv.org/abs/2409.10357v1"}
{"created":"2024-09-16 15:04:40","title":"Learnings from a Large-Scale Deployment of an LLM-Powered Expert-in-the-Loop Healthcare Chatbot","abstract":"Large Language Models (LLMs) are widely used in healthcare, but limitations like hallucinations, incomplete information, and bias hinder their reliability. To address these, researchers released the Build Your Own expert Bot (BYOeB) platform, enabling developers to create LLM-powered chatbots with integrated expert verification. CataractBot, its first implementation, provides expert-verified responses to cataract surgery questions. A pilot evaluation showed its potential; however the study had a small sample size and was primarily qualitative. In this work, we conducted a large-scale 24-week deployment of CataractBot involving 318 patients and attendants who sent 1,992 messages, with 91.71\\% of responses verified by seven experts. Analysis of interaction logs revealed that medical questions significantly outnumbered logistical ones, hallucinations were negligible, and experts rated 84.52\\% of medical answers as accurate. As the knowledge base expanded with expert corrections, system performance improved by 19.02\\%, reducing expert workload. These insights guide the design of future LLM-powered chatbots.","sentences":["Large Language Models (LLMs) are widely used in healthcare, but limitations like hallucinations, incomplete information, and bias hinder their reliability.","To address these, researchers released the Build Your Own expert Bot (BYOeB) platform, enabling developers to create LLM-powered chatbots with integrated expert verification.","CataractBot, its first implementation, provides expert-verified responses to cataract surgery questions.","A pilot evaluation showed its potential; however the study had a small sample size and was primarily qualitative.","In this work, we conducted a large-scale 24-week deployment of CataractBot involving 318 patients and attendants who sent 1,992 messages, with 91.71\\% of responses verified by seven experts.","Analysis of interaction logs revealed that medical questions significantly outnumbered logistical ones, hallucinations were negligible, and experts rated 84.52\\% of medical answers as accurate.","As the knowledge base expanded with expert corrections, system performance improved by 19.02\\%, reducing expert workload.","These insights guide the design of future LLM-powered chatbots."],"url":"http://arxiv.org/abs/2409.10354v1"}
{"created":"2024-09-16 15:04:14","title":"Taming Diffusion Models for Image Restoration: A Review","abstract":"Diffusion models have achieved remarkable progress in generative modelling, particularly in enhancing image quality to conform to human preferences. Recently, these models have also been applied to low-level computer vision for photo-realistic image restoration (IR) in tasks such as image denoising, deblurring, dehazing, etc. In this review paper, we introduce key constructions in diffusion models and survey contemporary techniques that make use of diffusion models in solving general IR tasks. Furthermore, we point out the main challenges and limitations of existing diffusion-based IR frameworks and provide potential directions for future work.","sentences":["Diffusion models have achieved remarkable progress in generative modelling, particularly in enhancing image quality to conform to human preferences.","Recently, these models have also been applied to low-level computer vision for photo-realistic image restoration (IR) in tasks such as image denoising, deblurring, dehazing, etc.","In this review paper, we introduce key constructions in diffusion models and survey contemporary techniques that make use of diffusion models in solving general IR tasks.","Furthermore, we point out the main challenges and limitations of existing diffusion-based IR frameworks and provide potential directions for future work."],"url":"http://arxiv.org/abs/2409.10353v1"}
{"created":"2024-09-16 15:01:28","title":"Point2Graph: An End-to-end Point Cloud-based 3D Open-Vocabulary Scene Graph for Robot Navigation","abstract":"Current open-vocabulary scene graph generation algorithms highly rely on both 3D scene point cloud data and posed RGB-D images and thus have limited applications in scenarios where RGB-D images or camera poses are not readily available. To solve this problem, we propose Point2Graph, a novel end-to-end point cloud-based 3D open-vocabulary scene graph generation framework in which the requirement of posed RGB-D image series is eliminated. This hierarchical framework contains room and object detection/segmentation and open-vocabulary classification. For the room layer, we leverage the advantage of merging the geometry-based border detection algorithm with the learning-based region detection to segment rooms and create a \"Snap-Lookup\" framework for open-vocabulary room classification. In addition, we create an end-to-end pipeline for the object layer to detect and classify 3D objects based solely on 3D point cloud data. Our evaluation results show that our framework can outperform the current state-of-the-art (SOTA) open-vocabulary object and room segmentation and classification algorithm on widely used real-scene datasets.","sentences":["Current open-vocabulary scene graph generation algorithms highly rely on both 3D scene point cloud data and posed RGB-D images and thus have limited applications in scenarios where RGB-D images or camera poses are not readily available.","To solve this problem, we propose Point2Graph, a novel end-to-end point cloud-based 3D open-vocabulary scene graph generation framework in which the requirement of posed RGB-D image series is eliminated.","This hierarchical framework contains room and object detection/segmentation and open-vocabulary classification.","For the room layer, we leverage the advantage of merging the geometry-based border detection algorithm with the learning-based region detection to segment rooms and create a \"Snap-Lookup\" framework for open-vocabulary room classification.","In addition, we create an end-to-end pipeline for the object layer to detect and classify 3D objects based solely on 3D point cloud data.","Our evaluation results show that our framework can outperform the current state-of-the-art (SOTA) open-vocabulary object and room segmentation and classification algorithm on widely used real-scene datasets."],"url":"http://arxiv.org/abs/2409.10350v1"}
{"created":"2024-09-16 14:58:00","title":"Digital Twins Meet the Koopman Operator: Data-Driven Learning for Robust Autonomy","abstract":"Contrary to on-road autonomous navigation, off-road autonomy is complicated by various factors ranging from sensing challenges to terrain variability. In such a milieu, data-driven approaches have been commonly employed to capture intricate vehicle-environment interactions effectively. However, the success of data-driven methods depends crucially on the quality and quantity of data, which can be compromised by large variability in off-road environments. To address these concerns, we present a novel workflow to recreate the exact vehicle and its target operating conditions digitally for domain-specific data generation. This enables us to effectively model off-road vehicle dynamics from simulation data using the Koopman operator theory, and employ the obtained models for local motion planning and optimal vehicle control. The capabilities of the proposed methodology are demonstrated through an autonomous navigation problem of a 1:5 scale vehicle, where a terrain-informed planner is employed for global mission planning. Results indicate a substantial improvement in off-road navigation performance with the proposed algorithm (5.84x) and underscore the efficacy of digital twinning in terms of improving the sample efficiency (3.2x) and reducing the sim2real gap (5.2%).","sentences":["Contrary to on-road autonomous navigation, off-road autonomy is complicated by various factors ranging from sensing challenges to terrain variability.","In such a milieu, data-driven approaches have been commonly employed to capture intricate vehicle-environment interactions effectively.","However, the success of data-driven methods depends crucially on the quality and quantity of data, which can be compromised by large variability in off-road environments.","To address these concerns, we present a novel workflow to recreate the exact vehicle and its target operating conditions digitally for domain-specific data generation.","This enables us to effectively model off-road vehicle dynamics from simulation data using the Koopman operator theory, and employ the obtained models for local motion planning and optimal vehicle control.","The capabilities of the proposed methodology are demonstrated through an autonomous navigation problem of a 1:5 scale vehicle, where a terrain-informed planner is employed for global mission planning.","Results indicate a substantial improvement in off-road navigation performance with the proposed algorithm (5.84x) and underscore the efficacy of digital twinning in terms of improving the sample efficiency (3.2x) and reducing the sim2real gap (5.2%)."],"url":"http://arxiv.org/abs/2409.10347v1"}
{"created":"2024-09-16 14:57:09","title":"Large Language Model Enhanced Hard Sample Identification for Denoising Recommendation","abstract":"Implicit feedback, often used to build recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias. Previous studies have attempted to alleviate this by identifying noisy samples based on their diverged patterns, such as higher loss values, and mitigating the noise through sample dropping or reweighting. Despite the progress, we observe existing approaches struggle to distinguish hard samples and noise samples, as they often exhibit similar patterns, thereby limiting their effectiveness in denoising recommendations. To address this challenge, we propose a Large Language Model Enhanced Hard Sample Denoising (LLMHD) framework. Specifically, we construct an LLM-based scorer to evaluate the semantic consistency of items with the user preference, which is quantified based on summarized historical user interactions. The resulting scores are used to assess the hardness of samples for the pointwise or pairwise training objectives. To ensure efficiency, we introduce a variance-based sample pruning strategy to filter potential hard samples before scoring. Besides, we propose an iterative preference update module designed to continuously refine summarized user preference, which may be biased due to false-positive user-item interactions. Extensive experiments on three real-world datasets and four backbone recommenders demonstrate the effectiveness of our approach.","sentences":["Implicit feedback, often used to build recommender systems, unavoidably confronts noise due to factors such as misclicks and position bias.","Previous studies have attempted to alleviate this by identifying noisy samples based on their diverged patterns, such as higher loss values, and mitigating the noise through sample dropping or reweighting.","Despite the progress, we observe existing approaches struggle to distinguish hard samples and noise samples, as they often exhibit similar patterns, thereby limiting their effectiveness in denoising recommendations.","To address this challenge, we propose a Large Language Model Enhanced Hard Sample Denoising (LLMHD) framework.","Specifically, we construct an LLM-based scorer to evaluate the semantic consistency of items with the user preference, which is quantified based on summarized historical user interactions.","The resulting scores are used to assess the hardness of samples for the pointwise or pairwise training objectives.","To ensure efficiency, we introduce a variance-based sample pruning strategy to filter potential hard samples before scoring.","Besides, we propose an iterative preference update module designed to continuously refine summarized user preference, which may be biased due to false-positive user-item interactions.","Extensive experiments on three real-world datasets and four backbone recommenders demonstrate the effectiveness of our approach."],"url":"http://arxiv.org/abs/2409.10343v1"}
{"created":"2024-09-16 14:56:59","title":"Detecting Sexism in German Online Newspaper Comments with Open-Source Text Embeddings (Team GDA, GermEval2024 Shared Task 1: GerMS-Detect, Subtasks 1 and 2, Closed Track)","abstract":"Sexism in online media comments is a pervasive challenge that often manifests subtly, complicating moderation efforts as interpretations of what constitutes sexism can vary among individuals. We study monolingual and multilingual open-source text embeddings to reliably detect sexism and misogyny in German-language online comments from an Austrian newspaper. We observed classifiers trained on text embeddings to mimic closely the individual judgements of human annotators. Our method showed robust performance in the GermEval 2024 GerMS-Detect Subtask 1 challenge, achieving an average macro F1 score of 0.597 (4th place, as reported on Codabench). It also accurately predicted the distribution of human annotations in GerMS-Detect Subtask 2, with an average Jensen-Shannon distance of 0.301 (2nd place). The computational efficiency of our approach suggests potential for scalable applications across various languages and linguistic contexts.","sentences":["Sexism in online media comments is a pervasive challenge that often manifests subtly, complicating moderation efforts as interpretations of what constitutes sexism can vary among individuals.","We study monolingual and multilingual open-source text embeddings to reliably detect sexism and misogyny in German-language online comments from an Austrian newspaper.","We observed classifiers trained on text embeddings to mimic closely the individual judgements of human annotators.","Our method showed robust performance in the GermEval 2024 GerMS-Detect Subtask 1 challenge, achieving an average macro F1 score of 0.597 (4th place, as reported on Codabench).","It also accurately predicted the distribution of human annotations in GerMS-Detect Subtask 2, with an average Jensen-Shannon distance of 0.301 (2nd place).","The computational efficiency of our approach suggests potential for scalable applications across various languages and linguistic contexts."],"url":"http://arxiv.org/abs/2409.10341v1"}
{"created":"2024-09-16 14:56:10","title":"Hyperedge Modeling in Hypergraph Neural Networks by using Densest Overlapping Subgraphs","abstract":"Hypergraphs tackle the limitations of traditional graphs by introducing {\\em hyperedges}. While graph edges connect only two nodes, hyperedges connect an arbitrary number of nodes along their edges. Also, the underlying message-passing mechanisms in Hypergraph Neural Networks (HGNNs) are in the form of vertex-hyperedge-vertex, which let HGNNs capture and utilize richer and more complex structural information than traditional Graph Neural Networks (GNNs). More recently, the idea of overlapping subgraphs has emerged. These subgraphs can capture more information about subgroups of vertices without limiting one vertex belonging to just one group, allowing vertices to belong to multiple groups or subgraphs. In addition, one of the most important problems in graph clustering is to find densest overlapping subgraphs (DOS). In this paper, we propose a solution to the DOS problem via Agglomerative Greedy Enumeration (DOSAGE) algorithm as a novel approach to enhance the process of generating the densest overlapping subgraphs and, hence, a robust construction of the hypergraphs. Experiments on standard benchmarks show that the DOSAGE algorithm significantly outperforms the HGNNs and six other methods on the node classification task.","sentences":["Hypergraphs tackle the limitations of traditional graphs by introducing {\\em hyperedges}.","While graph edges connect only two nodes, hyperedges connect an arbitrary number of nodes along their edges.","Also, the underlying message-passing mechanisms in Hypergraph Neural Networks (HGNNs) are in the form of vertex-hyperedge-vertex, which let HGNNs capture and utilize richer and more complex structural information than traditional Graph Neural Networks (GNNs).","More recently, the idea of overlapping subgraphs has emerged.","These subgraphs can capture more information about subgroups of vertices without limiting one vertex belonging to just one group, allowing vertices to belong to multiple groups or subgraphs.","In addition, one of the most important problems in graph clustering is to find densest overlapping subgraphs (DOS).","In this paper, we propose a solution to the DOS problem via Agglomerative Greedy Enumeration (DOSAGE) algorithm as a novel approach to enhance the process of generating the densest overlapping subgraphs and, hence, a robust construction of the hypergraphs.","Experiments on standard benchmarks show that the DOSAGE algorithm significantly outperforms the HGNNs and six other methods on the node classification task."],"url":"http://arxiv.org/abs/2409.10340v1"}
{"created":"2024-09-16 14:50:29","title":"The 20 questions game to distinguish large language models","abstract":"In a parallel with the 20 questions game, we present a method to determine whether two large language models (LLMs), placed in a black-box context, are the same or not. The goal is to use a small set of (benign) binary questions, typically under 20. We formalize the problem and first establish a baseline using a random selection of questions from known benchmark datasets, achieving an accuracy of nearly 100% within 20 questions. After showing optimal bounds for this problem, we introduce two effective questioning heuristics able to discriminate 22 LLMs by using half as many questions for the same task. These methods offer significant advantages in terms of stealth and are thus of interest to auditors or copyright owners facing suspicions of model leaks.","sentences":["In a parallel with the 20 questions game, we present a method to determine whether two large language models (LLMs), placed in a black-box context, are the same or not.","The goal is to use a small set of (benign) binary questions, typically under 20.","We formalize the problem and first establish a baseline using a random selection of questions from known benchmark datasets, achieving an accuracy of nearly 100% within 20 questions.","After showing optimal bounds for this problem, we introduce two effective questioning heuristics able to discriminate 22 LLMs by using half as many questions for the same task.","These methods offer significant advantages in terms of stealth and are thus of interest to auditors or copyright owners facing suspicions of model leaks."],"url":"http://arxiv.org/abs/2409.10338v1"}
{"created":"2024-09-16 14:48:20","title":"Security, Trust and Privacy challenges in AI-driven 6G Networks","abstract":"The advent of 6G networks promises unprecedented advancements in wireless communication, offering wider bandwidth and lower latency compared to its predecessors. This article explores the evolving infrastructure of 6G networks, emphasizing the transition towards a more disaggregated structure and the integration of artificial intelligence (AI) technologies. Furthermore, it explores the security, trust and privacy challenges and attacks in 6G networks, particularly those related to the use of AI. It presents a classification of network attacks stemming from its AI-centric architecture and explores technologies designed to detect or mitigate these emerging threats. The paper concludes by examining the implications and risks linked to the utilization of AI in ensuring a robust network.","sentences":["The advent of 6G networks promises unprecedented advancements in wireless communication, offering wider bandwidth and lower latency compared to its predecessors.","This article explores the evolving infrastructure of 6G networks, emphasizing the transition towards a more disaggregated structure and the integration of artificial intelligence (AI) technologies.","Furthermore, it explores the security, trust and privacy challenges and attacks in 6G networks, particularly those related to the use of AI.","It presents a classification of network attacks stemming from its AI-centric architecture and explores technologies designed to detect or mitigate these emerging threats.","The paper concludes by examining the implications and risks linked to the utilization of AI in ensuring a robust network."],"url":"http://arxiv.org/abs/2409.10337v1"}
{"created":"2024-09-16 14:46:52","title":"Execution-time opacity control for timed automata","abstract":"Timing leaks in timed automata (TA) can occur whenever an attacker is able to deduce a secret by observing some timed behavior. In execution-time opacity, the attacker aims at deducing whether a private location was visited, by observing only the execution time. It can be decided whether a TA is opaque in this setting. In this work, we tackle control, and show that we are able to decide whether a TA can be controlled at runtime to ensure opacity. Our method is constructive, in the sense that we can exhibit such a controller. We also address the case when the attacker cannot have an infinite precision in its observations.","sentences":["Timing leaks in timed automata (TA) can occur whenever an attacker is able to deduce a secret by observing some timed behavior.","In execution-time opacity, the attacker aims at deducing whether a private location was visited, by observing only the execution time.","It can be decided whether a TA is opaque in this setting.","In this work, we tackle control, and show that we are able to decide whether a TA can be controlled at runtime to ensure opacity.","Our method is constructive, in the sense that we can exhibit such a controller.","We also address the case when the attacker cannot have an infinite precision in its observations."],"url":"http://arxiv.org/abs/2409.10336v1"}
{"created":"2024-09-16 14:46:36","title":"Phys3DGS: Physically-based 3D Gaussian Splatting for Inverse Rendering","abstract":"We propose two novel ideas (adoption of deferred rendering and mesh-based representation) to improve the quality of 3D Gaussian splatting (3DGS) based inverse rendering. We first report a problem incurred by hidden Gaussians, where Gaussians beneath the surface adversely affect the pixel color in the volume rendering adopted by the existing methods. In order to resolve the problem, we propose applying deferred rendering and report new problems incurred in a naive application of deferred rendering to the existing 3DGS-based inverse rendering. In an effort to improve the quality of 3DGS-based inverse rendering under deferred rendering, we propose a novel two-step training approach which (1) exploits mesh extraction and utilizes a hybrid mesh-3DGS representation and (2) applies novel regularization methods to better exploit the mesh. Our experiments show that, under relighting, the proposed method offers significantly better rendering quality than the existing 3DGS-based inverse rendering methods. Compared with the SOTA voxel grid-based inverse rendering method, it gives better rendering quality while offering real-time rendering.","sentences":["We propose two novel ideas (adoption of deferred rendering and mesh-based representation) to improve the quality of 3D Gaussian splatting (3DGS) based inverse rendering.","We first report a problem incurred by hidden Gaussians, where Gaussians beneath the surface adversely affect the pixel color in the volume rendering adopted by the existing methods.","In order to resolve the problem, we propose applying deferred rendering and report new problems incurred in a naive application of deferred rendering to the existing 3DGS-based inverse rendering.","In an effort to improve the quality of 3DGS-based inverse rendering under deferred rendering, we propose a novel two-step training approach which (1) exploits mesh extraction and utilizes a hybrid mesh-3DGS representation and (2) applies novel regularization methods to better exploit the mesh.","Our experiments show that, under relighting, the proposed method offers significantly better rendering quality than the existing 3DGS-based inverse rendering methods.","Compared with the SOTA voxel grid-based inverse rendering method, it gives better rendering quality while offering real-time rendering."],"url":"http://arxiv.org/abs/2409.10335v1"}
{"created":"2024-09-16 14:45:39","title":"Stretchable Arduinos embedded in soft robots","abstract":"To achieve real-world functionality, robots must have the ability to carry out decision-making computations. However, soft robots stretch and therefore need a solution other than rigid computers. Examples of embedding computing capacity into soft robots currently include appending rigid printed circuit boards (PCBs) to the robot, integrating soft logic gates, and exploiting material responses for material-embedded computation. Although promising, these approaches introduce limitations such as rigidity, tethers, or low logic gate density. The field of stretchable electronics has sought to solve these challenges, but a complete pipeline for direct integration of single-board computers, microcontrollers, and other complex circuitry into soft robots has remained elusive. We present a generalized method to translate any complex two-layer circuit into a soft, stretchable form. This enabled the creation of stretchable single-board microcontrollers (including Arduinos) and other commercial circuits (including Sparkfun circuits), without design simplifications. As demonstrations of the method's utility, we embed highly stretchable (>300% strain) Arduino Pro Minis into the bodies of multiple soft robots. This makes use of otherwise inert structural material, fulfilling the promise of the stretchable electronics field to integrate state-of-the-art computational power into robust, stretchable systems during active use.","sentences":["To achieve real-world functionality, robots must have the ability to carry out decision-making computations.","However, soft robots stretch and therefore need a solution other than rigid computers.","Examples of embedding computing capacity into soft robots currently include appending rigid printed circuit boards (PCBs) to the robot, integrating soft logic gates, and exploiting material responses for material-embedded computation.","Although promising, these approaches introduce limitations such as rigidity, tethers, or low logic gate density.","The field of stretchable electronics has sought to solve these challenges, but a complete pipeline for direct integration of single-board computers, microcontrollers, and other complex circuitry into soft robots has remained elusive.","We present a generalized method to translate any complex two-layer circuit into a soft, stretchable form.","This enabled the creation of stretchable single-board microcontrollers (including Arduinos) and other commercial circuits (including Sparkfun circuits), without design simplifications.","As demonstrations of the method's utility, we embed highly stretchable (>300% strain) Arduino Pro Minis into the bodies of multiple soft robots.","This makes use of otherwise inert structural material, fulfilling the promise of the stretchable electronics field to integrate state-of-the-art computational power into robust, stretchable systems during active use."],"url":"http://arxiv.org/abs/2409.10333v1"}
{"created":"2024-09-16 14:41:55","title":"Escaping Local Minima: Hybrid Artificial Potential Field with Wall-Follower for Decentralized Multi-Robot Navigation","abstract":"We tackle the challenges of decentralized multi-robot navigation in environments with nonconvex obstacles, where complete environmental knowledge is unavailable. While reactive methods like Artificial Potential Field (APF) offer simplicity and efficiency, they suffer from local minima, causing robots to become trapped due to their lack of global environmental awareness. Other existing solutions either rely on inter-robot communication, are limited to single-robot scenarios, or struggle to overcome nonconvex obstacles effectively.   Our proposed methods enable collision-free navigation using only local sensor and state information without a map. By incorporating a wall-following (WF) behavior into the APF approach, our method allows robots to escape local minima, even in the presence of nonconvex and dynamic obstacles including other robots. We introduce two algorithms for switching between APF and WF: a rule-based system and an encoder network trained on expert demonstrations. Experimental results show that our approach achieves substantially higher success rates compared to state-of-the-art methods, highlighting its ability to overcome the limitations of local minima in complex environments","sentences":["We tackle the challenges of decentralized multi-robot navigation in environments with nonconvex obstacles, where complete environmental knowledge is unavailable.","While reactive methods like Artificial Potential Field (APF) offer simplicity and efficiency, they suffer from local minima, causing robots to become trapped due to their lack of global environmental awareness.","Other existing solutions either rely on inter-robot communication, are limited to single-robot scenarios, or struggle to overcome nonconvex obstacles effectively.   ","Our proposed methods enable collision-free navigation using only local sensor and state information without a map.","By incorporating a wall-following (WF) behavior into the APF approach, our method allows robots to escape local minima, even in the presence of nonconvex and dynamic obstacles including other robots.","We introduce two algorithms for switching between APF and WF: a rule-based system and an encoder network trained on expert demonstrations.","Experimental results show that our approach achieves substantially higher success rates compared to state-of-the-art methods, highlighting its ability to overcome the limitations of local minima in complex environments"],"url":"http://arxiv.org/abs/2409.10332v1"}
{"created":"2024-09-16 14:40:47","title":"DRIVE: Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving","abstract":"Recent advancements in autonomous driving have seen a paradigm shift towards end-to-end learning paradigms, which map sensory inputs directly to driving actions, thereby enhancing the robustness and adaptability of autonomous vehicles. However, these models often sacrifice interpretability, posing significant challenges to trust, safety, and regulatory compliance. To address these issues, we introduce DRIVE -- Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving, a comprehensive framework designed to improve the dependability and stability of explanations in end-to-end unsupervised autonomous driving models. Our work specifically targets the inherent instability problems observed in the Driving through the Concept Gridlock (DCG) model, which undermine the trustworthiness of its explanations and decision-making processes. We define four key attributes of DRIVE: consistent interpretability, stable interpretability, consistent output, and stable output. These attributes collectively ensure that explanations remain reliable and robust across different scenarios and perturbations. Through extensive empirical evaluations, we demonstrate the effectiveness of our framework in enhancing the stability and dependability of explanations, thereby addressing the limitations of current models. Our contributions include an in-depth analysis of the dependability issues within the DCG model, a rigorous definition of DRIVE with its fundamental properties, a framework to implement DRIVE, and novel metrics for evaluating the dependability of concept-based explainable autonomous driving models. These advancements lay the groundwork for the development of more reliable and trusted autonomous driving systems, paving the way for their broader acceptance and deployment in real-world applications.","sentences":["Recent advancements in autonomous driving have seen a paradigm shift towards end-to-end learning paradigms, which map sensory inputs directly to driving actions, thereby enhancing the robustness and adaptability of autonomous vehicles.","However, these models often sacrifice interpretability, posing significant challenges to trust, safety, and regulatory compliance.","To address these issues, we introduce DRIVE -- Dependable Robust Interpretable Visionary Ensemble Framework in Autonomous Driving, a comprehensive framework designed to improve the dependability and stability of explanations in end-to-end unsupervised autonomous driving models.","Our work specifically targets the inherent instability problems observed in the Driving through the Concept Gridlock (DCG) model, which undermine the trustworthiness of its explanations and decision-making processes.","We define four key attributes of DRIVE: consistent interpretability, stable interpretability, consistent output, and stable output.","These attributes collectively ensure that explanations remain reliable and robust across different scenarios and perturbations.","Through extensive empirical evaluations, we demonstrate the effectiveness of our framework in enhancing the stability and dependability of explanations, thereby addressing the limitations of current models.","Our contributions include an in-depth analysis of the dependability issues within the DCG model, a rigorous definition of DRIVE with its fundamental properties, a framework to implement DRIVE, and novel metrics for evaluating the dependability of concept-based explainable autonomous driving models.","These advancements lay the groundwork for the development of more reliable and trusted autonomous driving systems, paving the way for their broader acceptance and deployment in real-world applications."],"url":"http://arxiv.org/abs/2409.10330v1"}
{"created":"2024-09-16 14:39:15","title":"InfoDisent: Explainability of Image Classification Models by Information Disentanglement","abstract":"Understanding the decisions made by image classification networks is a critical area of research in deep learning. This task is traditionally divided into two distinct approaches: post-hoc methods and intrinsic methods. Post-hoc methods, such as GradCam, aim to interpret the decisions of pre-trained models by identifying regions of the image where the network focuses its attention. However, these methods provide only a high-level overview, making it difficult to fully understand the network's decision-making process. Conversely, intrinsic methods, like prototypical parts models, offer a more detailed understanding of network predictions but are constrained by specific architectures, training methods, and datasets.   In this paper, we introduce InfoDisent, a hybrid model that combines the advantages of both approaches. By utilizing an information bottleneck, InfoDisent disentangles the information in the final layer of a pre-trained deep network, enabling the breakdown of classification decisions into basic, understandable atomic components. Unlike standard prototypical parts approaches, InfoDisent can interpret the decisions of pre-trained classification networks and be used for making classification decisions, similar to intrinsic models. We validate the effectiveness of InfoDisent on benchmark datasets such as ImageNet, CUB-200-2011, Stanford Cars, and Stanford Dogs for both convolutional and transformer backbones.","sentences":["Understanding the decisions made by image classification networks is a critical area of research in deep learning.","This task is traditionally divided into two distinct approaches: post-hoc methods and intrinsic methods.","Post-hoc methods, such as GradCam, aim to interpret the decisions of pre-trained models by identifying regions of the image where the network focuses its attention.","However, these methods provide only a high-level overview, making it difficult to fully understand the network's decision-making process.","Conversely, intrinsic methods, like prototypical parts models, offer a more detailed understanding of network predictions but are constrained by specific architectures, training methods, and datasets.   ","In this paper, we introduce InfoDisent, a hybrid model that combines the advantages of both approaches.","By utilizing an information bottleneck, InfoDisent disentangles the information in the final layer of a pre-trained deep network, enabling the breakdown of classification decisions into basic, understandable atomic components.","Unlike standard prototypical parts approaches, InfoDisent can interpret the decisions of pre-trained classification networks and be used for making classification decisions, similar to intrinsic models.","We validate the effectiveness of InfoDisent on benchmark datasets such as ImageNet, CUB-200-2011, Stanford Cars, and Stanford Dogs for both convolutional and transformer backbones."],"url":"http://arxiv.org/abs/2409.10329v1"}
{"created":"2024-09-16 14:39:04","title":"Fuse4Seg: Image-Level Fusion Based Multi-Modality Medical Image Segmentation","abstract":"Although multi-modality medical image segmentation holds significant potential for enhancing the diagnosis and understanding of complex diseases by integrating diverse imaging modalities, existing methods predominantly rely on feature-level fusion strategies. We argue the current feature-level fusion strategy is prone to semantic inconsistencies and misalignments across various imaging modalities because it merges features at intermediate layers in a neural network without evaluative control. To mitigate this, we introduce a novel image-level fusion based multi-modality medical image segmentation method, Fuse4Seg, which is a bi-level learning framework designed to model the intertwined dependencies between medical image segmentation and medical image fusion. The image-level fusion process is seamlessly employed to guide and enhance the segmentation results through a layered optimization approach. Besides, the knowledge gained from the segmentation module can effectively enhance the fusion module. This ensures that the resultant fused image is a coherent representation that accurately amalgamates information from all modalities. Moreover, we construct a BraTS-Fuse benchmark based on BraTS dataset, which includes 2040 paired original images, multi-modal fusion images, and ground truth. This benchmark not only serves image-level medical segmentation but is also the largest dataset for medical image fusion to date. Extensive experiments on several public datasets and our benchmark demonstrate the superiority of our approach over prior state-of-the-art (SOTA) methodologies.","sentences":["Although multi-modality medical image segmentation holds significant potential for enhancing the diagnosis and understanding of complex diseases by integrating diverse imaging modalities, existing methods predominantly rely on feature-level fusion strategies.","We argue the current feature-level fusion strategy is prone to semantic inconsistencies and misalignments across various imaging modalities because it merges features at intermediate layers in a neural network without evaluative control.","To mitigate this, we introduce a novel image-level fusion based multi-modality medical image segmentation method, Fuse4Seg, which is a bi-level learning framework designed to model the intertwined dependencies between medical image segmentation and medical image fusion.","The image-level fusion process is seamlessly employed to guide and enhance the segmentation results through a layered optimization approach.","Besides, the knowledge gained from the segmentation module can effectively enhance the fusion module.","This ensures that the resultant fused image is a coherent representation that accurately amalgamates information from all modalities.","Moreover, we construct a BraTS-Fuse benchmark based on BraTS dataset, which includes 2040 paired original images, multi-modal fusion images, and ground truth.","This benchmark not only serves image-level medical segmentation but is also the largest dataset for medical image fusion to date.","Extensive experiments on several public datasets and our benchmark demonstrate the superiority of our approach over prior state-of-the-art (SOTA) methodologies."],"url":"http://arxiv.org/abs/2409.10328v1"}
{"created":"2024-09-16 14:38:26","title":"Baking Relightable NeRF for Real-time Direct/Indirect Illumination Rendering","abstract":"Relighting, which synthesizes a novel view under a given lighting condition (unseen in training time), is a must feature for immersive photo-realistic experience. However, real-time relighting is challenging due to high computation cost of the rendering equation which requires shape and material decomposition and visibility test to model shadow. Additionally, for indirect illumination, additional computation of rendering equation on each secondary surface point (where reflection occurs) is required rendering real-time relighting challenging. We propose a novel method that executes a CNN renderer to compute primary surface points and rendering parameters, required for direct illumination. We also present a lightweight hash grid-based renderer, for indirect illumination, which is recursively executed to perform the secondary ray tracing process. Both renderers are trained in a distillation from a pre-trained teacher model and provide real-time physically-based rendering under unseen lighting condition at a negligible loss of rendering quality.","sentences":["Relighting, which synthesizes a novel view under a given lighting condition (unseen in training time), is a must feature for immersive photo-realistic experience.","However, real-time relighting is challenging due to high computation cost of the rendering equation which requires shape and material decomposition and visibility test to model shadow.","Additionally, for indirect illumination, additional computation of rendering equation on each secondary surface point (where reflection occurs) is required rendering real-time relighting challenging.","We propose a novel method that executes a CNN renderer to compute primary surface points and rendering parameters, required for direct illumination.","We also present a lightweight hash grid-based renderer, for indirect illumination, which is recursively executed to perform the secondary ray tracing process.","Both renderers are trained in a distillation from a pre-trained teacher model and provide real-time physically-based rendering under unseen lighting condition at a negligible loss of rendering quality."],"url":"http://arxiv.org/abs/2409.10327v1"}
{"created":"2024-09-16 14:36:22","title":"PASS: An Asynchronous Probabilistic Processor for Next Generation Intelligence","abstract":"New computing paradigms are required to solve the most challenging computational problems where no exact polynomial time solution exists.Probabilistic Ising Accelerators has gained promise on these problems with the ability to model complex probability distributions and find ground states of intractable problems. In this context, we have demonstrated the Parallel Asynchronous Stochastic Sampler (PASS), the first fully on-chip integrated, asynchronous, probabilistic accelerator that takes advantage of the intrinsic fine-grained parallelism of the Ising Model and built in state of the art 14nm CMOS FinFET technology. We have demonstrated broad applicability of this accelerator on problems ranging from Combinatorial Optimization, Neural Simulation, to Machine Learning along with up to $23,000$x energy to solution improvement compared to CPUs on probabilistic problems.","sentences":["New computing paradigms are required to solve the most challenging computational problems where no exact polynomial time solution exists.","Probabilistic Ising Accelerators has gained promise on these problems with the ability to model complex probability distributions and find ground states of intractable problems.","In this context, we have demonstrated the Parallel Asynchronous Stochastic Sampler (PASS), the first fully on-chip integrated, asynchronous, probabilistic accelerator that takes advantage of the intrinsic fine-grained parallelism of the Ising Model and built in state of the art 14nm CMOS FinFET technology.","We have demonstrated broad applicability of this accelerator on problems ranging from Combinatorial Optimization, Neural Simulation, to Machine Learning along with up to $23,000$x energy to solution improvement compared to CPUs on probabilistic problems."],"url":"http://arxiv.org/abs/2409.10325v1"}
{"created":"2024-09-16 14:33:21","title":"SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary Learning for Closed-Loop Scenario Generation","abstract":"Verification and validation of autonomous driving (AD) systems and components is of increasing importance, as such technology increases in real-world prevalence. Safety-critical scenario generation is a key approach to robustify AD policies through closed-loop training. However, existing approaches for scenario generation rely on simplistic objectives, resulting in overly-aggressive or non-reactive adversarial behaviors. To generate diverse adversarial yet realistic scenarios, we propose SEAL, a scenario perturbation approach which leverages learned scoring functions and adversarial, human-like skills. SEAL-perturbed scenarios are more realistic than SOTA baselines, leading to improved ego task success across real-world, in-distribution, and out-of-distribution scenarios, of more than 20%. To facilitate future research, we release our code and tools: https://github.com/cmubig/SEAL","sentences":["Verification and validation of autonomous driving (AD) systems and components is of increasing importance, as such technology increases in real-world prevalence.","Safety-critical scenario generation is a key approach to robustify AD policies through closed-loop training.","However, existing approaches for scenario generation rely on simplistic objectives, resulting in overly-aggressive or non-reactive adversarial behaviors.","To generate diverse adversarial yet realistic scenarios, we propose SEAL, a scenario perturbation approach which leverages learned scoring functions and adversarial, human-like skills.","SEAL-perturbed scenarios are more realistic than SOTA baselines, leading to improved ego task success across real-world, in-distribution, and out-of-distribution scenarios, of more than 20%.","To facilitate future research, we release our code and tools: https://github.com/cmubig/SEAL"],"url":"http://arxiv.org/abs/2409.10320v1"}
{"created":"2024-09-16 14:32:25","title":"Catch It! Learning to Catch in Flight with Mobile Dexterous Hands","abstract":"Catching objects in flight (i.e., thrown objects) is a common daily skill for humans, yet it presents a significant challenge for robots. This task requires a robot with agile and accurate motion, a large spatial workspace, and the ability to interact with diverse objects. In this paper, we build a mobile manipulator composed of a mobile base, a 6-DoF arm, and a 12-DoF dexterous hand to tackle such a challenging task. We propose a two-stage reinforcement learning framework to efficiently train a whole-body-control catching policy for this high-DoF system in simulation. The objects' throwing configurations, shapes, and sizes are randomized during training to enhance policy adaptivity to various trajectories and object characteristics in flight. The results show that our trained policy catches diverse objects with randomly thrown trajectories, at a high success rate of about 80\\% in simulation, with a significant improvement over the baselines. The policy trained in simulation can be directly deployed in the real world with onboard sensing and computation, which achieves catching sandbags in various shapes, randomly thrown by humans. Our project page is available at https://mobile-dex-catch.github.io/.","sentences":["Catching objects in flight (i.e., thrown objects) is a common daily skill for humans, yet it presents a significant challenge for robots.","This task requires a robot with agile and accurate motion, a large spatial workspace, and the ability to interact with diverse objects.","In this paper, we build a mobile manipulator composed of a mobile base, a 6-DoF arm, and a 12-DoF dexterous hand to tackle such a challenging task.","We propose a two-stage reinforcement learning framework to efficiently train a whole-body-control catching policy for this high-DoF system in simulation.","The objects' throwing configurations, shapes, and sizes are randomized during training to enhance policy adaptivity to various trajectories and object characteristics in flight.","The results show that our trained policy catches diverse objects with randomly thrown trajectories, at a high success rate of about 80\\% in simulation, with a significant improvement over the baselines.","The policy trained in simulation can be directly deployed in the real world with onboard sensing and computation, which achieves catching sandbags in various shapes, randomly thrown by humans.","Our project page is available at https://mobile-dex-catch.github.io/."],"url":"http://arxiv.org/abs/2409.10319v1"}
{"created":"2024-09-16 14:25:37","title":"Rate-Splitting Multiple Access for Coexistence of Semantic and Bit Communications","abstract":"In the sixth generation (6G) of cellular networks, the demands for capacity and connectivity will increase dramatically to meet the requirements of emerging services for both humans and machines. Semantic communication has shown great potential because of its efficiency, and suitability for users who only care about the semantic meaning. But bit communication is still needed for users requiring original messages. Therefore, there will be a coexistence of semantic and bit communications in future networks. This motivates us to explore how to allocate resources in such a coexistence scenario. We investigate different uplink multiple access (MA) schemes for the coexistence of semantic users and a bit user, namely orthogonal multiple access (OMA), non-orthogonal multiple access (NOMA) and rate-splitting multiple access (RSMA). We characterize the rate regions achieved by those MA schemes. The simulation results show that RSMA always outperforms NOMA and has better performance in high semantic rate regimes compared to OMA. We find that RSMA scheme design, rate region, and power allocation are quite different in the coexistence scenario compared to the bit-only communication, primarily due to the need to consider the understandability in semantic communications. Interestingly, in contrast to bit-only communications where RSMA is capacity achieving without any need for time sharing, in the coexistence scenario, time sharing helps enlarging RSMA rate region.","sentences":["In the sixth generation (6G) of cellular networks, the demands for capacity and connectivity will increase dramatically to meet the requirements of emerging services for both humans and machines.","Semantic communication has shown great potential because of its efficiency, and suitability for users who only care about the semantic meaning.","But bit communication is still needed for users requiring original messages.","Therefore, there will be a coexistence of semantic and bit communications in future networks.","This motivates us to explore how to allocate resources in such a coexistence scenario.","We investigate different uplink multiple access (MA) schemes for the coexistence of semantic users and a bit user, namely orthogonal multiple access (OMA), non-orthogonal multiple access (NOMA) and rate-splitting multiple access (RSMA).","We characterize the rate regions achieved by those MA schemes.","The simulation results show that RSMA always outperforms NOMA and has better performance in high semantic rate regimes compared to OMA.","We find that RSMA scheme design, rate region, and power allocation are quite different in the coexistence scenario compared to the bit-only communication, primarily due to the need to consider the understandability in semantic communications.","Interestingly, in contrast to bit-only communications where RSMA is capacity achieving without any need for time sharing, in the coexistence scenario, time sharing helps enlarging RSMA rate region."],"url":"http://arxiv.org/abs/2409.10314v1"}
