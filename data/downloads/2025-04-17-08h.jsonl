{"created":"2025-04-16 17:59:54","title":"Adapting a World Model for Trajectory Following in a 3D Game","abstract":"Imitation learning is a powerful tool for training agents by leveraging expert knowledge, and being able to replicate a given trajectory is an integral part of it. In complex environments, like modern 3D video games, distribution shift and stochasticity necessitate robust approaches beyond simple action replay. In this study, we apply Inverse Dynamics Models (IDM) with different encoders and policy heads to trajectory following in a modern 3D video game -- Bleeding Edge. Additionally, we investigate several future alignment strategies that address the distribution shift caused by the aleatoric uncertainty and imperfections of the agent. We measure both the trajectory deviation distance and the first significant deviation point between the reference and the agent's trajectory and show that the optimal configuration depends on the chosen setting. Our results show that in a diverse data setting, a GPT-style policy head with an encoder trained from scratch performs the best, DINOv2 encoder with the GPT-style policy head gives the best results in the low data regime, and both GPT-style and MLP-style policy heads had comparable results when pre-trained on a diverse setting and fine-tuned for a specific behaviour setting.","sentences":["Imitation learning is a powerful tool for training agents by leveraging expert knowledge, and being able to replicate a given trajectory is an integral part of it.","In complex environments, like modern 3D video games, distribution shift and stochasticity necessitate robust approaches beyond simple action replay.","In this study, we apply Inverse Dynamics Models (IDM) with different encoders and policy heads to trajectory following in a modern 3D video game -- Bleeding Edge.","Additionally, we investigate several future alignment strategies that address the distribution shift caused by the aleatoric uncertainty and imperfections of the agent.","We measure both the trajectory deviation distance and the first significant deviation point between the reference and the agent's trajectory and show that the optimal configuration depends on the chosen setting.","Our results show that in a diverse data setting, a GPT-style policy head with an encoder trained from scratch performs the best, DINOv2 encoder with the GPT-style policy head gives the best results in the low data regime, and both GPT-style and MLP-style policy heads had comparable results when pre-trained on a diverse setting and fine-tuned for a specific behaviour setting."],"url":"http://arxiv.org/abs/2504.12299v1"}
{"created":"2025-04-16 17:55:02","title":"SHeaP: Self-Supervised Head Geometry Predictor Learned via 2D Gaussians","abstract":"Accurate, real-time 3D reconstruction of human heads from monocular images and videos underlies numerous visual applications. As 3D ground truth data is hard to come by at scale, previous methods have sought to learn from abundant 2D videos in a self-supervised manner. Typically, this involves the use of differentiable mesh rendering, which is effective but faces limitations. To improve on this, we propose SHeaP (Self-supervised Head Geometry Predictor Learned via 2D Gaussians). Given a source image, we predict a 3DMM mesh and a set of Gaussians that are rigged to this mesh. We then reanimate this rigged head avatar to match a target frame, and backpropagate photometric losses to both the 3DMM and Gaussian prediction networks. We find that using Gaussians for rendering substantially improves the effectiveness of this self-supervised approach. Training solely on 2D data, our method surpasses existing self-supervised approaches in geometric evaluations on the NoW benchmark for neutral faces and a new benchmark for non-neutral expressions. Our method also produces highly expressive meshes, outperforming state-of-the-art in emotion classification.","sentences":["Accurate, real-time 3D reconstruction of human heads from monocular images and videos underlies numerous visual applications.","As 3D ground truth data is hard to come by at scale, previous methods have sought to learn from abundant 2D videos in a self-supervised manner.","Typically, this involves the use of differentiable mesh rendering, which is effective but faces limitations.","To improve on this, we propose SHeaP (Self-supervised Head Geometry Predictor Learned via 2D Gaussians).","Given a source image, we predict a 3DMM mesh and a set of Gaussians that are rigged to this mesh.","We then reanimate this rigged head avatar to match a target frame, and backpropagate photometric losses to both the 3DMM and Gaussian prediction networks.","We find that using Gaussians for rendering substantially improves the effectiveness of this self-supervised approach.","Training solely on 2D data, our method surpasses existing self-supervised approaches in geometric evaluations on the NoW benchmark for neutral faces and a new benchmark for non-neutral expressions.","Our method also produces highly expressive meshes, outperforming state-of-the-art in emotion classification."],"url":"http://arxiv.org/abs/2504.12292v1"}
{"created":"2025-04-16 17:51:43","title":"BitNet b1.58 2B4T Technical Report","abstract":"We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.","sentences":["We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale.","Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability.","Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency.","To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures."],"url":"http://arxiv.org/abs/2504.12285v1"}
{"created":"2025-04-16 17:48:12","title":"How Do I Do That? Synthesizing 3D Hand Motion and Contacts for Everyday Interactions","abstract":"We tackle the novel problem of predicting 3D hand motion and contact maps (or Interaction Trajectories) given a single RGB view, action text, and a 3D contact point on the object as input. Our approach consists of (1) Interaction Codebook: a VQVAE model to learn a latent codebook of hand poses and contact points, effectively tokenizing interaction trajectories, (2) Interaction Predictor: a transformer-decoder module to predict the interaction trajectory from test time inputs by using an indexer module to retrieve a latent affordance from the learned codebook. To train our model, we develop a data engine that extracts 3D hand poses and contact trajectories from the diverse HoloAssist dataset. We evaluate our model on a benchmark that is 2.5-10X larger than existing works, in terms of diversity of objects and interactions observed, and test for generalization of the model across object categories, action categories, tasks, and scenes. Experimental results show the effectiveness of our approach over transformer & diffusion baselines across all settings.","sentences":["We tackle the novel problem of predicting 3D hand motion and contact maps (or Interaction Trajectories) given a single RGB view, action text, and a 3D contact point on the object as input.","Our approach consists of (1) Interaction Codebook: a VQVAE model to learn a latent codebook of hand poses and contact points, effectively tokenizing interaction trajectories, (2) Interaction Predictor: a transformer-decoder module to predict the interaction trajectory from test time inputs by using an indexer module to retrieve a latent affordance from the learned codebook.","To train our model, we develop a data engine that extracts 3D hand poses and contact trajectories from the diverse HoloAssist dataset.","We evaluate our model on a benchmark that is 2.5-10X larger than existing works, in terms of diversity of objects and interactions observed, and test for generalization of the model across object categories, action categories, tasks, and scenes.","Experimental results show the effectiveness of our approach over transformer & diffusion baselines across all settings."],"url":"http://arxiv.org/abs/2504.12284v1"}
{"created":"2025-04-16 17:44:33","title":"A Near-Optimal Kernel for a Coloring Problem","abstract":"For a fixed integer $q$, the $q$-Coloring problem asks to decide if a given graph has a vertex coloring with $q$ colors such that no two adjacent vertices receive the same color. In a series of papers, it has been shown that for every $q \\geq 3$, the $q$-Coloring problem parameterized by the vertex cover number $k$ admits a kernel of bit-size $\\widetilde{O}(k^{q-1})$, but admits no kernel of bit-size $O(k^{q-1-\\varepsilon})$ for $\\varepsilon >0$ unless $\\mathsf{NP} \\subseteq \\mathsf{coNP/poly}$ (Jansen and Kratsch, 2013; Jansen and Pieterse, 2019). In 2020, Schalken proposed the question of the kernelizability of the $q$-Coloring problem parameterized by the number $k$ of vertices whose removal results in a disjoint union of edges and isolated vertices. He proved that for every $q \\geq 3$, the problem admits a kernel of bit-size $\\widetilde{O}(k^{2q-2})$, but admits no kernel of bit-size $O(k^{2q-3-\\varepsilon})$ for $\\varepsilon >0$ unless $\\mathsf{NP} \\subseteq \\mathsf{coNP/poly}$. He further proved that for $q \\in \\{3,4\\}$ the problem admits a near-optimal kernel of bit-size $\\widetilde{O}(k^{2q-3})$ and asked whether such a kernel is achievable for all integers $q \\geq 3$. In this short paper, we settle this question in the affirmative.","sentences":["For a fixed integer $q$, the $q$-Coloring problem asks to decide if a given graph has a vertex coloring with $q$ colors such that no two adjacent vertices receive the same color.","In a series of papers, it has been shown that for every $q \\geq 3$, the $q$-Coloring problem parameterized by the vertex cover number $k$ admits a kernel of bit-size $\\widetilde{O}(k^{q-1})$, but admits no kernel of bit-size $O(k^{q-1-\\varepsilon})$ for $\\varepsilon >0$ unless $\\mathsf{NP} \\subseteq \\mathsf{coNP/poly}$ (Jansen and Kratsch, 2013; Jansen and Pieterse, 2019).","In 2020, Schalken proposed the question of the kernelizability of the $q$-Coloring problem parameterized by the number $k$ of vertices whose removal results in a disjoint union of edges and isolated vertices.","He proved that for every $q \\geq 3$, the problem admits a kernel of bit-size $\\widetilde{O}(k^{2q-2})$, but admits no kernel of bit-size $O(k^{2q-3-\\varepsilon})$ for $\\varepsilon >0$ unless $\\mathsf{NP} \\subseteq \\mathsf{coNP/poly}$. He further proved that for $q \\in \\{3,4\\}$ the problem admits a near-optimal kernel of bit-size $\\widetilde{O}(k^{2q-3})$ and asked whether such a kernel is achievable for all integers $q \\geq 3$.","In this short paper, we settle this question in the affirmative."],"url":"http://arxiv.org/abs/2504.12281v1"}
{"created":"2025-04-16 17:41:19","title":"Dysarthria Normalization via Local Lie Group Transformations for Robust ASR","abstract":"We present a geometry-driven method for normalizing dysarthric speech using local Lie group transformations of spectrograms. Time, frequency, and amplitude distortions are modeled as smooth, invertible deformations, parameterized by scalar fields and applied via exponential maps. A neural network is trained to infer these fields from synthetic distortions of typical speech-without using any pathological data. At test time, the model applies an approximate inverse to real dysarthric inputs. Despite zero-shot generalization, we observe substantial ASR gains, including up to 16 percentage points WER reduction on challenging TORGO samples, with no degradation on clean speech. This work introduces a principled, interpretable approach for robust speech recognition under motor speech disorders","sentences":["We present a geometry-driven method for normalizing dysarthric speech using local Lie group transformations of spectrograms.","Time, frequency, and amplitude distortions are modeled as smooth, invertible deformations, parameterized by scalar fields and applied via exponential maps.","A neural network is trained to infer these fields from synthetic distortions of typical speech-without using any pathological data.","At test time, the model applies an approximate inverse to real dysarthric inputs.","Despite zero-shot generalization, we observe substantial ASR gains, including up to 16 percentage points WER reduction on challenging TORGO samples, with no degradation on clean speech.","This work introduces a principled, interpretable approach for robust speech recognition under motor speech disorders"],"url":"http://arxiv.org/abs/2504.12279v1"}
{"created":"2025-04-16 17:35:09","title":"The Tenth NTIRE 2025 Image Denoising Challenge Report","abstract":"This paper presents an overview of the NTIRE 2025 Image Denoising Challenge ({\\sigma} = 50), highlighting the proposed methodologies and corresponding results. The primary objective is to develop a network architecture capable of achieving high-quality denoising performance, quantitatively evaluated using PSNR, without constraints on computational complexity or model size. The task assumes independent additive white Gaussian noise (AWGN) with a fixed noise level of 50. A total of 290 participants registered for the challenge, with 20 teams successfully submitting valid results, providing insights into the current state-of-the-art in image denoising.","sentences":["This paper presents an overview of the NTIRE 2025 Image Denoising Challenge ({\\sigma} = 50), highlighting the proposed methodologies and corresponding results.","The primary objective is to develop a network architecture capable of achieving high-quality denoising performance, quantitatively evaluated using PSNR, without constraints on computational complexity or model size.","The task assumes independent additive white Gaussian noise (AWGN) with a fixed noise level of 50.","A total of 290 participants registered for the challenge, with 20 teams successfully submitting valid results, providing insights into the current state-of-the-art in image denoising."],"url":"http://arxiv.org/abs/2504.12276v1"}
{"created":"2025-04-16 17:34:58","title":"Kernels for Storage Capacity and Dual Index Coding","abstract":"The storage capacity of a graph measures the maximum amount of information that can be stored across its vertices, such that the information at any vertex can be recovered from the information stored at its neighborhood. The study of this graph quantity is motivated by applications in distributed storage and by its intimate relations to the index coding problem from the area of network information theory. In the latter, one wishes to minimize the amount of information that has to be transmitted to a collection of receivers, in a way that enables each of them to discover its required data using some prior side information.   In this paper, we initiate the study of the Storage Capacity and Index Coding problems from the perspective of parameterized complexity. We prove that the Storage Capacity problem parameterized by the solution size admits a kernelization algorithm producing kernels of linear size. We also provide such a result for the Index Coding problem, in the linear and non-linear settings, where it is parameterized by the dual value of the solution, i.e., the length of the transmission that can be saved using the side information. A key ingredient in the proofs is the crown decomposition technique due to Chor, Fellows, and Juedes (WG 2003, WG 2004). As an application, we significantly extend an algorithmic result of Dau, Skachek, and Chee (IEEE Trans. Inform. Theory, 2014).","sentences":["The storage capacity of a graph measures the maximum amount of information that can be stored across its vertices, such that the information at any vertex can be recovered from the information stored at its neighborhood.","The study of this graph quantity is motivated by applications in distributed storage and by its intimate relations to the index coding problem from the area of network information theory.","In the latter, one wishes to minimize the amount of information that has to be transmitted to a collection of receivers, in a way that enables each of them to discover its required data using some prior side information.   ","In this paper, we initiate the study of the Storage Capacity and Index Coding problems from the perspective of parameterized complexity.","We prove that the Storage Capacity problem parameterized by the solution size admits a kernelization algorithm producing kernels of linear size.","We also provide such a result for the Index Coding problem, in the linear and non-linear settings, where it is parameterized by the dual value of the solution, i.e., the length of the transmission that can be saved using the side information.","A key ingredient in the proofs is the crown decomposition technique due to Chor, Fellows, and Juedes (WG 2003, WG 2004).","As an application, we significantly extend an algorithmic result of Dau, Skachek, and Chee (IEEE Trans.","Inform.","Theory, 2014)."],"url":"http://arxiv.org/abs/2504.12274v1"}
{"created":"2025-04-16 17:32:50","title":"Beyond Reconstruction: A Physics Based Neural Deferred Shader for Photo-realistic Rendering","abstract":"Deep learning based rendering has demonstrated major improvements for photo-realistic image synthesis, applicable to various applications including visual effects in movies and photo-realistic scene building in video games. However, a significant limitation is the difficulty of decomposing the illumination and material parameters, which limits such methods to reconstruct an input scene, without any possibility to control these parameters. This paper introduces a novel physics based neural deferred shading pipeline to decompose the data-driven rendering process, learn a generalizable shading function to produce photo-realistic results for shading and relighting tasks, we also provide a shadow estimator to efficiently mimic shadowing effect. Our model achieves improved performance compared to classical models and a state-of-art neural shading model, and enables generalizable photo-realistic shading from arbitrary illumination input.","sentences":["Deep learning based rendering has demonstrated major improvements for photo-realistic image synthesis, applicable to various applications including visual effects in movies and photo-realistic scene building in video games.","However, a significant limitation is the difficulty of decomposing the illumination and material parameters, which limits such methods to reconstruct an input scene, without any possibility to control these parameters.","This paper introduces a novel physics based neural deferred shading pipeline to decompose the data-driven rendering process, learn a generalizable shading function to produce photo-realistic results for shading and relighting tasks, we also provide a shadow estimator to efficiently mimic shadowing effect.","Our model achieves improved performance compared to classical models and a state-of-art neural shading model, and enables generalizable photo-realistic shading from arbitrary illumination input."],"url":"http://arxiv.org/abs/2504.12273v1"}
{"created":"2025-04-16 17:30:36","title":"HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level Synthesis Design Tasks","abstract":"The rapid scaling of large language model (LLM) training and inference has driven their adoption in semiconductor design across academia and industry. While most prior work evaluates LLMs on hardware description language (HDL) tasks, particularly Verilog, designers are increasingly using high-level synthesis (HLS) to build domain-specific accelerators and complex hardware systems. However, benchmarks and tooling to comprehensively evaluate LLMs for HLS design tasks remain scarce.   To address this, we introduce HLS-Eval, the first complete benchmark and evaluation framework for LLM-driven HLS design. HLS-Eval targets two core tasks: (1) generating HLS code from natural language descriptions, and (2) performing HLS-specific code edits to optimize performance and hardware efficiency. The benchmark includes 94 unique designs drawn from standard HLS benchmarks and novel sources. Each case is prepared via a semi-automated flow that produces a natural language description and a paired testbench for C-simulation and synthesis validation, ensuring each task is \"LLM-ready.\"   Beyond the benchmark, HLS-Eval offers a modular Python framework for automated, parallel evaluation of both local and hosted LLMs. It includes a parallel evaluation engine, direct HLS tool integration, and abstractions for to support different LLM interaction paradigms, enabling rapid prototyping of new benchmarks, tasks, and LLM methods.   We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on Vitis HLS, measuring outputs across four key metrics - parseability, compilability, runnability, and synthesizability - reflecting the iterative HLS design cycle. We also report pass@k metrics, establishing clear baselines and reusable infrastructure for the broader LLM-for-hardware community.   All benchmarks, framework code, and results are open-sourced at https://github.com/stefanpie/hls-eval.","sentences":["The rapid scaling of large language model (LLM) training and inference has driven their adoption in semiconductor design across academia and industry.","While most prior work evaluates LLMs on hardware description language (HDL) tasks, particularly Verilog, designers are increasingly using high-level synthesis (HLS) to build domain-specific accelerators and complex hardware systems.","However, benchmarks and tooling to comprehensively evaluate LLMs for HLS design tasks remain scarce.   ","To address this, we introduce HLS-Eval, the first complete benchmark and evaluation framework for LLM-driven HLS design.","HLS-Eval targets two core tasks: (1) generating HLS code from natural language descriptions, and (2) performing HLS-specific code edits to optimize performance and hardware efficiency.","The benchmark includes 94 unique designs drawn from standard HLS benchmarks and novel sources.","Each case is prepared via a semi-automated flow that produces a natural language description and a paired testbench for C-simulation and synthesis validation, ensuring each task is \"LLM-ready.\"   ","Beyond the benchmark, HLS-Eval offers a modular Python framework for automated, parallel evaluation of both local and hosted LLMs.","It includes a parallel evaluation engine, direct HLS tool integration, and abstractions for to support different LLM interaction paradigms, enabling rapid prototyping of new benchmarks, tasks, and LLM methods.   ","We demonstrate HLS-Eval through baseline evaluations of open-source LLMs on Vitis HLS, measuring outputs across four key metrics - parseability, compilability, runnability, and synthesizability - reflecting the iterative HLS design cycle.","We also report pass@k metrics, establishing clear baselines and reusable infrastructure for the broader LLM-for-hardware community.   ","All benchmarks, framework code, and results are open-sourced at https://github.com/stefanpie/hls-eval."],"url":"http://arxiv.org/abs/2504.12268v1"}
{"created":"2025-04-16 17:21:55","title":"Towards Learning to Complete Anything in Lidar","abstract":"We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion in-the-wild. This is closely related to Lidar-based semantic/panoptic scene completion. However, contemporary methods can only complete and recognize objects from a closed vocabulary labeled in existing Lidar datasets. Different to that, our zero-shot approach leverages the temporal context from multi-modal sensor sequences to mine object shapes and semantic features of observed objects. These are then distilled into a Lidar-only instance-level completion and recognition model. Although we only mine partial shape completions, we find that our distilled model learns to infer full object shapes from multiple such partial observations across the dataset. We show that our model can be prompted on standard benchmarks for Semantic and Panoptic Scene Completion, localize objects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class vocabularies. Our project page is https://research.nvidia.com/labs/dvl/projects/complete-anything-lidar","sentences":["We propose CAL (Complete Anything in Lidar) for Lidar-based shape-completion in-the-wild.","This is closely related to Lidar-based semantic/panoptic scene completion.","However, contemporary methods can only complete and recognize objects from a closed vocabulary labeled in existing Lidar datasets.","Different to that, our zero-shot approach leverages the temporal context from multi-modal sensor sequences to mine object shapes and semantic features of observed objects.","These are then distilled into a Lidar-only instance-level completion and recognition model.","Although we only mine partial shape completions, we find that our distilled model learns to infer full object shapes from multiple such partial observations across the dataset.","We show that our model can be prompted on standard benchmarks for Semantic and Panoptic Scene Completion, localize objects as (amodal) 3D bounding boxes, and recognize objects beyond fixed class vocabularies.","Our project page is https://research.nvidia.com/labs/dvl/projects/complete-anything-lidar"],"url":"http://arxiv.org/abs/2504.12264v1"}
{"created":"2025-04-16 17:17:31","title":"SCENT: Robust Spatiotemporal Learning for Continuous Scientific Data via Scalable Conditioned Neural Fields","abstract":"Spatiotemporal learning is challenging due to the intricate interplay between spatial and temporal dependencies, the high dimensionality of the data, and scalability constraints. These challenges are further amplified in scientific domains, where data is often irregularly distributed (e.g., missing values from sensor failures) and high-volume (e.g., high-fidelity simulations), posing additional computational and modeling difficulties. In this paper, we present SCENT, a novel framework for scalable and continuity-informed spatiotemporal representation learning. SCENT unifies interpolation, reconstruction, and forecasting within a single architecture. Built on a transformer-based encoder-processor-decoder backbone, SCENT introduces learnable queries to enhance generalization and a query-wise cross-attention mechanism to effectively capture multi-scale dependencies. To ensure scalability in both data size and model complexity, we incorporate a sparse attention mechanism, enabling flexible output representations and efficient evaluation at arbitrary resolutions. We validate SCENT through extensive simulations and real-world experiments, demonstrating state-of-the-art performance across multiple challenging tasks while achieving superior scalability.","sentences":["Spatiotemporal learning is challenging due to the intricate interplay between spatial and temporal dependencies, the high dimensionality of the data, and scalability constraints.","These challenges are further amplified in scientific domains, where data is often irregularly distributed (e.g., missing values from sensor failures) and high-volume (e.g., high-fidelity simulations), posing additional computational and modeling difficulties.","In this paper, we present SCENT, a novel framework for scalable and continuity-informed spatiotemporal representation learning.","SCENT unifies interpolation, reconstruction, and forecasting within a single architecture.","Built on a transformer-based encoder-processor-decoder backbone, SCENT introduces learnable queries to enhance generalization and a query-wise cross-attention mechanism to effectively capture multi-scale dependencies.","To ensure scalability in both data size and model complexity, we incorporate a sparse attention mechanism, enabling flexible output representations and efficient evaluation at arbitrary resolutions.","We validate SCENT through extensive simulations and real-world experiments, demonstrating state-of-the-art performance across multiple challenging tasks while achieving superior scalability."],"url":"http://arxiv.org/abs/2504.12262v1"}
{"created":"2025-04-16 17:15:58","title":"Dependency Dilemmas: A Comparative Study of Independent and Dependent Artifacts in Maven Central Ecosystem","abstract":"The Maven Central ecosystem forms the backbone of Java dependency management, hosting artifacts that vary significantly in their adoption, security, and ecosystem roles. Artifact reuse is fundamental in software development, with ecosystems like Maven Central facilitating this process. However, prior studies predominantly analyzed popular artifacts with numerous dependencies, leaving those without incoming dependencies (independent artifacts) unexplored. In this study, we analyzed 658,078 artifacts, of which 635,003 had at least one release. Among these, 93,101 artifacts (15.4%) were identified as independent (in-degree = 0), while the rest were classified as dependent. We looked at the impact of separate artifacts using PageRank and out-degree centrality and discovered that they were very important to the ecosystem. Further analysis across 18 different metrics revealed several advantages and comparability of independent artifacts with dependent artifacts: comparable popularity (25.58 vs. 7.30), fewer vulnerabilities (60 CVEs vs. 179 CVEs), and zero propagated vulnerabilities. Based on these results, it seems that independent artifacts make a big difference in the ecosystem and give developers a safe, self-contained alternative to traditional dependencies. These findings suggest that independent artifacts might be a beneficial choice for dependencies but have some maintainability issues. Therefore, developers should carefully incorporate independent artifacts into their projects, and artifact maintainers should prioritize this group of artifacts to mitigate the risk of transitive vulnerability propagation and improve software sustainability.","sentences":["The Maven Central ecosystem forms the backbone of Java dependency management, hosting artifacts that vary significantly in their adoption, security, and ecosystem roles.","Artifact reuse is fundamental in software development, with ecosystems like Maven Central facilitating this process.","However, prior studies predominantly analyzed popular artifacts with numerous dependencies, leaving those without incoming dependencies (independent artifacts) unexplored.","In this study, we analyzed 658,078 artifacts, of which 635,003 had at least one release.","Among these, 93,101 artifacts (15.4%) were identified as independent (in-degree = 0), while the rest were classified as dependent.","We looked at the impact of separate artifacts using PageRank and out-degree centrality and discovered that they were very important to the ecosystem.","Further analysis across 18 different metrics revealed several advantages and comparability of independent artifacts with dependent artifacts: comparable popularity (25.58 vs. 7.30), fewer vulnerabilities (60 CVEs vs. 179 CVEs), and zero propagated vulnerabilities.","Based on these results, it seems that independent artifacts make a big difference in the ecosystem and give developers a safe, self-contained alternative to traditional dependencies.","These findings suggest that independent artifacts might be a beneficial choice for dependencies but have some maintainability issues.","Therefore, developers should carefully incorporate independent artifacts into their projects, and artifact maintainers should prioritize this group of artifacts to mitigate the risk of transitive vulnerability propagation and improve software sustainability."],"url":"http://arxiv.org/abs/2504.12261v1"}
{"created":"2025-04-16 17:09:13","title":"VGDFR: Diffusion-based Video Generation with Dynamic Latent Frame Rate","abstract":"Diffusion Transformer(DiT)-based generation models have achieved remarkable success in video generation. However, their inherent computational demands pose significant efficiency challenges. In this paper, we exploit the inherent temporal non-uniformity of real-world videos and observe that videos exhibit dynamic information density, with high-motion segments demanding greater detail preservation than static scenes. Inspired by this temporal non-uniformity, we propose VGDFR, a training-free approach for Diffusion-based Video Generation with Dynamic Latent Frame Rate. VGDFR adaptively adjusts the number of elements in latent space based on the motion frequency of the latent space content, using fewer tokens for low-frequency segments while preserving detail in high-frequency segments. Specifically, our key contributions are: (1) A dynamic frame rate scheduler for DiT video generation that adaptively assigns frame rates for video segments. (2) A novel latent-space frame merging method to align latent representations with their denoised counterparts before merging those redundant in low-resolution space. (3) A preference analysis of Rotary Positional Embeddings (RoPE) across DiT layers, informing a tailored RoPE strategy optimized for semantic and local information capture. Experiments show that VGDFR can achieve a speedup up to 3x for video generation with minimal quality degradation.","sentences":["Diffusion Transformer(DiT)-based generation models have achieved remarkable success in video generation.","However, their inherent computational demands pose significant efficiency challenges.","In this paper, we exploit the inherent temporal non-uniformity of real-world videos and observe that videos exhibit dynamic information density, with high-motion segments demanding greater detail preservation than static scenes.","Inspired by this temporal non-uniformity, we propose VGDFR, a training-free approach for Diffusion-based Video Generation with Dynamic Latent Frame Rate.","VGDFR adaptively adjusts the number of elements in latent space based on the motion frequency of the latent space content, using fewer tokens for low-frequency segments while preserving detail in high-frequency segments.","Specifically, our key contributions are: (1) A dynamic frame rate scheduler for DiT video generation that adaptively assigns frame rates for video segments.","(2) A novel latent-space frame merging method to align latent representations with their denoised counterparts before merging those redundant in low-resolution space.","(3) A preference analysis of Rotary Positional Embeddings (RoPE) across DiT layers, informing a tailored RoPE strategy optimized for semantic and local information capture.","Experiments show that VGDFR can achieve a speedup up to 3x for video generation with minimal quality degradation."],"url":"http://arxiv.org/abs/2504.12259v1"}
{"created":"2025-04-16 17:07:16","title":"FLIP Reasoning Challenge","abstract":"Over the past years, advances in artificial intelligence (AI) have demonstrated how AI can solve many perception and generation tasks, such as image classification and text writing, yet reasoning remains a challenge. This paper introduces the FLIP dataset, a benchmark for evaluating AI reasoning capabilities based on human verification tasks on the Idena blockchain. FLIP challenges present users with two orderings of 4 images, requiring them to identify the logically coherent one. By emphasizing sequential reasoning, visual storytelling, and common sense, FLIP provides a unique testbed for multimodal AI systems. Our experiments evaluate state-of-the-art models, leveraging both vision-language models (VLMs) and large language models (LLMs). Results reveal that even the best open-sourced and closed-sourced models achieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot settings, compared to human performance of 95.3%. Captioning models aid reasoning models by providing text descriptions of images, yielding better results than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5 Pro. Combining the predictions from 15 models in an ensemble increases the accuracy to 85.2%. These findings highlight the limitations of existing reasoning models and the need for robust multimodal benchmarks like FLIP. The full codebase and dataset will be available at https://github.com/aplesner/FLIP-Reasoning-Challenge.","sentences":["Over the past years, advances in artificial intelligence (AI) have demonstrated how AI can solve many perception and generation tasks, such as image classification and text writing, yet reasoning remains a challenge.","This paper introduces the FLIP dataset, a benchmark for evaluating AI reasoning capabilities based on human verification tasks on the Idena blockchain.","FLIP challenges present users with two orderings of 4 images, requiring them to identify the logically coherent one.","By emphasizing sequential reasoning, visual storytelling, and common sense, FLIP provides a unique testbed for multimodal AI systems.","Our experiments evaluate state-of-the-art models, leveraging both vision-language models (VLMs) and large language models (LLMs).","Results reveal that even the best open-sourced and closed-sourced models achieve maximum accuracies of 75.5% and 77.9%, respectively, in zero-shot settings, compared to human performance of 95.3%.","Captioning models aid reasoning models by providing text descriptions of images, yielding better results than when using the raw images directly, 69.6% vs. 75.2% for Gemini 1.5 Pro.","Combining the predictions from 15 models in an ensemble increases the accuracy to 85.2%.","These findings highlight the limitations of existing reasoning models and the need for robust multimodal benchmarks like FLIP.","The full codebase and dataset will be available at https://github.com/aplesner/FLIP-Reasoning-Challenge."],"url":"http://arxiv.org/abs/2504.12256v1"}
{"created":"2025-04-16 17:05:58","title":"Human Aligned Compression for Robust Models","abstract":"Adversarial attacks on image models threaten system robustness by introducing imperceptible perturbations that cause incorrect predictions. We investigate human-aligned learned lossy compression as a defense mechanism, comparing two learned models (HiFiC and ELIC) against traditional JPEG across various quality levels. Our experiments on ImageNet subsets demonstrate that learned compression methods outperform JPEG, particularly for Vision Transformer architectures, by preserving semantically meaningful content while removing adversarial noise. Even in white-box settings where attackers can access the defense, these methods maintain substantial effectiveness. We also show that sequential compression--applying rounds of compression/decompression--significantly enhances defense efficacy while maintaining classification performance. Our findings reveal that human-aligned compression provides an effective, computationally efficient defense that protects the image features most relevant to human and machine understanding. It offers a practical approach to improving model robustness against adversarial threats.","sentences":["Adversarial attacks on image models threaten system robustness by introducing imperceptible perturbations that cause incorrect predictions.","We investigate human-aligned learned lossy compression as a defense mechanism, comparing two learned models (HiFiC and ELIC) against traditional JPEG across various quality levels.","Our experiments on ImageNet subsets demonstrate that learned compression methods outperform JPEG, particularly for Vision Transformer architectures, by preserving semantically meaningful content while removing adversarial noise.","Even in white-box settings where attackers can access the defense, these methods maintain substantial effectiveness.","We also show that sequential compression--applying rounds of compression/decompression--significantly enhances defense efficacy while maintaining classification performance.","Our findings reveal that human-aligned compression provides an effective, computationally efficient defense that protects the image features most relevant to human and machine understanding.","It offers a practical approach to improving model robustness against adversarial threats."],"url":"http://arxiv.org/abs/2504.12255v1"}
{"created":"2025-04-16 17:05:14","title":"Advancing Arabic Speech Recognition Through Large-Scale Weakly Supervised Learning","abstract":"Automatic speech recognition (ASR) is crucial for human-machine interaction in diverse applications like conversational agents, industrial robotics, call center automation, and automated subtitling. However, developing high-performance ASR models remains challenging, particularly for low-resource languages like Arabic, due to the scarcity of large, labeled speech datasets, which are costly and labor-intensive to produce. In this work, we employ weakly supervised learning to train an Arabic ASR model using the Conformer architecture. Our model is trained from scratch on 15,000 hours of weakly annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal Arabic (DA), eliminating the need for costly manual transcriptions. Despite the absence of human-verified labels, our approach attains state-of-the-art (SOTA) performance, exceeding all previous efforts in the field of Arabic ASR on the standard benchmarks. By demonstrating the effectiveness of weak supervision as a scalable, cost-efficient alternative to traditional supervised approaches, paving the way for improved ASR systems in low resource settings.","sentences":["Automatic speech recognition (ASR) is crucial for human-machine interaction in diverse applications like conversational agents, industrial robotics, call center automation, and automated subtitling.","However, developing high-performance ASR models remains challenging, particularly for low-resource languages like Arabic, due to the scarcity of large, labeled speech datasets, which are costly and labor-intensive to produce.","In this work, we employ weakly supervised learning to train an Arabic ASR model using the Conformer architecture.","Our model is trained from scratch on 15,000 hours of weakly annotated speech data covering both Modern Standard Arabic (MSA) and Dialectal Arabic (DA), eliminating the need for costly manual transcriptions.","Despite the absence of human-verified labels, our approach attains state-of-the-art (SOTA) performance, exceeding all previous efforts in the field of Arabic ASR on the standard benchmarks.","By demonstrating the effectiveness of weak supervision as a scalable, cost-efficient alternative to traditional supervised approaches, paving the way for improved ASR systems in low resource settings."],"url":"http://arxiv.org/abs/2504.12254v1"}
{"created":"2025-04-16 16:58:26","title":"An Evaluation of N-Gram Selection Strategies for Regular Expression Indexing in Contemporary Text Analysis Tasks","abstract":"Efficient evaluation of regular expressions (regex, for short) is crucial for text analysis, and n-gram indexes are fundamental to achieving fast regex evaluation performance. However, these indexes face scalability challenges because of the exponential number of possible n-grams that must be indexed. Many existing selection strategies, developed decades ago, have not been rigorously evaluated on contemporary large-scale workloads and lack comprehensive performance comparisons. Therefore, a unified and comprehensive evaluation framework is necessary to compare these methods under the same experimental settings. This paper presents the first systematic evaluation of three representative n-gram selection strategies across five workloads, including real-time production logs and genomic sequence analysis. We examine their trade-offs in terms of index construction time, storage overhead, false positive rates, and end-to-end query performance. Through empirical results, this study provides a modern perspective on existing n-gram based regular expression evaluation methods, extensive observations, valuable discoveries, and an adaptable testing framework to guide future research in this domain. We make our implementations of these methods and our test framework available as open-source at https://github.com/mush-zhang/RegexIndexComparison.","sentences":["Efficient evaluation of regular expressions (regex, for short) is crucial for text analysis, and n-gram indexes are fundamental to achieving fast regex evaluation performance.","However, these indexes face scalability challenges because of the exponential number of possible n-grams that must be indexed.","Many existing selection strategies, developed decades ago, have not been rigorously evaluated on contemporary large-scale workloads and lack comprehensive performance comparisons.","Therefore, a unified and comprehensive evaluation framework is necessary to compare these methods under the same experimental settings.","This paper presents the first systematic evaluation of three representative n-gram selection strategies across five workloads, including real-time production logs and genomic sequence analysis.","We examine their trade-offs in terms of index construction time, storage overhead, false positive rates, and end-to-end query performance.","Through empirical results, this study provides a modern perspective on existing n-gram based regular expression evaluation methods, extensive observations, valuable discoveries, and an adaptable testing framework to guide future research in this domain.","We make our implementations of these methods and our test framework available as open-source at https://github.com/mush-zhang/RegexIndexComparison."],"url":"http://arxiv.org/abs/2504.12251v1"}
{"created":"2025-04-16 16:54:38","title":"AnomalyGen: An Automated Semantic Log Sequence Generation Framework with LLM for Anomaly Detection","abstract":"The scarcity of high-quality public log datasets has become a critical bottleneck in advancing log-based anomaly detection techniques. Current datasets exhibit three fundamental limitations: (1) incomplete event coverage, (2) artificial patterns introduced by static analysis-based generation frameworks, and (3) insufficient semantic awareness. To address these challenges, we present AnomalyGen, the first automated log synthesis framework specifically designed for anomaly detection. Our framework introduces a novel four-phase architecture that integrates enhanced program analysis with Chain-of-Thought reasoning (CoT reasoning), enabling iterative log generation and anomaly annotation without requiring physical system execution. Evaluations on Hadoop and HDFS distributed systems demonstrate that AnomalyGen achieves substantially broader log event coverage (38-95 times improvement over existing datasets) while producing more operationally realistic log sequences compared to static analysis-based approaches. When augmenting benchmark datasets with synthesized logs, we observe maximum F1-score improvements of 3.7% (average 1.8% improvement across three state-of-the-art anomaly detection models). This work not only establishes a high-quality benchmarking resource for automated log analysis but also pioneers a new paradigm for applying large language models (LLMs) in software engineering workflows.","sentences":["The scarcity of high-quality public log datasets has become a critical bottleneck in advancing log-based anomaly detection techniques.","Current datasets exhibit three fundamental limitations: (1) incomplete event coverage, (2) artificial patterns introduced by static analysis-based generation frameworks, and (3) insufficient semantic awareness.","To address these challenges, we present AnomalyGen, the first automated log synthesis framework specifically designed for anomaly detection.","Our framework introduces a novel four-phase architecture that integrates enhanced program analysis with Chain-of-Thought reasoning (CoT reasoning), enabling iterative log generation and anomaly annotation without requiring physical system execution.","Evaluations on Hadoop and HDFS distributed systems demonstrate that AnomalyGen achieves substantially broader log event coverage (38-95 times improvement over existing datasets) while producing more operationally realistic log sequences compared to static analysis-based approaches.","When augmenting benchmark datasets with synthesized logs, we observe maximum F1-score improvements of 3.7% (average 1.8% improvement across three state-of-the-art anomaly detection models).","This work not only establishes a high-quality benchmarking resource for automated log analysis but also pioneers a new paradigm for applying large language models (LLMs) in software engineering workflows."],"url":"http://arxiv.org/abs/2504.12250v1"}
{"created":"2025-04-16 16:52:07","title":"Branching Bisimulation Learning","abstract":"We introduce a bisimulation learning algorithm for non-deterministic transition systems. We generalise bisimulation learning to systems with bounded branching and extend its applicability to model checking branching-time temporal logic, while previously it was limited to deterministic systems and model checking linear-time properties. Our method computes a finite stutter-insensitive bisimulation quotient of the system under analysis, represented as a decision tree. We adapt the proof rule for well-founded bisimulations to an iterative procedure that trains candidate decision trees from sample transitions of the system, and checks their validity over the entire transition relation using SMT solving. This results in a new technology for model checking CTL* without the next-time operator. Our technique is sound, entirely automated, and yields abstractions that are succinct and effective for formal verification and system diagnostics. We demonstrate the efficacy of our method on diverse benchmarks comprising concurrent software, communication protocols and robotic scenarios. Our method performs comparably to mature tools in the special case of LTL model checking, and outperforms the state of the art in CTL and CTL* model checking for systems with very large and countably infinite state space.","sentences":["We introduce a bisimulation learning algorithm for non-deterministic transition systems.","We generalise bisimulation learning to systems with bounded branching and extend its applicability to model checking branching-time temporal logic, while previously it was limited to deterministic systems and model checking linear-time properties.","Our method computes a finite stutter-insensitive bisimulation quotient of the system under analysis, represented as a decision tree.","We adapt the proof rule for well-founded bisimulations to an iterative procedure that trains candidate decision trees from sample transitions of the system, and checks their validity over the entire transition relation using SMT solving.","This results in a new technology for model checking CTL* without the next-time operator.","Our technique is sound, entirely automated, and yields abstractions that are succinct and effective for formal verification and system diagnostics.","We demonstrate the efficacy of our method on diverse benchmarks comprising concurrent software, communication protocols and robotic scenarios.","Our method performs comparably to mature tools in the special case of LTL model checking, and outperforms the state of the art in CTL and CTL* model checking for systems with very large and countably infinite state space."],"url":"http://arxiv.org/abs/2504.12246v1"}
{"created":"2025-04-16 16:50:41","title":"SIDME: Self-supervised Image Demoir\u00e9ing via Masked Encoder-Decoder Reconstruction","abstract":"Moir\\'e patterns, resulting from aliasing between object light signals and camera sampling frequencies, often degrade image quality during capture. Traditional demoir\\'eing methods have generally treated images as a whole for processing and training, neglecting the unique signal characteristics of different color channels. Moreover, the randomness and variability of moir\\'e pattern generation pose challenges to the robustness of existing methods when applied to real-world data. To address these issues, this paper presents SIDME (Self-supervised Image Demoir\\'eing via Masked Encoder-Decoder Reconstruction), a novel model designed to generate high-quality visual images by effectively processing moir\\'e patterns. SIDME combines a masked encoder-decoder architecture with self-supervised learning, allowing the model to reconstruct images using the inherent properties of camera sampling frequencies. A key innovation is the random masked image reconstructor, which utilizes an encoder-decoder structure to handle the reconstruction task. Furthermore, since the green channel in camera sampling has a higher sampling frequency compared to red and blue channels, a specialized self-supervised loss function is designed to improve the training efficiency and effectiveness. To ensure the generalization ability of the model, a self-supervised moir\\'e image generation method has been developed to produce a dataset that closely mimics real-world conditions. Extensive experiments demonstrate that SIDME outperforms existing methods in processing real moir\\'e pattern data, showing its superior generalization performance and robustness.","sentences":["Moir\\'e patterns, resulting from aliasing between object light signals and camera sampling frequencies, often degrade image quality during capture.","Traditional demoir\\'eing methods have generally treated images as a whole for processing and training, neglecting the unique signal characteristics of different color channels.","Moreover, the randomness and variability of moir\\'e pattern generation pose challenges to the robustness of existing methods when applied to real-world data.","To address these issues, this paper presents SIDME (Self-supervised Image Demoir\\'eing via Masked Encoder-Decoder Reconstruction), a novel model designed to generate high-quality visual images by effectively processing moir\\'e patterns.","SIDME combines a masked encoder-decoder architecture with self-supervised learning, allowing the model to reconstruct images using the inherent properties of camera sampling frequencies.","A key innovation is the random masked image reconstructor, which utilizes an encoder-decoder structure to handle the reconstruction task.","Furthermore, since the green channel in camera sampling has a higher sampling frequency compared to red and blue channels, a specialized self-supervised loss function is designed to improve the training efficiency and effectiveness.","To ensure the generalization ability of the model, a self-supervised moir\\'e image generation method has been developed to produce a dataset that closely mimics real-world conditions.","Extensive experiments demonstrate that SIDME outperforms existing methods in processing real moir\\'e pattern data, showing its superior generalization performance and robustness."],"url":"http://arxiv.org/abs/2504.12245v1"}
{"created":"2025-04-16 16:45:19","title":"Cobra: Efficient Line Art COlorization with BRoAder References","abstract":"The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control. A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process. Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control. We investigate the necessity of extensive contextual image guidance on the quality of line art colorization. To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency. Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency. Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands. We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/.","sentences":["The comic production industry requires reference-based line art colorization with high accuracy, efficiency, contextual consistency, and flexible control.","A comic page often involves diverse characters, objects, and backgrounds, which complicates the coloring process.","Despite advancements in diffusion models for image generation, their application in line art colorization remains limited, facing challenges related to handling extensive reference images, time-consuming inference, and flexible control.","We investigate the necessity of extensive contextual image guidance on the quality of line art colorization.","To address these challenges, we introduce Cobra, an efficient and versatile method that supports color hints and utilizes over 200 reference images while maintaining low latency.","Central to Cobra is a Causal Sparse DiT architecture, which leverages specially designed positional encodings, causal sparse attention, and Key-Value Cache to effectively manage long-context references and ensure color identity consistency.","Results demonstrate that Cobra achieves accurate line art colorization through extensive contextual reference, significantly enhancing inference speed and interactivity, thereby meeting critical industrial demands.","We release our codes and models on our project page: https://zhuang2002.github.io/Cobra/."],"url":"http://arxiv.org/abs/2504.12240v1"}
{"created":"2025-04-16 16:42:13","title":"Stereoscopic Cylindrical Screen (SCS) Projection","abstract":"We present a technique for Stereoscopic Cylindrical Screen (SCS) Projection of a world scene to a 360-degree canvas for viewing with 3D glasses. To optimize the rendering pipeline, we render the scene to four cubemaps, before sampling relevant cubemaps onto the canvas. For an interactive user experience, we perform stereoscopic view rendering and off-axis projection to anchor the image to the viewer. This technique is being used to project virtual worlds at CMU ETC, and is a step in creating immersive viewing experiences.","sentences":["We present a technique for Stereoscopic Cylindrical Screen (SCS) Projection of a world scene to a 360-degree canvas for viewing with 3D glasses.","To optimize the rendering pipeline, we render the scene to four cubemaps, before sampling relevant cubemaps onto the canvas.","For an interactive user experience, we perform stereoscopic view rendering and off-axis projection to anchor the image to the viewer.","This technique is being used to project virtual worlds at CMU ETC, and is a step in creating immersive viewing experiences."],"url":"http://arxiv.org/abs/2504.12237v1"}
{"created":"2025-04-16 16:40:56","title":"Towards Human-Centered Early Prediction Models for Academic Performance in Real-World Contexts","abstract":"Supporting student success requires collaboration among multiple stakeholders. Researchers have explored machine learning models for academic performance prediction; yet key challenges remain in ensuring these models are interpretable, equitable, and actionable within real-world educational support systems. First, many models prioritize predictive accuracy but overlook human-centered considerations, limiting trust among students and reducing their usefulness for educators and institutional decision-makers. Second, most models require at least a month of data before making reliable predictions, delaying opportunities for early intervention. Third, current models primarily rely on sporadically collected, classroom-derived data, missing broader behavioral patterns that could provide more continuous and actionable insights. To address these gaps, we present three modeling approaches-LR, 1D-CNN, and MTL-1D-CNN-to classify students as low or high academic performers. We evaluate them based on explainability, fairness, and generalizability to assess their alignment with key social values. Using behavioral and self-reported data collected within the first week of two Spring terms, we demonstrate that these models can identify at-risk students as early as week one. However, trade-offs across human-centered considerations highlight the complexity of designing predictive models that effectively support multi-stakeholder decision-making and intervention strategies. We discuss these trade-offs and their implications for different stakeholders, outlining how predictive models can be integrated into student support systems. Finally, we examine broader socio-technical challenges in deploying these models and propose future directions for advancing human-centered, collaborative academic prediction systems.","sentences":["Supporting student success requires collaboration among multiple stakeholders.","Researchers have explored machine learning models for academic performance prediction; yet key challenges remain in ensuring these models are interpretable, equitable, and actionable within real-world educational support systems.","First, many models prioritize predictive accuracy but overlook human-centered considerations, limiting trust among students and reducing their usefulness for educators and institutional decision-makers.","Second, most models require at least a month of data before making reliable predictions, delaying opportunities for early intervention.","Third, current models primarily rely on sporadically collected, classroom-derived data, missing broader behavioral patterns that could provide more continuous and actionable insights.","To address these gaps, we present three modeling approaches-LR, 1D-CNN, and MTL-1D-CNN-to classify students as low or high academic performers.","We evaluate them based on explainability, fairness, and generalizability to assess their alignment with key social values.","Using behavioral and self-reported data collected within the first week of two Spring terms, we demonstrate that these models can identify at-risk students as early as week one.","However, trade-offs across human-centered considerations highlight the complexity of designing predictive models that effectively support multi-stakeholder decision-making and intervention strategies.","We discuss these trade-offs and their implications for different stakeholders, outlining how predictive models can be integrated into student support systems.","Finally, we examine broader socio-technical challenges in deploying these models and propose future directions for advancing human-centered, collaborative academic prediction systems."],"url":"http://arxiv.org/abs/2504.12236v1"}
{"created":"2025-04-16 16:33:53","title":"MOS: Towards Effective Smart Contract Vulnerability Detection through Mixture-of-Experts Tuning of Large Language Models","abstract":"Smart contract vulnerabilities pose significant security risks to blockchain systems, potentially leading to severe financial losses. Existing methods face several limitations: (1) Program analysis-based approaches rely on predefined patterns, lacking flexibility for new vulnerability types; (2) Deep learning-based methods lack explanations; (3) Large language model-based approaches suffer from high false positives. We propose MOS, a smart contract vulnerability detection framework based on mixture-of-experts tuning (MOE-Tuning) of large language models. First, we conduct continual pre-training on a large-scale smart contract dataset to provide domain-enhanced initialization. Second, we construct a high-quality MOE-Tuning dataset through a multi-stage pipeline combining LLM generation and expert verification for reliable explanations. Third, we design a vulnerability-aware routing mechanism that activates the most relevant expert networks by analyzing code features and their matching degree with experts. Finally, we extend the feed-forward layers into multiple parallel expert networks, each specializing in specific vulnerability patterns. We employ a dual-objective loss function: one for optimizing detection and explanation performance, and another for ensuring reasonable distribution of vulnerability types to experts through entropy calculation. Experiments show that MOS significantly outperforms existing methods with average improvements of 6.32% in F1 score and 4.80% in accuracy. The vulnerability explanations achieve positive ratings (scores of 3-4 on a 4-point scale) of 82.96%, 85.21% and 94.58% for correctness, completeness, and conciseness through human and LLM evaluation.","sentences":["Smart contract vulnerabilities pose significant security risks to blockchain systems, potentially leading to severe financial losses.","Existing methods face several limitations: (1) Program analysis-based approaches rely on predefined patterns, lacking flexibility for new vulnerability types; (2) Deep learning-based methods lack explanations; (3) Large language model-based approaches suffer from high false positives.","We propose MOS, a smart contract vulnerability detection framework based on mixture-of-experts tuning (MOE-Tuning) of large language models.","First, we conduct continual pre-training on a large-scale smart contract dataset to provide domain-enhanced initialization.","Second, we construct a high-quality MOE-Tuning dataset through a multi-stage pipeline combining LLM generation and expert verification for reliable explanations.","Third, we design a vulnerability-aware routing mechanism that activates the most relevant expert networks by analyzing code features and their matching degree with experts.","Finally, we extend the feed-forward layers into multiple parallel expert networks, each specializing in specific vulnerability patterns.","We employ a dual-objective loss function: one for optimizing detection and explanation performance, and another for ensuring reasonable distribution of vulnerability types to experts through entropy calculation.","Experiments show that MOS significantly outperforms existing methods with average improvements of 6.32% in F1 score and 4.80% in accuracy.","The vulnerability explanations achieve positive ratings (scores of 3-4 on a 4-point scale) of 82.96%, 85.21% and 94.58% for correctness, completeness, and conciseness through human and LLM evaluation."],"url":"http://arxiv.org/abs/2504.12234v1"}
{"created":"2025-04-16 16:25:26","title":"Watermarking Needs Input Repetition Masking","abstract":"Recent advancements in Large Language Models (LLMs) raised concerns over potential misuse, such as for spreading misinformation. In response two counter measures emerged: machine learning-based detectors that predict if text is synthetic, and LLM watermarking, which subtly marks generated text for identification and attribution. Meanwhile, humans are known to adjust language to their conversational partners both syntactically and lexically. By implication, it is possible that humans or unwatermarked LLMs could unintentionally mimic properties of LLM generated text, making counter measures unreliable. In this work we investigate the extent to which such conversational adaptation happens. We call the concept $\\textit{mimicry}$ and demonstrate that both humans and LLMs end up mimicking, including the watermarking signal even in seemingly improbable settings. This challenges current academic assumptions and suggests that for long-term watermarking to be reliable, the likelihood of false positives needs to be significantly lower, while longer word sequences should be used for seeding watermarking mechanisms.","sentences":["Recent advancements in Large Language Models (LLMs) raised concerns over potential misuse, such as for spreading misinformation.","In response two counter measures emerged: machine learning-based detectors that predict if text is synthetic, and LLM watermarking, which subtly marks generated text for identification and attribution.","Meanwhile, humans are known to adjust language to their conversational partners both syntactically and lexically.","By implication, it is possible that humans or unwatermarked LLMs could unintentionally mimic properties of LLM generated text, making counter measures unreliable.","In this work we investigate the extent to which such conversational adaptation happens.","We call the concept $\\textit{mimicry}$ and demonstrate that both humans and LLMs end up mimicking, including the watermarking signal even in seemingly improbable settings.","This challenges current academic assumptions and suggests that for long-term watermarking to be reliable, the likelihood of false positives needs to be significantly lower, while longer word sequences should be used for seeding watermarking mechanisms."],"url":"http://arxiv.org/abs/2504.12229v1"}
{"created":"2025-04-16 16:14:43","title":"Coding-Prior Guided Diffusion Network for Video Deblurring","abstract":"While recent video deblurring methods have advanced significantly, they often overlook two valuable prior information: (1) motion vectors (MVs) and coding residuals (CRs) from video codecs, which provide efficient inter-frame alignment cues, and (2) the rich real-world knowledge embedded in pre-trained diffusion generative models. We present CPGDNet, a novel two-stage framework that effectively leverages both coding priors and generative diffusion priors for high-quality deblurring. First, our coding-prior feature propagation (CPFP) module utilizes MVs for efficient frame alignment and CRs to generate attention masks, addressing motion inaccuracies and texture variations. Second, a coding-prior controlled generation (CPC) module network integrates coding priors into a pretrained diffusion model, guiding it to enhance critical regions and synthesize realistic details. Experiments demonstrate our method achieves state-of-the-art perceptual quality with up to 30% improvement in IQA metrics. Both the code and the codingprior-augmented dataset will be open-sourced.","sentences":["While recent video deblurring methods have advanced significantly, they often overlook two valuable prior information: (1) motion vectors (MVs) and coding residuals (CRs) from video codecs, which provide efficient inter-frame alignment cues, and (2) the rich real-world knowledge embedded in pre-trained diffusion generative models.","We present CPGDNet, a novel two-stage framework that effectively leverages both coding priors and generative diffusion priors for high-quality deblurring.","First, our coding-prior feature propagation (CPFP) module utilizes MVs for efficient frame alignment and CRs to generate attention masks, addressing motion inaccuracies and texture variations.","Second, a coding-prior controlled generation (CPC) module network integrates coding priors into a pretrained diffusion model, guiding it to enhance critical regions and synthesize realistic details.","Experiments demonstrate our method achieves state-of-the-art perceptual quality with up to 30% improvement in IQA metrics.","Both the code and the codingprior-augmented dataset will be open-sourced."],"url":"http://arxiv.org/abs/2504.12222v1"}
{"created":"2025-04-16 16:13:09","title":"Accountable Liveness","abstract":"Safety and liveness are the two classical security properties of consensus protocols. Recent works have strengthened safety with accountability: should any safety violation occur, a sizable fraction of adversary nodes can be proven to be protocol violators. This paper studies to what extent analogous accountability guarantees are achievable for liveness. To reveal the full complexity of this question, we introduce an interpolation between the classical synchronous and partially-synchronous models that we call the $x$-partially-synchronous network model in which, intuitively, at most an $x$ fraction of the time steps in any sufficiently long interval are asynchronous (and, as with a partially-synchronous network, all time steps are synchronous following the passage of an unknown \"global stablization time\"). We prove a precise characterization of the parameter regime in which accountable liveness is achievable: if and only if $x < 1/2$ and $f < n/2$, where $n$ denotes the number of nodes and $f$ the number of nodes controlled by an adversary. We further refine the problem statement and our analysis by parameterizing by the number of violating nodes identified following a liveness violation, and provide evidence that the guarantees achieved by our protocol are near-optimal (as a function of $x$ and $f$). Our results provide rigorous foundations for liveness-accountability heuristics such as the \"inactivity leaks\" employed in Ethereum.","sentences":["Safety and liveness are the two classical security properties of consensus protocols.","Recent works have strengthened safety with accountability: should any safety violation occur, a sizable fraction of adversary nodes can be proven to be protocol violators.","This paper studies to what extent analogous accountability guarantees are achievable for liveness.","To reveal the full complexity of this question, we introduce an interpolation between the classical synchronous and partially-synchronous models that we call the $x$-partially-synchronous network model in which, intuitively, at most an $x$ fraction of the time steps in any sufficiently long interval are asynchronous (and, as with a partially-synchronous network, all time steps are synchronous following the passage of an unknown \"global stablization time\").","We prove a precise characterization of the parameter regime in which accountable liveness is achievable: if and only if $x < 1/2$ and $f < n/2$, where $n$ denotes the number of nodes and $f$ the number of nodes controlled by an adversary.","We further refine the problem statement and our analysis by parameterizing by the number of violating nodes identified following a liveness violation, and provide evidence that the guarantees achieved by our protocol are near-optimal (as a function of $x$ and $f$).","Our results provide rigorous foundations for liveness-accountability heuristics such as the \"inactivity leaks\" employed in Ethereum."],"url":"http://arxiv.org/abs/2504.12218v1"}
{"created":"2025-04-16 16:11:11","title":"zkVC: Fast Zero-Knowledge Proof for Private and Verifiable Computing","abstract":"In the context of cloud computing, services are held on cloud servers, where the clients send their data to the server and obtain the results returned by server. However, the computation, data and results are prone to tampering due to the vulnerabilities on the server side. Thus, verifying the integrity of computation is important in the client-server setting. The cryptographic method known as Zero-Knowledge Proof (ZKP) is renowned for facilitating private and verifiable computing. ZKP allows the client to validate that the results from the server are computed correctly without violating the privacy of the server's intellectual property. Zero-Knowledge Succinct Non-Interactive Argument of Knowledge (zkSNARKs), in particular, has been widely applied in various applications like blockchain and verifiable machine learning. Despite their popularity, existing zkSNARKs approaches remain highly computationally intensive. For instance, even basic operations like matrix multiplication require an extensive number of constraints, resulting in significant overhead. In addressing this challenge, we introduce \\textit{zkVC}, which optimizes the ZKP computation for matrix multiplication, enabling rapid proof generation on the server side and efficient verification on the client side. zkVC integrates optimized ZKP modules, such as Constraint-reduced Polynomial Circuit (CRPC) and Prefix-Sum Query (PSQ), collectively yielding a more than 12-fold increase in proof speed over prior methods. The code is available at https://github.com/UCF-Lou-Lab-PET/zkformer","sentences":["In the context of cloud computing, services are held on cloud servers, where the clients send their data to the server and obtain the results returned by server.","However, the computation, data and results are prone to tampering due to the vulnerabilities on the server side.","Thus, verifying the integrity of computation is important in the client-server setting.","The cryptographic method known as Zero-Knowledge Proof (ZKP) is renowned for facilitating private and verifiable computing.","ZKP allows the client to validate that the results from the server are computed correctly without violating the privacy of the server's intellectual property.","Zero-Knowledge Succinct Non-Interactive Argument of Knowledge (zkSNARKs), in particular, has been widely applied in various applications like blockchain and verifiable machine learning.","Despite their popularity, existing zkSNARKs approaches remain highly computationally intensive.","For instance, even basic operations like matrix multiplication require an extensive number of constraints, resulting in significant overhead.","In addressing this challenge, we introduce \\textit{zkVC}, which optimizes the ZKP computation for matrix multiplication, enabling rapid proof generation on the server side and efficient verification on the client side.","zkVC integrates optimized ZKP modules, such as Constraint-reduced Polynomial Circuit (CRPC) and Prefix-Sum Query (PSQ), collectively yielding a more than 12-fold increase in proof speed over prior methods.","The code is available at https://github.com/UCF-Lou-Lab-PET/zkformer"],"url":"http://arxiv.org/abs/2504.12217v1"}
{"created":"2025-04-16 16:08:45","title":"d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning","abstract":"Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL). These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning. To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL. Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM.","sentences":["Recent large language models (LLMs) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning (RL).","These capabilities have primarily been demonstrated within the left-to-right autoregressive (AR) generation paradigm.","In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner.","Although recent diffusion-based large language models (dLLMs) have achieved competitive language modeling performance compared to their AR counterparts, it remains unclear if dLLMs can also leverage recent advances in LLM reasoning.","To this end, we propose d1, a framework to adapt pre-trained masked dLLMs into reasoning models via a combination of supervised finetuning (SFT) and RL.","Specifically, we develop and extend techniques to improve reasoning in pretrained dLLMs: (a) we utilize a masked SFT technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based RL algorithm called diffu-GRPO.","Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks.","We find that d1 yields the best performance and significantly improves performance of a state-of-the-art dLLM."],"url":"http://arxiv.org/abs/2504.12216v1"}
{"created":"2025-04-16 16:08:38","title":"Uncertainty-Guided Coarse-to-Fine Tumor Segmentation with Anatomy-Aware Post-Processing","abstract":"Reliable tumor segmentation in thoracic computed tomography (CT) remains challenging due to boundary ambiguity, class imbalance, and anatomical variability. We propose an uncertainty-guided, coarse-to-fine segmentation framework that combines full-volume tumor localization with refined region-of-interest (ROI) segmentation, enhanced by anatomically aware post-processing. The first-stage model generates a coarse prediction, followed by anatomically informed filtering based on lung overlap, proximity to lung surfaces, and component size. The resulting ROIs are segmented by a second-stage model trained with uncertainty-aware loss functions to improve accuracy and boundary calibration in ambiguous regions. Experiments on private and public datasets demonstrate improvements in Dice and Hausdorff scores, with fewer false positives and enhanced spatial interpretability. These results highlight the value of combining uncertainty modeling and anatomical priors in cascaded segmentation pipelines for robust and clinically meaningful tumor delineation. On the Orlando dataset, our framework improved Swin UNETR Dice from 0.4690 to 0.6447. Reduction in spurious components was strongly correlated with segmentation gains, underscoring the value of anatomically informed post-processing.","sentences":["Reliable tumor segmentation in thoracic computed tomography (CT) remains challenging due to boundary ambiguity, class imbalance, and anatomical variability.","We propose an uncertainty-guided, coarse-to-fine segmentation framework that combines full-volume tumor localization with refined region-of-interest (ROI) segmentation, enhanced by anatomically aware post-processing.","The first-stage model generates a coarse prediction, followed by anatomically informed filtering based on lung overlap, proximity to lung surfaces, and component size.","The resulting ROIs are segmented by a second-stage model trained with uncertainty-aware loss functions to improve accuracy and boundary calibration in ambiguous regions.","Experiments on private and public datasets demonstrate improvements in Dice and Hausdorff scores, with fewer false positives and enhanced spatial interpretability.","These results highlight the value of combining uncertainty modeling and anatomical priors in cascaded segmentation pipelines for robust and clinically meaningful tumor delineation.","On the Orlando dataset, our framework improved Swin UNETR Dice from 0.4690 to 0.6447.","Reduction in spurious components was strongly correlated with segmentation gains, underscoring the value of anatomically informed post-processing."],"url":"http://arxiv.org/abs/2504.12215v1"}
{"created":"2025-04-16 15:58:33","title":"Creating benchmarkable components to measure the quality ofAI-enhanced developer tools","abstract":"In the AI community, benchmarks to evaluate model quality are well established, but an equivalent approach to benchmarking products built upon generative AI models is still missing. This has had two consequences. First, it has made teams focus on model quality over the developer experience, while successful products combine both. Second, product team have struggled to answer questions about their products in relation to their competitors.   In this case study, we share: (1) our process to create robust, enterprise-grade and modular components to support the benchmarking of the developer experience (DX) dimensions of our team's AI for code offerings, and (2) the components we have created to do so, including demographics and attitudes towards AI surveys, a benchmarkable task, and task and feature surveys. By doing so, we hope to lower the barrier to the DX benchmarking of genAI-enhanced code products.","sentences":["In the AI community, benchmarks to evaluate model quality are well established, but an equivalent approach to benchmarking products built upon generative AI models is still missing.","This has had two consequences.","First, it has made teams focus on model quality over the developer experience, while successful products combine both.","Second, product team have struggled to answer questions about their products in relation to their competitors.   ","In this case study, we share: (1) our process to create robust, enterprise-grade and modular components to support the benchmarking of the developer experience (DX) dimensions of our team's AI for code offerings, and (2) the components we have created to do so, including demographics and attitudes towards AI surveys, a benchmarkable task, and task and feature surveys.","By doing so, we hope to lower the barrier to the DX benchmarking of genAI-enhanced code products."],"url":"http://arxiv.org/abs/2504.12211v1"}
{"created":"2025-04-16 15:56:57","title":"Communication Optimization for Decentralized Learning atop Bandwidth-limited Edge Networks","abstract":"Decentralized federated learning (DFL) is a promising machine learning paradigm for bringing artificial intelligence (AI) capabilities to the network edge. Running DFL on top of edge networks, however, faces severe performance challenges due to the extensive parameter exchanges between agents. Most existing solutions for these challenges were based on simplistic communication models, which cannot capture the case of learning over a multi-hop bandwidth-limited network. In this work, we address this problem by jointly designing the communication scheme for the overlay network formed by the agents and the mixing matrix that controls the communication demands between the agents. By carefully analyzing the properties of our problem, we cast each design problem into a tractable optimization and develop an efficient algorithm with guaranteed performance. Our evaluations based on real topology and data show that the proposed algorithm can reduce the total training time by over $80\\%$ compared to the baseline without sacrificing accuracy, while significantly improving the computational efficiency over the state of the art.","sentences":["Decentralized federated learning (DFL) is a promising machine learning paradigm for bringing artificial intelligence (AI) capabilities to the network edge.","Running DFL on top of edge networks, however, faces severe performance challenges due to the extensive parameter exchanges between agents.","Most existing solutions for these challenges were based on simplistic communication models, which cannot capture the case of learning over a multi-hop bandwidth-limited network.","In this work, we address this problem by jointly designing the communication scheme for the overlay network formed by the agents and the mixing matrix that controls the communication demands between the agents.","By carefully analyzing the properties of our problem, we cast each design problem into a tractable optimization and develop an efficient algorithm with guaranteed performance.","Our evaluations based on real topology and data show that the proposed algorithm can reduce the total training time by over $80\\%$ compared to the baseline without sacrificing accuracy, while significantly improving the computational efficiency over the state of the art."],"url":"http://arxiv.org/abs/2504.12210v1"}
{"created":"2025-04-16 15:53:53","title":"Towards Realistic Low-Light Image Enhancement via ISP Driven Data Modeling","abstract":"Deep neural networks (DNNs) have recently become the leading method for low-light image enhancement (LLIE). However, despite significant progress, their outputs may still exhibit issues such as amplified noise, incorrect white balance, or unnatural enhancements when deployed in real world applications. A key challenge is the lack of diverse, large scale training data that captures the complexities of low-light conditions and imaging pipelines. In this paper, we propose a novel image signal processing (ISP) driven data synthesis pipeline that addresses these challenges by generating unlimited paired training data. Specifically, our pipeline begins with easily collected high-quality normal-light images, which are first unprocessed into the RAW format using a reverse ISP. We then synthesize low-light degradations directly in the RAW domain. The resulting data is subsequently processed through a series of ISP stages, including white balance adjustment, color space conversion, tone mapping, and gamma correction, with controlled variations introduced at each stage. This broadens the degradation space and enhances the diversity of the training data, enabling the generated data to capture a wide range of degradations and the complexities inherent in the ISP pipeline. To demonstrate the effectiveness of our synthetic pipeline, we conduct extensive experiments using a vanilla UNet model consisting solely of convolutional layers, group normalization, GeLU activation, and convolutional block attention modules (CBAMs). Extensive testing across multiple datasets reveals that the vanilla UNet model trained with our data synthesis pipeline delivers high fidelity, visually appealing enhancement results, surpassing state-of-the-art (SOTA) methods both quantitatively and qualitatively.","sentences":["Deep neural networks (DNNs) have recently become the leading method for low-light image enhancement (LLIE).","However, despite significant progress, their outputs may still exhibit issues such as amplified noise, incorrect white balance, or unnatural enhancements when deployed in real world applications.","A key challenge is the lack of diverse, large scale training data that captures the complexities of low-light conditions and imaging pipelines.","In this paper, we propose a novel image signal processing (ISP) driven data synthesis pipeline that addresses these challenges by generating unlimited paired training data.","Specifically, our pipeline begins with easily collected high-quality normal-light images, which are first unprocessed into the RAW format using a reverse ISP.","We then synthesize low-light degradations directly in the RAW domain.","The resulting data is subsequently processed through a series of ISP stages, including white balance adjustment, color space conversion, tone mapping, and gamma correction, with controlled variations introduced at each stage.","This broadens the degradation space and enhances the diversity of the training data, enabling the generated data to capture a wide range of degradations and the complexities inherent in the ISP pipeline.","To demonstrate the effectiveness of our synthetic pipeline, we conduct extensive experiments using a vanilla UNet model consisting solely of convolutional layers, group normalization, GeLU activation, and convolutional block attention modules (CBAMs).","Extensive testing across multiple datasets reveals that the vanilla UNet model trained with our data synthesis pipeline delivers high fidelity, visually appealing enhancement results, surpassing state-of-the-art (SOTA) methods both quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2504.12204v1"}
{"created":"2025-04-16 15:48:21","title":"Beyond Patches: Mining Interpretable Part-Prototypes for Explainable AI","abstract":"Deep learning has provided considerable advancements for multimedia systems, yet the interpretability of deep models remains a challenge. State-of-the-art post-hoc explainability methods, such as GradCAM, provide visual interpretation based on heatmaps but lack conceptual clarity. Prototype-based approaches, like ProtoPNet and PIPNet, offer a more structured explanation but rely on fixed patches, limiting their robustness and semantic consistency.   To address these limitations, a part-prototypical concept mining network (PCMNet) is proposed that dynamically learns interpretable prototypes from meaningful regions. PCMNet clusters prototypes into concept groups, creating semantically grounded explanations without requiring additional annotations. Through a joint process of unsupervised part discovery and concept activation vector extraction, PCMNet effectively captures discriminative concepts and makes interpretable classification decisions.   Our extensive experiments comparing PCMNet against state-of-the-art methods on multiple datasets show that it can provide a high level of interpretability, stability, and robustness under clean and occluded scenarios.","sentences":["Deep learning has provided considerable advancements for multimedia systems, yet the interpretability of deep models remains a challenge.","State-of-the-art post-hoc explainability methods, such as GradCAM, provide visual interpretation based on heatmaps but lack conceptual clarity.","Prototype-based approaches, like ProtoPNet and PIPNet, offer a more structured explanation but rely on fixed patches, limiting their robustness and semantic consistency.   ","To address these limitations, a part-prototypical concept mining network (PCMNet) is proposed that dynamically learns interpretable prototypes from meaningful regions.","PCMNet clusters prototypes into concept groups, creating semantically grounded explanations without requiring additional annotations.","Through a joint process of unsupervised part discovery and concept activation vector extraction, PCMNet effectively captures discriminative concepts and makes interpretable classification decisions.   ","Our extensive experiments comparing PCMNet against state-of-the-art methods on multiple datasets show that it can provide a high level of interpretability, stability, and robustness under clean and occluded scenarios."],"url":"http://arxiv.org/abs/2504.12197v1"}
{"created":"2025-04-16 15:47:44","title":"Validating and monitoring bibliographic and citation data in OpenCitations collections","abstract":"Purpose. The increasing emphasis on data quantity in research infrastructures has highlighted the need for equally robust mechanisms ensuring data quality, particularly in bibliographic and citation datasets. This paper addresses the challenge of maintaining high-quality open research information within OpenCitations, a community-guided Open Science Infrastructure, by introducing tools for validating and monitoring bibliographic metadata and citation data.   Methods. We developed a custom validation tool tailored to the OpenCitations Data Model (OCDM), designed to detect and explain ingestion errors from heterogeneous sources, whether due to upstream data inconsistencies or internal software bugs. Additionally, a quality monitoring tool was created to track known data issues post-publication. These tools were applied in two scenarios: (1) validating metadata and citations from Matilda, a potential future source, and (2) monitoring data quality in the existing OpenCitations Meta dataset.   Results. The validation tool successfully identified a variety of structural and semantic issues in the Matilda dataset, demonstrating its precision. The monitoring tool enabled the detection of recurring problems in the OpenCitations Meta collection, as well as their quantification. Together, these tools proved effective in enhancing the reliability of OpenCitations' published data.   Conclusion. The presented validation and monitoring tools represent a step toward ensuring high-quality bibliographic data in open research infrastructures, though they are limited to the data model adopted by OpenCitations. Future developments are aimed at expanding to additional data sources, with particular regard to crowdsourced data.","sentences":["Purpose.","The increasing emphasis on data quantity in research infrastructures has highlighted the need for equally robust mechanisms ensuring data quality, particularly in bibliographic and citation datasets.","This paper addresses the challenge of maintaining high-quality open research information within OpenCitations, a community-guided Open Science Infrastructure, by introducing tools for validating and monitoring bibliographic metadata and citation data.   Methods.","We developed a custom validation tool tailored to the OpenCitations Data Model (OCDM), designed to detect and explain ingestion errors from heterogeneous sources, whether due to upstream data inconsistencies or internal software bugs.","Additionally, a quality monitoring tool was created to track known data issues post-publication.","These tools were applied in two scenarios: (1) validating metadata and citations from Matilda, a potential future source, and (2) monitoring data quality in the existing OpenCitations Meta dataset.   ","Results.","The validation tool successfully identified a variety of structural and semantic issues in the Matilda dataset, demonstrating its precision.","The monitoring tool enabled the detection of recurring problems in the OpenCitations Meta collection, as well as their quantification.","Together, these tools proved effective in enhancing the reliability of OpenCitations' published data.   ","Conclusion.","The presented validation and monitoring tools represent a step toward ensuring high-quality bibliographic data in open research infrastructures, though they are limited to the data model adopted by OpenCitations.","Future developments are aimed at expanding to additional data sources, with particular regard to crowdsourced data."],"url":"http://arxiv.org/abs/2504.12195v1"}
{"created":"2025-04-16 15:47:38","title":"The Optimal Condition Number for ReLU Function","abstract":"ReLU is a widely used activation function in deep neural networks. This paper explores the stability properties of the ReLU map. For any weight matrix $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ and bias vector $\\boldsymbol{b} \\in \\mathbb{R}^{m}$ at a given layer, we define the condition number $\\beta_{\\boldsymbol{A},\\boldsymbol{b}}$ as $\\beta_{\\boldsymbol{A},\\boldsymbol{b}} = \\frac{\\mathcal{U}_{\\boldsymbol{A},\\boldsymbol{b}}}{\\mathcal{L}_{\\boldsymbol{A},\\boldsymbol{b}}}$, where $\\mathcal{U}_{\\boldsymbol{A},\\boldsymbol{b}}$   and $\\mathcal{L}_{\\boldsymbol{A},\\boldsymbol{b}}$ are the upper and lower Lipschitz constants, respectively. We first demonstrate that for any given $\\boldsymbol{A}$ and $\\boldsymbol{b}$, the condition number satisfies $\\beta_{\\boldsymbol{A},\\boldsymbol{b}} \\geq \\sqrt{2}$. Moreover, when the weights of the network at a given layer are initialized as random i.i.d. Gaussian variables and the bias term is set to zero, the condition number asymptotically approaches this lower bound. This theoretical finding suggests that Gaussian weight initialization is optimal for preserving distances in the context of random deep neural network weights.","sentences":["ReLU is a widely used activation function in deep neural networks.","This paper explores the stability properties of the ReLU map.","For any weight matrix $\\boldsymbol{A} \\in \\mathbb{R}^{m \\times n}$ and bias vector $\\boldsymbol{b} \\in \\mathbb{R}^{m}$ at a given layer, we define the condition number $\\beta_{\\boldsymbol{A},\\boldsymbol{b}}$ as $\\beta_{\\boldsymbol{A},\\boldsymbol{b}} = \\frac{\\mathcal{U}_{\\boldsymbol{A},\\boldsymbol{b}}}{\\mathcal{L}_{\\boldsymbol{A},\\boldsymbol{b}}}$, where $\\mathcal{U}_{\\boldsymbol{A},\\boldsymbol{b}}$   and $\\mathcal{L}_{\\boldsymbol{A},\\boldsymbol{b}}$ are the upper and lower Lipschitz constants, respectively.","We first demonstrate that for any given $\\boldsymbol{A}$ and $\\boldsymbol{b}$, the condition number satisfies $\\beta_{\\boldsymbol{A},\\boldsymbol{b}} \\geq \\sqrt{2}$. Moreover, when the weights of the network at a given layer are initialized as random i.i.d.","Gaussian variables and the bias term is set to zero, the condition number asymptotically approaches this lower bound.","This theoretical finding suggests that Gaussian weight initialization is optimal for preserving distances in the context of random deep neural network weights."],"url":"http://arxiv.org/abs/2504.12194v1"}
{"created":"2025-04-16 15:46:56","title":"From Requirements to Architecture: Semi-Automatically Generating Software Architectures","abstract":"To support junior and senior architects, I propose developing a new architecture creation method that leverages LLMs' evolving capabilities to support the architect. This method involves the architect's close collaboration with LLM-fueled tooling over the whole process. The architect is guided through Domain Model creation, Use Case specification, architectural decisions, and architecture evaluation. While the architect can take complete control of the process and the results, and use the tooling as a building set, they can follow the intended process for maximum tooling support. The preliminary results suggest the feasibility of this process and indicate major time savings for the architect.","sentences":["To support junior and senior architects, I propose developing a new architecture creation method that leverages LLMs' evolving capabilities to support the architect.","This method involves the architect's close collaboration with LLM-fueled tooling over the whole process.","The architect is guided through Domain Model creation, Use Case specification, architectural decisions, and architecture evaluation.","While the architect can take complete control of the process and the results, and use the tooling as a building set, they can follow the intended process for maximum tooling support.","The preliminary results suggest the feasibility of this process and indicate major time savings for the architect."],"url":"http://arxiv.org/abs/2504.12192v1"}
{"created":"2025-04-16 15:42:33","title":"What Do Large Language Models Know? Tacit Knowledge as a Potential Causal-Explanatory Structure","abstract":"It is sometimes assumed that Large Language Models (LLMs) know language, or for example that they know that Paris is the capital of France. But what -- if anything -- do LLMs actually know? In this paper, I argue that LLMs can acquire tacit knowledge as defined by Martin Davies (1990). Whereas Davies himself denies that neural networks can acquire tacit knowledge, I demonstrate that certain architectural features of LLMs satisfy the constraints of semantic description, syntactic structure, and causal systematicity. Thus, tacit knowledge may serve as a conceptual framework for describing, explaining, and intervening on LLMs and their behavior.","sentences":["It is sometimes assumed that Large Language Models (LLMs) know language, or for example that they know that Paris is the capital of France.","But what -- if anything -- do LLMs actually know?","In this paper, I argue that LLMs can acquire tacit knowledge as defined by Martin Davies (1990).","Whereas Davies himself denies that neural networks can acquire tacit knowledge, I demonstrate that certain architectural features of LLMs satisfy the constraints of semantic description, syntactic structure, and causal systematicity.","Thus, tacit knowledge may serve as a conceptual framework for describing, explaining, and intervening on LLMs and their behavior."],"url":"http://arxiv.org/abs/2504.12187v1"}
{"created":"2025-04-16 15:40:15","title":"CoMotion: Concurrent Multi-person 3D Motion","abstract":"We introduce an approach for detecting and tracking detailed 3D poses of multiple people from a single monocular camera stream. Our system maintains temporally coherent predictions in crowded scenes filled with difficult poses and occlusions. Our model performs both strong per-frame detection and a learned pose update to track people from frame to frame. Rather than match detections across time, poses are updated directly from a new input image, which enables online tracking through occlusion. We train on numerous image and video datasets leveraging pseudo-labeled annotations to produce a model that matches state-of-the-art systems in 3D pose estimation accuracy while being faster and more accurate in tracking multiple people through time. Code and weights are provided at https://github.com/apple/ml-comotion","sentences":["We introduce an approach for detecting and tracking detailed 3D poses of multiple people from a single monocular camera stream.","Our system maintains temporally coherent predictions in crowded scenes filled with difficult poses and occlusions.","Our model performs both strong per-frame detection and a learned pose update to track people from frame to frame.","Rather than match detections across time, poses are updated directly from a new input image, which enables online tracking through occlusion.","We train on numerous image and video datasets leveraging pseudo-labeled annotations to produce a model that matches state-of-the-art systems in 3D pose estimation accuracy while being faster and more accurate in tracking multiple people through time.","Code and weights are provided at https://github.com/apple/ml-comotion"],"url":"http://arxiv.org/abs/2504.12186v1"}
{"created":"2025-04-16 15:40:10","title":"SALAD: Improving Robustness and Generalization through Contrastive Learning with Structure-Aware and LLM-Driven Augmented Data","abstract":"In various natural language processing (NLP) tasks, fine-tuning Pre-trained Language Models (PLMs) often leads to the issue of spurious correlations, which negatively impacts performance, particularly when dealing with out-of-distribution data. To address this problem, we propose SALAD}(Structure Aware and LLM-driven Augmented Data), a novel approach designed to enhance model robustness and generalization by generating structure-aware and counterfactually augmented data for contrastive learning. Our method leverages a tagging-based approach to generate structure-aware positive samples and utilizes large language models (LLMs) to generate counterfactual negative samples with diverse sentence patterns. By applying contrastive learning, SALAD enables the model to focus on learning the structural relationships between key sentence components while minimizing reliance on spurious correlations. We validate our approach through experiments on three tasks: Sentiment Classification, Sexism Detection, and Natural Language Inference. The results demonstrate that SALAD not only improves model robustness and performance across different environments but also enhances generalization to out-of-distribution datasets and cross-domain scenarios.","sentences":["In various natural language processing (NLP) tasks, fine-tuning Pre-trained Language Models (PLMs) often leads to the issue of spurious correlations, which negatively impacts performance, particularly when dealing with out-of-distribution data.","To address this problem, we propose SALAD}(Structure Aware and LLM-driven Augmented Data), a novel approach designed to enhance model robustness and generalization by generating structure-aware and counterfactually augmented data for contrastive learning.","Our method leverages a tagging-based approach to generate structure-aware positive samples and utilizes large language models (LLMs) to generate counterfactual negative samples with diverse sentence patterns.","By applying contrastive learning, SALAD enables the model to focus on learning the structural relationships between key sentence components while minimizing reliance on spurious correlations.","We validate our approach through experiments on three tasks: Sentiment Classification, Sexism Detection, and Natural Language Inference.","The results demonstrate that SALAD not only improves model robustness and performance across different environments but also enhances generalization to out-of-distribution datasets and cross-domain scenarios."],"url":"http://arxiv.org/abs/2504.12185v1"}
{"created":"2025-04-16 15:38:59","title":"Domains, Information Frames, and Their Logic","abstract":"In \\cite{sp25}, continuous information frames were introduced that capture exactly all continuous domains. They are obtained from the information frames considered in \\cite{sp21} by omitting the conservativity requirement. Information frames generalise Scott's information systems~\\cite{sc82}: Instead of the global consistency predicate, there is now a local consistency predicate for each token. Strong information frames are obtained by strengthening the conditions for these predicates. Let $\\CIF$ and $\\SIF$ be the corresponding categories.   In \\cite{sxx08} another generalisation of Scott's information systems was introduced which also exactly captures all continuous domains. As shown in \\cite{hzl15}, the definition can be simplified while maintaining the representation result. Let $\\CIS$ and $\\SCIS$ be the corresponding categories. It is shown that all these categories are equivalent. Moreover, the equivalence extends to the subcategories of (strong) continuous information frames with truth elements. Such information frames capture exactly all pointed continuous domains.   Continuous information frames are families of rudimentary logics, associated with each token is a local consistency predicate and an entailment relation. However, they lack the expressive power of propositional logic. In an attempt to make each of this logics more expressible, continuous stratified conjunctive logics are introduced. These are families of conjunctive logics. The category $\\CSL$ of such logics is shown to be isomorphic to $\\SIF_{\\bt}$, the category of strong continuous information frames with a truth element.","sentences":["In \\cite{sp25}, continuous information frames were introduced that capture exactly all continuous domains.","They are obtained from the information frames considered in \\cite{sp21} by omitting the conservativity requirement.","Information frames generalise Scott's information systems~\\cite{sc82}: Instead of the global consistency predicate, there is now a local consistency predicate for each token.","Strong information frames are obtained by strengthening the conditions for these predicates.","Let $\\CIF$ and $\\SIF$ be the corresponding categories.   ","In \\cite{sxx08} another generalisation of Scott's information systems was introduced which also exactly captures all continuous domains.","As shown in \\cite{hzl15}, the definition can be simplified while maintaining the representation result.","Let $\\CIS$ and $\\SCIS$ be the corresponding categories.","It is shown that all these categories are equivalent.","Moreover, the equivalence extends to the subcategories of (strong) continuous information frames with truth elements.","Such information frames capture exactly all pointed continuous domains.   ","Continuous information frames are families of rudimentary logics, associated with each token is a local consistency predicate and an entailment relation.","However, they lack the expressive power of propositional logic.","In an attempt to make each of this logics more expressible, continuous stratified conjunctive logics are introduced.","These are families of conjunctive logics.","The category $\\CSL$ of such logics is shown to be isomorphic to $\\SIF_{\\bt}$, the category of strong continuous information frames with a truth element."],"url":"http://arxiv.org/abs/2504.12182v1"}
{"created":"2025-04-16 15:38:38","title":"Battery-aware Cyclic Scheduling in Energy-harvesting Federated Learning","abstract":"Federated Learning (FL) has emerged as a promising framework for distributed learning, but its growing complexity has led to significant energy consumption, particularly from computations on the client side. This challenge is especially critical in energy-harvesting FL (EHFL) systems, where device availability fluctuates due to limited and time-varying energy resources. We propose FedBacys, a battery-aware FL framework that introduces cyclic client participation based on users' battery levels to cope with these issues. FedBacys enables clients to save energy and strategically perform local training just before their designated transmission time by clustering clients and scheduling their involvement sequentially. This design minimizes redundant computation, reduces system-wide energy usage, and improves learning stability. Our experiments demonstrate that FedBacys outperforms existing approaches in terms of energy efficiency and performance consistency, exhibiting robustness even under non-i.i.d. training data distributions and with very infrequent battery charging. This work presents the first comprehensive evaluation of cyclic client participation in EHFL, incorporating both communication and computation costs into a unified, resource-aware scheduling strategy.","sentences":["Federated Learning (FL) has emerged as a promising framework for distributed learning, but its growing complexity has led to significant energy consumption, particularly from computations on the client side.","This challenge is especially critical in energy-harvesting FL (EHFL) systems, where device availability fluctuates due to limited and time-varying energy resources.","We propose FedBacys, a battery-aware FL framework that introduces cyclic client participation based on users' battery levels to cope with these issues.","FedBacys enables clients to save energy and strategically perform local training just before their designated transmission time by clustering clients and scheduling their involvement sequentially.","This design minimizes redundant computation, reduces system-wide energy usage, and improves learning stability.","Our experiments demonstrate that FedBacys outperforms existing approaches in terms of energy efficiency and performance consistency, exhibiting robustness even under non-i.i.d. training data distributions and with very infrequent battery charging.","This work presents the first comprehensive evaluation of cyclic client participation in EHFL, incorporating both communication and computation costs into a unified, resource-aware scheduling strategy."],"url":"http://arxiv.org/abs/2504.12181v1"}
{"created":"2025-04-16 15:37:09","title":"Trusting CHATGPT: how minor tweaks in the prompts lead to major differences in sentiment classification","abstract":"One fundamental question for the social sciences today is: how much can we trust highly complex predictive models like ChatGPT? This study tests the hypothesis that subtle changes in the structure of prompts do not produce significant variations in the classification results of sentiment polarity analysis generated by the Large Language Model GPT-4o mini. Using a dataset of 100.000 comments in Spanish on four Latin American presidents, the model classified the comments as positive, negative, or neutral on 10 occasions, varying the prompts slightly each time. The experimental methodology included exploratory and confirmatory analyses to identify significant discrepancies among classifications.   The results reveal that even minor modifications to prompts such as lexical, syntactic, or modal changes, or even their lack of structure impact the classifications. In certain cases, the model produced inconsistent responses, such as mixing categories, providing unsolicited explanations, or using languages other than Spanish. Statistical analysis using Chi-square tests confirmed significant differences in most comparisons between prompts, except in one case where linguistic structures were highly similar.   These findings challenge the robustness and trust of Large Language Models for classification tasks, highlighting their vulnerability to variations in instructions. Moreover, it was evident that the lack of structured grammar in prompts increases the frequency of hallucinations. The discussion underscores that trust in Large Language Models is based not only on technical performance but also on the social and institutional relationships underpinning their use.","sentences":["One fundamental question for the social sciences today is: how much can we trust highly complex predictive models like ChatGPT?","This study tests the hypothesis that subtle changes in the structure of prompts do not produce significant variations in the classification results of sentiment polarity analysis generated by the Large Language Model GPT-4o mini.","Using a dataset of 100.000 comments in Spanish on four Latin American presidents, the model classified the comments as positive, negative, or neutral on 10 occasions, varying the prompts slightly each time.","The experimental methodology included exploratory and confirmatory analyses to identify significant discrepancies among classifications.   ","The results reveal that even minor modifications to prompts such as lexical, syntactic, or modal changes, or even their lack of structure impact the classifications.","In certain cases, the model produced inconsistent responses, such as mixing categories, providing unsolicited explanations, or using languages other than Spanish.","Statistical analysis using Chi-square tests confirmed significant differences in most comparisons between prompts, except in one case where linguistic structures were highly similar.   ","These findings challenge the robustness and trust of Large Language Models for classification tasks, highlighting their vulnerability to variations in instructions.","Moreover, it was evident that the lack of structured grammar in prompts increases the frequency of hallucinations.","The discussion underscores that trust in Large Language Models is based not only on technical performance but also on the social and institutional relationships underpinning their use."],"url":"http://arxiv.org/abs/2504.12180v1"}
{"created":"2025-04-16 15:27:57","title":"Mapping Controversies Using Artificial Intelligence: An Analysis of the Hamas-Israel Conflict on YouTube","abstract":"This article analyzes the Hamas-Israel controversy through 253,925 Spanish-language YouTube comments posted between October 2023 and January 2024, following the October 7 attack that escalated the conflict. Adopting an interdisciplinary approach, the study combines the analysis of controversies from Science and Technology Studies (STS) with advanced computational methodologies, specifically Natural Language Processing (NLP) using the BERT (Bidirectional Encoder Representations from Transformers) model. Using this approach, the comments were automatically classified into seven categories, reflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli positions, among others. The results show a predominance of pro- Palestinian comments, although pro-Israeli and anti-Palestinian comments received more \"likes.\" This study also applies the agenda-setting theory to demonstrate how media coverage significantly influences public perception, observing a notable shift in public opinion, transitioning from a pro- Palestinian stance to a more critical position towards Israel. This work highlights the importance of combining social science perspectives with technological tools in the analysis of controversies, presenting a methodological innovation by integrating computational analysis with critical social theories to address complex public opinion phenomena and media narratives.","sentences":["This article analyzes the Hamas-Israel controversy through 253,925 Spanish-language YouTube comments posted between October 2023 and January 2024, following the October 7 attack that escalated the conflict.","Adopting an interdisciplinary approach, the study combines the analysis of controversies from Science and Technology Studies (STS) with advanced computational methodologies, specifically Natural Language Processing (NLP) using the BERT (Bidirectional Encoder Representations from Transformers) model.","Using this approach, the comments were automatically classified into seven categories, reflecting pro-Palestinian, pro-Israeli, anti- Palestinian, anti-Israeli positions, among others.","The results show a predominance of pro- Palestinian comments, although pro-Israeli and anti-Palestinian comments received more \"likes.\"","This study also applies the agenda-setting theory to demonstrate how media coverage significantly influences public perception, observing a notable shift in public opinion, transitioning from a pro- Palestinian stance to a more critical position towards Israel.","This work highlights the importance of combining social science perspectives with technological tools in the analysis of controversies, presenting a methodological innovation by integrating computational analysis with critical social theories to address complex public opinion phenomena and media narratives."],"url":"http://arxiv.org/abs/2504.12177v1"}
{"created":"2025-04-16 15:25:45","title":"Poem Meter Classification of Recited Arabic Poetry: Integrating High-Resource Systems for a Low-Resource Task","abstract":"Arabic poetry is an essential and integral part of Arabic language and culture. It has been used by the Arabs to spot lights on their major events such as depicting brutal battles and conflicts. They also used it, as in many other languages, for various purposes such as romance, pride, lamentation, etc. Arabic poetry has received major attention from linguistics over the decades. One of the main characteristics of Arabic poetry is its special rhythmic structure as opposed to prose. This structure is referred to as a meter. Meters, along with other poetic characteristics, are intensively studied in an Arabic linguistic field called \"\\textit{Aroud}\". Identifying these meters for a verse is a lengthy and complicated process. It also requires technical knowledge in \\textit{Aruod}. For recited poetry, it adds an extra layer of processing. Developing systems for automatic identification of poem meters for recited poems need large amounts of labelled data. In this study, we propose a state-of-the-art framework to identify the poem meters of recited Arabic poetry, where we integrate two separate high-resource systems to perform the low-resource task. To ensure generalization of our proposed architecture, we publish a benchmark for this task for future research.","sentences":["Arabic poetry is an essential and integral part of Arabic language and culture.","It has been used by the Arabs to spot lights on their major events such as depicting brutal battles and conflicts.","They also used it, as in many other languages, for various purposes such as romance, pride, lamentation, etc.","Arabic poetry has received major attention from linguistics over the decades.","One of the main characteristics of Arabic poetry is its special rhythmic structure as opposed to prose.","This structure is referred to as a meter.","Meters, along with other poetic characteristics, are intensively studied in an Arabic linguistic field called \"\\textit{Aroud}\".","Identifying these meters for a verse is a lengthy and complicated process.","It also requires technical knowledge in \\textit{Aruod}.","For recited poetry, it adds an extra layer of processing.","Developing systems for automatic identification of poem meters for recited poems need large amounts of labelled data.","In this study, we propose a state-of-the-art framework to identify the poem meters of recited Arabic poetry, where we integrate two separate high-resource systems to perform the low-resource task.","To ensure generalization of our proposed architecture, we publish a benchmark for this task for future research."],"url":"http://arxiv.org/abs/2504.12172v1"}
{"created":"2025-04-16 15:21:13","title":"AI Behind Closed Doors: a Primer on The Governance of Internal Deployment","abstract":"The most advanced future AI systems will first be deployed inside the frontier AI companies developing them. According to these companies and independent experts, AI systems may reach or even surpass human intelligence and capabilities by 2030. Internal deployment is, therefore, a key source of benefits and risks from frontier AI systems. Despite this, the governance of the internal deployment of highly advanced frontier AI systems appears absent. This report aims to address this absence by priming a conversation around the governance of internal deployment. It presents a conceptualization of internal deployment, learnings from other sectors, reviews of existing legal frameworks and their applicability, and illustrative examples of the type of scenarios we are most concerned about. Specifically, it discusses the risks correlated to the loss of control via the internal application of a misaligned AI system to the AI research and development pipeline, and unconstrained and undetected power concentration behind closed doors. The report culminates with a small number of targeted recommendations that provide a first blueprint for the governance of internal deployment.","sentences":["The most advanced future AI systems will first be deployed inside the frontier AI companies developing them.","According to these companies and independent experts, AI systems may reach or even surpass human intelligence and capabilities by 2030.","Internal deployment is, therefore, a key source of benefits and risks from frontier AI systems.","Despite this, the governance of the internal deployment of highly advanced frontier AI systems appears absent.","This report aims to address this absence by priming a conversation around the governance of internal deployment.","It presents a conceptualization of internal deployment, learnings from other sectors, reviews of existing legal frameworks and their applicability, and illustrative examples of the type of scenarios we are most concerned about.","Specifically, it discusses the risks correlated to the loss of control via the internal application of a misaligned AI system to the AI research and development pipeline, and unconstrained and undetected power concentration behind closed doors.","The report culminates with a small number of targeted recommendations that provide a first blueprint for the governance of internal deployment."],"url":"http://arxiv.org/abs/2504.12170v1"}
{"created":"2025-04-16 15:19:11","title":"Towards a General-Purpose Zero-Shot Synthetic Low-Light Image and Video Pipeline","abstract":"Low-light conditions pose significant challenges for both human and machine annotation. This in turn has led to a lack of research into machine understanding for low-light images and (in particular) videos. A common approach is to apply annotations obtained from high quality datasets to synthetically created low light versions. In addition, these approaches are often limited through the use of unrealistic noise models. In this paper, we propose a new Degradation Estimation Network (DEN), which synthetically generates realistic standard RGB (sRGB) noise without the requirement for camera metadata. This is achieved by estimating the parameters of physics-informed noise distributions, trained in a self-supervised manner. This zero-shot approach allows our method to generate synthetic noisy content with a diverse range of realistic noise characteristics, unlike other methods which focus on recreating the noise characteristics of the training data. We evaluate our proposed synthetic pipeline using various methods trained on its synthetic data for typical low-light tasks including synthetic noise replication, video enhancement, and object detection, showing improvements of up to 24\\% KLD, 21\\% LPIPS, and 62\\% AP$_{50-95}$, respectively.","sentences":["Low-light conditions pose significant challenges for both human and machine annotation.","This in turn has led to a lack of research into machine understanding for low-light images and (in particular) videos.","A common approach is to apply annotations obtained from high quality datasets to synthetically created low light versions.","In addition, these approaches are often limited through the use of unrealistic noise models.","In this paper, we propose a new Degradation Estimation Network (DEN), which synthetically generates realistic standard RGB (sRGB) noise without the requirement for camera metadata.","This is achieved by estimating the parameters of physics-informed noise distributions, trained in a self-supervised manner.","This zero-shot approach allows our method to generate synthetic noisy content with a diverse range of realistic noise characteristics, unlike other methods which focus on recreating the noise characteristics of the training data.","We evaluate our proposed synthetic pipeline using various methods trained on its synthetic data for typical low-light tasks including synthetic noise replication, video enhancement, and object detection, showing improvements of up to 24\\% KLD, 21\\% LPIPS, and 62\\% AP$_{50-95}$, respectively."],"url":"http://arxiv.org/abs/2504.12169v1"}
{"created":"2025-04-16 15:18:56","title":"RADLER: Radar Object Detection Leveraging Semantic 3D City Models and Self-Supervised Radar-Image Learning","abstract":"Semantic 3D city models are worldwide easy-accessible, providing accurate, object-oriented, and semantic-rich 3D priors. To date, their potential to mitigate the noise impact on radar object detection remains under-explored. In this paper, we first introduce a unique dataset, RadarCity, comprising 54K synchronized radar-image pairs and semantic 3D city models. Moreover, we propose a novel neural network, RADLER, leveraging the effectiveness of contrastive self-supervised learning (SSL) and semantic 3D city models to enhance radar object detection of pedestrians, cyclists, and cars. Specifically, we first obtain the robust radar features via a SSL network in the radar-image pretext task. We then use a simple yet effective feature fusion strategy to incorporate semantic-depth features from semantic 3D city models. Having prior 3D information as guidance, RADLER obtains more fine-grained details to enhance radar object detection. We extensively evaluate RADLER on the collected RadarCity dataset and demonstrate average improvements of 5.46% in mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over previous radar object detection methods. We believe this work will foster further research on semantic-guided and map-supported radar object detection. Our project page is publicly available athttps://gpp-communication.github.io/RADLER .","sentences":["Semantic 3D city models are worldwide easy-accessible, providing accurate, object-oriented, and semantic-rich 3D priors.","To date, their potential to mitigate the noise impact on radar object detection remains under-explored.","In this paper, we first introduce a unique dataset, RadarCity, comprising 54K synchronized radar-image pairs and semantic 3D city models.","Moreover, we propose a novel neural network, RADLER, leveraging the effectiveness of contrastive self-supervised learning (SSL) and semantic 3D city models to enhance radar object detection of pedestrians, cyclists, and cars.","Specifically, we first obtain the robust radar features via a SSL network in the radar-image pretext task.","We then use a simple yet effective feature fusion strategy to incorporate semantic-depth features from semantic 3D city models.","Having prior 3D information as guidance, RADLER obtains more fine-grained details to enhance radar object detection.","We extensively evaluate RADLER on the collected RadarCity dataset and demonstrate average improvements of 5.46% in mean avarage precision (mAP) and 3.51% in mean avarage recall (mAR) over previous radar object detection methods.","We believe this work will foster further research on semantic-guided and map-supported radar object detection.","Our project page is publicly available athttps://gpp-communication.github.io/RADLER ."],"url":"http://arxiv.org/abs/2504.12167v1"}
{"created":"2025-04-16 15:18:11","title":"CodingHomo: Bootstrapping Deep Homography With Video Coding","abstract":"Homography estimation is a fundamental task in computer vision with applications in diverse fields. Recent advances in deep learning have improved homography estimation, particularly with unsupervised learning approaches, offering increased robustness and generalizability. However, accurately predicting homography, especially in complex motions, remains a challenge. In response, this work introduces a novel method leveraging video coding, particularly by harnessing inherent motion vectors (MVs) present in videos. We present CodingHomo, an unsupervised framework for homography estimation. Our framework features a Mask-Guided Fusion (MGF) module that identifies and utilizes beneficial features among the MVs, thereby enhancing the accuracy of homography prediction. Additionally, the Mask-Guided Homography Estimation (MGHE) module is presented for eliminating undesired features in the coarse-to-fine homography refinement process. CodingHomo outperforms existing state-of-the-art unsupervised methods, delivering good robustness and generalizability. The code and dataset are available at: \\href{github}{https://github.com/liuyike422/CodingHomo","sentences":["Homography estimation is a fundamental task in computer vision with applications in diverse fields.","Recent advances in deep learning have improved homography estimation, particularly with unsupervised learning approaches, offering increased robustness and generalizability.","However, accurately predicting homography, especially in complex motions, remains a challenge.","In response, this work introduces a novel method leveraging video coding, particularly by harnessing inherent motion vectors (MVs) present in videos.","We present CodingHomo, an unsupervised framework for homography estimation.","Our framework features a Mask-Guided Fusion (MGF) module that identifies and utilizes beneficial features among the MVs, thereby enhancing the accuracy of homography prediction.","Additionally, the Mask-Guided Homography Estimation (MGHE) module is presented for eliminating undesired features in the coarse-to-fine homography refinement process.","CodingHomo outperforms existing state-of-the-art unsupervised methods, delivering good robustness and generalizability.","The code and dataset are available at: \\href{github}{https://github.com/liuyike422/CodingHomo"],"url":"http://arxiv.org/abs/2504.12165v1"}
{"created":"2025-04-16 15:05:25","title":"Deep Material Network: Overview, applications and current directions","abstract":"Deep Material Network (DMN) has emerged as a powerful framework for multiscale material modeling, enabling efficient and accurate predictions of material behavior across different length scales. Unlike traditional machine learning approaches, the trainable parameters in DMN have direct physical interpretations, capturing the geometric characteristics of the microstructure rather than serving as purely statistical fitting parameters. Its hierarchical tree structure effectively encodes microstructural interactions and deformation mechanisms, allowing DMN to achieve a balance between accuracy and computational efficiency. This physics-informed architecture significantly reduces computational costs compared to direct numerical simulations while preserving essential microstructural physics. Furthermore, DMN can be trained solely on a linear elastic dataset while effectively extrapolating nonlinear responses during online prediction, making it a highly efficient and scalable approach for multiscale material modeling. This article provides a comprehensive review of DMN, detailing its motivation, underlying methodology, and recent advancements. We discuss key modeling aspects, including its hierarchical structure, training process, and the role of physics-based constraints in enhancing predictive accuracy. Furthermore, we highlight its applications in component-scale multiscale analysis and inverse parameter identification, demonstrating its capability to bridge microscale material behavior with macroscale engineering predictions. Finally, we discuss challenges and future directions in improving DMN's generalization capabilities and its potential extensions for broader applications in multiscale modeling.","sentences":["Deep Material Network (DMN) has emerged as a powerful framework for multiscale material modeling, enabling efficient and accurate predictions of material behavior across different length scales.","Unlike traditional machine learning approaches, the trainable parameters in DMN have direct physical interpretations, capturing the geometric characteristics of the microstructure rather than serving as purely statistical fitting parameters.","Its hierarchical tree structure effectively encodes microstructural interactions and deformation mechanisms, allowing DMN to achieve a balance between accuracy and computational efficiency.","This physics-informed architecture significantly reduces computational costs compared to direct numerical simulations while preserving essential microstructural physics.","Furthermore, DMN can be trained solely on a linear elastic dataset while effectively extrapolating nonlinear responses during online prediction, making it a highly efficient and scalable approach for multiscale material modeling.","This article provides a comprehensive review of DMN, detailing its motivation, underlying methodology, and recent advancements.","We discuss key modeling aspects, including its hierarchical structure, training process, and the role of physics-based constraints in enhancing predictive accuracy.","Furthermore, we highlight its applications in component-scale multiscale analysis and inverse parameter identification, demonstrating its capability to bridge microscale material behavior with macroscale engineering predictions.","Finally, we discuss challenges and future directions in improving DMN's generalization capabilities and its potential extensions for broader applications in multiscale modeling."],"url":"http://arxiv.org/abs/2504.12159v1"}
{"created":"2025-04-16 15:04:14","title":"FocusedAD: Character-centric Movie Audio Description","abstract":"Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions. It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD .","sentences":["Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences.","Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.","To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions.","It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters.","To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks.","FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset.","Code and data will be released at https://github.com/Thorin215/FocusedAD ."],"url":"http://arxiv.org/abs/2504.12157v1"}
{"created":"2025-04-16 15:04:00","title":"Predictive Multiplicity in Survival Models: A Method for Quantifying Model Uncertainty in Predictive Maintenance Applications","abstract":"In many applications, especially those involving prediction, models may yield near-optimal performance yet significantly disagree on individual-level outcomes. This phenomenon, known as predictive multiplicity, has been formally defined in binary, probabilistic, and multi-target classification, and undermines the reliability of predictive systems. However, its implications remain unexplored in the context of survival analysis, which involves estimating the time until a failure or similar event while properly handling censored data. We frame predictive multiplicity as a critical concern in survival-based models and introduce formal measures -- ambiguity, discrepancy, and obscurity -- to quantify it. This is particularly relevant for downstream tasks such as maintenance scheduling, where precise individual risk estimates are essential. Understanding and reporting predictive multiplicity helps build trust in models deployed in high-stakes environments. We apply our methodology to benchmark datasets from predictive maintenance, extending the notion of multiplicity to survival models. Our findings show that ambiguity steadily increases, reaching up to 40-45% of observations; discrepancy is lower but exhibits a similar trend; and obscurity remains mild and concentrated in a few models. These results demonstrate that multiple accurate survival models may yield conflicting estimations of failure risk and degradation progression for the same equipment. This highlights the need to explicitly measure and communicate predictive multiplicity to ensure reliable decision-making in process health management.","sentences":["In many applications, especially those involving prediction, models may yield near-optimal performance yet significantly disagree on individual-level outcomes.","This phenomenon, known as predictive multiplicity, has been formally defined in binary, probabilistic, and multi-target classification, and undermines the reliability of predictive systems.","However, its implications remain unexplored in the context of survival analysis, which involves estimating the time until a failure or similar event while properly handling censored data.","We frame predictive multiplicity as a critical concern in survival-based models and introduce formal measures -- ambiguity, discrepancy, and obscurity -- to quantify it.","This is particularly relevant for downstream tasks such as maintenance scheduling, where precise individual risk estimates are essential.","Understanding and reporting predictive multiplicity helps build trust in models deployed in high-stakes environments.","We apply our methodology to benchmark datasets from predictive maintenance, extending the notion of multiplicity to survival models.","Our findings show that ambiguity steadily increases, reaching up to 40-45% of observations; discrepancy is lower but exhibits a similar trend; and obscurity remains mild and concentrated in a few models.","These results demonstrate that multiple accurate survival models may yield conflicting estimations of failure risk and degradation progression for the same equipment.","This highlights the need to explicitly measure and communicate predictive multiplicity to ensure reliable decision-making in process health management."],"url":"http://arxiv.org/abs/2504.12156v1"}
{"created":"2025-04-16 15:00:06","title":"Towards Explainable Fusion and Balanced Learning in Multimodal Sentiment Analysis","abstract":"Multimodal Sentiment Analysis (MSA) faces two critical challenges: the lack of interpretability in the decision logic of multimodal fusion and modality imbalance caused by disparities in inter-modal information density. To address these issues, we propose KAN-MCP, a novel framework that integrates the interpretability of Kolmogorov-Arnold Networks (KAN) with the robustness of the Multimodal Clean Pareto (MCPareto) framework. First, KAN leverages its univariate function decomposition to achieve transparent analysis of cross-modal interactions. This structural design allows direct inspection of feature transformations without relying on external interpretation tools, thereby ensuring both high expressiveness and interpretability. Second, the proposed MCPareto enhances robustness by addressing modality imbalance and noise interference. Specifically, we introduce the Dimensionality Reduction and Denoising Modal Information Bottleneck (DRD-MIB) method, which jointly denoises and reduces feature dimensionality. This approach provides KAN with discriminative low-dimensional inputs to reduce the modeling complexity of KAN while preserving critical sentiment-related information. Furthermore, MCPareto dynamically balances gradient contributions across modalities using the purified features output by DRD-MIB, ensuring lossless transmission of auxiliary signals and effectively alleviating modality imbalance. This synergy of interpretability and robustness not only achieves superior performance on benchmark datasets such as CMU-MOSI, CMU-MOSEI, and CH-SIMS v2 but also offers an intuitive visualization interface through KAN's interpretable architecture.","sentences":["Multimodal Sentiment Analysis (MSA) faces two critical challenges: the lack of interpretability in the decision logic of multimodal fusion and modality imbalance caused by disparities in inter-modal information density.","To address these issues, we propose KAN-MCP, a novel framework that integrates the interpretability of Kolmogorov-Arnold Networks (KAN) with the robustness of the Multimodal Clean Pareto (MCPareto) framework.","First, KAN leverages its univariate function decomposition to achieve transparent analysis of cross-modal interactions.","This structural design allows direct inspection of feature transformations without relying on external interpretation tools, thereby ensuring both high expressiveness and interpretability.","Second, the proposed MCPareto enhances robustness by addressing modality imbalance and noise interference.","Specifically, we introduce the Dimensionality Reduction and Denoising Modal Information Bottleneck (DRD-MIB) method, which jointly denoises and reduces feature dimensionality.","This approach provides KAN with discriminative low-dimensional inputs to reduce the modeling complexity of KAN while preserving critical sentiment-related information.","Furthermore, MCPareto dynamically balances gradient contributions across modalities using the purified features output by DRD-MIB, ensuring lossless transmission of auxiliary signals and effectively alleviating modality imbalance.","This synergy of interpretability and robustness not only achieves superior performance on benchmark datasets such as CMU-MOSI, CMU-MOSEI, and CH-SIMS v2 but also offers an intuitive visualization interface through KAN's interpretable architecture."],"url":"http://arxiv.org/abs/2504.12151v1"}
{"created":"2025-04-16 14:53:28","title":"ARCeR: an Agentic RAG for the Automated Definition of Cyber Ranges","abstract":"The growing and evolving landscape of cybersecurity threats necessitates the development of supporting tools and platforms that allow for the creation of realistic IT environments operating within virtual, controlled settings as Cyber Ranges (CRs). CRs can be exploited for analyzing vulnerabilities and experimenting with the effectiveness of devised countermeasures, as well as serving as training environments for building cyber security skills and abilities for IT operators. This paper proposes ARCeR as an innovative solution for the automatic generation and deployment of CRs, starting from user-provided descriptions in a natural language. ARCeR relies on the Agentic RAG paradigm, which allows it to fully exploit state-of-art AI technologies. Experimental results show that ARCeR is able to successfully process prompts even in cases that LLMs or basic RAG systems are not able to cope with. Furthermore, ARCeR is able to target any CR framework provided that specific knowledge is made available to it.","sentences":["The growing and evolving landscape of cybersecurity threats necessitates the development of supporting tools and platforms that allow for the creation of realistic IT environments operating within virtual, controlled settings as Cyber Ranges (CRs).","CRs can be exploited for analyzing vulnerabilities and experimenting with the effectiveness of devised countermeasures, as well as serving as training environments for building cyber security skills and abilities for IT operators.","This paper proposes ARCeR as an innovative solution for the automatic generation and deployment of CRs, starting from user-provided descriptions in a natural language.","ARCeR relies on the Agentic RAG paradigm, which allows it to fully exploit state-of-art AI technologies.","Experimental results show that ARCeR is able to successfully process prompts even in cases that LLMs or basic RAG systems are not able to cope with.","Furthermore, ARCeR is able to target any CR framework provided that specific knowledge is made available to it."],"url":"http://arxiv.org/abs/2504.12143v1"}
{"created":"2025-04-16 14:53:12","title":"Overlapping Error Correction Codes on Two-Dimensional Structures","abstract":"The growing demand for highly reliable communication systems drives the research and development of algorithms that identify and correct errors during data transmission and storage. This need becomes even more critical in hard-to-access or sensitive systems, such as those used in space applications, passenger transportation, and the financial sector. In this context, Error Correction Codes (ECCs) are essential tools for ensuring a certain level of reliability. This work proposes a technique to enhance ECC error correction capability by overlapping data regions. The approach consists of protecting the same data area with multiple ECCs organized in a two-dimensional structure, enabling logical inferences that correlate the codes and improve their error detection and correction capabilities. More specifically, the overlapping is characterized by the organization of multiple ECCs, whose intersection exclusively covers the entire data region. Different configurations of overlapping ECCs were analyzed to evaluate the proposal regarding error detection and correction capability, scalability, and reliability. Experimental results confirm the technique's effectiveness and demonstrate its high scalability potential, reducing the need for redundancy bits relative to the number of data bits. Furthermore, comparisons with state-of-the-art ECC approaches indicate the technique's applicability in critical systems that require high reliability.","sentences":["The growing demand for highly reliable communication systems drives the research and development of algorithms that identify and correct errors during data transmission and storage.","This need becomes even more critical in hard-to-access or sensitive systems, such as those used in space applications, passenger transportation, and the financial sector.","In this context, Error Correction Codes (ECCs) are essential tools for ensuring a certain level of reliability.","This work proposes a technique to enhance ECC error correction capability by overlapping data regions.","The approach consists of protecting the same data area with multiple ECCs organized in a two-dimensional structure, enabling logical inferences that correlate the codes and improve their error detection and correction capabilities.","More specifically, the overlapping is characterized by the organization of multiple ECCs, whose intersection exclusively covers the entire data region.","Different configurations of overlapping ECCs were analyzed to evaluate the proposal regarding error detection and correction capability, scalability, and reliability.","Experimental results confirm the technique's effectiveness and demonstrate its high scalability potential, reducing the need for redundancy bits relative to the number of data bits.","Furthermore, comparisons with state-of-the-art ECC approaches indicate the technique's applicability in critical systems that require high reliability."],"url":"http://arxiv.org/abs/2504.12142v1"}
{"created":"2025-04-16 14:52:22","title":"Multilingual Contextualization of Large Language Models for Document-Level Machine Translation","abstract":"Large language models (LLMs) have demonstrated strong performance in sentence-level machine translation, but scaling to document-level translation remains challenging, particularly in modeling long-range dependencies and discourse phenomena across sentences and paragraphs. In this work, we propose a method to improve LLM-based long-document translation through targeted fine-tuning on high-quality document-level data, which we curate and introduce as DocBlocks. Our approach supports multiple translation paradigms, including direct document-to-document and chunk-level translation, by integrating instructions both with and without surrounding context. This enables models to better capture cross-sentence dependencies while maintaining strong sentence-level translation performance. Experimental results show that incorporating multiple translation paradigms improves document-level translation quality and inference speed compared to prompting and agent-based methods.","sentences":["Large language models (LLMs) have demonstrated strong performance in sentence-level machine translation, but scaling to document-level translation remains challenging, particularly in modeling long-range dependencies and discourse phenomena across sentences and paragraphs.","In this work, we propose a method to improve LLM-based long-document translation through targeted fine-tuning on high-quality document-level data, which we curate and introduce as DocBlocks.","Our approach supports multiple translation paradigms, including direct document-to-document and chunk-level translation, by integrating instructions both with and without surrounding context.","This enables models to better capture cross-sentence dependencies while maintaining strong sentence-level translation performance.","Experimental results show that incorporating multiple translation paradigms improves document-level translation quality and inference speed compared to prompting and agent-based methods."],"url":"http://arxiv.org/abs/2504.12140v1"}
{"created":"2025-04-16 14:50:25","title":"Efficient Contrastive Decoding with Probabilistic Hallucination Detection - Mitigating Hallucinations in Large Vision Language Models -","abstract":"Despite recent advances in Large Vision Language Models (LVLMs), these models still suffer from generating hallucinatory responses that do not align with the visual input provided. To mitigate such hallucinations, we introduce Efficient Contrastive Decoding (ECD), a simple method that leverages probabilistic hallucination detection to shift the output distribution towards contextually accurate answers at inference time. By contrasting token probabilities and hallucination scores, ECD subtracts hallucinated concepts from the original distribution, effectively suppressing hallucinations. Notably, our proposed method can be applied to any open-source LVLM and does not require additional LVLM training. We evaluate our method on several benchmark datasets and across different LVLMs. Our experiments show that ECD effectively mitigates hallucinations, outperforming state-of-the-art methods with respect to performance on LVLM benchmarks and computation time.","sentences":["Despite recent advances in Large Vision Language Models (LVLMs), these models still suffer from generating hallucinatory responses that do not align with the visual input provided.","To mitigate such hallucinations, we introduce Efficient Contrastive Decoding (ECD), a simple method that leverages probabilistic hallucination detection to shift the output distribution towards contextually accurate answers at inference time.","By contrasting token probabilities and hallucination scores, ECD subtracts hallucinated concepts from the original distribution, effectively suppressing hallucinations.","Notably, our proposed method can be applied to any open-source LVLM and does not require additional LVLM training.","We evaluate our method on several benchmark datasets and across different LVLMs.","Our experiments show that ECD effectively mitigates hallucinations, outperforming state-of-the-art methods with respect to performance on LVLM benchmarks and computation time."],"url":"http://arxiv.org/abs/2504.12137v1"}
{"created":"2025-04-16 14:45:26","title":"Weakly Semi-supervised Whole Slide Image Classification by Two-level Cross Consistency Supervision","abstract":"Computer-aided Whole Slide Image (WSI) classification has the potential to enhance the accuracy and efficiency of clinical pathological diagnosis. It is commonly formulated as a Multiple Instance Learning (MIL) problem, where each WSI is treated as a bag and the small patches extracted from the WSI are considered instances within that bag. However, obtaining labels for a large number of bags is a costly and time-consuming process, particularly when utilizing existing WSIs for new classification tasks. This limitation renders most existing WSI classification methods ineffective. To address this issue, we propose a novel WSI classification problem setting, more aligned with clinical practice, termed Weakly Semi-supervised Whole slide image Classification (WSWC). In WSWC, a small number of bags are labeled, while a significant number of bags remain unlabeled. The MIL nature of the WSWC problem, coupled with the absence of patch labels, distinguishes it from typical semi-supervised image classification problems, making existing algorithms for natural images unsuitable for directly solving the WSWC problem. In this paper, we present a concise and efficient framework, named CroCo, to tackle the WSWC problem through two-level Cross Consistency supervision. CroCo comprises two heterogeneous classifier branches capable of performing both instance classification and bag classification. The fundamental idea is to establish cross-consistency supervision at both the bag-level and instance-level between the two branches during training. Extensive experiments conducted on four datasets demonstrate that CroCo achieves superior bag classification and instance classification performance compared to other comparative methods when limited WSIs with bag labels are available. To the best of our knowledge, this paper presents for the first time the WSWC problem and gives a successful resolution.","sentences":["Computer-aided Whole Slide Image (WSI) classification has the potential to enhance the accuracy and efficiency of clinical pathological diagnosis.","It is commonly formulated as a Multiple Instance Learning (MIL) problem, where each WSI is treated as a bag and the small patches extracted from the WSI are considered instances within that bag.","However, obtaining labels for a large number of bags is a costly and time-consuming process, particularly when utilizing existing WSIs for new classification tasks.","This limitation renders most existing WSI classification methods ineffective.","To address this issue, we propose a novel WSI classification problem setting, more aligned with clinical practice, termed Weakly Semi-supervised Whole slide image Classification (WSWC).","In WSWC, a small number of bags are labeled, while a significant number of bags remain unlabeled.","The MIL nature of the WSWC problem, coupled with the absence of patch labels, distinguishes it from typical semi-supervised image classification problems, making existing algorithms for natural images unsuitable for directly solving the WSWC problem.","In this paper, we present a concise and efficient framework, named CroCo, to tackle the WSWC problem through two-level Cross Consistency supervision.","CroCo comprises two heterogeneous classifier branches capable of performing both instance classification and bag classification.","The fundamental idea is to establish cross-consistency supervision at both the bag-level and instance-level between the two branches during training.","Extensive experiments conducted on four datasets demonstrate that CroCo achieves superior bag classification and instance classification performance compared to other comparative methods when limited WSIs with bag labels are available.","To the best of our knowledge, this paper presents for the first time the WSWC problem and gives a successful resolution."],"url":"http://arxiv.org/abs/2504.12132v1"}
{"created":"2025-04-16 14:44:00","title":"Anti-Aesthetics: Protecting Facial Privacy against Customized Text-to-Image Synthesis","abstract":"The rise of customized diffusion models has spurred a boom in personalized visual content creation, but also poses risks of malicious misuse, severely threatening personal privacy and copyright protection. Some studies show that the aesthetic properties of images are highly positively correlated with human perception of image quality. Inspired by this, we approach the problem from a novel and intriguing aesthetic perspective to degrade the generation quality of maliciously customized models, thereby achieving better protection of facial identity. Specifically, we propose a Hierarchical Anti-Aesthetic (HAA) framework to fully explore aesthetic cues, which consists of two key branches: 1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward mechanism and a global anti-aesthetic loss, it can degrade the overall aesthetics of the generated content; 2) Local Anti-Aesthetics: A local anti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to guide adversarial perturbations to disrupt local facial identity. By seamlessly integrating both branches, our HAA effectively achieves the goal of anti-aesthetics from a global to a local level during customized generation. Extensive experiments show that HAA outperforms existing SOTA methods largely in identity removal, providing a powerful tool for protecting facial privacy and copyright.","sentences":["The rise of customized diffusion models has spurred a boom in personalized visual content creation, but also poses risks of malicious misuse, severely threatening personal privacy and copyright protection.","Some studies show that the aesthetic properties of images are highly positively correlated with human perception of image quality.","Inspired by this, we approach the problem from a novel and intriguing aesthetic perspective to degrade the generation quality of maliciously customized models, thereby achieving better protection of facial identity.","Specifically, we propose a Hierarchical Anti-Aesthetic (HAA) framework to fully explore aesthetic cues, which consists of two key branches: 1) Global Anti-Aesthetics: By establishing a global anti-aesthetic reward mechanism and a global anti-aesthetic loss, it can degrade the overall aesthetics of the generated content; 2) Local Anti-Aesthetics: A local anti-aesthetic reward mechanism and a local anti-aesthetic loss are designed to guide adversarial perturbations to disrupt local facial identity.","By seamlessly integrating both branches, our HAA effectively achieves the goal of anti-aesthetics from a global to a local level during customized generation.","Extensive experiments show that HAA outperforms existing SOTA methods largely in identity removal, providing a powerful tool for protecting facial privacy and copyright."],"url":"http://arxiv.org/abs/2504.12129v1"}
{"created":"2025-04-16 14:43:44","title":"An Algebraic Extension of Intuitionistic Linear Logic: The $L_!^S$-Calculus and Its Categorical Model","abstract":"We introduce the $L_!^S$-calculus, a linear lambda-calculus extended with scalar multiplication and term addition, that acts as a proof language for intuitionistic linear logic (ILL). These algebraic operations enable the direct expression of linearity at the syntactic level, a property not typically available in standard proof-term calculi. Building upon previous work, we develop the $L_!^S$-calculus as an extension of the $L^S$-calculus with the $!$ modality. We prove key meta-theoretical properties--subject reduction, confluence, strong normalisation, and an introduction property--as well as preserve the expressiveness of the original $L^S$-calculus, including the encoding of vectors and matrices, and the correspondence between proof-terms and linear functions. A denotational semantics is provided in the framework of linear categories with biproducts, ensuring a sound and adequate interpretation of the calculus. This work is part of a broader programme aiming to build a measurement-free quantum programming language grounded in linear logic.","sentences":["We introduce the $L_!^S$-calculus, a linear lambda-calculus extended with scalar multiplication and term addition, that acts as a proof language for intuitionistic linear logic (ILL).","These algebraic operations enable the direct expression of linearity at the syntactic level, a property not typically available in standard proof-term calculi.","Building upon previous work, we develop the $L_!^S$-calculus as an extension of the $L^S$-calculus with the $!$ modality.","We prove key meta-theoretical properties--subject reduction, confluence, strong normalisation, and an introduction property--as well as preserve the expressiveness of the original $L^S$-calculus, including the encoding of vectors and matrices, and the correspondence between proof-terms and linear functions.","A denotational semantics is provided in the framework of linear categories with biproducts, ensuring a sound and adequate interpretation of the calculus.","This work is part of a broader programme aiming to build a measurement-free quantum programming language grounded in linear logic."],"url":"http://arxiv.org/abs/2504.12128v1"}
{"created":"2025-04-16 14:36:52","title":"EmoACT: a Framework to Embed Emotions into Artificial Agents Based on Affect Control Theory","abstract":"As robots and artificial agents become increasingly integrated into daily life, enhancing their ability to interact with humans is essential. Emotions, which play a crucial role in human interactions, can improve the naturalness and transparency of human-robot interactions (HRI) when embodied in artificial agents. This study aims to employ Affect Control Theory (ACT), a psychological model of emotions deeply rooted in interaction, for the generation of synthetic emotions. A platform-agnostic framework inspired by ACT was developed and implemented in a humanoid robot to assess its impact on human perception. Results show that the frequency of emotional displays impacts how users perceive the robot. Moreover, appropriate emotional expressions seem to enhance the robot's perceived emotional and cognitive agency. The findings suggest that ACT can be successfully employed to embed synthetic emotions into robots, resulting in effective human-robot interactions, where the robot is perceived more as a social agent than merely a machine.","sentences":["As robots and artificial agents become increasingly integrated into daily life, enhancing their ability to interact with humans is essential.","Emotions, which play a crucial role in human interactions, can improve the naturalness and transparency of human-robot interactions (HRI) when embodied in artificial agents.","This study aims to employ Affect Control Theory (ACT), a psychological model of emotions deeply rooted in interaction, for the generation of synthetic emotions.","A platform-agnostic framework inspired by ACT was developed and implemented in a humanoid robot to assess its impact on human perception.","Results show that the frequency of emotional displays impacts how users perceive the robot.","Moreover, appropriate emotional expressions seem to enhance the robot's perceived emotional and cognitive agency.","The findings suggest that ACT can be successfully employed to embed synthetic emotions into robots, resulting in effective human-robot interactions, where the robot is perceived more as a social agent than merely a machine."],"url":"http://arxiv.org/abs/2504.12125v1"}
{"created":"2025-04-16 14:33:57","title":"Remote sensing colour image semantic segmentation of trails created by large herbivorous Mammals","abstract":"Detection of spatial areas where biodiversity is at risk is of paramount importance for the conservation and monitoring of ecosystems. Large terrestrial mammalian herbivores are keystone species as their activity not only has deep effects on soils, plants, and animals, but also shapes landscapes, as large herbivores act as allogenic ecosystem engineers. One key landscape feature that indicates intense herbivore activity and potentially impacts biodiversity is the formation of grazing trails. Grazing trails are formed by the continuous trampling activity of large herbivores that can produce complex networks of tracks of bare soil. Here, we evaluated different algorithms based on machine learning techniques to identify grazing trails. Our goal is to automatically detect potential areas with intense herbivory activity, which might be beneficial for conservation and management plans.   We have applied five semantic segmentation methods combined with fourteen encoders aimed at mapping grazing trails on aerial images. Our results indicate that in most cases the chosen methodology successfully mapped the trails, although there were a few instances where the actual trail structure was underestimated. The UNet architecture with the MambaOut encoder was the best architecture for mapping trails. The proposed approach could be applied to develop tools for mapping and monitoring temporal changes in these landscape structures to support habitat conservation and land management programs. This is the first time, to the best of our knowledge, that competitive image segmentation results are obtained for the detection and delineation of trails of large herbivorous mammals.","sentences":["Detection of spatial areas where biodiversity is at risk is of paramount importance for the conservation and monitoring of ecosystems.","Large terrestrial mammalian herbivores are keystone species as their activity not only has deep effects on soils, plants, and animals, but also shapes landscapes, as large herbivores act as allogenic ecosystem engineers.","One key landscape feature that indicates intense herbivore activity and potentially impacts biodiversity is the formation of grazing trails.","Grazing trails are formed by the continuous trampling activity of large herbivores that can produce complex networks of tracks of bare soil.","Here, we evaluated different algorithms based on machine learning techniques to identify grazing trails.","Our goal is to automatically detect potential areas with intense herbivory activity, which might be beneficial for conservation and management plans.   ","We have applied five semantic segmentation methods combined with fourteen encoders aimed at mapping grazing trails on aerial images.","Our results indicate that in most cases the chosen methodology successfully mapped the trails, although there were a few instances where the actual trail structure was underestimated.","The UNet architecture with the MambaOut encoder was the best architecture for mapping trails.","The proposed approach could be applied to develop tools for mapping and monitoring temporal changes in these landscape structures to support habitat conservation and land management programs.","This is the first time, to the best of our knowledge, that competitive image segmentation results are obtained for the detection and delineation of trails of large herbivorous mammals."],"url":"http://arxiv.org/abs/2504.12121v1"}
{"created":"2025-04-16 14:26:51","title":"Improvement of the square-root low bounds on the minimum distances of BCH codes and Matrix-product codes","abstract":"The task of constructing infinite families of self-dual codes with unbounded lengths and minimum distances exhibiting square-root lower bounds is extremely challenging, especially when it comes to cyclic codes. Recently, the first infinite family of Euclidean self-dual binary and nonbinary cyclic codes, whose minimum distances have a square-root lower bound and have a lower bound better than square-root lower bounds are constructed in \\cite{Chen23} for the lengths of these codes being unbounded. Let $q$ be a power of a prime number and $Q=q^2$. In this paper, we first improve the lower bounds on the minimum distances of Euclidean and Hermitian duals of BCH codes with length $\\frac{q^m-1}{q^s-1}$ over $\\mathbb{F}_q$ and $\\frac{Q^m-1}{Q-1}$ over $\\mathbb{F}_Q$ in \\cite{Fan23,GDL21,Wang24} for the designed distances in some ranges, respectively, where $\\frac{m}{s}\\geq 3$. Then based on matrix-product construction and some lower bounds on the minimum distances of BCH codes and their duals, we obtain several classes of Euclidean and Hermitian self-dual codes, whose minimum distances have square-root lower bounds or a square-root-like lower bounds. Our lower bounds on the minimum distances of Euclidean and Hermitian self-dual cyclic codes improved many results in \\cite{Chen23}. In addition, our lower bounds on the minimum distances of the duals of BCH codes are almost $q^s-1$ or $q$ times that of the existing lower bounds.","sentences":["The task of constructing infinite families of self-dual codes with unbounded lengths and minimum distances exhibiting square-root lower bounds is extremely challenging, especially when it comes to cyclic codes.","Recently, the first infinite family of Euclidean self-dual binary and nonbinary cyclic codes, whose minimum distances have a square-root lower bound and have a lower bound better than square-root lower bounds are constructed in \\cite{Chen23} for the lengths of these codes being unbounded.","Let $q$ be a power of a prime number and $Q=q^2$. In this paper, we first improve the lower bounds on the minimum distances of Euclidean and Hermitian duals of BCH codes with length $\\frac{q^m-1}{q^s-1}$ over $\\mathbb{F}_q$ and","$\\frac{Q^m-1}{Q-1}$ over $\\mathbb{F}_Q$ in \\cite{Fan23,GDL21,Wang24} for the designed distances in some ranges, respectively, where $\\frac{m}{s}\\geq 3$. Then based on matrix-product construction and some lower bounds on the minimum distances of BCH codes and their duals, we obtain several classes of Euclidean and Hermitian self-dual codes, whose minimum distances have square-root lower bounds or a square-root-like lower bounds.","Our lower bounds on the minimum distances of Euclidean and Hermitian self-dual cyclic codes improved many results in \\cite{Chen23}.","In addition, our lower bounds on the minimum distances of the duals of BCH codes are almost $q^s-1$ or $q$ times that of the existing lower bounds."],"url":"http://arxiv.org/abs/2504.12116v1"}
{"created":"2025-04-16 14:25:29","title":"GripMap: An Efficient, Spatially Resolved Constraint Framework for Offline and Online Trajectory Planning in Autonomous Racing","abstract":"Conventional trajectory planning approaches for autonomous vehicles often assume a fixed vehicle model that remains constant regardless of the vehicle's location. This overlooks the critical fact that the tires and the surface are the two force-transmitting partners in vehicle dynamics; while the tires stay with the vehicle, surface conditions vary with location. Recognizing these challenges, this paper presents a novel framework for spatially resolving dynamic constraints in both offline and online planning algorithms applied to autonomous racing. We introduce the GripMap concept, which provides a spatial resolution of vehicle dynamic constraints in the Frenet frame, allowing adaptation to locally varying grip conditions. This enables compensation for location-specific effects, more efficient vehicle behavior, and increased safety, unattainable with spatially invariant vehicle models. The focus is on low storage demand and quick access through perfect hashing. This framework proved advantageous in real-world applications in the presented form. Experiments inspired by autonomous racing demonstrate its effectiveness. In future work, this framework can serve as a foundational layer for developing future interpretable learning algorithms that adjust to varying grip conditions in real-time.","sentences":["Conventional trajectory planning approaches for autonomous vehicles often assume a fixed vehicle model that remains constant regardless of the vehicle's location.","This overlooks the critical fact that the tires and the surface are the two force-transmitting partners in vehicle dynamics; while the tires stay with the vehicle, surface conditions vary with location.","Recognizing these challenges, this paper presents a novel framework for spatially resolving dynamic constraints in both offline and online planning algorithms applied to autonomous racing.","We introduce the GripMap concept, which provides a spatial resolution of vehicle dynamic constraints in the Frenet frame, allowing adaptation to locally varying grip conditions.","This enables compensation for location-specific effects, more efficient vehicle behavior, and increased safety, unattainable with spatially invariant vehicle models.","The focus is on low storage demand and quick access through perfect hashing.","This framework proved advantageous in real-world applications in the presented form.","Experiments inspired by autonomous racing demonstrate its effectiveness.","In future work, this framework can serve as a foundational layer for developing future interpretable learning algorithms that adjust to varying grip conditions in real-time."],"url":"http://arxiv.org/abs/2504.12115v1"}
{"created":"2025-04-16 14:23:12","title":"An Extended Generalized Prandtl-Ishlinskii Hysteresis Model for I2RIS Robot","abstract":"Retinal surgery requires extreme precision due to constrained anatomical spaces in the human retina. To assist surgeons achieve this level of accuracy, the Improved Integrated Robotic Intraocular Snake (I2RIS) with dexterous capability has been developed. However, such flexible tendon-driven robots often suffer from hysteresis problems, which significantly challenges precise control and positioning. In particular, we observed multi-stage hysteresis phenomena in the small-scale I2RIS. In this paper, we propose an Extended Generalized Prandtl-Ishlinskii (EGPI) model to increase the fitting accuracy of the hysteresis. The model incorporates a novel switching mechanism that enables it to describe multi-stage hysteresis in the regions of monotonic input. Experimental validation on I2RIS data demonstrate that the EGPI model outperforms the conventional Generalized Prandtl-Ishlinskii (GPI) model in terms of RMSE, NRMSE, and MAE across multiple motor input directions. The EGPI model in our study highlights the potential in modeling multi-stage hysteresis in minimally invasive flexible robots.","sentences":["Retinal surgery requires extreme precision due to constrained anatomical spaces in the human retina.","To assist surgeons achieve this level of accuracy, the Improved Integrated Robotic Intraocular Snake (I2RIS) with dexterous capability has been developed.","However, such flexible tendon-driven robots often suffer from hysteresis problems, which significantly challenges precise control and positioning.","In particular, we observed multi-stage hysteresis phenomena in the small-scale I2RIS.","In this paper, we propose an Extended Generalized Prandtl-Ishlinskii (EGPI) model to increase the fitting accuracy of the hysteresis.","The model incorporates a novel switching mechanism that enables it to describe multi-stage hysteresis in the regions of monotonic input.","Experimental validation on I2RIS data demonstrate that the EGPI model outperforms the conventional Generalized Prandtl-Ishlinskii (GPI) model in terms of RMSE, NRMSE, and MAE across multiple motor input directions.","The EGPI model in our study highlights the potential in modeling multi-stage hysteresis in minimally invasive flexible robots."],"url":"http://arxiv.org/abs/2504.12114v1"}
{"created":"2025-04-16 14:21:02","title":"Clarifying Ambiguities: on the Role of Ambiguity Types in Prompting Methods for Clarification Generation","abstract":"In information retrieval (IR), providing appropriate clarifications to better understand users' information needs is crucial for building a proactive search-oriented dialogue system. Due to the strong in-context learning ability of large language models (LLMs), recent studies investigate prompting methods to generate clarifications using few-shot or Chain of Thought (CoT) prompts. However, vanilla CoT prompting does not distinguish the characteristics of different information needs, making it difficult to understand how LLMs resolve ambiguities in user queries. In this work, we focus on the concept of ambiguity for clarification, seeking to model and integrate ambiguities in the clarification process. To this end, we comprehensively study the impact of prompting schemes based on reasoning and ambiguity for clarification. The idea is to enhance the reasoning abilities of LLMs by limiting CoT to predict first ambiguity types that can be interpreted as instructions to clarify, then correspondingly generate clarifications. We name this new prompting scheme Ambiguity Type-Chain of Thought (AT-CoT). Experiments are conducted on various datasets containing human-annotated clarifying questions to compare AT-CoT with multiple baselines. We also perform user simulations to implicitly measure the quality of generated clarifications under various IR scenarios.","sentences":["In information retrieval (IR), providing appropriate clarifications to better understand users' information needs is crucial for building a proactive search-oriented dialogue system.","Due to the strong in-context learning ability of large language models (LLMs), recent studies investigate prompting methods to generate clarifications using few-shot or Chain of Thought (CoT) prompts.","However, vanilla CoT prompting does not distinguish the characteristics of different information needs, making it difficult to understand how LLMs resolve ambiguities in user queries.","In this work, we focus on the concept of ambiguity for clarification, seeking to model and integrate ambiguities in the clarification process.","To this end, we comprehensively study the impact of prompting schemes based on reasoning and ambiguity for clarification.","The idea is to enhance the reasoning abilities of LLMs by limiting CoT to predict first ambiguity types that can be interpreted as instructions to clarify, then correspondingly generate clarifications.","We name this new prompting scheme Ambiguity Type-Chain of Thought (AT-CoT).","Experiments are conducted on various datasets containing human-annotated clarifying questions to compare AT-CoT with multiple baselines.","We also perform user simulations to implicitly measure the quality of generated clarifications under various IR scenarios."],"url":"http://arxiv.org/abs/2504.12113v1"}
{"created":"2025-04-16 14:19:57","title":"A Diffusion-Based Framework for Terrain-Aware Remote Sensing Image Reconstruction","abstract":"Remote sensing imagery is essential for environmental monitoring, agricultural management, and disaster response. However, data loss due to cloud cover, sensor failures, or incomplete acquisition-especially in high-resolution and high-frequency tasks-severely limits satellite imagery's effectiveness. Traditional interpolation methods struggle with large missing areas and complex structures. Remote sensing imagery consists of multiple bands, each with distinct meanings, and ensuring consistency across bands is critical to avoid anomalies in the combined images. This paper proposes SatelliteMaker, a diffusion-based method that reconstructs missing data across varying levels of data loss while maintaining spatial, spectral, and temporal consistency. We also propose Digital Elevation Model (DEM) as a conditioning input and use tailored prompts to generate realistic images, making diffusion models applicable to quantitative remote sensing tasks. Additionally, we propose a VGG-Adapter module based on Distribution Loss, which reduces distribution discrepancy and ensures style consistency. Extensive experiments show that SatelliteMaker achieves state-of-the-art performance across multiple tasks.","sentences":["Remote sensing imagery is essential for environmental monitoring, agricultural management, and disaster response.","However, data loss due to cloud cover, sensor failures, or incomplete acquisition-especially in high-resolution and high-frequency tasks-severely limits satellite imagery's effectiveness.","Traditional interpolation methods struggle with large missing areas and complex structures.","Remote sensing imagery consists of multiple bands, each with distinct meanings, and ensuring consistency across bands is critical to avoid anomalies in the combined images.","This paper proposes SatelliteMaker, a diffusion-based method that reconstructs missing data across varying levels of data loss while maintaining spatial, spectral, and temporal consistency.","We also propose Digital Elevation Model (DEM) as a conditioning input and use tailored prompts to generate realistic images, making diffusion models applicable to quantitative remote sensing tasks.","Additionally, we propose a VGG-Adapter module based on Distribution Loss, which reduces distribution discrepancy and ensures style consistency.","Extensive experiments show that SatelliteMaker achieves state-of-the-art performance across multiple tasks."],"url":"http://arxiv.org/abs/2504.12112v1"}
{"created":"2025-04-16 14:19:25","title":"Towards LLM Agents for Earth Observation","abstract":"Earth Observation (EO) provides critical planetary data for environmental monitoring, disaster management, climate science, and other scientific domains. Here we ask: Are AI systems ready for reliable Earth Observation? We introduce \\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth Observatory articles across 13 topics and 17 satellite sensors. Using Google Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33% because the code fails to run over 58% of the time. We improve the failure rate for open models by fine-tuning synthetic data, allowing much smaller models (Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g., DeepSeek-R1). Taken together, our findings identify significant challenges to be solved before AI agents can automate earth observation, and suggest paths forward. The project page is available at https://iandrover.github.io/UnivEarth.","sentences":["Earth Observation (EO) provides critical planetary data for environmental monitoring, disaster management, climate science, and other scientific domains.","Here we ask: Are AI systems ready for reliable Earth Observation?","We introduce \\datasetnamenospace, a benchmark of 140 yes/no questions from NASA Earth Observatory articles across 13 topics and 17 satellite sensors.","Using Google Earth Engine API as a tool, LLM agents can only achieve an accuracy of 33% because the code fails to run over 58% of the time.","We improve the failure rate for open models by fine-tuning synthetic data, allowing much smaller models (Llama-3.1-8B) to achieve comparable accuracy to much larger ones (e.g., DeepSeek-R1).","Taken together, our findings identify significant challenges to be solved before AI agents can automate earth observation, and suggest paths forward.","The project page is available at https://iandrover.github.io/UnivEarth."],"url":"http://arxiv.org/abs/2504.12110v1"}
{"created":"2025-04-16 14:17:31","title":"Self-Supervised Traversability Learning with Online Prototype Adaptation for Off-Road Autonomous Driving","abstract":"Achieving reliable and safe autonomous driving in off-road environments requires accurate and efficient terrain traversability analysis. However, this task faces several challenges, including the scarcity of large-scale datasets tailored for off-road scenarios, the high cost and potential errors of manual annotation, the stringent real-time requirements of motion planning, and the limited computational power of onboard units. To address these challenges, this paper proposes a novel traversability learning method that leverages self-supervised learning, eliminating the need for manual annotation. For the first time, a Birds-Eye View (BEV) representation is used as input, reducing computational burden and improving adaptability to downstream motion planning. During vehicle operation, the proposed method conducts online analysis of traversed regions and dynamically updates prototypes to adaptively assess the traversability of the current environment, effectively handling dynamic scene changes. We evaluate our approach against state-of-the-art benchmarks on both public datasets and our own dataset, covering diverse seasons and geographical locations. Experimental results demonstrate that our method significantly outperforms recent approaches. Additionally, real-world vehicle experiments show that our method operates at 10 Hz, meeting real-time requirements, while a 5.5 km autonomous driving experiment further validates the generated traversability cost maps compatibility with downstream motion planning.","sentences":["Achieving reliable and safe autonomous driving in off-road environments requires accurate and efficient terrain traversability analysis.","However, this task faces several challenges, including the scarcity of large-scale datasets tailored for off-road scenarios, the high cost and potential errors of manual annotation, the stringent real-time requirements of motion planning, and the limited computational power of onboard units.","To address these challenges, this paper proposes a novel traversability learning method that leverages self-supervised learning, eliminating the need for manual annotation.","For the first time, a Birds-Eye View (BEV) representation is used as input, reducing computational burden and improving adaptability to downstream motion planning.","During vehicle operation, the proposed method conducts online analysis of traversed regions and dynamically updates prototypes to adaptively assess the traversability of the current environment, effectively handling dynamic scene changes.","We evaluate our approach against state-of-the-art benchmarks on both public datasets and our own dataset, covering diverse seasons and geographical locations.","Experimental results demonstrate that our method significantly outperforms recent approaches.","Additionally, real-world vehicle experiments show that our method operates at 10 Hz, meeting real-time requirements, while a 5.5 km autonomous driving experiment further validates the generated traversability cost maps compatibility with downstream motion planning."],"url":"http://arxiv.org/abs/2504.12109v1"}
{"created":"2025-04-16 14:16:38","title":"Entropy-Guided Watermarking for LLMs: A Test-Time Framework for Robust and Traceable Text Generation","abstract":"The rapid development of Large Language Models (LLMs) has intensified concerns about content traceability and potential misuse. Existing watermarking schemes for sampled text often face trade-offs between maintaining text quality and ensuring robust detection against various attacks. To address these issues, we propose a novel watermarking scheme that improves both detectability and text quality by introducing a cumulative watermark entropy threshold. Our approach is compatible with and generalizes existing sampling functions, enhancing adaptability. Experimental results across multiple LLMs show that our scheme significantly outperforms existing methods, achieving over 80\\% improvements on widely-used datasets, e.g., MATH and GSM8K, while maintaining high detection accuracy.","sentences":["The rapid development of Large Language Models (LLMs) has intensified concerns about content traceability and potential misuse.","Existing watermarking schemes for sampled text often face trade-offs between maintaining text quality and ensuring robust detection against various attacks.","To address these issues, we propose a novel watermarking scheme that improves both detectability and text quality by introducing a cumulative watermark entropy threshold.","Our approach is compatible with and generalizes existing sampling functions, enhancing adaptability.","Experimental results across multiple LLMs show that our scheme significantly outperforms existing methods, achieving over 80\\% improvements on widely-used datasets, e.g., MATH and GSM8K, while maintaining high detection accuracy."],"url":"http://arxiv.org/abs/2504.12108v1"}
{"created":"2025-04-16 14:12:56","title":"Logits DeConfusion with CLIP for Few-Shot Learning","abstract":"With its powerful visual-language alignment capability, CLIP performs well in zero-shot and few-shot learning tasks. However, we found in experiments that CLIP's logits suffer from serious inter-class confusion problems in downstream tasks, and the ambiguity between categories seriously affects the accuracy. To address this challenge, we propose a novel method called Logits DeConfusion, which effectively learns and eliminates inter-class confusion in logits by combining our Multi-level Adapter Fusion (MAF) module with our Inter-Class Deconfusion (ICD) module. Our MAF extracts features from different levels and fuses them uniformly to enhance feature representation. Our ICD learnably eliminates inter-class confusion in logits with a residual structure. Experimental results show that our method can significantly improve the classification performance and alleviate the inter-class confusion problem. The code is available at https://github.com/LiShuo1001/LDC.","sentences":["With its powerful visual-language alignment capability, CLIP performs well in zero-shot and few-shot learning tasks.","However, we found in experiments that CLIP's logits suffer from serious inter-class confusion problems in downstream tasks, and the ambiguity between categories seriously affects the accuracy.","To address this challenge, we propose a novel method called Logits DeConfusion, which effectively learns and eliminates inter-class confusion in logits by combining our Multi-level Adapter Fusion (MAF) module with our Inter-Class Deconfusion (ICD) module.","Our MAF extracts features from different levels and fuses them uniformly to enhance feature representation.","Our ICD learnably eliminates inter-class confusion in logits with a residual structure.","Experimental results show that our method can significantly improve the classification performance and alleviate the inter-class confusion problem.","The code is available at https://github.com/LiShuo1001/LDC."],"url":"http://arxiv.org/abs/2504.12104v1"}
{"created":"2025-04-16 14:12:25","title":"Metric-Solver: Sliding Anchored Metric Depth Estimation from a Single Image","abstract":"Accurate and generalizable metric depth estimation is crucial for various computer vision applications but remains challenging due to the diverse depth scales encountered in indoor and outdoor environments. In this paper, we introduce Metric-Solver, a novel sliding anchor-based metric depth estimation method that dynamically adapts to varying scene scales. Our approach leverages an anchor-based representation, where a reference depth serves as an anchor to separate and normalize the scene depth into two components: scaled near-field depth and tapered far-field depth. The anchor acts as a normalization factor, enabling the near-field depth to be normalized within a consistent range while mapping far-field depth smoothly toward zero. Through this approach, any depth from zero to infinity in the scene can be represented within a unified representation, effectively eliminating the need to manually account for scene scale variations. More importantly, for the same scene, the anchor can slide along the depth axis, dynamically adjusting to different depth scales. A smaller anchor provides higher resolution in the near-field, improving depth precision for closer objects while a larger anchor improves depth estimation in far regions. This adaptability enables the model to handle depth predictions at varying distances and ensure strong generalization across datasets. Our design enables a unified and adaptive depth representation across diverse environments. Extensive experiments demonstrate that Metric-Solver outperforms existing methods in both accuracy and cross-dataset generalization.","sentences":["Accurate and generalizable metric depth estimation is crucial for various computer vision applications but remains challenging due to the diverse depth scales encountered in indoor and outdoor environments.","In this paper, we introduce Metric-Solver, a novel sliding anchor-based metric depth estimation method that dynamically adapts to varying scene scales.","Our approach leverages an anchor-based representation, where a reference depth serves as an anchor to separate and normalize the scene depth into two components: scaled near-field depth and tapered far-field depth.","The anchor acts as a normalization factor, enabling the near-field depth to be normalized within a consistent range while mapping far-field depth smoothly toward zero.","Through this approach, any depth from zero to infinity in the scene can be represented within a unified representation, effectively eliminating the need to manually account for scene scale variations.","More importantly, for the same scene, the anchor can slide along the depth axis, dynamically adjusting to different depth scales.","A smaller anchor provides higher resolution in the near-field, improving depth precision for closer objects while a larger anchor improves depth estimation in far regions.","This adaptability enables the model to handle depth predictions at varying distances and ensure strong generalization across datasets.","Our design enables a unified and adaptive depth representation across diverse environments.","Extensive experiments demonstrate that Metric-Solver outperforms existing methods in both accuracy and cross-dataset generalization."],"url":"http://arxiv.org/abs/2504.12103v1"}
{"created":"2025-04-16 14:08:25","title":"Successive-Cancellation Flip and Perturbation Decoder of Polar Codes","abstract":"In this paper, two decoding algorithms based on Successive Cancellation (SC) are proposed to improve the error-correction performance of cyclic redundancy check (CRC)-aided polar codes while aiming for a low-complexity implementation. Comparisons with Dynamic SC Flip (DSCF) and SC Perturbation (SCP) are carried out since the proposed DSCF and Perturbation (DSCFP) and Perturbed DSCF (PDSCF) algorithms combine both methods. The analysis includes comparisons with several code lengths $N$ and various number of decoding attempts $T_{max}$. For $N=1024$ and the coding rate $R=\\frac{1}{2}$, the DSCFP and the SCP algorithms with $T_{max}=17$ are bested by approximately $0.1$\\,dB at block error rate (BLER) of $0.001$. At $\\text{BLER}=10^{-6}$ and for $T_{max}=64$, the gain is of $0.375$ dB and $>0.5$ dB with respect to DSCF and SCP, respectively. At high signal-to-noise ratio, the average computational complexity of the proposed algorithms is virtually equivalent to that of SC.","sentences":["In this paper, two decoding algorithms based on Successive Cancellation (SC) are proposed to improve the error-correction performance of cyclic redundancy check (CRC)-aided polar codes while aiming for a low-complexity implementation.","Comparisons with Dynamic SC Flip (DSCF) and SC Perturbation (SCP) are carried out since the proposed DSCF and Perturbation (DSCFP) and Perturbed DSCF (PDSCF) algorithms combine both methods.","The analysis includes comparisons with several code lengths $N$ and various number of decoding attempts $T_{max}$. For $N=1024$ and the coding rate $R=\\frac{1}{2}$, the DSCFP and the SCP algorithms with $T_{max}=17$ are bested by approximately $0.1$\\,dB at block error rate (BLER) of $0.001$. At $\\text{BLER}=10^{-6}$ and for $T_{max}=64$, the gain is of $0.375$ dB and $>0.5$ dB with respect to DSCF and SCP, respectively.","At high signal-to-noise ratio, the average computational complexity of the proposed algorithms is virtually equivalent to that of SC."],"url":"http://arxiv.org/abs/2504.12102v1"}
{"created":"2025-04-16 14:03:24","title":"Generalized Visual Relation Detection with Diffusion Models","abstract":"Visual relation detection (VRD) aims to identify relationships (or interactions) between object pairs in an image. Although recent VRD models have achieved impressive performance, they are all restricted to pre-defined relation categories, while failing to consider the semantic ambiguity characteristic of visual relations. Unlike objects, the appearance of visual relations is always subtle and can be described by multiple predicate words from different perspectives, e.g., ``ride'' can be depicted as ``race'' and ``sit on'', from the sports and spatial position views, respectively. To this end, we propose to model visual relations as continuous embeddings, and design diffusion models to achieve generalized VRD in a conditional generative manner, termed Diff-VRD. We model the diffusion process in a latent space and generate all possible relations in the image as an embedding sequence. During the generation, the visual and text embeddings of subject-object pairs serve as conditional signals and are injected via cross-attention. After the generation, we design a subsequent matching stage to assign the relation words to subject-object pairs by considering their semantic similarities. Benefiting from the diffusion-based generative process, our Diff-VRD is able to generate visual relations beyond the pre-defined category labels of datasets. To properly evaluate this generalized VRD task, we introduce two evaluation metrics, i.e., text-to-image retrieval and SPICE PR Curve inspired by image captioning. Extensive experiments in both human-object interaction (HOI) detection and scene graph generation (SGG) benchmarks attest to the superiority and effectiveness of Diff-VRD.","sentences":["Visual relation detection (VRD) aims to identify relationships (or interactions) between object pairs in an image.","Although recent VRD models have achieved impressive performance, they are all restricted to pre-defined relation categories, while failing to consider the semantic ambiguity characteristic of visual relations.","Unlike objects, the appearance of visual relations is always subtle and can be described by multiple predicate words from different perspectives, e.g., ``ride'' can be depicted as ``race'' and ``sit on'', from the sports and spatial position views, respectively.","To this end, we propose to model visual relations as continuous embeddings, and design diffusion models to achieve generalized VRD in a conditional generative manner, termed Diff-VRD.","We model the diffusion process in a latent space and generate all possible relations in the image as an embedding sequence.","During the generation, the visual and text embeddings of subject-object pairs serve as conditional signals and are injected via cross-attention.","After the generation, we design a subsequent matching stage to assign the relation words to subject-object pairs by considering their semantic similarities.","Benefiting from the diffusion-based generative process, our Diff-VRD is able to generate visual relations beyond the pre-defined category labels of datasets.","To properly evaluate this generalized VRD task, we introduce two evaluation metrics, i.e., text-to-image retrieval and SPICE PR Curve inspired by image captioning.","Extensive experiments in both human-object interaction (HOI) detection and scene graph generation (SGG) benchmarks attest to the superiority and effectiveness of Diff-VRD."],"url":"http://arxiv.org/abs/2504.12100v1"}
{"created":"2025-04-16 14:02:21","title":"Gauging Overprecision in LLMs: An Empirical Study","abstract":"Recently, overconfidence in large language models (LLMs) has garnered considerable attention due to its fundamental importance in quantifying the trustworthiness of LLM generation. However, existing approaches prompt the \\textit{black box LLMs} to produce their confidence (\\textit{verbalized confidence}), which can be subject to many biases and hallucinations. Inspired by a different aspect of overconfidence in cognitive science called \\textit{overprecision}, we designed a framework for its study in black box LLMs. This framework contains three main phases: 1) generation, 2) refinement and 3) evaluation. In the generation phase we prompt the LLM to generate answers to numerical questions in the form of intervals with a certain level of confidence. This confidence level is imposed in the prompt and not required for the LLM to generate as in previous approaches. We use various prompting techniques and use the same prompt multiple times to gauge the effects of randomness in the generation process. In the refinement phase, answers from the previous phase are refined to generate better answers. The LLM answers are evaluated and studied in the evaluation phase to understand its internal workings. This study allowed us to gain various insights into LLM overprecision: 1) LLMs are highly uncalibrated for numerical tasks 2) {\\color{blue}there is no correlation between the length of the interval and the imposed confidence level, which can be symptomatic of a a) lack of understanding of the concept of confidence or b) inability to adjust self-confidence by following instructions}, {\\color{blue}3)} LLM numerical precision differs depending on the task, scale of answer and prompting technique {\\color{blue}4) Refinement of answers doesn't improve precision in most cases}. We believe this study offers new perspectives on LLM overconfidence and serves as a strong baseline for overprecision in LLMs.","sentences":["Recently, overconfidence in large language models (LLMs) has garnered considerable attention due to its fundamental importance in quantifying the trustworthiness of LLM generation.","However, existing approaches prompt the \\textit{black box LLMs} to produce their confidence (\\textit{verbalized confidence}), which can be subject to many biases and hallucinations.","Inspired by a different aspect of overconfidence in cognitive science called \\textit{overprecision}, we designed a framework for its study in black box LLMs.","This framework contains three main phases: 1) generation, 2) refinement and 3) evaluation.","In the generation phase we prompt the LLM to generate answers to numerical questions in the form of intervals with a certain level of confidence.","This confidence level is imposed in the prompt and not required for the LLM to generate as in previous approaches.","We use various prompting techniques and use the same prompt multiple times to gauge the effects of randomness in the generation process.","In the refinement phase, answers from the previous phase are refined to generate better answers.","The LLM answers are evaluated and studied in the evaluation phase to understand its internal workings.","This study allowed us to gain various insights into LLM overprecision: 1) LLMs are highly uncalibrated for numerical tasks 2) {\\color{blue}there is no correlation between the length of the interval and the imposed confidence level, which can be symptomatic of a a) lack of understanding of the concept of confidence or b) inability to adjust self-confidence by following instructions}, {\\color{blue}3)} LLM numerical precision differs depending on the task, scale of answer and prompting technique {\\color{blue}4) Refinement of answers doesn't improve precision in most cases}.","We believe this study offers new perspectives on LLM overconfidence and serves as a strong baseline for overprecision in LLMs."],"url":"http://arxiv.org/abs/2504.12098v1"}
