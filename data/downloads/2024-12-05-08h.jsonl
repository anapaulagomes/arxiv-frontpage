{"created":"2024-12-04 18:59:45","title":"Navigation World Models","abstract":"Navigation is a fundamental skill of agents with visual-motor capabilities. We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions. To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters. In familiar environments, NWM can plan navigation trajectories by simulating them and evaluating whether they achieve the desired goal. Unlike supervised navigation policies with fixed behavior, NWM can dynamically incorporate constraints during planning. Experiments demonstrate its effectiveness in planning trajectories from scratch or by ranking trajectories sampled from an external policy. Furthermore, NWM leverages its learned visual priors to imagine trajectories in unfamiliar environments from a single input image, making it a flexible and powerful tool for next-generation navigation systems.","sentences":["Navigation is a fundamental skill of agents with visual-motor capabilities.","We introduce a Navigation World Model (NWM), a controllable video generation model that predicts future visual observations based on past observations and navigation actions.","To capture complex environment dynamics, NWM employs a Conditional Diffusion Transformer (CDiT), trained on a diverse collection of egocentric videos of both human and robotic agents, and scaled up to 1 billion parameters.","In familiar environments, NWM can plan navigation trajectories by simulating them and evaluating whether they achieve the desired goal.","Unlike supervised navigation policies with fixed behavior, NWM can dynamically incorporate constraints during planning.","Experiments demonstrate its effectiveness in planning trajectories from scratch or by ranking trajectories sampled from an external policy.","Furthermore, NWM leverages its learned visual priors to imagine trajectories in unfamiliar environments from a single input image, making it a flexible and powerful tool for next-generation navigation systems."],"url":"http://arxiv.org/abs/2412.03572v1"}
{"created":"2024-12-04 18:59:38","title":"Style3D: Attention-guided Multi-view Style Transfer for 3D Object Generation","abstract":"We present Style3D, a novel approach for generating stylized 3D objects from a content image and a style image. Unlike most previous methods that require case- or style-specific training, Style3D supports instant 3D object stylization. Our key insight is that 3D object stylization can be decomposed into two interconnected processes: multi-view dual-feature alignment and sparse-view spatial reconstruction. We introduce MultiFusion Attention, an attention-guided technique to achieve multi-view stylization from the content-style pair. Specifically, the query features from the content image preserve geometric consistency across multiple views, while the key and value features from the style image are used to guide the stylistic transfer. This dual-feature alignment ensures that spatial coherence and stylistic fidelity are maintained across multi-view images. Finally, a large 3D reconstruction model is introduced to generate coherent stylized 3D objects. By establishing an interplay between structural and stylistic features across multiple views, our approach enables a holistic 3D stylization process. Extensive experiments demonstrate that Style3D offers a more flexible and scalable solution for generating style-consistent 3D assets, surpassing existing methods in both computational efficiency and visual quality.","sentences":["We present Style3D, a novel approach for generating stylized 3D objects from a content image and a style image.","Unlike most previous methods that require case- or style-specific training, Style3D supports instant 3D object stylization.","Our key insight is that 3D object stylization can be decomposed into two interconnected processes: multi-view dual-feature alignment and sparse-view spatial reconstruction.","We introduce MultiFusion Attention, an attention-guided technique to achieve multi-view stylization from the content-style pair.","Specifically, the query features from the content image preserve geometric consistency across multiple views, while the key and value features from the style image are used to guide the stylistic transfer.","This dual-feature alignment ensures that spatial coherence and stylistic fidelity are maintained across multi-view images.","Finally, a large 3D reconstruction model is introduced to generate coherent stylized 3D objects.","By establishing an interplay between structural and stylistic features across multiple views, our approach enables a holistic 3D stylization process.","Extensive experiments demonstrate that Style3D offers a more flexible and scalable solution for generating style-consistent 3D assets, surpassing existing methods in both computational efficiency and visual quality."],"url":"http://arxiv.org/abs/2412.03571v1"}
{"created":"2024-12-04 18:59:24","title":"Sparse-view Pose Estimation and Reconstruction via Analysis by Generative Synthesis","abstract":"Inferring the 3D structure underlying a set of multi-view images typically requires solving two co-dependent tasks -- accurate 3D reconstruction requires precise camera poses, and predicting camera poses relies on (implicitly or explicitly) modeling the underlying 3D. The classical framework of analysis by synthesis casts this inference as a joint optimization seeking to explain the observed pixels, and recent instantiations learn expressive 3D representations (e.g., Neural Fields) with gradient-descent-based pose refinement of initial pose estimates. However, given a sparse set of observed views, the observations may not provide sufficient direct evidence to obtain complete and accurate 3D. Moreover, large errors in pose estimation may not be easily corrected and can further degrade the inferred 3D. To allow robust 3D reconstruction and pose estimation in this challenging setup, we propose SparseAGS, a method that adapts this analysis-by-synthesis approach by: a) including novel-view-synthesis-based generative priors in conjunction with photometric objectives to improve the quality of the inferred 3D, and b) explicitly reasoning about outliers and using a discrete search with a continuous optimization-based strategy to correct them. We validate our framework across real-world and synthetic datasets in combination with several off-the-shelf pose estimation systems as initialization. We find that it significantly improves the base systems' pose accuracy while yielding high-quality 3D reconstructions that outperform the results from current multi-view reconstruction baselines.","sentences":["Inferring the 3D structure underlying a set of multi-view images typically requires solving two co-dependent tasks -- accurate 3D reconstruction requires precise camera poses, and predicting camera poses relies on (implicitly or explicitly) modeling the underlying 3D.","The classical framework of analysis by synthesis casts this inference as a joint optimization seeking to explain the observed pixels, and recent instantiations learn expressive 3D representations (e.g., Neural Fields) with gradient-descent-based pose refinement of initial pose estimates.","However, given a sparse set of observed views, the observations may not provide sufficient direct evidence to obtain complete and accurate 3D.","Moreover, large errors in pose estimation may not be easily corrected and can further degrade the inferred 3D.","To allow robust 3D reconstruction and pose estimation in this challenging setup, we propose SparseAGS, a method that adapts this analysis-by-synthesis approach by: a) including novel-view-synthesis-based generative priors in conjunction with photometric objectives to improve the quality of the inferred 3D, and b) explicitly reasoning about outliers and using a discrete search with a continuous optimization-based strategy to correct them.","We validate our framework across real-world and synthetic datasets in combination with several off-the-shelf pose estimation systems as initialization.","We find that it significantly improves the base systems' pose accuracy while yielding high-quality 3D reconstructions that outperform the results from current multi-view reconstruction baselines."],"url":"http://arxiv.org/abs/2412.03570v1"}
{"created":"2024-12-04 18:59:05","title":"The Matrix: Infinite-Horizon World Generation with Real-Time Moving Control","abstract":"We present The Matrix, the first foundational realistic world simulator capable of generating continuous 720p high-fidelity real-scene video streams with real-time, responsive control in both first- and third-person perspectives, enabling immersive exploration of richly dynamic environments. Trained on limited supervised data from AAA games like Forza Horizon 5 and Cyberpunk 2077, complemented by large-scale unsupervised footage from real-world settings like Tokyo streets, The Matrix allows users to traverse diverse terrains -- deserts, grasslands, water bodies, and urban landscapes -- in continuous, uncut hour-long sequences. Operating at 16 FPS, the system supports real-time interactivity and demonstrates zero-shot generalization, translating virtual game environments to real-world contexts where collecting continuous movement data is often infeasible. For example, The Matrix can simulate a BMW X3 driving through an office setting--an environment present in neither gaming data nor real-world sources. This approach showcases the potential of AAA game data to advance robust world models, bridging the gap between simulations and real-world applications in scenarios with limited data.","sentences":["We present The Matrix, the first foundational realistic world simulator capable of generating continuous 720p high-fidelity real-scene video streams with real-time, responsive control in both first- and third-person perspectives, enabling immersive exploration of richly dynamic environments.","Trained on limited supervised data from AAA games like Forza Horizon 5 and Cyberpunk 2077, complemented by large-scale unsupervised footage from real-world settings like Tokyo streets, The Matrix allows users to traverse diverse terrains -- deserts, grasslands, water bodies, and urban landscapes -- in continuous, uncut hour-long sequences.","Operating at 16 FPS, the system supports real-time interactivity and demonstrates zero-shot generalization, translating virtual game environments to real-world contexts where collecting continuous movement data is often infeasible.","For example, The Matrix can simulate a BMW X3 driving through an office setting--an environment present in neither gaming data nor real-world sources.","This approach showcases the potential of AAA game data to advance robust world models, bridging the gap between simulations and real-world applications in scenarios with limited data."],"url":"http://arxiv.org/abs/2412.03568v1"}
{"created":"2024-12-04 18:58:27","title":"Streaming Detection of Queried Event Start","abstract":"Robotics, autonomous driving, augmented reality, and many embodied computer vision applications must quickly react to user-defined events unfolding in real time. We address this setting by proposing a novel task for multimodal video understanding-Streaming Detection of Queried Event Start (SDQES). The goal of SDQES is to identify the beginning of a complex event as described by a natural language query, with high accuracy and low latency. We introduce a new benchmark based on the Ego4D dataset, as well as new task-specific metrics to study streaming multimodal detection of diverse events in an egocentric video setting. Inspired by parameter-efficient fine-tuning methods in NLP and for video tasks, we propose adapter-based baselines that enable image-to-video transfer learning, allowing for efficient online video modeling. We evaluate three vision-language backbones and three adapter architectures on both short-clip and untrimmed video settings.","sentences":["Robotics, autonomous driving, augmented reality, and many embodied computer vision applications must quickly react to user-defined events unfolding in real time.","We address this setting by proposing a novel task for multimodal video understanding-Streaming Detection of Queried Event Start (SDQES).","The goal of SDQES is to identify the beginning of a complex event as described by a natural language query, with high accuracy and low latency.","We introduce a new benchmark based on the Ego4D dataset, as well as new task-specific metrics to study streaming multimodal detection of diverse events in an egocentric video setting.","Inspired by parameter-efficient fine-tuning methods in NLP and for video tasks, we propose adapter-based baselines that enable image-to-video transfer learning, allowing for efficient online video modeling.","We evaluate three vision-language backbones and three adapter architectures on both short-clip and untrimmed video settings."],"url":"http://arxiv.org/abs/2412.03567v1"}
{"created":"2024-12-04 18:58:21","title":"FreeSim: Toward Free-viewpoint Camera Simulation in Driving Scenes","abstract":"We propose FreeSim, a camera simulation method for autonomous driving. FreeSim emphasizes high-quality rendering from viewpoints beyond the recorded ego trajectories. In such viewpoints, previous methods have unacceptable degradation because the training data of these viewpoints is unavailable. To address such data scarcity, we first propose a generative enhancement model with a matched data construction strategy. The resulting model can generate high-quality images in a viewpoint slightly deviated from the recorded trajectories, conditioned on the degraded rendering of this viewpoint. We then propose a progressive reconstruction strategy, which progressively adds generated images of unrecorded views into the reconstruction process, starting from slightly off-trajectory viewpoints and moving progressively farther away. With this progressive generation-reconstruction pipeline, FreeSim supports high-quality off-trajectory view synthesis under large deviations of more than 3 meters.","sentences":["We propose FreeSim, a camera simulation method for autonomous driving.","FreeSim emphasizes high-quality rendering from viewpoints beyond the recorded ego trajectories.","In such viewpoints, previous methods have unacceptable degradation because the training data of these viewpoints is unavailable.","To address such data scarcity, we first propose a generative enhancement model with a matched data construction strategy.","The resulting model can generate high-quality images in a viewpoint slightly deviated from the recorded trajectories, conditioned on the degraded rendering of this viewpoint.","We then propose a progressive reconstruction strategy, which progressively adds generated images of unrecorded views into the reconstruction process, starting from slightly off-trajectory viewpoints and moving progressively farther away.","With this progressive generation-reconstruction pipeline, FreeSim supports high-quality off-trajectory view synthesis under large deviations of more than 3 meters."],"url":"http://arxiv.org/abs/2412.03566v1"}
{"created":"2024-12-04 18:58:10","title":"Inst-IT: Boosting Multimodal Instance Understanding via Explicit Visual Prompt Instruction Tuning","abstract":"Large Multimodal Models (LMMs) have made significant breakthroughs with the advancement of instruction tuning. However, while existing models can understand images and videos at a holistic level, they still struggle with instance-level understanding that requires a more nuanced comprehension and alignment. Instance-level understanding is crucial, as it focuses on the specific elements that we are most interested in. Excitingly, existing works find that the state-of-the-art LMMs exhibit strong instance understanding capabilities when provided with explicit visual cues. Motivated by this, we introduce an automated annotation pipeline assisted by GPT-4o to extract instance-level information from images and videos through explicit visual prompting for instance guidance. Building upon this pipeline, we proposed Inst-IT, a solution to enhance LMMs in Instance understanding via explicit visual prompt Instruction Tuning. Inst-IT consists of a benchmark to diagnose multimodal instance-level understanding, a large-scale instruction-tuning dataset, and a continuous instruction-tuning training paradigm to effectively enhance spatial-temporal instance understanding capabilities of existing LMMs. Experimental results show that, with the boost of Inst-IT, our models not only achieve outstanding performance on Inst-IT Bench but also demonstrate significant improvements across various generic image and video understanding benchmarks. This highlights that our dataset not only boosts instance-level understanding but also strengthens the overall capabilities of generic image and video comprehension.","sentences":["Large Multimodal Models (LMMs) have made significant breakthroughs with the advancement of instruction tuning.","However, while existing models can understand images and videos at a holistic level, they still struggle with instance-level understanding that requires a more nuanced comprehension and alignment.","Instance-level understanding is crucial, as it focuses on the specific elements that we are most interested in.","Excitingly, existing works find that the state-of-the-art LMMs exhibit strong instance understanding capabilities when provided with explicit visual cues.","Motivated by this, we introduce an automated annotation pipeline assisted by GPT-4o to extract instance-level information from images and videos through explicit visual prompting for instance guidance.","Building upon this pipeline, we proposed Inst-IT, a solution to enhance LMMs in Instance understanding via explicit visual prompt Instruction Tuning.","Inst-IT consists of a benchmark to diagnose multimodal instance-level understanding, a large-scale instruction-tuning dataset, and a continuous instruction-tuning training paradigm to effectively enhance spatial-temporal instance understanding capabilities of existing LMMs.","Experimental results show that, with the boost of Inst-IT, our models not only achieve outstanding performance on Inst-IT Bench but also demonstrate significant improvements across various generic image and video understanding benchmarks.","This highlights that our dataset not only boosts instance-level understanding but also strengthens the overall capabilities of generic image and video comprehension."],"url":"http://arxiv.org/abs/2412.03565v1"}
{"created":"2024-12-04 18:56:37","title":"From Individual to Society: A Survey on Social Simulation Driven by Large Language Model-based Agents","abstract":"Traditional sociological research often relies on human participation, which, though effective, is expensive, challenging to scale, and with ethical concerns. Recent advancements in large language models (LLMs) highlight their potential to simulate human behavior, enabling the replication of individual responses and facilitating studies on many interdisciplinary studies. In this paper, we conduct a comprehensive survey of this field, illustrating the recent progress in simulation driven by LLM-empowered agents. We categorize the simulations into three types: (1) Individual Simulation, which mimics specific individuals or demographic groups; (2) Scenario Simulation, where multiple agents collaborate to achieve goals within specific contexts; and (3) Society Simulation, which models interactions within agent societies to reflect the complexity and variety of real-world dynamics. These simulations follow a progression, ranging from detailed individual modeling to large-scale societal phenomena. We provide a detailed discussion of each simulation type, including the architecture or key components of the simulation, the classification of objectives or scenarios and the evaluation method. Afterward, we summarize commonly used datasets and benchmarks. Finally, we discuss the trends across these three types of simulation. A repository for the related sources is at {\\url{https://github.com/FudanDISC/SocialAgent}}.","sentences":["Traditional sociological research often relies on human participation, which, though effective, is expensive, challenging to scale, and with ethical concerns.","Recent advancements in large language models (LLMs) highlight their potential to simulate human behavior, enabling the replication of individual responses and facilitating studies on many interdisciplinary studies.","In this paper, we conduct a comprehensive survey of this field, illustrating the recent progress in simulation driven by LLM-empowered agents.","We categorize the simulations into three types: (1) Individual Simulation, which mimics specific individuals or demographic groups; (2) Scenario Simulation, where multiple agents collaborate to achieve goals within specific contexts; and (3) Society Simulation, which models interactions within agent societies to reflect the complexity and variety of real-world dynamics.","These simulations follow a progression, ranging from detailed individual modeling to large-scale societal phenomena.","We provide a detailed discussion of each simulation type, including the architecture or key components of the simulation, the classification of objectives or scenarios and the evaluation method.","Afterward, we summarize commonly used datasets and benchmarks.","Finally, we discuss the trends across these three types of simulation.","A repository for the related sources is at {\\url{https://github.com/FudanDISC/SocialAgent}}."],"url":"http://arxiv.org/abs/2412.03563v1"}
{"created":"2024-12-04 18:56:19","title":"Characterizing the Distinguishability of Product Distributions through Multicalibration","abstract":"Given a sequence of samples $x_1, \\dots , x_k$ promised to be drawn from one of two distributions $X_0, X_1$, a well-studied problem in statistics is to decide $\\textit{which}$ distribution the samples are from. Information theoretically, the maximum advantage in distinguishing the two distributions given $k$ samples is captured by the total variation distance between $X_0^{\\otimes k}$ and $X_1^{\\otimes k}$. However, when we restrict our attention to $\\textit{efficient distinguishers}$ (i.e., small circuits) of these two distributions, exactly characterizing the ability to distinguish $X_0^{\\otimes k}$ and $X_1^{\\otimes k}$ is more involved and less understood.   In this work, we give a general way to reduce bounds on the computational indistinguishability of $X_0$ and $X_1$ to bounds on the $\\textit{information-theoretic}$ indistinguishability of some specific, related variables $\\widetilde{X}_0$ and $\\widetilde{X}_1$. As a consequence, we prove a new, tight characterization of the number of samples $k$ needed to efficiently distinguish $X_0^{\\otimes k}$ and $X_1^{\\otimes k}$ with constant advantage as   \\[   k = \\Theta\\left(d_H^{-2}\\left(\\widetilde{X}_0, \\widetilde{X}_1\\right)\\right),   \\] which is the inverse of the squared Hellinger distance $d_H$ between two distributions $\\widetilde{X}_0$ and $\\widetilde{X}_1$ that are computationally indistinguishable from $X_0$ and $X_1$. Likewise, our framework can be used to re-derive a result of Geier (TCC 2022), proving nearly-tight bounds on how computational indistinguishability scales with the number of samples for arbitrary product distributions.","sentences":["Given a sequence of samples $x_1, \\dots , x_k$ promised to be drawn from one of two distributions $X_0, X_1$, a well-studied problem in statistics is to decide $\\textit{which}$ distribution the samples are from.","Information theoretically, the maximum advantage in distinguishing the two distributions given $k$ samples is captured by the total variation distance between $X_0^{\\otimes k}$ and $X_1^{\\otimes k}$.","However, when we restrict our attention to $\\textit{efficient distinguishers}$ (i.e., small circuits) of these two distributions, exactly characterizing the ability to distinguish $X_0^{\\otimes k}$ and $X_1^{\\otimes k}$ is more involved and less understood.   ","In this work, we give a general way to reduce bounds on the computational indistinguishability of $X_0$ and $X_1$ to bounds on the $\\textit{information-theoretic}$ indistinguishability of some specific, related variables $\\widetilde{X}_0$ and $\\widetilde{X}_1$. As a consequence, we prove a new, tight characterization of the number of samples $k$ needed to efficiently distinguish $X_0^{\\otimes k}$ and $X_1^{\\otimes k}$ with constant advantage as   \\[   k = \\Theta\\left(d_H^{-2}\\left(\\widetilde{X}_0, \\widetilde{X}_1\\right)\\right),   \\] which is the inverse of the squared Hellinger distance $d_H$ between two distributions $\\widetilde{X}_0$ and $\\widetilde{X}_1$ that are computationally indistinguishable from $X_0$ and $X_1$. Likewise, our framework can be used to re-derive a result of Geier (TCC 2022), proving nearly-tight bounds on how computational indistinguishability scales with the number of samples for arbitrary product distributions."],"url":"http://arxiv.org/abs/2412.03562v1"}
{"created":"2024-12-04 18:56:04","title":"FLAIR: VLM with Fine-grained Language-informed Image Representations","abstract":"CLIP has shown impressive results in aligning images and texts at scale. However, its ability to capture detailed visual features remains limited because CLIP matches images and texts at a global level. To address this issue, we propose FLAIR, Fine-grained Language-informed Image Representations, an approach that utilizes long and detailed image descriptions to learn localized image embeddings. By sampling diverse sub-captions that describe fine-grained details about an image, we train our vision-language model to produce not only global embeddings but also text-specific image representations. Our model introduces text-conditioned attention pooling on top of local image tokens to produce fine-grained image representations that excel at retrieving detailed image content. We achieve state-of-the-art performance on both, existing multimodal retrieval benchmarks, as well as, our newly introduced fine-grained retrieval task which evaluates vision-language models' ability to retrieve partial image content. Furthermore, our experiments demonstrate the effectiveness of FLAIR trained on 30M image-text pairs in capturing fine-grained visual information, including zero-shot semantic segmentation, outperforming models trained on billions of pairs. Code is available at https://github.com/ExplainableML/flair .","sentences":["CLIP has shown impressive results in aligning images and texts at scale.","However, its ability to capture detailed visual features remains limited because CLIP matches images and texts at a global level.","To address this issue, we propose FLAIR, Fine-grained Language-informed Image Representations, an approach that utilizes long and detailed image descriptions to learn localized image embeddings.","By sampling diverse sub-captions that describe fine-grained details about an image, we train our vision-language model to produce not only global embeddings but also text-specific image representations.","Our model introduces text-conditioned attention pooling on top of local image tokens to produce fine-grained image representations that excel at retrieving detailed image content.","We achieve state-of-the-art performance on both, existing multimodal retrieval benchmarks, as well as, our newly introduced fine-grained retrieval task which evaluates vision-language models' ability to retrieve partial image content.","Furthermore, our experiments demonstrate the effectiveness of FLAIR trained on 30M image-text pairs in capturing fine-grained visual information, including zero-shot semantic segmentation, outperforming models trained on billions of pairs.","Code is available at https://github.com/ExplainableML/flair ."],"url":"http://arxiv.org/abs/2412.03561v1"}
{"created":"2024-12-04 18:52:40","title":"MIDI: Multi-Instance Diffusion for Single Image to 3D Scene Generation","abstract":"This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image. Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability. At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes. The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation. During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability. MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models.","sentences":["This paper introduces MIDI, a novel paradigm for compositional 3D scene generation from a single image.","Unlike existing methods that rely on reconstruction or retrieval techniques or recent approaches that employ multi-stage object-by-object generation, MIDI extends pre-trained image-to-3D object generation models to multi-instance diffusion models, enabling the simultaneous generation of multiple 3D instances with accurate spatial relationships and high generalizability.","At its core, MIDI incorporates a novel multi-instance attention mechanism, that effectively captures inter-object interactions and spatial coherence directly within the generation process, without the need for complex multi-step processes.","The method utilizes partial object images and global scene context as inputs, directly modeling object completion during 3D generation.","During training, we effectively supervise the interactions between 3D instances using a limited amount of scene-level data, while incorporating single-object data for regularization, thereby maintaining the pre-trained generalization ability.","MIDI demonstrates state-of-the-art performance in image-to-scene generation, validated through evaluations on synthetic data, real-world scene data, and stylized scene images generated by text-to-image diffusion models."],"url":"http://arxiv.org/abs/2412.03558v1"}
{"created":"2024-12-04 18:52:32","title":"Freshness and Informativity Weighted Cognitive Extent and Its Correlation with Cumulative Citation Count","abstract":"In this paper, we revisit cognitive extent, originally defined as the number of unique phrases in a quota. We introduce Freshness and Informative Weighted Cognitive Extent (FICE), calculated based on two novel weighting factors, the lifetime ratio and informativity of scientific entities. We model the lifetime of each scientific entity as the time-dependent document frequency, which is fit by the composition of multiple Gaussian profiles. The lifetime ratio is then calculated as the cumulative document frequency at the publication time $t_0$ divided by the cumulative document frequency over its entire lifetime. The informativity is calculated by normalizing the document frequency across all scientific entities recognized in a title. Using the ACL Anthology, we verified the trend formerly observed in several other domains that the number of unique scientific entities per quota increased gradually at a slower rate. We found that FICE exhibits a strong correlation with the average cumulative citation count within a quota. Our code is available at \\href{https://github.com/ZiheHerzWang/Freshness-and-Informativity-Weighted-Cognitive-Extent}{https://github.com/ZiheHerzWang/Freshness-and-Informativity-Weighted-Cognitive-Extent}","sentences":["In this paper, we revisit cognitive extent, originally defined as the number of unique phrases in a quota.","We introduce Freshness and Informative Weighted Cognitive Extent (FICE), calculated based on two novel weighting factors, the lifetime ratio and informativity of scientific entities.","We model the lifetime of each scientific entity as the time-dependent document frequency, which is fit by the composition of multiple Gaussian profiles.","The lifetime ratio is then calculated as the cumulative document frequency at the publication time $t_0$ divided by the cumulative document frequency over its entire lifetime.","The informativity is calculated by normalizing the document frequency across all scientific entities recognized in a title.","Using the ACL Anthology, we verified the trend formerly observed in several other domains that the number of unique scientific entities per quota increased gradually at a slower rate.","We found that FICE exhibits a strong correlation with the average cumulative citation count within a quota.","Our code is available at \\href{https://github.com/ZiheHerzWang/Freshness-and-Informativity-Weighted-Cognitive-Extent}{https://github.com/ZiheHerzWang/Freshness-and-Informativity-Weighted-Cognitive-Extent}"],"url":"http://arxiv.org/abs/2412.03557v1"}
{"created":"2024-12-04 18:51:32","title":"Best-of-N Jailbreaking","abstract":"We introduce Best-of-N (BoN) Jailbreaking, a simple black-box algorithm that jailbreaks frontier AI systems across modalities. BoN Jailbreaking works by repeatedly sampling variations of a prompt with a combination of augmentations - such as random shuffling or capitalization for textual prompts - until a harmful response is elicited. We find that BoN Jailbreaking achieves high attack success rates (ASRs) on closed-source language models, such as 89% on GPT-4o and 78% on Claude 3.5 Sonnet when sampling 10,000 augmented prompts. Further, it is similarly effective at circumventing state-of-the-art open-source defenses like circuit breakers. BoN also seamlessly extends to other modalities: it jailbreaks vision language models (VLMs) such as GPT-4o and audio language models (ALMs) like Gemini 1.5 Pro, using modality-specific augmentations. BoN reliably improves when we sample more augmented prompts. Across all modalities, ASR, as a function of the number of samples (N), empirically follows power-law-like behavior for many orders of magnitude. BoN Jailbreaking can also be composed with other black-box algorithms for even more effective attacks - combining BoN with an optimized prefix attack achieves up to a 35% increase in ASR. Overall, our work indicates that, despite their capability, language models are sensitive to seemingly innocuous changes to inputs, which attackers can exploit across modalities.","sentences":["We introduce Best-of-N (BoN) Jailbreaking, a simple black-box algorithm that jailbreaks frontier AI systems across modalities.","BoN Jailbreaking works by repeatedly sampling variations of a prompt with a combination of augmentations - such as random shuffling or capitalization for textual prompts - until a harmful response is elicited.","We find that BoN Jailbreaking achieves high attack success rates (ASRs) on closed-source language models, such as 89% on GPT-4o and 78% on Claude 3.5 Sonnet when sampling 10,000 augmented prompts.","Further, it is similarly effective at circumventing state-of-the-art open-source defenses like circuit breakers.","BoN also seamlessly extends to other modalities: it jailbreaks vision language models (VLMs) such as GPT-4o and audio language models (ALMs) like Gemini 1.5 Pro, using modality-specific augmentations.","BoN reliably improves when we sample more augmented prompts.","Across all modalities, ASR, as a function of the number of samples (N), empirically follows power-law-like behavior for many orders of magnitude.","BoN Jailbreaking can also be composed with other black-box algorithms for even more effective attacks - combining BoN with an optimized prefix attack achieves up to a 35% increase in ASR.","Overall, our work indicates that, despite their capability, language models are sensitive to seemingly innocuous changes to inputs, which attackers can exploit across modalities."],"url":"http://arxiv.org/abs/2412.03556v1"}
{"created":"2024-12-04 18:50:42","title":"PaliGemma 2: A Family of Versatile VLMs for Transfer","abstract":"PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.","sentences":["PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models.","We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model.","We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning.","The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution.","We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results."],"url":"http://arxiv.org/abs/2412.03555v1"}
{"created":"2024-12-04 18:50:12","title":"BinSparX: Sparsified Binary Neural Networks for Reduced Hardware Non-Idealities in Xbar Arrays","abstract":"Compute-in-memory (CiM)-based binary neural network (CiM-BNN) accelerators marry the benefits of CiM and ultra-low precision quantization, making them highly suitable for edge computing. However, CiM-enabled crossbar (Xbar) arrays are plagued with hardware non-idealities like parasitic resistances and device non-linearities that impair inference accuracy, especially in scaled technologies. In this work, we first analyze the impact of Xbar non-idealities on the inference accuracy of various CiM-BNNs, establishing that the unique properties of CiM-BNNs make them more prone to hardware non-idealities compared to higher precision deep neural networks (DNNs). To address this issue, we propose BinSparX, a training-free technique that mitigates non-idealities in CiM-BNNs. BinSparX utilizes the distinct attributes of BNNs to reduce the average current generated during the CiM operations in Xbar arrays. This is achieved by statically and dynamically sparsifying the BNN weights and activations, respectively (which, in the context of BNNs, is defined as reducing the number of +1 weights and activations). This minimizes the IR drops across the parasitic resistances, drastically mitigating their impact on inference accuracy. To evaluate our technique, we conduct experiments on ResNet-18 and VGG-small CiM-BNNs designed at the 7nm technology node using 8T-SRAM and 1T-1ReRAM. Our results show that BinSparX is highly effective in alleviating the impact of non-idealities, recouping the inference accuracy to near-ideal (software) levels in some cases and providing accuracy boost of up to 77.25%. These benefits are accompanied by energy reduction, albeit at the cost of mild latency/area increase.","sentences":["Compute-in-memory (CiM)-based binary neural network (CiM-BNN) accelerators marry the benefits of CiM and ultra-low precision quantization, making them highly suitable for edge computing.","However, CiM-enabled crossbar (Xbar) arrays are plagued with hardware non-idealities like parasitic resistances and device non-linearities that impair inference accuracy, especially in scaled technologies.","In this work, we first analyze the impact of Xbar non-idealities on the inference accuracy of various CiM-BNNs, establishing that the unique properties of CiM-BNNs make them more prone to hardware non-idealities compared to higher precision deep neural networks (DNNs).","To address this issue, we propose BinSparX, a training-free technique that mitigates non-idealities in CiM-BNNs.","BinSparX utilizes the distinct attributes of BNNs to reduce the average current generated during the CiM operations in Xbar arrays.","This is achieved by statically and dynamically sparsifying the BNN weights and activations, respectively (which, in the context of BNNs, is defined as reducing the number of +1 weights and activations).","This minimizes the IR drops across the parasitic resistances, drastically mitigating their impact on inference accuracy.","To evaluate our technique, we conduct experiments on ResNet-18 and VGG-small CiM-BNNs designed at the 7nm technology node using 8T-SRAM and 1T-1ReRAM.","Our results show that BinSparX is highly effective in alleviating the impact of non-idealities, recouping the inference accuracy to near-ideal (software) levels in some cases and providing accuracy boost of up to 77.25%.","These benefits are accompanied by energy reduction, albeit at the cost of mild latency/area increase."],"url":"http://arxiv.org/abs/2412.03553v1"}
{"created":"2024-12-04 18:50:08","title":"Imagine360: Immersive 360 Video Generation from Perspective Anchor","abstract":"$360^\\circ$ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees. To achieve more user-friendly and personalized content creation in $360^\\circ$ video format, we seek to lift standard perspective videos into $360^\\circ$ equirectangular videos. To this end, we introduce Imagine360, the first perspective-to-$360^\\circ$ video generation framework that creates high-quality $360^\\circ$ videos with rich and diverse motion patterns from video anchors. Imagine360 learns fine-grained spherical visual and motion patterns from limited $360^\\circ$ video data with several key designs. 1) Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for $360^\\circ$ video generation, with motion module and spatial LoRA layers fine-tuned on extended web $360^\\circ$ videos. 2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres. 3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames. Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art $360^\\circ$ video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive $360^\\circ$ video creation.","sentences":["$360^\\circ$ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees.","To achieve more user-friendly and personalized content creation in $360^\\circ$ video format, we seek to lift standard perspective videos into $360^\\circ$ equirectangular videos.","To this end, we introduce Imagine360, the first perspective-to-$360^\\circ$ video generation framework that creates high-quality $360^\\circ$ videos with rich and diverse motion patterns from video anchors.","Imagine360","learns fine-grained spherical visual and motion patterns from limited $360^\\circ$ video data with several key designs.","1)","Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for $360^\\circ$ video generation, with motion module and spatial LoRA layers fine-tuned on extended web $360^\\circ$ videos.","2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres.","3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames.","Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art $360^\\circ$ video generation methods.","We believe Imagine360 holds promise for advancing personalized, immersive $360^\\circ$ video creation."],"url":"http://arxiv.org/abs/2412.03552v1"}
{"created":"2024-12-04 18:49:26","title":"SPICE: Smart Projection Interface for Cooking Enhancement","abstract":"Tangible User Interfaces (TUI) for human--computer interaction (HCI) provide the user with physical representations of digital information with the aim to overcome the limitations of screen-based interfaces. Although many compelling demonstrations of TUIs exist in the literature, there is a lack of research on TUIs intended for daily two-handed tasks and processes, such as cooking. In response to this gap, we propose SPICE (Smart Projection Interface for Cooking Enhancement). SPICE investigates TUIs in a kitchen setting, aiming to transform the recipe following experience from simply text-based to tangibly interactive. SPICE includes a tracking system, an agent-based software, and vision large language models to create and interpret a kitchen environment where recipe information is projected directly onto the cooking surface. We conducted a comparative usability study of SPICE and text-based recipe following with 30 participants, assessing the task difficulty, total duration, and efficiency, as well as user confidence and taste perception. The results indicate that SPICE allowed participants to perform the recipe with less stops and in shorter time while also improving self-reported efficiency, confidence, and taste. Despite this, participants self-reported no change in overall difficulty, which is a direction for future research. Overall, the SPICE project demonstrates the potential of using TUIs to improve everyday activities, paving the way for future research in HCI and new computing interfaces.","sentences":["Tangible User Interfaces (TUI) for human--computer interaction (HCI) provide the user with physical representations of digital information with the aim to overcome the limitations of screen-based interfaces.","Although many compelling demonstrations of TUIs exist in the literature, there is a lack of research on TUIs intended for daily two-handed tasks and processes, such as cooking.","In response to this gap, we propose SPICE (Smart Projection Interface for Cooking Enhancement).","SPICE investigates TUIs in a kitchen setting, aiming to transform the recipe following experience from simply text-based to tangibly interactive.","SPICE includes a tracking system, an agent-based software, and vision large language models to create and interpret a kitchen environment where recipe information is projected directly onto the cooking surface.","We conducted a comparative usability study of SPICE and text-based recipe following with 30 participants, assessing the task difficulty, total duration, and efficiency, as well as user confidence and taste perception.","The results indicate that SPICE allowed participants to perform the recipe with less stops and in shorter time while also improving self-reported efficiency, confidence, and taste.","Despite this, participants self-reported no change in overall difficulty, which is a direction for future research.","Overall, the SPICE project demonstrates the potential of using TUIs to improve everyday activities, paving the way for future research in HCI and new computing interfaces."],"url":"http://arxiv.org/abs/2412.03551v1"}
{"created":"2024-12-04 18:47:11","title":"Teaching an Old Dog New Tricks: Verifiable FHE Using Commodity Hardware","abstract":"We present Argos, a simple approach for adding verifiability to fully homomorphic encryption (FHE) schemes using trusted hardware. Traditional approaches to verifiable FHE require expensive cryptographic proofs, which incur an overhead of up to seven orders of magnitude on top of FHE, making them impractical.   With Argos, we show that trusted hardware can be securely used to provide verifiability for FHE computations, with minimal overhead relative to the baseline FHE computation. An important contribution of Argos is showing that the major security pitfall associated with trusted hardware, microarchitectural side channels, can be completely mitigated by excluding any secrets from the CPU and the memory hierarchy. This is made possible by focusing on building a platform that only enforces program and data integrity and not confidentiality (which is sufficient for verifiable FHE, since all data remain encrypted at all times). All secrets related to the attestation mechanism are kept in a separate coprocessor (e.g., a TPM) inaccessible to any software-based attacker. Relying on a discrete TPM typically incurs significant performance overhead, which is why (insecure) software-based TPMs are used in practice. As a second contribution, we show that for FHE applications, the attestation protocol can be adapted to only incur a fixed cost.   Argos requires no dedicated hardware extensions and is supported on commodity processors from 2008 onward. Our prototype implementation introduces 6% overhead to the FHE evaluation, and 8% for more complex protocols. In particular, we show that Argos can be adapted for real-world applications of FHE, such as PIR and PSI. By demonstrating how to combine cryptography with trusted hardware, Argos paves the way for widespread deployment of FHE-based protocols beyond the semi-honest setting, without the overhead of cryptographic proofs.","sentences":["We present Argos, a simple approach for adding verifiability to fully homomorphic encryption (FHE) schemes using trusted hardware.","Traditional approaches to verifiable FHE require expensive cryptographic proofs, which incur an overhead of up to seven orders of magnitude on top of FHE, making them impractical.   ","With Argos, we show that trusted hardware can be securely used to provide verifiability for FHE computations, with minimal overhead relative to the baseline FHE computation.","An important contribution of Argos is showing that the major security pitfall associated with trusted hardware, microarchitectural side channels, can be completely mitigated by excluding any secrets from the CPU and the memory hierarchy.","This is made possible by focusing on building a platform that only enforces program and data integrity and not confidentiality (which is sufficient for verifiable FHE, since all data remain encrypted at all times).","All secrets related to the attestation mechanism are kept in a separate coprocessor (e.g., a TPM) inaccessible to any software-based attacker.","Relying on a discrete TPM typically incurs significant performance overhead, which is why (insecure) software-based TPMs are used in practice.","As a second contribution, we show that for FHE applications, the attestation protocol can be adapted to only incur a fixed cost.   ","Argos requires no dedicated hardware extensions and is supported on commodity processors from 2008 onward.","Our prototype implementation introduces 6% overhead to the FHE evaluation, and 8% for more complex protocols.","In particular, we show that Argos can be adapted for real-world applications of FHE, such as PIR and PSI.","By demonstrating how to combine cryptography with trusted hardware, Argos paves the way for widespread deployment of FHE-based protocols beyond the semi-honest setting, without the overhead of cryptographic proofs."],"url":"http://arxiv.org/abs/2412.03550v1"}
{"created":"2024-12-04 18:45:35","title":"Perception Tokens Enhance Visual Reasoning in Multimodal Language Models","abstract":"Multimodal language models (MLMs) still face challenges in fundamental visual perception tasks where specialized models excel. Tasks requiring reasoning about 3D structures benefit from depth estimation, and reasoning about 2D object instances benefits from object detection. Yet, MLMs can not produce intermediate depth or boxes to reason over. Finetuning MLMs on relevant data doesn't generalize well and outsourcing computation to specialized vision tools is too compute-intensive and memory-inefficient. To address this, we introduce Perception Tokens, intrinsic image representations designed to assist reasoning tasks where language is insufficient. Perception tokens act as auxiliary reasoning tokens, akin to chain-of-thought prompts in language models. For example, in a depth-related task, an MLM augmented with perception tokens can reason by generating a depth map as tokens, enabling it to solve the problem effectively. We propose AURORA, a training method that augments MLMs with perception tokens for improved reasoning over visual inputs. AURORA leverages a VQVAE to transform intermediate image representations, such as depth maps into a tokenized format and bounding box tokens, which is then used in a multi-task training framework. AURORA achieves notable improvements across counting benchmarks: +10.8% on BLINK, +11.3% on CVBench, and +8.3% on SEED-Bench, outperforming finetuning approaches in generalization across datasets. It also improves on relative depth: over +6% on BLINK. With perception tokens, AURORA expands the scope of MLMs beyond language-based reasoning, paving the way for more effective visual reasoning capabilities.","sentences":["Multimodal language models (MLMs) still face challenges in fundamental visual perception tasks where specialized models excel.","Tasks requiring reasoning about 3D structures benefit from depth estimation, and reasoning about 2D object instances benefits from object detection.","Yet, MLMs can not produce intermediate depth or boxes to reason over.","Finetuning MLMs on relevant data doesn't generalize well and outsourcing computation to specialized vision tools is too compute-intensive and memory-inefficient.","To address this, we introduce Perception Tokens, intrinsic image representations designed to assist reasoning tasks where language is insufficient.","Perception tokens act as auxiliary reasoning tokens, akin to chain-of-thought prompts in language models.","For example, in a depth-related task, an MLM augmented with perception tokens can reason by generating a depth map as tokens, enabling it to solve the problem effectively.","We propose AURORA, a training method that augments MLMs with perception tokens for improved reasoning over visual inputs.","AURORA leverages a VQVAE to transform intermediate image representations, such as depth maps into a tokenized format and bounding box tokens, which is then used in a multi-task training framework.","AURORA achieves notable improvements across counting benchmarks: +10.8% on BLINK, +11.3% on CVBench, and +8.3% on SEED-Bench, outperforming finetuning approaches in generalization across datasets.","It also improves on relative depth: over +6% on BLINK.","With perception tokens, AURORA expands the scope of MLMs beyond language-based reasoning, paving the way for more effective visual reasoning capabilities."],"url":"http://arxiv.org/abs/2412.03548v1"}
{"created":"2024-12-04 18:36:09","title":"NODE-AdvGAN: Improving the transferability and perceptual similarity of adversarial examples by dynamic-system-driven adversarial generative model","abstract":"Understanding adversarial examples is crucial for improving the model's robustness, as they introduce imperceptible perturbations that deceive models. Effective adversarial examples, therefore, offer the potential to train more robust models by removing their singularities. We propose NODE-AdvGAN, a novel approach that treats adversarial generation as a continuous process and employs a Neural Ordinary Differential Equation (NODE) for simulating the dynamics of the generator. By mimicking the iterative nature of traditional gradient-based methods, NODE-AdvGAN generates smoother and more precise perturbations that preserve high perceptual similarity when added to benign images. We also propose a new training strategy, NODE-AdvGAN-T, which enhances transferability in black-box attacks by effectively tuning noise parameters during training. Experiments demonstrate that NODE-AdvGAN and NODE-AdvGAN-T generate more effective adversarial examples that achieve higher attack success rates while preserving better perceptual quality than traditional GAN-based methods.","sentences":["Understanding adversarial examples is crucial for improving the model's robustness, as they introduce imperceptible perturbations that deceive models.","Effective adversarial examples, therefore, offer the potential to train more robust models by removing their singularities.","We propose NODE-AdvGAN, a novel approach that treats adversarial generation as a continuous process and employs a Neural Ordinary Differential Equation (NODE) for simulating the dynamics of the generator.","By mimicking the iterative nature of traditional gradient-based methods, NODE-AdvGAN generates smoother and more precise perturbations that preserve high perceptual similarity when added to benign images.","We also propose a new training strategy, NODE-AdvGAN-T, which enhances transferability in black-box attacks by effectively tuning noise parameters during training.","Experiments demonstrate that NODE-AdvGAN and NODE-AdvGAN-T generate more effective adversarial examples that achieve higher attack success rates while preserving better perceptual quality than traditional GAN-based methods."],"url":"http://arxiv.org/abs/2412.03539v1"}
{"created":"2024-12-04 18:32:42","title":"Evaluating Gender Bias Transfer between Pre-trained and Prompt-Adapted Language Models","abstract":"Large language models (LLMs) are increasingly being adapted to achieve task-specificity for deployment in real-world decision systems. Several previous works have investigated the bias transfer hypothesis (BTH) by studying the effect of the fine-tuning adaptation strategy on model fairness to find that fairness in pre-trained masked language models have limited effect on the fairness of models when adapted using fine-tuning. In this work, we expand the study of BTH to causal models under prompt adaptations, as prompting is an accessible, and compute-efficient way to deploy models in real-world systems. In contrast to previous works, we establish that intrinsic biases in pre-trained Mistral, Falcon and Llama models are strongly correlated (rho >= 0.94) with biases when the same models are zero- and few-shot prompted, using a pronoun co-reference resolution task. Further, we find that bias transfer remains strongly correlated even when LLMs are specifically prompted to exhibit fair or biased behavior (rho >= 0.92), and few-shot length and stereotypical composition are varied (rho >= 0.97). Our findings highlight the importance of ensuring fairness in pre-trained LLMs, especially when they are later used to perform downstream tasks via prompt adaptation.","sentences":["Large language models (LLMs) are increasingly being adapted to achieve task-specificity for deployment in real-world decision systems.","Several previous works have investigated the bias transfer hypothesis (BTH) by studying the effect of the fine-tuning adaptation strategy on model fairness to find that fairness in pre-trained masked language models have limited effect on the fairness of models when adapted using fine-tuning.","In this work, we expand the study of BTH to causal models under prompt adaptations, as prompting is an accessible, and compute-efficient way to deploy models in real-world systems.","In contrast to previous works, we establish that intrinsic biases in pre-trained Mistral, Falcon and Llama models are strongly correlated (rho >= 0.94) with biases when the same models are zero- and few-shot prompted, using a pronoun co-reference resolution task.","Further, we find that bias transfer remains strongly correlated even when LLMs are specifically prompted to exhibit fair or biased behavior (rho >= 0.92), and few-shot length and stereotypical composition are varied (rho >= 0.97).","Our findings highlight the importance of ensuring fairness in pre-trained LLMs, especially when they are later used to perform downstream tasks via prompt adaptation."],"url":"http://arxiv.org/abs/2412.03537v1"}
{"created":"2024-12-04 18:26:13","title":"A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences","abstract":"The rapid advancement of large language models (LLMs) has opened new boundaries in the extraction and synthesis of medical knowledge, particularly within evidence synthesis. This paper reviews the state-of-the-art applications of LLMs in the biomedical domain, exploring their effectiveness in automating complex tasks such as evidence synthesis and data extraction from a biomedical corpus of documents. While LLMs demonstrate remarkable potential, significant challenges remain, including issues related to hallucinations, contextual understanding, and the ability to generalize across diverse medical tasks. We highlight critical gaps in the current research literature, particularly the need for unified benchmarks to standardize evaluations and ensure reliability in real-world applications. In addition, we propose directions for future research, emphasizing the integration of state-of-the-art techniques such as retrieval-augmented generation (RAG) to enhance LLM performance in evidence synthesis. By addressing these challenges and utilizing the strengths of LLMs, we aim to improve access to medical literature and facilitate meaningful discoveries in healthcare.","sentences":["The rapid advancement of large language models (LLMs) has opened new boundaries in the extraction and synthesis of medical knowledge, particularly within evidence synthesis.","This paper reviews the state-of-the-art applications of LLMs in the biomedical domain, exploring their effectiveness in automating complex tasks such as evidence synthesis and data extraction from a biomedical corpus of documents.","While LLMs demonstrate remarkable potential, significant challenges remain, including issues related to hallucinations, contextual understanding, and the ability to generalize across diverse medical tasks.","We highlight critical gaps in the current research literature, particularly the need for unified benchmarks to standardize evaluations and ensure reliability in real-world applications.","In addition, we propose directions for future research, emphasizing the integration of state-of-the-art techniques such as retrieval-augmented generation (RAG) to enhance LLM performance in evidence synthesis.","By addressing these challenges and utilizing the strengths of LLMs, we aim to improve access to medical literature and facilitate meaningful discoveries in healthcare."],"url":"http://arxiv.org/abs/2412.03531v1"}
{"created":"2024-12-04 18:15:41","title":"FANAL -- Financial Activity News Alerting Language Modeling Framework","abstract":"In the rapidly evolving financial sector, the accurate and timely interpretation of market news is essential for stakeholders needing to navigate unpredictable events. This paper introduces FANAL (Financial Activity News Alerting Language Modeling Framework), a specialized BERT-based framework engineered for real-time financial event detection and analysis, categorizing news into twelve distinct financial categories. FANAL leverages silver-labeled data processed through XGBoost and employs advanced fine-tuning techniques, alongside ORBERT (Odds Ratio BERT), a novel variant of BERT fine-tuned with ORPO (Odds Ratio Preference Optimization) for superior class-wise probability calibration and alignment with financial event relevance. We evaluate FANAL's performance against leading large language models, including GPT-4o, Llama-3.1 8B, and Phi-3, demonstrating its superior accuracy and cost efficiency. This framework sets a new standard for financial intelligence and responsiveness, significantly outstripping existing models in both performance and affordability.","sentences":["In the rapidly evolving financial sector, the accurate and timely interpretation of market news is essential for stakeholders needing to navigate unpredictable events.","This paper introduces FANAL (Financial Activity News Alerting Language Modeling Framework), a specialized BERT-based framework engineered for real-time financial event detection and analysis, categorizing news into twelve distinct financial categories.","FANAL leverages silver-labeled data processed through XGBoost and employs advanced fine-tuning techniques, alongside ORBERT (Odds Ratio BERT), a novel variant of BERT fine-tuned with ORPO (Odds Ratio Preference Optimization) for superior class-wise probability calibration and alignment with financial event relevance.","We evaluate FANAL's performance against leading large language models, including GPT-4o, Llama-3.1 8B, and Phi-3, demonstrating its superior accuracy and cost efficiency.","This framework sets a new standard for financial intelligence and responsiveness, significantly outstripping existing models in both performance and affordability."],"url":"http://arxiv.org/abs/2412.03527v1"}
{"created":"2024-12-04 18:15:06","title":"Feed-Forward Bullet-Time Reconstruction of Dynamic Scenes from Monocular Videos","abstract":"Recent advancements in static feed-forward scene reconstruction have demonstrated significant progress in high-quality novel view synthesis. However, these models often struggle with generalizability across diverse environments and fail to effectively handle dynamic content. We present BTimer (short for BulletTimer), the first motion-aware feed-forward model for real-time reconstruction and novel view synthesis of dynamic scenes. Our approach reconstructs the full scene in a 3D Gaussian Splatting representation at a given target ('bullet') timestamp by aggregating information from all the context frames. Such a formulation allows BTimer to gain scalability and generalization by leveraging both static and dynamic scene datasets. Given a casual monocular dynamic video, BTimer reconstructs a bullet-time scene within 150ms while reaching state-of-the-art performance on both static and dynamic scene datasets, even compared with optimization-based approaches.","sentences":["Recent advancements in static feed-forward scene reconstruction have demonstrated significant progress in high-quality novel view synthesis.","However, these models often struggle with generalizability across diverse environments and fail to effectively handle dynamic content.","We present BTimer (short for BulletTimer), the first motion-aware feed-forward model for real-time reconstruction and novel view synthesis of dynamic scenes.","Our approach reconstructs the full scene in a 3D Gaussian Splatting representation at a given target ('bullet') timestamp by aggregating information from all the context frames.","Such a formulation allows BTimer to gain scalability and generalization by leveraging both static and dynamic scene datasets.","Given a casual monocular dynamic video, BTimer reconstructs a bullet-time scene within 150ms while reaching state-of-the-art performance on both static and dynamic scene datasets, even compared with optimization-based approaches."],"url":"http://arxiv.org/abs/2412.03526v1"}
{"created":"2024-12-04 18:02:49","title":"Seeing Beyond Views: Multi-View Driving Scene Video Generation with Holistic Attention","abstract":"Generating multi-view videos for autonomous driving training has recently gained much attention, with the challenge of addressing both cross-view and cross-frame consistency. Existing methods typically apply decoupled attention mechanisms for spatial, temporal, and view dimensions. However, these approaches often struggle to maintain consistency across dimensions, particularly when handling fast-moving objects that appear at different times and viewpoints. In this paper, we present CogDriving, a novel network designed for synthesizing high-quality multi-view driving videos. CogDriving leverages a Diffusion Transformer architecture with holistic-4D attention modules, enabling simultaneous associations across the spatial, temporal, and viewpoint dimensions. We also propose a lightweight controller tailored for CogDriving, i.e., Micro-Controller, which uses only 1.1% of the parameters of the standard ControlNet, enabling precise control over Bird's-Eye-View layouts. To enhance the generation of object instances crucial for autonomous driving, we propose a re-weighted learning objective, dynamically adjusting the learning weights for object instances during training. CogDriving demonstrates strong performance on the nuScenes validation set, achieving an FVD score of 37.8, highlighting its ability to generate realistic driving videos. The project can be found at https://luhannan.github.io/CogDrivingPage/.","sentences":["Generating multi-view videos for autonomous driving training has recently gained much attention, with the challenge of addressing both cross-view and cross-frame consistency.","Existing methods typically apply decoupled attention mechanisms for spatial, temporal, and view dimensions.","However, these approaches often struggle to maintain consistency across dimensions, particularly when handling fast-moving objects that appear at different times and viewpoints.","In this paper, we present CogDriving, a novel network designed for synthesizing high-quality multi-view driving videos.","CogDriving leverages a Diffusion Transformer architecture with holistic-4D attention modules, enabling simultaneous associations across the spatial, temporal, and viewpoint dimensions.","We also propose a lightweight controller tailored for CogDriving, i.e., Micro-Controller, which uses only 1.1% of the parameters of the standard ControlNet, enabling precise control over Bird's-Eye-View layouts.","To enhance the generation of object instances crucial for autonomous driving, we propose a re-weighted learning objective, dynamically adjusting the learning weights for object instances during training.","CogDriving demonstrates strong performance on the nuScenes validation set, achieving an FVD score of 37.8, highlighting its ability to generate realistic driving videos.","The project can be found at https://luhannan.github.io/CogDrivingPage/."],"url":"http://arxiv.org/abs/2412.03520v1"}
{"created":"2024-12-04 17:59:04","title":"Dense Scene Reconstruction from Light-Field Images Affected by Rolling Shutter","abstract":"This paper presents a dense depth estimation approach from light-field (LF) images that is able to compensate for strong rolling shutter (RS) effects. Our method estimates RS compensated views and dense RS compensated disparity maps. We present a two-stage method based on a 2D Gaussians Splatting that allows for a ``render and compare\" strategy with a point cloud formulation. In the first stage, a subset of sub-aperture images is used to estimate an RS agnostic 3D shape that is related to the scene target shape ``up to a motion\". In the second stage, the deformation of the 3D shape is computed by estimating an admissible camera motion. We demonstrate the effectiveness and advantages of this approach through several experiments conducted for different scenes and types of motions. Due to lack of suitable datasets for evaluation, we also present a new carefully designed synthetic dataset of RS LF images. The source code, trained models and dataset will be made publicly available at: https://github.com/ICB-Vision-AI/DenseRSLF","sentences":["This paper presents a dense depth estimation approach from light-field (LF) images that is able to compensate for strong rolling shutter (RS) effects.","Our method estimates RS compensated views and dense RS compensated disparity maps.","We present a two-stage method based on a 2D Gaussians Splatting that allows for a ``render and compare\" strategy with a point cloud formulation.","In the first stage, a subset of sub-aperture images is used to estimate an RS agnostic 3D shape that is related to the scene target shape ``up to a motion\".","In the second stage, the deformation of the 3D shape is computed by estimating an admissible camera motion.","We demonstrate the effectiveness and advantages of this approach through several experiments conducted for different scenes and types of motions.","Due to lack of suitable datasets for evaluation, we also present a new carefully designed synthetic dataset of RS LF images.","The source code, trained models and dataset will be made publicly available at: https://github.com/ICB-Vision-AI/DenseRSLF"],"url":"http://arxiv.org/abs/2412.03518v1"}
{"created":"2024-12-04 17:58:03","title":"NVComposer: Boosting Generative Novel View Synthesis with Multiple Sparse and Unposed Images","abstract":"Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data. However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views. In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment. NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training. Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility. Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems.","sentences":["Recent advancements in generative models have significantly improved novel view synthesis (NVS) from multi-view data.","However, existing methods depend on external multi-view alignment processes, such as explicit pose estimation or pre-reconstruction, which limits their flexibility and accessibility, especially when alignment is unstable due to insufficient overlap or occlusions between views.","In this paper, we propose NVComposer, a novel approach that eliminates the need for explicit external alignment.","NVComposer enables the generative model to implicitly infer spatial and geometric relationships between multiple conditional views by introducing two key components: 1) an image-pose dual-stream diffusion model that simultaneously generates target novel views and condition camera poses, and 2) a geometry-aware feature alignment module that distills geometric priors from dense stereo models during training.","Extensive experiments demonstrate that NVComposer achieves state-of-the-art performance in generative multi-view NVS tasks, removing the reliance on external alignment and thus improving model accessibility.","Our approach shows substantial improvements in synthesis quality as the number of unposed input views increases, highlighting its potential for more flexible and accessible generative NVS systems."],"url":"http://arxiv.org/abs/2412.03517v1"}
{"created":"2024-12-04 17:57:39","title":"You're (Not) My Type -- Can LLMs Generate Feedback of Specific Types for Introductory Programming Tasks?","abstract":"Background: Feedback as one of the most influential factors for learning has been subject to a great body of research. It plays a key role in the development of educational technology systems and is traditionally rooted in deterministic feedback defined by experts and their experience. However, with the rise of generative AI and especially Large Language Models (LLMs), we expect feedback as part of learning systems to transform, especially for the context of programming. In the past, it was challenging to automate feedback for learners of programming. LLMs may create new possibilities to provide richer, and more individual feedback than ever before.   Objectives: This paper aims to generate specific types of feedback for introductory programming tasks using LLMs. We revisit existing feedback taxonomies to capture the specifics of the generated feedback, such as randomness, uncertainty, and degrees of variation.   Methods: We iteratively designed prompts for the generation of specific feedback types (as part of existing feedback taxonomies) in response to authentic student programs. We then evaluated the generated output and determined to what extent it reflected certain feedback types.   Results and Conclusion: The present work provides a better understanding of different feedback dimensions and characteristics. The results have implications for future feedback research with regard to, for example, feedback effects and learners' informational needs. It further provides a basis for the development of new tools and learning systems for novice programmers including feedback generated by AI.","sentences":["Background: Feedback as one of the most influential factors for learning has been subject to a great body of research.","It plays a key role in the development of educational technology systems and is traditionally rooted in deterministic feedback defined by experts and their experience.","However, with the rise of generative AI and especially Large Language Models (LLMs), we expect feedback as part of learning systems to transform, especially for the context of programming.","In the past, it was challenging to automate feedback for learners of programming.","LLMs may create new possibilities to provide richer, and more individual feedback than ever before.   Objectives:","This paper aims to generate specific types of feedback for introductory programming tasks using LLMs.","We revisit existing feedback taxonomies to capture the specifics of the generated feedback, such as randomness, uncertainty, and degrees of variation.   ","Methods: We iteratively designed prompts for the generation of specific feedback types (as part of existing feedback taxonomies) in response to authentic student programs.","We then evaluated the generated output and determined to what extent it reflected certain feedback types.   ","Results and Conclusion: The present work provides a better understanding of different feedback dimensions and characteristics.","The results have implications for future feedback research with regard to, for example, feedback effects and learners' informational needs.","It further provides a basis for the development of new tools and learning systems for novice programmers including feedback generated by AI."],"url":"http://arxiv.org/abs/2412.03516v1"}
{"created":"2024-12-04 17:57:25","title":"Distilling Diffusion Models to Efficient 3D LiDAR Scene Completion","abstract":"Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality. However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments. This paper proposes a novel distillation method tailored for 3D LiDAR scene completion models, dubbed $\\textbf{ScoreLiDAR}$, which achieves efficient yet high-quality scene completion. ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation. To improve completion quality, we also introduce a novel $\\textbf{Structural Loss}$, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene. The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration. Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame ($>$5$\\times$) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models. Our code is publicly available at https://github.com/happyw1nd/ScoreLiDAR.","sentences":["Diffusion models have been applied to 3D LiDAR scene completion due to their strong training stability and high completion quality.","However, the slow sampling speed limits the practical application of diffusion-based scene completion models since autonomous vehicles require an efficient perception of surrounding environments.","This paper proposes a novel distillation method tailored for 3D LiDAR scene completion models, dubbed $\\textbf{ScoreLiDAR}$, which achieves efficient yet high-quality scene completion.","ScoreLiDAR enables the distilled model to sample in significantly fewer steps after distillation.","To improve completion quality, we also introduce a novel $\\textbf{Structural Loss}$, which encourages the distilled model to capture the geometric structure of the 3D LiDAR scene.","The loss contains a scene-wise term constraining the holistic structure and a point-wise term constraining the key landmark points and their relative configuration.","Extensive experiments demonstrate that ScoreLiDAR significantly accelerates the completion time from 30.55 to 5.37 seconds per frame ($>$5$\\times$) on SemanticKITTI and achieves superior performance compared to state-of-the-art 3D LiDAR scene completion models.","Our code is publicly available at https://github.com/happyw1nd/ScoreLiDAR."],"url":"http://arxiv.org/abs/2412.03515v1"}
{"created":"2024-12-04 17:57:04","title":"Adaptive Personalized Over-the-Air Federated Learning with Reflecting Intelligent Surfaces","abstract":"Over-the-air federated learning (OTA-FL) unifies communication and model aggregation by leveraging the inherent superposition property of the wireless medium. This strategy can enable scalable and bandwidth-efficient learning via simultaneous transmission of model updates using the same frequency resources, if care is exercised to design the physical layer jointly with learning. In this paper, a federated learning system facilitated by a heterogeneous edge-intelligent network is considered. The edge users (clients) have differing user resources and non-i.i.d. local dataset distributions. A general non-convex learning objective is considered for the model training task(s) at hand. We augment the network with Reconfigurable Intelligent Surfaces (RIS) in order to enhance the learning system. We propose a cross-layer algorithm that jointly assigns communication, computation and learning resources. In particular, we adaptively adjust the number of local steps in conjunction with RIS configuration to boost the learning performance. Our system model considers channel noise and channel estimation errors in both the uplink (model updates) and downlink (global model broadcast), employing dynamic power control for both. We provide the convergence analysis for the proposed algorithms and extend the frameworks to personalized learning. Our experimental results demonstrate that the proposed algorithms outperform the state-of-the-art joint communication and learning baselines.","sentences":["Over-the-air federated learning (OTA-FL) unifies communication and model aggregation by leveraging the inherent superposition property of the wireless medium.","This strategy can enable scalable and bandwidth-efficient learning via simultaneous transmission of model updates using the same frequency resources, if care is exercised to design the physical layer jointly with learning.","In this paper, a federated learning system facilitated by a heterogeneous edge-intelligent network is considered.","The edge users (clients) have differing user resources and non-i.i.d. local dataset distributions.","A general non-convex learning objective is considered for the model training task(s) at hand.","We augment the network with Reconfigurable Intelligent Surfaces (RIS) in order to enhance the learning system.","We propose a cross-layer algorithm that jointly assigns communication, computation and learning resources.","In particular, we adaptively adjust the number of local steps in conjunction with RIS configuration to boost the learning performance.","Our system model considers channel noise and channel estimation errors in both the uplink (model updates) and downlink (global model broadcast), employing dynamic power control for both.","We provide the convergence analysis for the proposed algorithms and extend the frameworks to personalized learning.","Our experimental results demonstrate that the proposed algorithms outperform the state-of-the-art joint communication and learning baselines."],"url":"http://arxiv.org/abs/2412.03514v1"}
{"created":"2024-12-04 17:56:49","title":"KKLIP: Knowledge Distillation Exploiting K-means Clustering for Language-Image Pre-Training","abstract":"Recently, CLIP has emerged as a valuable model for aligning image and text information in multi-modal scenarios. However, researchers have observed limitations in the ability of CLIP's text and image encoders to extract detailed knowledge from caption-image pairs. In response, this paper introduces KKLIP, a novel approach designed to enhance the quality of CLIP by incorporating a new knowledge distillation (KD) method derived from Llama 2. Our method comprises three objectives: Text Embedding Distillation, Concept Learning, and Contrastive Learning. Firstly, Text Embedding Distillation involves training the KKLIP text encoder to emulate the teacher model, Llama 2. Secondly, Concept Learning assigns a soft concept label to each caption-image pair through offline k-means clustering of text information from Llama 2, allowing KKLIP to learn from these soft concept labels. Finally, Contrastive Learning harmonizes text and image embeddings. Our experimental results demonstrate that KKLIP enhances the quality of both text and image encoders.","sentences":["Recently, CLIP has emerged as a valuable model for aligning image and text information in multi-modal scenarios.","However, researchers have observed limitations in the ability of CLIP's text and image encoders to extract detailed knowledge from caption-image pairs.","In response, this paper introduces KKLIP, a novel approach designed to enhance the quality of CLIP by incorporating a new knowledge distillation (KD) method derived from Llama 2.","Our method comprises three objectives:","Text Embedding Distillation, Concept Learning, and Contrastive Learning.","Firstly, Text Embedding Distillation involves training the KKLIP text encoder to emulate the teacher model, Llama 2.","Secondly, Concept Learning assigns a soft concept label to each caption-image pair through offline k-means clustering of text information from Llama 2, allowing KKLIP to learn from these soft concept labels.","Finally, Contrastive Learning harmonizes text and image embeddings.","Our experimental results demonstrate that KKLIP enhances the quality of both text and image encoders."],"url":"http://arxiv.org/abs/2412.03513v1"}
{"created":"2024-12-04 17:55:33","title":"Distillation of Diffusion Features for Semantic Correspondence","abstract":"Semantic correspondence, the task of determining relationships between different parts of images, underpins various applications including 3D reconstruction, image-to-image translation, object tracking, and visual place recognition. Recent studies have begun to explore representations learned in large generative image models for semantic correspondence, demonstrating promising results. Building on this progress, current state-of-the-art methods rely on combining multiple large models, resulting in high computational demands and reduced efficiency. In this work, we address this challenge by proposing a more computationally efficient approach. We propose a novel knowledge distillation technique to overcome the problem of reduced efficiency. We show how to use two large vision foundation models and distill the capabilities of these complementary models into one smaller model that maintains high accuracy at reduced computational cost. Furthermore, we demonstrate that by incorporating 3D data, we are able to further improve performance, without the need for human-annotated correspondences. Overall, our empirical results demonstrate that our distilled model with 3D data augmentation achieves performance superior to current state-of-the-art methods while significantly reducing computational load and enhancing practicality for real-world applications, such as semantic video correspondence. Our code and weights are publicly available on our project page.","sentences":["Semantic correspondence, the task of determining relationships between different parts of images, underpins various applications including 3D reconstruction, image-to-image translation, object tracking, and visual place recognition.","Recent studies have begun to explore representations learned in large generative image models for semantic correspondence, demonstrating promising results.","Building on this progress, current state-of-the-art methods rely on combining multiple large models, resulting in high computational demands and reduced efficiency.","In this work, we address this challenge by proposing a more computationally efficient approach.","We propose a novel knowledge distillation technique to overcome the problem of reduced efficiency.","We show how to use two large vision foundation models and distill the capabilities of these complementary models into one smaller model that maintains high accuracy at reduced computational cost.","Furthermore, we demonstrate that by incorporating 3D data, we are able to further improve performance, without the need for human-annotated correspondences.","Overall, our empirical results demonstrate that our distilled model with 3D data augmentation achieves performance superior to current state-of-the-art methods while significantly reducing computational load and enhancing practicality for real-world applications, such as semantic video correspondence.","Our code and weights are publicly available on our project page."],"url":"http://arxiv.org/abs/2412.03512v1"}
{"created":"2024-12-04 17:50:51","title":"Design and Control of an Ultra-Slender Push-Pull Multisection Continuum Manipulator for In-Situ Inspection of Aeroengine","abstract":"Since the shape of industrial endoscopes is passively altered according to the contact around it, manual inspection approaches of aeroengines through the inspection ports have unreachable areas, and it's difficult to traverse multistage blades and inspect them simultaneously, which requires engine disassembly or the cooperation of multiple operators, resulting in efficiency decline and increased costs. To this end, this paper proposes a novel continuum manipulator with push-pull multisection structure which provides a potential solution for the disadvantages mentioned above due to its higher flexibility, passability, and controllability in confined spaces. The ultra-slender design combined with a tendon-driven mechanism makes the manipulator acquire enough workspace and more flexible postures while maintaining a light weight. Considering the coupling between the tendons in multisection, a innovative kinematics decoupling control method is implemented, which can realize real-time control in the case of limited computational resources. A prototype is built to validate the capabilities of mechatronic design and the performance of the control algorithm. The experimental results demonstrate the advantages of our continuum manipulator in the in-situ inspection of aeroengines' multistage blades, which has the potential to be a replacement solution for industrial endoscopes.","sentences":["Since the shape of industrial endoscopes is passively altered according to the contact around it, manual inspection approaches of aeroengines through the inspection ports have unreachable areas, and it's difficult to traverse multistage blades and inspect them simultaneously, which requires engine disassembly or the cooperation of multiple operators, resulting in efficiency decline and increased costs.","To this end, this paper proposes a novel continuum manipulator with push-pull multisection structure which provides a potential solution for the disadvantages mentioned above due to its higher flexibility, passability, and controllability in confined spaces.","The ultra-slender design combined with a tendon-driven mechanism makes the manipulator acquire enough workspace and more flexible postures while maintaining a light weight.","Considering the coupling between the tendons in multisection, a innovative kinematics decoupling control method is implemented, which can realize real-time control in the case of limited computational resources.","A prototype is built to validate the capabilities of mechatronic design and the performance of the control algorithm.","The experimental results demonstrate the advantages of our continuum manipulator in the in-situ inspection of aeroengines' multistage blades, which has the potential to be a replacement solution for industrial endoscopes."],"url":"http://arxiv.org/abs/2412.03508v1"}
{"created":"2024-12-04 17:39:55","title":"A Bidirectional Siamese Recurrent Neural Network for Accurate Gait Recognition Using Body Landmarks","abstract":"Gait recognition is a significant biometric technique for person identification, particularly in scenarios where other physiological biometrics are impractical or ineffective. In this paper, we address the challenges associated with gait recognition and present a novel approach to improve its accuracy and reliability. The proposed method leverages advanced techniques, including sequential gait landmarks obtained through the Mediapipe pose estimation model, Procrustes analysis for alignment, and a Siamese biGRU-dualStack Neural Network architecture for capturing temporal dependencies. Extensive experiments were conducted on large-scale cross-view datasets to demonstrate the effectiveness of the approach, achieving high recognition accuracy compared to other models. The model demonstrated accuracies of 95.7%, 94.44%, 87.71%, and 86.6% on CASIA-B, SZU RGB-D, OU-MVLP, and Gait3D datasets respectively. The results highlight the potential applications of the proposed method in various practical domains, indicating its significant contribution to the field of gait recognition.","sentences":["Gait recognition is a significant biometric technique for person identification, particularly in scenarios where other physiological biometrics are impractical or ineffective.","In this paper, we address the challenges associated with gait recognition and present a novel approach to improve its accuracy and reliability.","The proposed method leverages advanced techniques, including sequential gait landmarks obtained through the Mediapipe pose estimation model, Procrustes analysis for alignment, and a Siamese biGRU-dualStack Neural Network architecture for capturing temporal dependencies.","Extensive experiments were conducted on large-scale cross-view datasets to demonstrate the effectiveness of the approach, achieving high recognition accuracy compared to other models.","The model demonstrated accuracies of 95.7%, 94.44%, 87.71%, and 86.6% on CASIA-B, SZU RGB-D, OU-MVLP, and Gait3D datasets respectively.","The results highlight the potential applications of the proposed method in various practical domains, indicating its significant contribution to the field of gait recognition."],"url":"http://arxiv.org/abs/2412.03498v1"}
{"created":"2024-12-04 17:39:01","title":"Soft Checksums to Flag Untrustworthy Machine Learning Surrogate Predictions and Application to Atomic Physics Simulations","abstract":"Trained neural networks (NN) are attractive as surrogate models to replace costly calculations in physical simulations, but are often unknowingly applied to states not adequately represented in the training dataset. We present the novel technique of soft checksums for scientific machine learning, a general-purpose method to differentiate between trustworthy predictions with small errors on in-distribution (ID) data points, and untrustworthy predictions with large errors on out-of-distribution (OOD) data points. By adding a check node to the existing output layer, we train the model to learn the chosen checksum function encoded within the NN predictions and show that violations of this function correlate with high prediction errors. As the checksum function depends only on the NN predictions, we can calculate the checksum error for any prediction with a single forward pass, incurring negligible time and memory costs. Additionally, we find that incorporating the checksum function into the loss function and exposing the NN to OOD data points during the training process improves separation between ID and OOD predictions. By applying soft checksums to a physically complex and high-dimensional non-local thermodynamic equilibrium atomic physics dataset, we show that a well-chosen threshold checksum error can effectively separate ID and OOD predictions.","sentences":["Trained neural networks (NN) are attractive as surrogate models to replace costly calculations in physical simulations, but are often unknowingly applied to states not adequately represented in the training dataset.","We present the novel technique of soft checksums for scientific machine learning, a general-purpose method to differentiate between trustworthy predictions with small errors on in-distribution (ID) data points, and untrustworthy predictions with large errors on out-of-distribution (OOD) data points.","By adding a check node to the existing output layer, we train the model to learn the chosen checksum function encoded within the NN predictions and show that violations of this function correlate with high prediction errors.","As the checksum function depends only on the NN predictions, we can calculate the checksum error for any prediction with a single forward pass, incurring negligible time and memory costs.","Additionally, we find that incorporating the checksum function into the loss function and exposing the NN to OOD data points during the training process improves separation between ID and OOD predictions.","By applying soft checksums to a physically complex and high-dimensional non-local thermodynamic equilibrium atomic physics dataset, we show that a well-chosen threshold checksum error can effectively separate ID and OOD predictions."],"url":"http://arxiv.org/abs/2412.03497v1"}
{"created":"2024-12-04 17:26:30","title":"Data Fusion of Semantic and Depth Information in the Context of Object Detection","abstract":"Considerable study has already been conducted regarding autonomous driving in modern era. An autonomous driving system must be extremely good at detecting objects surrounding the car to ensure safety. In this paper, classification, and estimation of an object's (pedestrian) position (concerning an ego 3D coordinate system) are studied and the distance between the ego vehicle and the object in the context of autonomous driving is measured. To classify the object, faster Region-based Convolution Neural Network (R-CNN) with inception v2 is utilized. First, a network is trained with customized dataset to estimate the reference position of objects as well as the distance from the vehicle. From camera calibration to computing the distance, cutting-edge technologies of computer vision algorithms in a series of processes are applied to generate a 3D reference point of the region of interest. The foremost step in this process is generating a disparity map using the concept of stereo vision.","sentences":["Considerable study has already been conducted regarding autonomous driving in modern era.","An autonomous driving system must be extremely good at detecting objects surrounding the car to ensure safety.","In this paper, classification, and estimation of an object's (pedestrian) position (concerning an ego 3D coordinate system) are studied and the distance between the ego vehicle and the object in the context of autonomous driving is measured.","To classify the object, faster Region-based Convolution Neural Network (R-CNN) with inception v2 is utilized.","First, a network is trained with customized dataset to estimate the reference position of objects as well as the distance from the vehicle.","From camera calibration to computing the distance, cutting-edge technologies of computer vision algorithms in a series of processes are applied to generate a 3D reference point of the region of interest.","The foremost step in this process is generating a disparity map using the concept of stereo vision."],"url":"http://arxiv.org/abs/2412.03490v1"}
{"created":"2024-12-04 17:26:21","title":"Higher-order Differentiable Rendering","abstract":"We derive methods to compute higher order differentials (Hessians and Hessian-vector products) of the rendering operator. Our approach is based on importance sampling of a convolution that represents the differentials of rendering parameters and shows to be applicable to both rasterization and path tracing. We further suggest an aggregate sampling strategy to importance-sample multiple dimensions of one convolution kernel simultaneously. We demonstrate that this information improves convergence when used in higher-order optimizers such as Newton or Conjugate Gradient relative to a gradient descent baseline in several inverse rendering tasks.","sentences":["We derive methods to compute higher order differentials (Hessians and Hessian-vector products) of the rendering operator.","Our approach is based on importance sampling of a convolution that represents the differentials of rendering parameters and shows to be applicable to both rasterization and path tracing.","We further suggest an aggregate sampling strategy to importance-sample multiple dimensions of one convolution kernel simultaneously.","We demonstrate that this information improves convergence when used in higher-order optimizers such as Newton or Conjugate Gradient relative to a gradient descent baseline in several inverse rendering tasks."],"url":"http://arxiv.org/abs/2412.03489v1"}
{"created":"2024-12-04 17:24:35","title":"Flow Matching with General Discrete Paths: A Kinetic-Optimal Perspective","abstract":"The design space of discrete-space diffusion or flow generative models are significantly less well-understood than their continuous-space counterparts, with many works focusing only on a simple masked construction. In this work, we aim to take a holistic approach to the construction of discrete generative models based on continuous-time Markov chains, and for the first time, allow the use of arbitrary discrete probability paths, or colloquially, corruption processes. Through the lens of optimizing the symmetric kinetic energy, we propose velocity formulas that can be applied to any given probability path, completely decoupling the probability and velocity, and giving the user the freedom to specify any desirable probability path based on expert knowledge specific to the data domain. Furthermore, we find that a special construction of mixture probability paths optimizes the symmetric kinetic energy for the discrete case. We empirically validate the usefulness of this new design space across multiple modalities: text generation, inorganic material generation, and image generation. We find that we can outperform the mask construction even in text with kinetic-optimal mixture paths, while we can make use of domain-specific constructions of the probability path over the visual domain.","sentences":["The design space of discrete-space diffusion or flow generative models are significantly less well-understood than their continuous-space counterparts, with many works focusing only on a simple masked construction.","In this work, we aim to take a holistic approach to the construction of discrete generative models based on continuous-time Markov chains, and for the first time, allow the use of arbitrary discrete probability paths, or colloquially, corruption processes.","Through the lens of optimizing the symmetric kinetic energy, we propose velocity formulas that can be applied to any given probability path, completely decoupling the probability and velocity, and giving the user the freedom to specify any desirable probability path based on expert knowledge specific to the data domain.","Furthermore, we find that a special construction of mixture probability paths optimizes the symmetric kinetic energy for the discrete case.","We empirically validate the usefulness of this new design space across multiple modalities: text generation, inorganic material generation, and image generation.","We find that we can outperform the mask construction even in text with kinetic-optimal mixture paths, while we can make use of domain-specific constructions of the probability path over the visual domain."],"url":"http://arxiv.org/abs/2412.03487v1"}
{"created":"2024-12-04 17:20:01","title":"Convolutional Neural Networks and Mixture of Experts for Intrusion Detection in 5G Networks and beyond","abstract":"The advent of 6G/NextG networks comes along with a series of benefits, including extreme capacity, reliability, and efficiency. However, these networks may become vulnerable to new security threats. Therefore, 6G/NextG networks must be equipped with advanced Artificial Intelligence algorithms, in order to evade these attacks. Existing studies on the intrusion detection task rely on the train of shallow machine learning classifiers, including Logistic Regression, Decision Trees, and so on, yielding suboptimal performance. Others are based on deep neural networks consisting of static components, which are not conditional on the input. This limits their representation power and efficiency. To resolve these issues, we present the first study integrating Mixture of Experts (MoE) for identifying malicious traffic. Specifically, we use network traffic data and convert the 1D array of features into a 2D matrix. Next, we pass this matrix through convolutional neural network (CNN) layers followed by batch normalization and max pooling layers. After obtaining the representation vector via the CNN layers, a sparsely gated MoE layer is used. This layer consists of a set of experts (dense layers) and a router, where the router assigns weights to the output of each expert. Sparsity is achieved by choosing the most relevant experts of the total ones. Finally, we perform a series of ablation experiments to prove the effectiveness of our proposed model. Experiments are conducted on the 5G-NIDD dataset, a network intrusion detection dataset generated from a real 5G test network. Results show that our introduced approach reaches weighted F1-score up to 99.95% achieving comparable performance to existing approaches. Findings also show that our proposed model achieves multiple advantages over state-of-the-art approaches.","sentences":["The advent of 6G/NextG networks comes along with a series of benefits, including extreme capacity, reliability, and efficiency.","However, these networks may become vulnerable to new security threats.","Therefore, 6G/NextG networks must be equipped with advanced Artificial Intelligence algorithms, in order to evade these attacks.","Existing studies on the intrusion detection task rely on the train of shallow machine learning classifiers, including Logistic Regression, Decision Trees, and so on, yielding suboptimal performance.","Others are based on deep neural networks consisting of static components, which are not conditional on the input.","This limits their representation power and efficiency.","To resolve these issues, we present the first study integrating Mixture of Experts (MoE) for identifying malicious traffic.","Specifically, we use network traffic data and convert the 1D array of features into a 2D matrix.","Next, we pass this matrix through convolutional neural network (CNN) layers followed by batch normalization and max pooling layers.","After obtaining the representation vector via the CNN layers, a sparsely gated MoE layer is used.","This layer consists of a set of experts (dense layers) and a router, where the router assigns weights to the output of each expert.","Sparsity is achieved by choosing the most relevant experts of the total ones.","Finally, we perform a series of ablation experiments to prove the effectiveness of our proposed model.","Experiments are conducted on the 5G-NIDD dataset, a network intrusion detection dataset generated from a real 5G test network.","Results show that our introduced approach reaches weighted F1-score up to 99.95% achieving comparable performance to existing approaches.","Findings also show that our proposed model achieves multiple advantages over state-of-the-art approaches."],"url":"http://arxiv.org/abs/2412.03483v1"}
{"created":"2024-12-04 16:59:49","title":"Urban4D: Semantic-Guided 4D Gaussian Splatting for Urban Scene Reconstruction","abstract":"Reconstructing dynamic urban scenes presents significant challenges due to their intrinsic geometric structures and spatiotemporal dynamics. Existing methods that attempt to model dynamic urban scenes without leveraging priors on potentially moving regions often produce suboptimal results. Meanwhile, approaches based on manual 3D annotations yield improved reconstruction quality but are impractical due to labor-intensive labeling. In this paper, we revisit the potential of 2D semantic maps for classifying dynamic and static Gaussians and integrating spatial and temporal dimensions for urban scene representation. We introduce Urban4D, a novel framework that employs a semantic-guided decomposition strategy inspired by advances in deep 2D semantic map generation. Our approach distinguishes potentially dynamic objects through reliable semantic Gaussians. To explicitly model dynamic objects, we propose an intuitive and effective 4D Gaussian splatting (4DGS) representation that aggregates temporal information through learnable time embeddings for each Gaussian, predicting their deformations at desired timestamps using a multilayer perceptron (MLP). For more accurate static reconstruction, we also design a k-nearest neighbor (KNN)-based consistency regularization to handle the ground surface due to its low-texture characteristic. Extensive experiments on real-world datasets demonstrate that Urban4D not only achieves comparable or better quality than previous state-of-the-art methods but also effectively captures dynamic objects while maintaining high visual fidelity for static elements.","sentences":["Reconstructing dynamic urban scenes presents significant challenges due to their intrinsic geometric structures and spatiotemporal dynamics.","Existing methods that attempt to model dynamic urban scenes without leveraging priors on potentially moving regions often produce suboptimal results.","Meanwhile, approaches based on manual 3D annotations yield improved reconstruction quality but are impractical due to labor-intensive labeling.","In this paper, we revisit the potential of 2D semantic maps for classifying dynamic and static Gaussians and integrating spatial and temporal dimensions for urban scene representation.","We introduce Urban4D, a novel framework that employs a semantic-guided decomposition strategy inspired by advances in deep 2D semantic map generation.","Our approach distinguishes potentially dynamic objects through reliable semantic Gaussians.","To explicitly model dynamic objects, we propose an intuitive and effective 4D Gaussian splatting (4DGS) representation that aggregates temporal information through learnable time embeddings for each Gaussian, predicting their deformations at desired timestamps using a multilayer perceptron (MLP).","For more accurate static reconstruction, we also design a k-nearest neighbor (KNN)-based consistency regularization to handle the ground surface due to its low-texture characteristic.","Extensive experiments on real-world datasets demonstrate that Urban4D not only achieves comparable or better quality than previous state-of-the-art methods but also effectively captures dynamic objects while maintaining high visual fidelity for static elements."],"url":"http://arxiv.org/abs/2412.03473v1"}
{"created":"2024-12-04 16:59:44","title":"Measure Anything: Real-time, Multi-stage Vision-based Dimensional Measurement using Segment Anything","abstract":"We present Measure Anything, a comprehensive vision-based framework for dimensional measurement of objects with circular cross-sections, leveraging the Segment Anything Model (SAM). Our approach estimates key geometric features -- including diameter, length, and volume -- for rod-like geometries with varying curvature and general objects with constant skeleton slope. The framework integrates segmentation, mask processing, skeleton construction, and 2D-3D transformation, packaged in a user-friendly interface. We validate our framework by estimating the diameters of Canola stems -- collected from agricultural fields in North Dakota -- which are thin and non-uniform, posing challenges for existing methods. Measuring its diameters is critical, as it is a phenotypic traits that correlates with the health and yield of Canola crops. This application also exemplifies the potential of Measure Anything, where integrating intelligent models -- such as keypoint detection -- extends its scalability to fully automate the measurement process for high-throughput applications. Furthermore, we showcase its versatility in robotic grasping, leveraging extracted geometric features to identify optimal grasp points.","sentences":["We present Measure Anything, a comprehensive vision-based framework for dimensional measurement of objects with circular cross-sections, leveraging the Segment Anything Model (SAM).","Our approach estimates key geometric features -- including diameter, length, and volume -- for rod-like geometries with varying curvature and general objects with constant skeleton slope.","The framework integrates segmentation, mask processing, skeleton construction, and 2D-3D transformation, packaged in a user-friendly interface.","We validate our framework by estimating the diameters of Canola stems -- collected from agricultural fields in North Dakota -- which are thin and non-uniform, posing challenges for existing methods.","Measuring its diameters is critical, as it is a phenotypic traits that correlates with the health and yield of Canola crops.","This application also exemplifies the potential of Measure Anything, where integrating intelligent models -- such as keypoint detection -- extends its scalability to fully automate the measurement process for high-throughput applications.","Furthermore, we showcase its versatility in robotic grasping, leveraging extracted geometric features to identify optimal grasp points."],"url":"http://arxiv.org/abs/2412.03472v1"}
{"created":"2024-12-04 16:59:37","title":"Cluster Specific Representation Learning","abstract":"Representation learning aims to extract meaningful lower-dimensional embeddings from data, known as representations. Despite its widespread application, there is no established definition of a ``good'' representation. Typically, the representation quality is evaluated based on its performance in downstream tasks such as clustering, de-noising, etc. However, this task-specific approach has a limitation where a representation that performs well for one task may not necessarily be effective for another. This highlights the need for a more agnostic formulation, which is the focus of our work. We propose a downstream-agnostic formulation: when inherent clusters exist in the data, the representations should be specific to each cluster. Under this idea, we develop a meta-algorithm that jointly learns cluster-specific representations and cluster assignments. As our approach is easy to integrate with any representation learning framework, we demonstrate its effectiveness in various setups, including Autoencoders, Variational Autoencoders, Contrastive learning models, and Restricted Boltzmann Machines. We qualitatively compare our cluster-specific embeddings to standard embeddings and downstream tasks such as de-noising and clustering. While our method slightly increases runtime and parameters compared to the standard model, the experiments clearly show that it extracts the inherent cluster structures in the data, resulting in improved performance in relevant applications.","sentences":["Representation learning aims to extract meaningful lower-dimensional embeddings from data, known as representations.","Despite its widespread application, there is no established definition of a ``good'' representation.","Typically, the representation quality is evaluated based on its performance in downstream tasks such as clustering, de-noising, etc.","However, this task-specific approach has a limitation where a representation that performs well for one task may not necessarily be effective for another.","This highlights the need for a more agnostic formulation, which is the focus of our work.","We propose a downstream-agnostic formulation: when inherent clusters exist in the data, the representations should be specific to each cluster.","Under this idea, we develop a meta-algorithm that jointly learns cluster-specific representations and cluster assignments.","As our approach is easy to integrate with any representation learning framework, we demonstrate its effectiveness in various setups, including Autoencoders, Variational Autoencoders, Contrastive learning models, and Restricted Boltzmann Machines.","We qualitatively compare our cluster-specific embeddings to standard embeddings and downstream tasks such as de-noising and clustering.","While our method slightly increases runtime and parameters compared to the standard model, the experiments clearly show that it extracts the inherent cluster structures in the data, resulting in improved performance in relevant applications."],"url":"http://arxiv.org/abs/2412.03471v1"}
{"created":"2024-12-04 16:56:20","title":"Training-Free Mitigation of Language Reasoning Degradation After Multimodal Instruction Tuning","abstract":"Multimodal models typically combine a powerful large language model (LLM) with a vision encoder and are then trained on multimodal data via instruction tuning. While this process adapts LLMs to multimodal settings, it remains unclear whether this adaptation compromises their original language reasoning capabilities. In this work, we explore the effects of multimodal instruction tuning on language reasoning performance. We focus on LLaVA, a leading multimodal framework that integrates LLMs such as Vicuna or Mistral with the CLIP vision encoder. We compare the performance of the original LLMs with their multimodal-adapted counterparts across eight language reasoning tasks. Our experiments yield several key insights. First, the impact of multimodal learning varies between Vicuna and Mistral: we observe a degradation in language reasoning for Mistral but improvements for Vicuna across most tasks. Second, while multimodal instruction learning consistently degrades performance on mathematical reasoning tasks (e.g., GSM8K), it enhances performance on commonsense reasoning tasks (e.g., CommonsenseQA). Finally, we demonstrate that a training-free model merging technique can effectively mitigate the language reasoning degradation observed in multimodal-adapted Mistral and even improve performance on visual tasks.","sentences":["Multimodal models typically combine a powerful large language model (LLM) with a vision encoder and are then trained on multimodal data via instruction tuning.","While this process adapts LLMs to multimodal settings, it remains unclear whether this adaptation compromises their original language reasoning capabilities.","In this work, we explore the effects of multimodal instruction tuning on language reasoning performance.","We focus on LLaVA, a leading multimodal framework that integrates LLMs such as Vicuna or Mistral with the CLIP vision encoder.","We compare the performance of the original LLMs with their multimodal-adapted counterparts across eight language reasoning tasks.","Our experiments yield several key insights.","First, the impact of multimodal learning varies between Vicuna and Mistral: we observe a degradation in language reasoning for Mistral but improvements for Vicuna across most tasks.","Second, while multimodal instruction learning consistently degrades performance on mathematical reasoning tasks (e.g., GSM8K), it enhances performance on commonsense reasoning tasks (e.g., CommonsenseQA).","Finally, we demonstrate that a training-free model merging technique can effectively mitigate the language reasoning degradation observed in multimodal-adapted Mistral and even improve performance on visual tasks."],"url":"http://arxiv.org/abs/2412.03467v1"}
{"created":"2024-12-04 16:54:58","title":"YT-30M: A multi-lingual multi-category dataset of YouTube comments","abstract":"This paper introduces two large-scale multilingual comment datasets, YT-30M (and YT-100K) from YouTube. The analysis in this paper is performed on a smaller sample (YT-100K) of YT-30M. Both the datasets: YT-30M (full) and YT-100K (randomly selected 100K sample from YT-30M) are publicly released for further research. YT-30M (YT-100K) contains 32236173 (108694) comments posted by YouTube channel that belong to YouTube categories. Each comment is associated with a video ID, comment ID, commentor name, commentor channel ID, comment text, upvotes, original channel ID and category of the YouTube channel (e.g., 'News & Politics', 'Science & Technology', etc.).","sentences":["This paper introduces two large-scale multilingual comment datasets, YT-30M (and YT-100K) from YouTube.","The analysis in this paper is performed on a smaller sample (YT-100K) of YT-30M. Both the datasets: YT-30M (full) and YT-100K (randomly selected 100K sample from YT-30M) are publicly released for further research.","YT-30M (YT-100K) contains 32236173 (108694) comments posted by YouTube channel that belong to YouTube categories.","Each comment is associated with a video ID, comment ID, commentor name, commentor channel ID, comment text, upvotes, original channel ID and category of the YouTube channel (e.g., 'News & Politics', 'Science & Technology', etc.)."],"url":"http://arxiv.org/abs/2412.03465v1"}
{"created":"2024-12-04 16:51:59","title":"Multi-Momentum Observer Contact Estimation for Bipedal Robots","abstract":"As bipedal robots become more and more popular in commercial and industrial settings, the ability to control them with a high degree of reliability is critical. To that end, this paper considers how to accurately estimate which feet are currently in contact with the ground so as to avoid improper control actions that could jeopardize the stability of the robot. Additionally, modern algorithms for estimating the position and orientation of a robot's base frame rely heavily on such contact mode estimates. Dedicated contact sensors on the feet can be used to estimate this contact mode, but these sensors are prone to noise, time delays, damage/yielding from repeated impacts with the ground, and are not available on every robot. To overcome these limitations, we propose a momentum observer based method for contact mode estimation that does not rely on such contact sensors. Often, momentum observers assume that the robot's base frame can be treated as an inertial frame. However, since many humanoids' legs represent a significant portion of the overall mass, the proposed method instead utilizes multiple simultaneous dynamic models. Each of these models assumes a different contact condition. A given contact assumption is then used to constrain the full dynamics in order to avoid assuming that either the body is an inertial frame or that a fully accurate estimate of body velocity is known. The (dis)agreement between each model's estimates and measurements is used to determine which contact mode is most likely using a Markov-style fusion method. The proposed method produces contact detection accuracy of up to 98.44% with a low noise simulation and 77.12% when utilizing data collect on the Sarcos Guardian XO robot (a hybrid humanoid/exoskeleton).","sentences":["As bipedal robots become more and more popular in commercial and industrial settings, the ability to control them with a high degree of reliability is critical.","To that end, this paper considers how to accurately estimate which feet are currently in contact with the ground so as to avoid improper control actions that could jeopardize the stability of the robot.","Additionally, modern algorithms for estimating the position and orientation of a robot's base frame rely heavily on such contact mode estimates.","Dedicated contact sensors on the feet can be used to estimate this contact mode, but these sensors are prone to noise, time delays, damage/yielding from repeated impacts with the ground, and are not available on every robot.","To overcome these limitations, we propose a momentum observer based method for contact mode estimation that does not rely on such contact sensors.","Often, momentum observers assume that the robot's base frame can be treated as an inertial frame.","However, since many humanoids' legs represent a significant portion of the overall mass, the proposed method instead utilizes multiple simultaneous dynamic models.","Each of these models assumes a different contact condition.","A given contact assumption is then used to constrain the full dynamics in order to avoid assuming that either the body is an inertial frame or that a fully accurate estimate of body velocity is known.","The (dis)agreement between each model's estimates and measurements is used to determine which contact mode is most likely using a Markov-style fusion method.","The proposed method produces contact detection accuracy of up to 98.44% with a low noise simulation and 77.12% when utilizing data collect on the Sarcos Guardian XO robot (a hybrid humanoid/exoskeleton)."],"url":"http://arxiv.org/abs/2412.03462v1"}
{"created":"2024-12-04 16:45:02","title":"Gesture Classification in Artworks Using Contextual Image Features","abstract":"Recognizing gestures in artworks can add a valuable dimension to art understanding and help to acknowledge the role of the sense of smell in cultural heritage. We propose a method to recognize smell gestures in historical artworks. We show that combining local features with global image context improves classification performance notably on different backbones.","sentences":["Recognizing gestures in artworks can add a valuable dimension to art understanding and help to acknowledge the role of the sense of smell in cultural heritage.","We propose a method to recognize smell gestures in historical artworks.","We show that combining local features with global image context improves classification performance notably on different backbones."],"url":"http://arxiv.org/abs/2412.03456v1"}
{"created":"2024-12-04 16:40:56","title":"Pre-trained Multiple Latent Variable Generative Models are good defenders against Adversarial Attacks","abstract":"Attackers can deliberately perturb classifiers' input with subtle noise, altering final predictions. Among proposed countermeasures, adversarial purification employs generative networks to preprocess input images, filtering out adversarial noise. In this study, we propose specific generators, defined Multiple Latent Variable Generative Models (MLVGMs), for adversarial purification. These models possess multiple latent variables that naturally disentangle coarse from fine features. Taking advantage of these properties, we autoencode images to maintain class-relevant information, while discarding and re-sampling any detail, including adversarial noise. The procedure is completely training-free, exploring the generalization abilities of pre-trained MLVGMs on the adversarial purification downstream task. Despite the lack of large models, trained on billions of samples, we show that smaller MLVGMs are already competitive with traditional methods, and can be used as foundation models. Official code released at https://github.com/SerezD/gen_adversarial.","sentences":["Attackers can deliberately perturb classifiers' input with subtle noise, altering final predictions.","Among proposed countermeasures, adversarial purification employs generative networks to preprocess input images, filtering out adversarial noise.","In this study, we propose specific generators, defined Multiple Latent Variable Generative Models (MLVGMs), for adversarial purification.","These models possess multiple latent variables that naturally disentangle coarse from fine features.","Taking advantage of these properties, we autoencode images to maintain class-relevant information, while discarding and re-sampling any detail, including adversarial noise.","The procedure is completely training-free, exploring the generalization abilities of pre-trained MLVGMs on the adversarial purification downstream task.","Despite the lack of large models, trained on billions of samples, we show that smaller MLVGMs are already competitive with traditional methods, and can be used as foundation models.","Official code released at https://github.com/SerezD/gen_adversarial."],"url":"http://arxiv.org/abs/2412.03453v1"}
{"created":"2024-12-04 16:38:07","title":"PlanarSplatting: Accurate Planar Surface Reconstruction in 3 Minutes","abstract":"This paper presents PlanarSplatting, an ultra-fast and accurate surface reconstruction approach for multiview indoor images. We take the 3D planes as the main objective due to their compactness and structural expressiveness in indoor scenes, and develop an explicit optimization framework that learns to fit the expected surface of indoor scenes by splatting the 3D planes into 2.5D depth and normal maps. As our PlanarSplatting operates directly on the 3D plane primitives, it eliminates the dependencies on 2D/3D plane detection and plane matching and tracking for planar surface reconstruction. Furthermore, the essential merits of plane-based representation plus CUDA-based implementation of planar splatting functions, PlanarSplatting reconstructs an indoor scene in 3 minutes while having significantly better geometric accuracy. Thanks to our ultra-fast reconstruction speed, the largest quantitative evaluation on the ScanNet and ScanNet++ datasets over hundreds of scenes clearly demonstrated the advantages of our method. We believe that our accurate and ultrafast planar surface reconstruction method will be applied in the structured data curation for surface reconstruction in the future. The code of our CUDA implementation will be publicly available. Project page: https://icetttb.github.io/PlanarSplatting/","sentences":["This paper presents PlanarSplatting, an ultra-fast and accurate surface reconstruction approach for multiview indoor images.","We take the 3D planes as the main objective due to their compactness and structural expressiveness in indoor scenes, and develop an explicit optimization framework that learns to fit the expected surface of indoor scenes by splatting the 3D planes into 2.5D depth and normal maps.","As our PlanarSplatting operates directly on the 3D plane primitives, it eliminates the dependencies on 2D/3D plane detection and plane matching and tracking for planar surface reconstruction.","Furthermore, the essential merits of plane-based representation plus CUDA-based implementation of planar splatting functions, PlanarSplatting reconstructs an indoor scene in 3 minutes while having significantly better geometric accuracy.","Thanks to our ultra-fast reconstruction speed, the largest quantitative evaluation on the ScanNet and ScanNet++ datasets over hundreds of scenes clearly demonstrated the advantages of our method.","We believe that our accurate and ultrafast planar surface reconstruction method will be applied in the structured data curation for surface reconstruction in the future.","The code of our CUDA implementation will be publicly available.","Project page: https://icetttb.github.io/PlanarSplatting/"],"url":"http://arxiv.org/abs/2412.03451v1"}
{"created":"2024-12-04 16:34:35","title":"From Words to Workflows: Automating Business Processes","abstract":"As businesses increasingly rely on automation to streamline operations, the limitations of Robotic Process Automation (RPA) have become apparent, particularly its dependence on expert knowledge and inability to handle complex decision-making tasks. Recent advancements in Artificial Intelligence (AI), particularly Generative AI (GenAI) and Large Language Models (LLMs), have paved the way for Intelligent Automation (IA), which integrates cognitive capabilities to overcome the shortcomings of RPA. This paper introduces Text2Workflow, a novel method that automatically generates workflows from natural language user requests. Unlike traditional automation approaches, Text2Workflow offers a generalized solution for automating any business process, translating user inputs into a sequence of executable steps represented in JavaScript Object Notation (JSON) format. Leveraging the decision-making and instruction-following capabilities of LLMs, this method provides a scalable, adaptable framework that enables users to visualize and execute workflows with minimal manual intervention. This research outlines the Text2Workflow methodology and its broader implications for automating complex business processes.","sentences":["As businesses increasingly rely on automation to streamline operations, the limitations of Robotic Process Automation (RPA) have become apparent, particularly its dependence on expert knowledge and inability to handle complex decision-making tasks.","Recent advancements in Artificial Intelligence (AI), particularly Generative AI (GenAI) and Large Language Models (LLMs), have paved the way for Intelligent Automation (IA), which integrates cognitive capabilities to overcome the shortcomings of RPA.","This paper introduces Text2Workflow, a novel method that automatically generates workflows from natural language user requests.","Unlike traditional automation approaches, Text2Workflow offers a generalized solution for automating any business process, translating user inputs into a sequence of executable steps represented in JavaScript Object Notation (JSON) format.","Leveraging the decision-making and instruction-following capabilities of LLMs, this method provides a scalable, adaptable framework that enables users to visualize and execute workflows with minimal manual intervention.","This research outlines the Text2Workflow methodology and its broader implications for automating complex business processes."],"url":"http://arxiv.org/abs/2412.03446v1"}
{"created":"2024-12-04 16:30:35","title":"State Frequency Estimation for Anomaly Detection","abstract":"Many works have studied the efficacy of state machines for detecting anomalies within NetFlows. These works typically learn a model from unlabeled data and compute anomaly scores for arbitrary traces based on their likelihood of occurrence or how well they fit within the model. However, these methods do not dynamically adapt their scores based on the traces seen at test time. This becomes a problem when an adversary produces seemingly common traces in their attack, causing the model to miss the detection by assigning low anomaly scores. We propose SEQUENT, a new approach that uses the state visit frequency to adapt its scoring for anomaly detection dynamically. SEQUENT subsequently uses the scores to generate root causes for anomalies. These allow the grouping of alarms and simplify the analysis of anomalies. Our evaluation of SEQUENT on three NetFlow datasets indicates that our approach outperforms existing methods, demonstrating its effectiveness in detecting anomalies.","sentences":["Many works have studied the efficacy of state machines for detecting anomalies within NetFlows.","These works typically learn a model from unlabeled data and compute anomaly scores for arbitrary traces based on their likelihood of occurrence or how well they fit within the model.","However, these methods do not dynamically adapt their scores based on the traces seen at test time.","This becomes a problem when an adversary produces seemingly common traces in their attack, causing the model to miss the detection by assigning low anomaly scores.","We propose SEQUENT, a new approach that uses the state visit frequency to adapt its scoring for anomaly detection dynamically.","SEQUENT subsequently uses the scores to generate root causes for anomalies.","These allow the grouping of alarms and simplify the analysis of anomalies.","Our evaluation of SEQUENT on three NetFlow datasets indicates that our approach outperforms existing methods, demonstrating its effectiveness in detecting anomalies."],"url":"http://arxiv.org/abs/2412.03442v1"}
{"created":"2024-12-04 16:30:03","title":"PBP: Post-training Backdoor Purification for Malware Classifiers","abstract":"In recent years, the rise of machine learning (ML) in cybersecurity has brought new challenges, including the increasing threat of backdoor poisoning attacks on ML malware classifiers. For instance, adversaries could inject malicious samples into public malware repositories, contaminating the training data and potentially misclassifying malware by the ML model. Current countermeasures predominantly focus on detecting poisoned samples by leveraging disagreements within the outputs of a diverse set of ensemble models on training data points. However, these methods are not suitable for scenarios where Machine Learning-as-a-Service (MLaaS) is used or when users aim to remove backdoors from a model after it has been trained. Addressing this scenario, we introduce PBP, a post-training defense for malware classifiers that mitigates various types of backdoor embeddings without assuming any specific backdoor embedding mechanism. Our method exploits the influence of backdoor attacks on the activation distribution of neural networks, independent of the trigger-embedding method. In the presence of a backdoor attack, the activation distribution of each layer is distorted into a mixture of distributions. By regulating the statistics of the batch normalization layers, we can guide a backdoored model to perform similarly to a clean one. Our method demonstrates substantial advantages over several state-of-the-art methods, as evidenced by experiments on two datasets, two types of backdoor methods, and various attack configurations. Notably, our approach requires only a small portion of the training data -- only 1\\% -- to purify the backdoor and reduce the attack success rate from 100\\% to almost 0\\%, a 100-fold improvement over the baseline methods. Our code is available at \\url{https://github.com/judydnguyen/pbp-backdoor-purification-official}.","sentences":["In recent years, the rise of machine learning (ML) in cybersecurity has brought new challenges, including the increasing threat of backdoor poisoning attacks on ML malware classifiers.","For instance, adversaries could inject malicious samples into public malware repositories, contaminating the training data and potentially misclassifying malware by the ML model.","Current countermeasures predominantly focus on detecting poisoned samples by leveraging disagreements within the outputs of a diverse set of ensemble models on training data points.","However, these methods are not suitable for scenarios where Machine Learning-as-a-Service (MLaaS) is used or when users aim to remove backdoors from a model after it has been trained.","Addressing this scenario, we introduce PBP, a post-training defense for malware classifiers that mitigates various types of backdoor embeddings without assuming any specific backdoor embedding mechanism.","Our method exploits the influence of backdoor attacks on the activation distribution of neural networks, independent of the trigger-embedding method.","In the presence of a backdoor attack, the activation distribution of each layer is distorted into a mixture of distributions.","By regulating the statistics of the batch normalization layers, we can guide a backdoored model to perform similarly to a clean one.","Our method demonstrates substantial advantages over several state-of-the-art methods, as evidenced by experiments on two datasets, two types of backdoor methods, and various attack configurations.","Notably, our approach requires only a small portion of the training data -- only 1\\% -- to purify the backdoor and reduce the attack success rate from 100\\% to almost 0\\%, a 100-fold improvement over the baseline methods.","Our code is available at \\url{https://github.com/judydnguyen/pbp-backdoor-purification-official}."],"url":"http://arxiv.org/abs/2412.03441v1"}
{"created":"2024-12-04 16:29:04","title":"CleanDIFT: Diffusion Features without Noise","abstract":"Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks. Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise. We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises. We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features. We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost.","sentences":["Internal features from large-scale pre-trained diffusion models have recently been established as powerful semantic descriptors for a wide range of downstream tasks.","Works that use these features generally need to add noise to images before passing them through the model to obtain the semantic features, as the models do not offer the most useful features when given images with little to no noise.","We show that this noise has a critical impact on the usefulness of these features that cannot be remedied by ensembling with different random noises.","We address this issue by introducing a lightweight, unsupervised fine-tuning method that enables diffusion backbones to provide high-quality, noise-free semantic features.","We show that these features readily outperform previous diffusion features by a wide margin in a wide variety of extraction setups and downstream tasks, offering better performance than even ensemble-based methods at a fraction of the cost."],"url":"http://arxiv.org/abs/2412.03439v1"}
{"created":"2024-12-04 16:26:17","title":"BIMCaP: BIM-based AI-supported LiDAR-Camera Pose Refinement","abstract":"This paper introduces BIMCaP, a novel method to integrate mobile 3D sparse LiDAR data and camera measurements with pre-existing building information models (BIMs), enhancing fast and accurate indoor mapping with affordable sensors. BIMCaP refines sensor poses by leveraging a 3D BIM and employing a bundle adjustment technique to align real-world measurements with the model. Experiments using real-world open-access data show that BIMCaP achieves superior accuracy, reducing translational error by over 4 cm compared to current state-of-the-art methods. This advancement enhances the accuracy and cost-effectiveness of 3D mapping methodologies like SLAM. BIMCaP's improvements benefit various fields, including construction site management and emergency response, by providing up-to-date, aligned digital maps for better decision-making and productivity. Link to the repository: https://github.com/MigVega/BIMCaP","sentences":["This paper introduces BIMCaP, a novel method to integrate mobile 3D sparse LiDAR data and camera measurements with pre-existing building information models (BIMs), enhancing fast and accurate indoor mapping with affordable sensors.","BIMCaP refines sensor poses by leveraging a 3D BIM and employing a bundle adjustment technique to align real-world measurements with the model.","Experiments using real-world open-access data show that BIMCaP achieves superior accuracy, reducing translational error by over 4 cm compared to current state-of-the-art methods.","This advancement enhances the accuracy and cost-effectiveness of 3D mapping methodologies like SLAM.","BIMCaP's improvements benefit various fields, including construction site management and emergency response, by providing up-to-date, aligned digital maps for better decision-making and productivity.","Link to the repository: https://github.com/MigVega/BIMCaP"],"url":"http://arxiv.org/abs/2412.03434v1"}
{"created":"2024-12-04 16:24:41","title":"Genetic Algorithm Based System for Path Planning with Unmanned Aerial Vehicles Swarms in Cell-Grid Environments","abstract":"Path Planning methods for autonomously controlling swarms of unmanned aerial vehicles (UAVs) are gaining momentum due to their operational advantages. An increasing number of scenarios now require autonomous control of multiple UAVs, as autonomous operation can significantly reduce labor costs. Additionally, obtaining optimal flight paths can lower energy consumption, thereby extending battery life for other critical operations. Many of these scenarios, however, involve obstacles such as power lines and trees, which complicate Path Planning. This paper presents an evolutionary computation-based system employing genetic algorithms to address this problem in environments with obstacles. The proposed approach aims to ensure complete coverage of areas with fixed obstacles, such as in field exploration tasks, while minimizing flight time regardless of map size or the number of UAVs in the swarm. No specific goal points or prior information beyond the provided map is required. The experiments conducted in this study used five maps of varying sizes and obstacle densities, as well as a control map without obstacles, with different numbers of UAVs. The results demonstrate that this method can determine optimal paths for all UAVs during full map traversal, thus minimizing resource consumption. A comparative analysis with other state-of-the-art approach is presented to highlight the advantages and potential limitations of the proposed method.","sentences":["Path Planning methods for autonomously controlling swarms of unmanned aerial vehicles (UAVs) are gaining momentum due to their operational advantages.","An increasing number of scenarios now require autonomous control of multiple UAVs, as autonomous operation can significantly reduce labor costs.","Additionally, obtaining optimal flight paths can lower energy consumption, thereby extending battery life for other critical operations.","Many of these scenarios, however, involve obstacles such as power lines and trees, which complicate Path Planning.","This paper presents an evolutionary computation-based system employing genetic algorithms to address this problem in environments with obstacles.","The proposed approach aims to ensure complete coverage of areas with fixed obstacles, such as in field exploration tasks, while minimizing flight time regardless of map size or the number of UAVs in the swarm.","No specific goal points or prior information beyond the provided map is required.","The experiments conducted in this study used five maps of varying sizes and obstacle densities, as well as a control map without obstacles, with different numbers of UAVs.","The results demonstrate that this method can determine optimal paths for all UAVs during full map traversal, thus minimizing resource consumption.","A comparative analysis with other state-of-the-art approach is presented to highlight the advantages and potential limitations of the proposed method."],"url":"http://arxiv.org/abs/2412.03433v1"}
{"created":"2024-12-04 16:19:47","title":"SINGER: Vivid Audio-driven Singing Video Generation with Multi-scale Spectral Diffusion Model","abstract":"Recent advancements in generative models have significantly enhanced talking face video generation, yet singing video generation remains underexplored. The differences between human talking and singing limit the performance of existing talking face video generation models when applied to singing. The fundamental differences between talking and singing-specifically in audio characteristics and behavioral expressions-limit the effectiveness of existing models. We observe that the differences between singing and talking audios manifest in terms of frequency and amplitude. To address this, we have designed a multi-scale spectral module to help the model learn singing patterns in the spectral domain. Additionally, we develop a spectral-filtering module that aids the model in learning the human behaviors associated with singing audio. These two modules are integrated into the diffusion model to enhance singing video generation performance, resulting in our proposed model, SINGER. Furthermore, the lack of high-quality real-world singing face videos has hindered the development of the singing video generation community. To address this gap, we have collected an in-the-wild audio-visual singing dataset to facilitate research in this area. Our experiments demonstrate that SINGER is capable of generating vivid singing videos and outperforms state-of-the-art methods in both objective and subjective evaluations.","sentences":["Recent advancements in generative models have significantly enhanced talking face video generation, yet singing video generation remains underexplored.","The differences between human talking and singing limit the performance of existing talking face video generation models when applied to singing.","The fundamental differences between talking and singing-specifically in audio characteristics and behavioral expressions-limit the effectiveness of existing models.","We observe that the differences between singing and talking audios manifest in terms of frequency and amplitude.","To address this, we have designed a multi-scale spectral module to help the model learn singing patterns in the spectral domain.","Additionally, we develop a spectral-filtering module that aids the model in learning the human behaviors associated with singing audio.","These two modules are integrated into the diffusion model to enhance singing video generation performance, resulting in our proposed model, SINGER.","Furthermore, the lack of high-quality real-world singing face videos has hindered the development of the singing video generation community.","To address this gap, we have collected an in-the-wild audio-visual singing dataset to facilitate research in this area.","Our experiments demonstrate that SINGER is capable of generating vivid singing videos and outperforms state-of-the-art methods in both objective and subjective evaluations."],"url":"http://arxiv.org/abs/2412.03430v1"}
{"created":"2024-12-04 16:17:47","title":"2DGS-Room: Seed-Guided 2D Gaussian Splatting with Geometric Constrains for High-Fidelity Indoor Scene Reconstruction","abstract":"The reconstruction of indoor scenes remains challenging due to the inherent complexity of spatial structures and the prevalence of textureless regions. Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable performance in surface reconstruction. In this paper, we introduce 2DGS-Room, a novel method leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction. Specifically, we employ a seed-guided mechanism to control the distribution of 2D Gaussians, with the density of seed points dynamically optimized through adaptive growth and pruning mechanisms. To further improve geometric accuracy, we incorporate monocular depth and normal priors to provide constraints for details and textureless regions respectively. Additionally, multi-view consistency constraints are employed to mitigate artifacts and further enhance reconstruction quality. Extensive experiments on ScanNet and ScanNet++ datasets demonstrate that our method achieves state-of-the-art performance in indoor scene reconstruction.","sentences":["The reconstruction of indoor scenes remains challenging due to the inherent complexity of spatial structures and the prevalence of textureless regions.","Recent advancements in 3D Gaussian Splatting have improved novel view synthesis with accelerated processing but have yet to deliver comparable performance in surface reconstruction.","In this paper, we introduce 2DGS-Room, a novel method leveraging 2D Gaussian Splatting for high-fidelity indoor scene reconstruction.","Specifically, we employ a seed-guided mechanism to control the distribution of 2D Gaussians, with the density of seed points dynamically optimized through adaptive growth and pruning mechanisms.","To further improve geometric accuracy, we incorporate monocular depth and normal priors to provide constraints for details and textureless regions respectively.","Additionally, multi-view consistency constraints are employed to mitigate artifacts and further enhance reconstruction quality.","Extensive experiments on ScanNet and ScanNet++ datasets demonstrate that our method achieves state-of-the-art performance in indoor scene reconstruction."],"url":"http://arxiv.org/abs/2412.03428v1"}
{"created":"2024-12-04 16:17:09","title":"Assessing Foundation Models' Transferability to Physiological Signals in Precision Medicine","abstract":"The success of precision medicine requires computational models that can effectively process and interpret diverse physiological signals across heterogeneous patient populations. While foundation models have demonstrated remarkable transfer capabilities across various domains, their effectiveness in handling individual-specific physiological signals - crucial for precision medicine - remains largely unexplored. This work introduces a systematic pipeline for rapidly and efficiently evaluating foundation models' transfer capabilities in medical contexts. Our pipeline employs a three-stage approach. First, it leverages physiological simulation software to generate diverse, clinically relevant scenarios, particularly focusing on data-scarce medical conditions. This simulation-based approach enables both targeted capability assessment and subsequent model fine-tuning. Second, the pipeline projects these simulated signals through the foundation model to obtain embeddings, which are then evaluated using linear methods. This evaluation quantifies the model's ability to capture three critical aspects: physiological feature independence, temporal dynamics preservation, and medical scenario differentiation. Finally, the pipeline validates these representations through specific downstream medical tasks. Initial testing of our pipeline on the Moirai time series foundation model revealed significant limitations in physiological signal processing, including feature entanglement, temporal dynamics distortion, and reduced scenario discrimination. These findings suggest that current foundation models may require substantial architectural modifications or targeted fine-tuning before deployment in clinical settings.","sentences":["The success of precision medicine requires computational models that can effectively process and interpret diverse physiological signals across heterogeneous patient populations.","While foundation models have demonstrated remarkable transfer capabilities across various domains, their effectiveness in handling individual-specific physiological signals - crucial for precision medicine - remains largely unexplored.","This work introduces a systematic pipeline for rapidly and efficiently evaluating foundation models' transfer capabilities in medical contexts.","Our pipeline employs a three-stage approach.","First, it leverages physiological simulation software to generate diverse, clinically relevant scenarios, particularly focusing on data-scarce medical conditions.","This simulation-based approach enables both targeted capability assessment and subsequent model fine-tuning.","Second, the pipeline projects these simulated signals through the foundation model to obtain embeddings, which are then evaluated using linear methods.","This evaluation quantifies the model's ability to capture three critical aspects: physiological feature independence, temporal dynamics preservation, and medical scenario differentiation.","Finally, the pipeline validates these representations through specific downstream medical tasks.","Initial testing of our pipeline on the Moirai time series foundation model revealed significant limitations in physiological signal processing, including feature entanglement, temporal dynamics distortion, and reduced scenario discrimination.","These findings suggest that current foundation models may require substantial architectural modifications or targeted fine-tuning before deployment in clinical settings."],"url":"http://arxiv.org/abs/2412.03427v1"}
{"created":"2024-12-04 16:14:02","title":"Tango*: Constrained synthesis planning using chemically informed value functions","abstract":"Computer-aided synthesis planning (CASP) has made significant strides in generating retrosynthetic pathways for simple molecules in a non-constrained fashion. Recent work introduces a specialised bidirectional search algorithm with forward and retro expansion to address the starting material-constrained synthesis problem, allowing CASP systems to provide synthesis pathways from specified starting materials, such as waste products or renewable feed-stocks. In this work, we introduce a simple guided search which allows solving the starting material-constrained synthesis planning problem using an existing, uni-directional search algorithm, Retro*. We show that by optimising a single hyperparameter, Tango* outperforms existing methods in terms of efficiency and solve rate. We find the Tango* cost function catalyses strong improvements for the bidirectional DESP methods. Our method also achieves lower wall clock times while proposing synthetic routes of similar length, a common metric for route quality. Finally, we highlight potential reasons for the strong performance of Tango over neural guided search methods","sentences":["Computer-aided synthesis planning (CASP) has made significant strides in generating retrosynthetic pathways for simple molecules in a non-constrained fashion.","Recent work introduces a specialised bidirectional search algorithm with forward and retro expansion to address the starting material-constrained synthesis problem, allowing CASP systems to provide synthesis pathways from specified starting materials, such as waste products or renewable feed-stocks.","In this work, we introduce a simple guided search which allows solving the starting material-constrained synthesis planning problem using an existing, uni-directional search algorithm, Retro*.","We show that by optimising a single hyperparameter, Tango* outperforms existing methods in terms of efficiency and solve rate.","We find the Tango* cost function catalyses strong improvements for the bidirectional DESP methods.","Our method also achieves lower wall clock times while proposing synthetic routes of similar length, a common metric for route quality.","Finally, we highlight potential reasons for the strong performance of Tango over neural guided search methods"],"url":"http://arxiv.org/abs/2412.03424v1"}
{"created":"2024-12-04 16:00:14","title":"Automated Test-Case Generation for REST APIs Using Model Inference Search Heuristic","abstract":"The rising popularity of the microservice architectural style has led to a growing demand for automated testing approaches tailored to these systems. EvoMaster is a state-of-the-art tool that uses Evolutionary Algorithms (EAs) to automatically generate test cases for microservices' REST APIs. One limitation of these EAs is the use of unit-level search heuristics, such as branch distances, which focus on fine-grained code coverage and may not effectively capture the complex, interconnected behaviors characteristic of system-level testing. To address this limitation, we propose a new search heuristic (MISH) that uses real-time automaton learning to guide the test case generation process. We capture the sequential call patterns exhibited by a test case by learning an automaton from the stream of log events outputted by different microservices within the same system. Therefore, MISH learns a representation of the systemwide behavior, allowing us to define the fitness of a test case based on the path it traverses within the inferred automaton. We empirically evaluate MISH's effectiveness on six real-world benchmark microservice applications and compare it against a state-of-the-art technique, MOSA, for testing REST APIs. Our evaluation shows promising results for using MISH to guide the automated test case generation within EvoMaster.","sentences":["The rising popularity of the microservice architectural style has led to a growing demand for automated testing approaches tailored to these systems.","EvoMaster is a state-of-the-art tool that uses Evolutionary Algorithms (EAs) to automatically generate test cases for microservices' REST APIs.","One limitation of these EAs is the use of unit-level search heuristics, such as branch distances, which focus on fine-grained code coverage and may not effectively capture the complex, interconnected behaviors characteristic of system-level testing.","To address this limitation, we propose a new search heuristic (MISH) that uses real-time automaton learning to guide the test case generation process.","We capture the sequential call patterns exhibited by a test case by learning an automaton from the stream of log events outputted by different microservices within the same system.","Therefore, MISH learns a representation of the systemwide behavior, allowing us to define the fitness of a test case based on the path it traverses within the inferred automaton.","We empirically evaluate MISH's effectiveness on six real-world benchmark microservice applications and compare it against a state-of-the-art technique, MOSA, for testing REST APIs.","Our evaluation shows promising results for using MISH to guide the automated test case generation within EvoMaster."],"url":"http://arxiv.org/abs/2412.03420v1"}
{"created":"2024-12-04 15:53:45","title":"Learning Semantic Association Rules from Internet of Things Data","abstract":"Association Rule Mining (ARM) is the task of discovering commonalities in data in the form of logical implications. ARM is used in the Internet of Things (IoT) for different tasks including monitoring and decision-making. However, existing methods give limited consideration to IoT-specific requirements such as heterogeneity and volume. Furthermore, they do not utilize important static domain-specific description data about IoT systems, which is increasingly represented as knowledge graphs. In this paper, we propose a novel ARM pipeline for IoT data that utilizes both dynamic sensor data and static IoT system metadata. Furthermore, we propose an Autoencoder-based Neurosymbolic ARM method (Aerial) as part of the pipeline to address the high volume of IoT data and reduce the total number of rules that are resource-intensive to process. Aerial learns a neural representation of a given data and extracts association rules from this representation by exploiting the reconstruction (decoding) mechanism of an autoencoder. Extensive evaluations on 3 IoT datasets from 2 domains show that ARM on both static and dynamic IoT data results in more generically applicable rules while Aerial can learn a more concise set of high-quality association rules than the state-of-the-art with full coverage over the datasets.","sentences":["Association Rule Mining (ARM) is the task of discovering commonalities in data in the form of logical implications.","ARM is used in the Internet of Things (IoT) for different tasks including monitoring and decision-making.","However, existing methods give limited consideration to IoT-specific requirements such as heterogeneity and volume.","Furthermore, they do not utilize important static domain-specific description data about IoT systems, which is increasingly represented as knowledge graphs.","In this paper, we propose a novel ARM pipeline for IoT data that utilizes both dynamic sensor data and static IoT system metadata.","Furthermore, we propose an Autoencoder-based Neurosymbolic ARM method (Aerial) as part of the pipeline to address the high volume of IoT data and reduce the total number of rules that are resource-intensive to process.","Aerial learns a neural representation of a given data and extracts association rules from this representation by exploiting the reconstruction (decoding) mechanism of an autoencoder.","Extensive evaluations on 3 IoT datasets from 2 domains show that ARM on both static and dynamic IoT data results in more generically applicable rules while Aerial can learn a more concise set of high-quality association rules than the state-of-the-art with full coverage over the datasets."],"url":"http://arxiv.org/abs/2412.03417v1"}
{"created":"2024-12-04 15:49:49","title":"Deep Learning for Sea Surface Temperature Reconstruction under Cloud Occlusion","abstract":"Sea Surface Temperature (SST) is crucial for understanding Earth's oceans and climate, significantly influencing weather patterns, ocean currents, marine ecosystem health, and the global energy balance. Large-scale SST monitoring relies on satellite infrared radiation detection, but cloud cover presents a major challenge, creating extensive observational gaps and hampering our ability to fully capture large-scale ocean temperature patterns. Efforts to address these gaps in existing L4 datasets have been made, but they often exhibit notable local and seasonal biases, compromising data reliability and accuracy. To tackle this challenge, we employed deep neural networks to reconstruct cloud-covered portions of satellite imagery while preserving the integrity of observed values in cloud-free areas, using MODIS satellite derived observations of SST. Our best-performing architecture showed significant skill improvements over established methodologies, achieving substantial reductions in error metrics when benchmarked against widely used approaches and datasets. These results underscore the potential of advanced AI techniques to enhance the completeness of satellite observations in Earth-science remote sensing, providing more accurate and reliable datasets for environmental assessments, data-driven model training, climate research, and seamless integration into model data assimilation workflows.","sentences":["Sea Surface Temperature (SST) is crucial for understanding Earth's oceans and climate, significantly influencing weather patterns, ocean currents, marine ecosystem health, and the global energy balance.","Large-scale SST monitoring relies on satellite infrared radiation detection, but cloud cover presents a major challenge, creating extensive observational gaps and hampering our ability to fully capture large-scale ocean temperature patterns.","Efforts to address these gaps in existing L4 datasets have been made, but they often exhibit notable local and seasonal biases, compromising data reliability and accuracy.","To tackle this challenge, we employed deep neural networks to reconstruct cloud-covered portions of satellite imagery while preserving the integrity of observed values in cloud-free areas, using MODIS satellite derived observations of SST.","Our best-performing architecture showed significant skill improvements over established methodologies, achieving substantial reductions in error metrics when benchmarked against widely used approaches and datasets.","These results underscore the potential of advanced AI techniques to enhance the completeness of satellite observations in Earth-science remote sensing, providing more accurate and reliable datasets for environmental assessments, data-driven model training, climate research, and seamless integration into model data assimilation workflows."],"url":"http://arxiv.org/abs/2412.03413v1"}
{"created":"2024-12-04 15:48:59","title":"PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following Models Need for Efficient Generation","abstract":"Recently, large vision-language models (LVLMs) have rapidly gained popularity for their strong generation and reasoning capabilities given diverse multimodal inputs. However, these models incur significant computational and memory overhead during inference, which greatly hinders the efficient deployment in practical scenarios. The extensive key-value (KV) cache, necessitated by the lengthy input and output sequences, notably contributes to the high inference cost. Based on this, recent works have investigated ways to reduce the KV cache size for higher efficiency. Although effective, they generally overlook the distinct importance distributions of KV vectors across layers and maintain the same cache size for each layer during the next token prediction. This results in the significant contextual information loss for certain layers, leading to notable performance decline. To address this, we present PrefixKV. It reframes the challenge of determining KV cache sizes for all layers into the task of searching for the optimal global prefix configuration. With an adaptive layer-wise KV retention recipe based on binary search, the maximum contextual information can thus be preserved in each layer, facilitating the generation. Extensive experiments demonstrate that our method achieves the state-of-the-art performance compared with others. It exhibits superior inference efficiency and generation quality trade-offs, showing promising potential for practical applications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.","sentences":["Recently, large vision-language models (LVLMs) have rapidly gained popularity for their strong generation and reasoning capabilities given diverse multimodal inputs.","However, these models incur significant computational and memory overhead during inference, which greatly hinders the efficient deployment in practical scenarios.","The extensive key-value (KV) cache, necessitated by the lengthy input and output sequences, notably contributes to the high inference cost.","Based on this, recent works have investigated ways to reduce the KV cache size for higher efficiency.","Although effective, they generally overlook the distinct importance distributions of KV vectors across layers and maintain the same cache size for each layer during the next token prediction.","This results in the significant contextual information loss for certain layers, leading to notable performance decline.","To address this, we present PrefixKV.","It reframes the challenge of determining KV cache sizes for all layers into the task of searching for the optimal global prefix configuration.","With an adaptive layer-wise KV retention recipe based on binary search, the maximum contextual information can thus be preserved in each layer, facilitating the generation.","Extensive experiments demonstrate that our method achieves the state-of-the-art performance compared with others.","It exhibits superior inference efficiency and generation quality trade-offs, showing promising potential for practical applications.","Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."],"url":"http://arxiv.org/abs/2412.03409v1"}
{"created":"2024-12-04 15:45:20","title":"Skel3D: Skeleton Guided Novel View Synthesis","abstract":"In this paper, we present an approach for monocular open-set novel view synthesis (NVS) that leverages object skeletons to guide the underlying diffusion model. Building upon a baseline that utilizes a pre-trained 2D image generator, our method takes advantage of the Objaverse dataset, which includes animated objects with bone structures. By introducing a skeleton guide layer following the existing ray conditioning normalization (RCN) layer, our approach enhances pose accuracy and multi-view consistency. The skeleton guide layer provides detailed structural information for the generative model, improving the quality of synthesized views. Experimental results demonstrate that our skeleton-guided method significantly enhances consistency and accuracy across diverse object categories within the Objaverse dataset. Our method outperforms existing state-of-the-art NVS techniques both quantitatively and qualitatively, without relying on explicit 3D representations.","sentences":["In this paper, we present an approach for monocular open-set novel view synthesis (NVS) that leverages object skeletons to guide the underlying diffusion model.","Building upon a baseline that utilizes a pre-trained 2D image generator, our method takes advantage of the Objaverse dataset, which includes animated objects with bone structures.","By introducing a skeleton guide layer following the existing ray conditioning normalization (RCN) layer, our approach enhances pose accuracy and multi-view consistency.","The skeleton guide layer provides detailed structural information for the generative model, improving the quality of synthesized views.","Experimental results demonstrate that our skeleton-guided method significantly enhances consistency and accuracy across diverse object categories within the Objaverse dataset.","Our method outperforms existing state-of-the-art NVS techniques both quantitatively and qualitatively, without relying on explicit 3D representations."],"url":"http://arxiv.org/abs/2412.03407v1"}
{"created":"2024-12-04 15:32:37","title":"Benchmarking Pretrained Attention-based Models for Real-Time Recognition in Robot-Assisted Esophagectomy","abstract":"Esophageal cancer is among the most common types of cancer worldwide. It is traditionally treated using open esophagectomy, but in recent years, robot-assisted minimally invasive esophagectomy (RAMIE) has emerged as a promising alternative. However, robot-assisted surgery can be challenging for novice surgeons, as they often suffer from a loss of spatial orientation. Computer-aided anatomy recognition holds promise for improving surgical navigation, but research in this area remains limited. In this study, we developed a comprehensive dataset for semantic segmentation in RAMIE, featuring the largest collection of vital anatomical structures and surgical instruments to date. Handling this diverse set of classes presents challenges, including class imbalance and the recognition of complex structures such as nerves. This study aims to understand the challenges and limitations of current state-of-the-art algorithms on this novel dataset and problem. Therefore, we benchmarked eight real-time deep learning models using two pretraining datasets. We assessed both traditional and attention-based networks, hypothesizing that attention-based networks better capture global patterns and address challenges such as occlusion caused by blood or other tissues. The benchmark includes our RAMIE dataset and the publicly available CholecSeg8k dataset, enabling a thorough assessment of surgical segmentation tasks. Our findings indicate that pretraining on ADE20k, a dataset for semantic segmentation, is more effective than pretraining on ImageNet. Furthermore, attention-based models outperform traditional convolutional neural networks, with SegNeXt and Mask2Former achieving higher Dice scores, and Mask2Former additionally excelling in average symmetric surface distance.","sentences":["Esophageal cancer is among the most common types of cancer worldwide.","It is traditionally treated using open esophagectomy, but in recent years, robot-assisted minimally invasive esophagectomy (RAMIE) has emerged as a promising alternative.","However, robot-assisted surgery can be challenging for novice surgeons, as they often suffer from a loss of spatial orientation.","Computer-aided anatomy recognition holds promise for improving surgical navigation, but research in this area remains limited.","In this study, we developed a comprehensive dataset for semantic segmentation in RAMIE, featuring the largest collection of vital anatomical structures and surgical instruments to date.","Handling this diverse set of classes presents challenges, including class imbalance and the recognition of complex structures such as nerves.","This study aims to understand the challenges and limitations of current state-of-the-art algorithms on this novel dataset and problem.","Therefore, we benchmarked eight real-time deep learning models using two pretraining datasets.","We assessed both traditional and attention-based networks, hypothesizing that attention-based networks better capture global patterns and address challenges such as occlusion caused by blood or other tissues.","The benchmark includes our RAMIE dataset and the publicly available CholecSeg8k dataset, enabling a thorough assessment of surgical segmentation tasks.","Our findings indicate that pretraining on ADE20k, a dataset for semantic segmentation, is more effective than pretraining on ImageNet.","Furthermore, attention-based models outperform traditional convolutional neural networks, with SegNeXt and Mask2Former achieving higher Dice scores, and Mask2Former additionally excelling in average symmetric surface distance."],"url":"http://arxiv.org/abs/2412.03401v1"}
{"created":"2024-12-04 15:31:30","title":"Implicit Priors Editing in Stable Diffusion via Targeted Token Adjustment","abstract":"Implicit assumptions and priors are often necessary in text-to-image generation tasks, especially when textual prompts lack sufficient context. However, these assumptions can sometimes reflect outdated concepts, inaccuracies, or societal bias embedded in the training data. We present Embedding-only Editing (Embedit), a method designed to efficiently adjust implict assumptions and priors in the model without affecting its interpretation of unrelated objects or overall performance. Given a \"source\" prompt (e.g., \"rose\") that elicits an implicit assumption (e.g., rose is red) and a \"destination\" prompt that specifies the desired attribute (e.g., \"blue rose\"), Embedit fine-tunes only the word token embedding (WTE) of the target object (\"rose\") to optimize the last hidden state of text encoder in Stable Diffusion, a SOTA text-to-image model. This targeted adjustment prevents unintended effects on other objects in the model's knowledge base, as the WTEs for unrelated objects and the model weights remain unchanged. Consequently, when a prompt does not contain the edited object, all representations, and the model outputs are identical to those of the original, unedited model. Our method is highly efficient, modifying only 768 parameters for Stable Diffusion 1.4 and 2048 for XL in a single edit, matching the WTE dimension of each respective model. This minimal scope, combined with rapid execution, makes Embedit highly practical for real-world applications. Additionally, changes are easily reversible by restoring the original WTE layers. Our experimental results demonstrate that Embedit consistently outperforms previous methods across various models, tasks, and editing scenarios (both single and sequential multiple edits), achieving at least a 6.01% improvement (from 87.17% to 93.18%).","sentences":["Implicit assumptions and priors are often necessary in text-to-image generation tasks, especially when textual prompts lack sufficient context.","However, these assumptions can sometimes reflect outdated concepts, inaccuracies, or societal bias embedded in the training data.","We present Embedding-only Editing (Embedit), a method designed to efficiently adjust implict assumptions and priors in the model without affecting its interpretation of unrelated objects or overall performance.","Given a \"source\" prompt (e.g., \"rose\") that elicits an implicit assumption (e.g., rose is red) and a \"destination\" prompt that specifies the desired attribute (e.g., \"blue rose\"), Embedit fine-tunes only the word token embedding (WTE) of the target object (\"rose\") to optimize the last hidden state of text encoder in Stable Diffusion, a SOTA text-to-image model.","This targeted adjustment prevents unintended effects on other objects in the model's knowledge base, as the WTEs for unrelated objects and the model weights remain unchanged.","Consequently, when a prompt does not contain the edited object, all representations, and the model outputs are identical to those of the original, unedited model.","Our method is highly efficient, modifying only 768 parameters for Stable Diffusion 1.4 and 2048 for XL in a single edit, matching the WTE dimension of each respective model.","This minimal scope, combined with rapid execution, makes Embedit highly practical for real-world applications.","Additionally, changes are easily reversible by restoring the original WTE layers.","Our experimental results demonstrate that Embedit consistently outperforms previous methods across various models, tasks, and editing scenarios (both single and sequential multiple edits), achieving at least a 6.01% improvement (from 87.17% to 93.18%)."],"url":"http://arxiv.org/abs/2412.03400v1"}
{"created":"2024-12-04 15:27:39","title":"RedStone: Curating General, Code, Math, and QA Data for Large Language Models","abstract":"Pre-training Large Language Models (LLMs) on high-quality, meticulously curated datasets is widely recognized as critical for enhancing their performance and generalization capabilities. This study explores the untapped potential of Common Crawl as a comprehensive and flexible resource for pre-training LLMs, addressing both general-purpose language understanding and specialized domain knowledge. We introduce RedStone, an innovative and scalable pipeline engineered to extract and process data from Common Crawl, facilitating the creation of extensive and varied pre-training datasets. Unlike traditional datasets, which often require expensive curation and domain-specific expertise, RedStone leverages the breadth of Common Crawl to deliver datasets tailored to a wide array of domains. In this work, we exemplify its capability by constructing pre-training datasets across multiple fields, including general language understanding, code, mathematics, and question-answering tasks. The flexibility of RedStone allows for easy adaptation to other specialized domains, significantly lowering the barrier to creating valuable domain-specific datasets. Our findings demonstrate that Common Crawl, when harnessed through effective pipelines like RedStone, can serve as a rich, renewable source of pre-training data, unlocking new avenues for domain adaptation and knowledge discovery in LLMs. This work also underscores the importance of innovative data acquisition strategies and highlights the role of web-scale data as a powerful resource in the continued evolution of LLMs. RedStone code and data samples will be publicly available at \\url{https://aka.ms/redstone}.","sentences":["Pre-training Large Language Models (LLMs) on high-quality, meticulously curated datasets is widely recognized as critical for enhancing their performance and generalization capabilities.","This study explores the untapped potential of Common Crawl as a comprehensive and flexible resource for pre-training LLMs, addressing both general-purpose language understanding and specialized domain knowledge.","We introduce RedStone, an innovative and scalable pipeline engineered to extract and process data from Common Crawl, facilitating the creation of extensive and varied pre-training datasets.","Unlike traditional datasets, which often require expensive curation and domain-specific expertise, RedStone leverages the breadth of Common Crawl to deliver datasets tailored to a wide array of domains.","In this work, we exemplify its capability by constructing pre-training datasets across multiple fields, including general language understanding, code, mathematics, and question-answering tasks.","The flexibility of RedStone allows for easy adaptation to other specialized domains, significantly lowering the barrier to creating valuable domain-specific datasets.","Our findings demonstrate that Common Crawl, when harnessed through effective pipelines like RedStone, can serve as a rich, renewable source of pre-training data, unlocking new avenues for domain adaptation and knowledge discovery in LLMs.","This work also underscores the importance of innovative data acquisition strategies and highlights the role of web-scale data as a powerful resource in the continued evolution of LLMs.","RedStone code and data samples will be publicly available at \\url{https://aka.ms/redstone}."],"url":"http://arxiv.org/abs/2412.03398v1"}
{"created":"2024-12-04 15:26:39","title":"Scarf's Algorithm on Arborescence Hypergraphs","abstract":"Scarf's algorithm--a pivoting procedure that finds a dominating extreme point in a down-monotone polytope--can be used to show the existence of a fractional stable matching in hypergraphs. The problem of finding a fractional stable matching in a hypergraph, however, is PPAD-complete. In this work, we study the behavior of Scarf's algorithm on arborescence hypergraphs, the family of hypergraphs in which hyperedges correspond to the paths of an arborescence. For arborescence hypergraphs, we prove that Scarf's algorithm can be implemented to find an integral stable matching in polynomial time. En route to our result, we uncover novel structural properties of bases and pivots for the more general family of network hypergraphs. Our work provides the first proof of polynomial-time convergence of Scarf's algorithm on hypergraphic stable matching problems, giving hope to the possibility of polynomial-time convergence of Scarf's algorithm for other families of polytope.","sentences":["Scarf's algorithm--a pivoting procedure that finds a dominating extreme point in a down-monotone polytope--can be used to show the existence of a fractional stable matching in hypergraphs.","The problem of finding a fractional stable matching in a hypergraph, however, is PPAD-complete.","In this work, we study the behavior of Scarf's algorithm on arborescence hypergraphs, the family of hypergraphs in which hyperedges correspond to the paths of an arborescence.","For arborescence hypergraphs, we prove that Scarf's algorithm can be implemented to find an integral stable matching in polynomial time.","En route to our result, we uncover novel structural properties of bases and pivots for the more general family of network hypergraphs.","Our work provides the first proof of polynomial-time convergence of Scarf's algorithm on hypergraphic stable matching problems, giving hope to the possibility of polynomial-time convergence of Scarf's algorithm for other families of polytope."],"url":"http://arxiv.org/abs/2412.03397v1"}
{"created":"2024-12-04 15:23:46","title":"An even simpler hard variant of Not-All-Equal 3-SAT","abstract":"We show that Not-All-Equal 3-Sat remains NP-complete when restricted to instances that simultaneously satisfy the following properties: (i) The clauses are given as the disjoint union of k partitions, for any fixed $k \\geq 4$, of the variable set into subsets of size 3, and (ii) each pair of distinct clauses shares at most one variable. Property (i) implies that each variable appears in exactly $k$ clauses and each clause consists of exactly 3 unnegated variables. Therewith, we improve upon our earlier result (Darmann and D\\\"ocker, 2020). Complementing the hardness result for at least $4$ partitions, we show that for $k\\leq 3$ the corresponding decision problem is in P. In particular, for $k\\in \\{1,2\\}$, all instances that satisfy Property (i) are nae-satisfiable.   By the well-known correspondence between Not-All-Equal 3-Sat} and hypergraph coloring, we obtain the following corollary of our results: For $k\\geq 4$, Bicolorability is NP-complete for linear 3-uniform $k$-regular hypergraphs even if the edges are given as a decomposition into $k$ perfect matchings; with the same restrictions, for $k \\leq 3$ Bicolorability is in P, and for $k \\in \\{1,2\\}$ all such hypergraphs are bicolorable.   Finally, we deduce from a construction in the work by Pilz (Pilz, 2019) that every instance of Positive Planar Not-All-Equal Sat with at least three distinct variables per clause is nae-satisfiable. Hence, when restricted to instances with a planar incidence graph, each of the above variants of Not-All-Equal 3-Sat turns into a trivial decision problem.","sentences":["We show that Not-All-Equal 3-Sat remains NP-complete when restricted to instances that simultaneously satisfy the following properties: (i) The clauses are given as the disjoint union of k partitions, for any fixed $k \\geq 4$, of the variable set into subsets of size 3, and (ii) each pair of distinct clauses shares at most one variable.","Property (i) implies that each variable appears in exactly $k$ clauses and each clause consists of exactly 3 unnegated variables.","Therewith, we improve upon our earlier result (Darmann and D\\\"ocker, 2020).","Complementing the hardness result for at least $4$ partitions, we show that for $k\\leq 3$ the corresponding decision problem is in P. In particular, for $k\\in \\{1,2\\}$, all instances that satisfy Property (i) are nae-satisfiable.   ","By the well-known correspondence between Not-All-Equal 3-Sat} and hypergraph coloring, we obtain the following corollary of our results: For $k\\geq 4$, Bicolorability is NP-complete for linear 3-uniform $k$-regular hypergraphs even if the edges are given as a decomposition into $k$ perfect matchings; with the same restrictions, for $k \\leq 3$ Bicolorability is in P, and for $k \\in \\{1,2\\}$ all such hypergraphs are bicolorable.   ","Finally, we deduce from a construction in the work by Pilz (Pilz, 2019) that every instance of Positive Planar Not-All-Equal Sat with at least three distinct variables per clause is nae-satisfiable.","Hence, when restricted to instances with a planar incidence graph, each of the above variants of Not-All-Equal 3-Sat turns into a trivial decision problem."],"url":"http://arxiv.org/abs/2412.03395v1"}
{"created":"2024-12-04 15:22:54","title":"Can neural operators always be continuously discretized?","abstract":"We consider the problem of discretization of neural operators between Hilbert spaces in a general framework including skip connections. We focus on bijective neural operators through the lens of diffeomorphisms in infinite dimensions. Framed using category theory, we give a no-go theorem that shows that diffeomorphisms between Hilbert spaces or Hilbert manifolds may not admit any continuous approximations by diffeomorphisms on finite-dimensional spaces, even if the approximations are nonlinear. The natural way out is the introduction of strongly monotone diffeomorphisms and layerwise strongly monotone neural operators which have continuous approximations by strongly monotone diffeomorphisms on finite-dimensional spaces. For these, one can guarantee discretization invariance, while ensuring that finite-dimensional approximations converge not only as sequences of functions, but that their representations converge in a suitable sense as well. Finally, we show that bilipschitz neural operators may always be written in the form of an alternating composition of strongly monotone neural operators, plus a simple isometry. Thus we realize a rigorous platform for discretization of a generalization of a neural operator. We also show that neural operators of this type may be approximated through the composition of finite-rank residual neural operators, where each block is strongly monotone, and may be inverted locally via iteration. We conclude by providing a quantitative approximation result for the discretization of general bilipschitz neural operators.","sentences":["We consider the problem of discretization of neural operators between Hilbert spaces in a general framework including skip connections.","We focus on bijective neural operators through the lens of diffeomorphisms in infinite dimensions.","Framed using category theory, we give a no-go theorem that shows that diffeomorphisms between Hilbert spaces or Hilbert manifolds may not admit any continuous approximations by diffeomorphisms on finite-dimensional spaces, even if the approximations are nonlinear.","The natural way out is the introduction of strongly monotone diffeomorphisms and layerwise strongly monotone neural operators which have continuous approximations by strongly monotone diffeomorphisms on finite-dimensional spaces.","For these, one can guarantee discretization invariance, while ensuring that finite-dimensional approximations converge not only as sequences of functions, but that their representations converge in a suitable sense as well.","Finally, we show that bilipschitz neural operators may always be written in the form of an alternating composition of strongly monotone neural operators, plus a simple isometry.","Thus we realize a rigorous platform for discretization of a generalization of a neural operator.","We also show that neural operators of this type may be approximated through the composition of finite-rank residual neural operators, where each block is strongly monotone, and may be inverted locally via iteration.","We conclude by providing a quantitative approximation result for the discretization of general bilipschitz neural operators."],"url":"http://arxiv.org/abs/2412.03393v1"}
{"created":"2024-12-04 15:20:26","title":"On one-way functions and the average time complexity of almost-optimal compression","abstract":"We show that one-way functions exist if and only there exists an efficient distribution relative to which almost-optimal compression is hard on average. The result is obtained by combining a theorem of Ilango, Ren, and Santhanam and one by Bauwens and Zimand.","sentences":["We show that one-way functions exist if and only there exists an efficient distribution relative to which almost-optimal compression is hard on average.","The result is obtained by combining a theorem of Ilango, Ren, and Santhanam and one by Bauwens and Zimand."],"url":"http://arxiv.org/abs/2412.03392v1"}
{"created":"2024-12-04 15:20:12","title":"Risk-aware Classification via Uncertainty Quantification","abstract":"Autonomous and semi-autonomous systems are using deep learning models to improve decision-making. However, deep classifiers can be overly confident in their incorrect predictions, a major issue especially in safety-critical domains. The present study introduces three foundational desiderata for developing real-world risk-aware classification systems. Expanding upon the previously proposed Evidential Deep Learning (EDL), we demonstrate the unity between these principles and EDL's operational attributes. We then augment EDL empowering autonomous agents to exercise discretion during structured decision-making when uncertainty and risks are inherent. We rigorously examine empirical scenarios to substantiate these theoretical innovations. In contrast to existing risk-aware classifiers, our proposed methodologies consistently exhibit superior performance, underscoring their transformative potential in risk-conscious classification strategies.","sentences":["Autonomous and semi-autonomous systems are using deep learning models to improve decision-making.","However, deep classifiers can be overly confident in their incorrect predictions, a major issue especially in safety-critical domains.","The present study introduces three foundational desiderata for developing real-world risk-aware classification systems.","Expanding upon the previously proposed Evidential Deep Learning (EDL), we demonstrate the unity between these principles and EDL's operational attributes.","We then augment EDL empowering autonomous agents to exercise discretion during structured decision-making when uncertainty and risks are inherent.","We rigorously examine empirical scenarios to substantiate these theoretical innovations.","In contrast to existing risk-aware classifiers, our proposed methodologies consistently exhibit superior performance, underscoring their transformative potential in risk-conscious classification strategies."],"url":"http://arxiv.org/abs/2412.03391v1"}
{"created":"2024-12-04 15:19:01","title":"Enhancing Supply Chain Visibility with Generative AI: An Exploratory Case Study on Relationship Prediction in Knowledge Graphs","abstract":"A key stumbling block in effective supply chain risk management for companies and policymakers is a lack of visibility on interdependent supply network relationships. Relationship prediction, also called link prediction is an emergent area of supply chain surveillance research that aims to increase the visibility of supply chains using data-driven techniques. Existing methods have been successful for predicting relationships but struggle to extract the context in which these relationships are embedded - such as the products being supplied or locations they are supplied from. Lack of context prevents practitioners from distinguishing transactional relations from established supply chain relations, hindering accurate estimations of risk. In this work, we develop a new Generative Artificial Intelligence (Gen AI) enhanced machine learning framework that leverages pre-trained language models as embedding models combined with machine learning models to predict supply chain relationships within knowledge graphs. By integrating Generative AI techniques, our approach captures the nuanced semantic relationships between entities, thereby improving supply chain visibility and facilitating more precise risk management. Using data from a real case study, we show that GenAI-enhanced link prediction surpasses all benchmarks, and demonstrate how GenAI models can be explored and effectively used in supply chain risk management.","sentences":["A key stumbling block in effective supply chain risk management for companies and policymakers is a lack of visibility on interdependent supply network relationships.","Relationship prediction, also called link prediction is an emergent area of supply chain surveillance research that aims to increase the visibility of supply chains using data-driven techniques.","Existing methods have been successful for predicting relationships but struggle to extract the context in which these relationships are embedded - such as the products being supplied or locations they are supplied from.","Lack of context prevents practitioners from distinguishing transactional relations from established supply chain relations, hindering accurate estimations of risk.","In this work, we develop a new Generative Artificial Intelligence (Gen AI) enhanced machine learning framework that leverages pre-trained language models as embedding models combined with machine learning models to predict supply chain relationships within knowledge graphs.","By integrating Generative AI techniques, our approach captures the nuanced semantic relationships between entities, thereby improving supply chain visibility and facilitating more precise risk management.","Using data from a real case study, we show that GenAI-enhanced link prediction surpasses all benchmarks, and demonstrate how GenAI models can be explored and effectively used in supply chain risk management."],"url":"http://arxiv.org/abs/2412.03390v1"}
{"created":"2024-12-04 15:17:25","title":"DiffStyleTTS: Diffusion-based Hierarchical Prosody Modeling for Text-to-Speech with Diverse and Controllable Styles","abstract":"Human speech exhibits rich and flexible prosodic variations. To address the one-to-many mapping problem from text to prosody in a reasonable and flexible manner, we propose DiffStyleTTS, a multi-speaker acoustic model based on a conditional diffusion module and an improved classifier-free guidance, which hierarchically models speech prosodic features, and controls different prosodic styles to guide prosody prediction. Experiments show that our method outperforms all baselines in naturalness and achieves superior synthesis speed compared to three diffusion-based baselines. Additionally, by adjusting the guiding scale, DiffStyleTTS effectively controls the guidance intensity of the synthetic prosody.","sentences":["Human speech exhibits rich and flexible prosodic variations.","To address the one-to-many mapping problem from text to prosody in a reasonable and flexible manner, we propose DiffStyleTTS, a multi-speaker acoustic model based on a conditional diffusion module and an improved classifier-free guidance, which hierarchically models speech prosodic features, and controls different prosodic styles to guide prosody prediction.","Experiments show that our method outperforms all baselines in naturalness and achieves superior synthesis speed compared to three diffusion-based baselines.","Additionally, by adjusting the guiding scale, DiffStyleTTS effectively controls the guidance intensity of the synthetic prosody."],"url":"http://arxiv.org/abs/2412.03388v1"}
{"created":"2024-12-04 15:12:00","title":"Reactive Orchestration for Hierarchical Federated Learning Under a Communication Cost Budget","abstract":"Deploying a Hierarchical Federated Learning (HFL) pipeline across the computing continuum (CC) requires careful organization of participants into a hierarchical structure with intermediate aggregation nodes between FL clients and the global FL server. This is challenging to achieve due to (i) cost constraints, (ii) varying data distributions, and (iii) the volatile operating environment of the CC. In response to these challenges, we present a framework for the adaptive orchestration of HFL pipelines, designed to be reactive to client churn and infrastructure-level events, while balancing communication cost and ML model accuracy. Our mechanisms identify and react to events that cause HFL reconfiguration actions at runtime, building on multi-level monitoring information (model accuracy, resource availability, resource cost). Moreover, our framework introduces a generic methodology for estimating reconfiguration costs to continuously re-evaluate the quality of adaptation actions, while being extensible to optimize for various HFL performance criteria. By extending the Kubernetes ecosystem, our framework demonstrates the ability to react promptly and effectively to changes in the operating environment, making the best of the available communication cost budget and effectively balancing costs and ML performance at runtime.","sentences":["Deploying a Hierarchical Federated Learning (HFL) pipeline across the computing continuum (CC) requires careful organization of participants into a hierarchical structure with intermediate aggregation nodes between FL clients and the global FL server.","This is challenging to achieve due to (i) cost constraints, (ii) varying data distributions, and (iii) the volatile operating environment of the CC.","In response to these challenges, we present a framework for the adaptive orchestration of HFL pipelines, designed to be reactive to client churn and infrastructure-level events, while balancing communication cost and ML model accuracy.","Our mechanisms identify and react to events that cause HFL reconfiguration actions at runtime, building on multi-level monitoring information (model accuracy, resource availability, resource cost).","Moreover, our framework introduces a generic methodology for estimating reconfiguration costs to continuously re-evaluate the quality of adaptation actions, while being extensible to optimize for various HFL performance criteria.","By extending the Kubernetes ecosystem, our framework demonstrates the ability to react promptly and effectively to changes in the operating environment, making the best of the available communication cost budget and effectively balancing costs and ML performance at runtime."],"url":"http://arxiv.org/abs/2412.03385v1"}
{"created":"2024-12-04 15:06:39","title":"Mapping using Transformers for Volumes -- Network for Super-Resolution with Long-Range Interactions","abstract":"Until now, it has been difficult for volumetric super-resolution to utilize the recent advances in transformer-based models seen in 2D super-resolution. The memory required for self-attention in 3D volumes limits the receptive field. Therefore, long-range interactions are not used in 3D to the extent done in 2D and the strength of transformers is not realized. We propose a multi-scale transformer-based model based on hierarchical attention blocks combined with carrier tokens at multiple scales to overcome this. Here information from larger regions at coarse resolution is sequentially carried on to finer-resolution regions to predict the super-resolved image. Using transformer layers at each resolution, our coarse-to-fine modeling limits the number of tokens at each scale and enables attention over larger regions than what has previously been possible. We experimentally compare our method, MTVNet, against state-of-the-art volumetric super-resolution models on five 3D datasets demonstrating the advantage of an increased receptive field. This advantage is especially pronounced for images that are larger than what is seen in popularly used 3D datasets. Our code is available at https://github.com/AugustHoeg/MTVNet","sentences":["Until now, it has been difficult for volumetric super-resolution to utilize the recent advances in transformer-based models seen in 2D super-resolution.","The memory required for self-attention in 3D volumes limits the receptive field.","Therefore, long-range interactions are not used in 3D to the extent done in 2D and the strength of transformers is not realized.","We propose a multi-scale transformer-based model based on hierarchical attention blocks combined with carrier tokens at multiple scales to overcome this.","Here information from larger regions at coarse resolution is sequentially carried on to finer-resolution regions to predict the super-resolved image.","Using transformer layers at each resolution, our coarse-to-fine modeling limits the number of tokens at each scale and enables attention over larger regions than what has previously been possible.","We experimentally compare our method, MTVNet, against state-of-the-art volumetric super-resolution models on five 3D datasets demonstrating the advantage of an increased receptive field.","This advantage is especially pronounced for images that are larger than what is seen in popularly used 3D datasets.","Our code is available at https://github.com/AugustHoeg/MTVNet"],"url":"http://arxiv.org/abs/2412.03379v1"}
{"created":"2024-12-04 15:05:43","title":"Volumetrically Consistent 3D Gaussian Rasterization","abstract":"Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view synthesis at high inference speeds. However, its splatting-based rendering model makes several approximations to the rendering equation, reducing physical accuracy. We show that splatting and its approximations are unnecessary, even within a rasterizer; we instead volumetrically integrate 3D Gaussians directly to compute the transmittance across them analytically. We use this analytic transmittance to derive more physically-accurate alpha values than 3DGS, which can directly be used within their framework. The result is a method that more closely follows the volume rendering equation (similar to ray-tracing) while enjoying the speed benefits of rasterization. Our method represents opaque surfaces with higher accuracy and fewer points than 3DGS. This enables it to outperform 3DGS for view synthesis (measured in SSIM and LPIPS). Being volumetrically consistent also enables our method to work out of the box for tomography. We match the state-of-the-art 3DGS-based tomography method with fewer points. Being volumetrically consistent also enables our method to work out of the box for tomography. We match the state-of-the-art 3DGS-based tomography method with fewer points.","sentences":["Recently, 3D Gaussian Splatting (3DGS) has enabled photorealistic view synthesis at high inference speeds.","However, its splatting-based rendering model makes several approximations to the rendering equation, reducing physical accuracy.","We show that splatting and its approximations are unnecessary, even within a rasterizer; we instead volumetrically integrate 3D Gaussians directly to compute the transmittance across them analytically.","We use this analytic transmittance to derive more physically-accurate alpha values than 3DGS, which can directly be used within their framework.","The result is a method that more closely follows the volume rendering equation (similar to ray-tracing) while enjoying the speed benefits of rasterization.","Our method represents opaque surfaces with higher accuracy and fewer points than 3DGS.","This enables it to outperform 3DGS for view synthesis (measured in SSIM and LPIPS).","Being volumetrically consistent also enables our method to work out of the box for tomography.","We match the state-of-the-art 3DGS-based tomography method with fewer points.","Being volumetrically consistent also enables our method to work out of the box for tomography.","We match the state-of-the-art 3DGS-based tomography method with fewer points."],"url":"http://arxiv.org/abs/2412.03378v1"}
{"created":"2024-12-04 15:02:28","title":"Granular Ball Twin Support Vector Machine with Universum Data","abstract":"Classification with support vector machines (SVM) often suffers from limited performance when relying solely on labeled data from target classes and is sensitive to noise and outliers. Incorporating prior knowledge from Universum data and more robust data representations can enhance accuracy and efficiency. Motivated by these findings, we propose a novel Granular Ball Twin Support Vector Machine with Universum Data (GBU-TSVM) that extends the TSVM framework to leverage both Universum samples and granular ball computing during model training. Unlike existing TSVM methods, the proposed GBU-TSVM represents data instances as hyper-balls rather than points in the feature space. This innovative approach improves the model's robustness and efficiency, particularly in handling noisy and large datasets. By grouping data points into granular balls, the model achieves superior computational efficiency, increased noise resistance, and enhanced interpretability. Additionally, the inclusion of Universum data, which consists of samples that are not strictly from the target classes, further refines the classification boundaries. This integration enriches the model with contextual information, refining classification boundaries and boosting overall accuracy. Experimental results on UCI benchmark datasets demonstrate that the GBU-TSVM outperforms existing TSVM models in both accuracy and computational efficiency. These findings highlight the potential of the GBU-TSVM model in setting a new standard in data representation and classification.","sentences":["Classification with support vector machines (SVM) often suffers from limited performance when relying solely on labeled data from target classes and is sensitive to noise and outliers.","Incorporating prior knowledge from Universum data and more robust data representations can enhance accuracy and efficiency.","Motivated by these findings, we propose a novel Granular Ball Twin Support Vector Machine with Universum Data (GBU-TSVM) that extends the TSVM framework to leverage both Universum samples and granular ball computing during model training.","Unlike existing TSVM methods, the proposed GBU-TSVM represents data instances as hyper-balls rather than points in the feature space.","This innovative approach improves the model's robustness and efficiency, particularly in handling noisy and large datasets.","By grouping data points into granular balls, the model achieves superior computational efficiency, increased noise resistance, and enhanced interpretability.","Additionally, the inclusion of Universum data, which consists of samples that are not strictly from the target classes, further refines the classification boundaries.","This integration enriches the model with contextual information, refining classification boundaries and boosting overall accuracy.","Experimental results on UCI benchmark datasets demonstrate that the GBU-TSVM outperforms existing TSVM models in both accuracy and computational efficiency.","These findings highlight the potential of the GBU-TSVM model in setting a new standard in data representation and classification."],"url":"http://arxiv.org/abs/2412.03375v1"}
{"created":"2024-12-04 15:01:20","title":"Exploring trends in audio mixes and masters: Insights from a dataset analysis","abstract":"We present an analysis of a dataset of audio metrics and aesthetic considerations about mixes and masters provided by the web platform MixCheck studio. The platform is designed for educational purposes, primarily targeting amateur music producers, and aimed at analysing their recordings prior to them being released. The analysis focuses on the following data points: integrated loudness, mono compatibility, presence of clipping and phase issues, compression and tonal profile across 30 user-specified genres. Both mixed (mixes) and mastered audio (masters) are included in the analysis, where mixes refer to the initial combination and balance of individual tracks, and masters refer to the final refined version optimized for distribution. Results show that loudness-related issues along with dynamics issues are the most prevalent, particularly in mastered audio. However mastered audio presents better results in compression than just mixed audio. Additionally, results show that mastered audio has a lower percentage of stereo field and phase issues.","sentences":["We present an analysis of a dataset of audio metrics and aesthetic considerations about mixes and masters provided by the web platform MixCheck studio.","The platform is designed for educational purposes, primarily targeting amateur music producers, and aimed at analysing their recordings prior to them being released.","The analysis focuses on the following data points: integrated loudness, mono compatibility, presence of clipping and phase issues, compression and tonal profile across 30 user-specified genres.","Both mixed (mixes) and mastered audio (masters) are included in the analysis, where mixes refer to the initial combination and balance of individual tracks, and masters refer to the final refined version optimized for distribution.","Results show that loudness-related issues along with dynamics issues are the most prevalent, particularly in mastered audio.","However mastered audio presents better results in compression than just mixed audio.","Additionally, results show that mastered audio has a lower percentage of stereo field and phase issues."],"url":"http://arxiv.org/abs/2412.03373v1"}
{"created":"2024-12-04 14:59:05","title":"SGSST: Scaling Gaussian Splatting StyleTransfer","abstract":"Applying style transfer to a full 3D environment is a challenging task that has seen many developments since the advent of neural rendering. 3D Gaussian splatting (3DGS) has recently pushed further many limits of neural rendering in terms of training speed and reconstruction quality. This work introduces SGSST: Scaling Gaussian Splatting Style Transfer, an optimization-based method to apply style transfer to pretrained 3DGS scenes. We demonstrate that a new multiscale loss based on global neural statistics, that we name SOS for Simultaneously Optimized Scales, enables style transfer to ultra-high resolution 3D scenes. Not only SGSST pioneers 3D scene style transfer at such high image resolutions, it also produces superior visual quality as assessed by thorough qualitative, quantitative and perceptual comparisons.","sentences":["Applying style transfer to a full 3D environment is a challenging task that has seen many developments since the advent of neural rendering.","3D Gaussian splatting (3DGS) has recently pushed further many limits of neural rendering in terms of training speed and reconstruction quality.","This work introduces SGSST:","Scaling Gaussian Splatting Style Transfer, an optimization-based method to apply style transfer to pretrained 3DGS scenes.","We demonstrate that a new multiscale loss based on global neural statistics, that we name SOS for Simultaneously Optimized Scales, enables style transfer to ultra-high resolution 3D scenes.","Not only SGSST pioneers 3D scene style transfer at such high image resolutions, it also produces superior visual quality as assessed by thorough qualitative, quantitative and perceptual comparisons."],"url":"http://arxiv.org/abs/2412.03371v1"}
