{"created":"2024-07-30 17:59:08","title":"ThinK: Thinner Key Cache by Query-Driven Pruning","abstract":"Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications by leveraging increased model sizes and sequence lengths. However, the associated rise in computational and memory costs poses significant challenges, particularly in managing long sequences due to the quadratic complexity of the transformer attention mechanism. This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference. Unlike existing approaches that optimize the memory based on the sequence lengths, we uncover that the channel dimension of the KV cache exhibits significant redundancy, characterized by unbalanced magnitude distribution and low-rank structure in attention weights. Based on these observations, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels. Our approach not only maintains or enhances model accuracy but also achieves a reduction in memory costs by over 20% compared with vanilla KV cache eviction methods. Extensive evaluations on the LLaMA3 and Mistral models across various long-sequence datasets confirm the efficacy of ThinK, setting a new precedent for efficient LLM deployment without compromising performance. We also outline the potential of extending our method to value cache pruning, demonstrating ThinK's versatility and broad applicability in reducing both memory and computational overheads.","sentences":["Large Language Models (LLMs) have revolutionized the field of natural language processing, achieving unprecedented performance across a variety of applications by leveraging increased model sizes and sequence lengths.","However, the associated rise in computational and memory costs poses significant challenges, particularly in managing long sequences due to the quadratic complexity of the transformer attention mechanism.","This paper focuses on the long-context scenario, addressing the inefficiencies in KV cache memory consumption during inference.","Unlike existing approaches that optimize the memory based on the sequence lengths, we uncover that the channel dimension of the KV cache exhibits significant redundancy, characterized by unbalanced magnitude distribution and low-rank structure in attention weights.","Based on these observations, we propose ThinK, a novel query-dependent KV cache pruning method designed to minimize attention weight loss while selectively pruning the least significant channels.","Our approach not only maintains or enhances model accuracy but also achieves a reduction in memory costs by over 20% compared with vanilla KV cache eviction methods.","Extensive evaluations on the LLaMA3 and Mistral models across various long-sequence datasets confirm the efficacy of ThinK, setting a new precedent for efficient LLM deployment without compromising performance.","We also outline the potential of extending our method to value cache pruning, demonstrating ThinK's versatility and broad applicability in reducing both memory and computational overheads."],"url":"http://arxiv.org/abs/2407.21018v1"}
{"created":"2024-07-30 17:58:52","title":"Matting by Generation","abstract":"This paper introduces an innovative approach for image matting that redefines the traditional regression-based task as a generative modeling challenge. Our method harnesses the capabilities of latent diffusion models, enriched with extensive pre-trained knowledge, to regularize the matting process. We present novel architectural innovations that empower our model to produce mattes with superior resolution and detail. The proposed method is versatile and can perform both guidance-free and guidance-based image matting, accommodating a variety of additional cues. Our comprehensive evaluation across three benchmark datasets demonstrates the superior performance of our approach, both quantitatively and qualitatively. The results not only reflect our method's robust effectiveness but also highlight its ability to generate visually compelling mattes that approach photorealistic quality. The project page for this paper is available at https://lightchaserx.github.io/matting-by-generation/","sentences":["This paper introduces an innovative approach for image matting that redefines the traditional regression-based task as a generative modeling challenge.","Our method harnesses the capabilities of latent diffusion models, enriched with extensive pre-trained knowledge, to regularize the matting process.","We present novel architectural innovations that empower our model to produce mattes with superior resolution and detail.","The proposed method is versatile and can perform both guidance-free and guidance-based image matting, accommodating a variety of additional cues.","Our comprehensive evaluation across three benchmark datasets demonstrates the superior performance of our approach, both quantitatively and qualitatively.","The results not only reflect our method's robust effectiveness but also highlight its ability to generate visually compelling mattes that approach photorealistic quality.","The project page for this paper is available at https://lightchaserx.github.io/matting-by-generation/"],"url":"http://arxiv.org/abs/2407.21017v1"}
{"created":"2024-07-30 17:58:13","title":"Add-SD: Rational Generation without Manual Reference","abstract":"Diffusion models have exhibited remarkable prowess in visual generalization. Building on this success, we introduce an instruction-based object addition pipeline, named Add-SD, which automatically inserts objects into realistic scenes with rational sizes and positions. Different from layout-conditioned methods, Add-SD is solely conditioned on simple text prompts rather than any other human-costly references like bounding boxes. Our work contributes in three aspects: proposing a dataset containing numerous instructed image pairs; fine-tuning a diffusion model for rational generation; and generating synthetic data to boost downstream tasks. The first aspect involves creating a RemovalDataset consisting of original-edited image pairs with textual instructions, where an object has been removed from the original image while maintaining strong pixel consistency in the background. These data pairs are then used for fine-tuning the Stable Diffusion (SD) model. Subsequently, the pretrained Add-SD model allows for the insertion of expected objects into an image with good rationale. Additionally, we generate synthetic instances for downstream task datasets at scale, particularly for tail classes, to alleviate the long-tailed problem. Downstream tasks benefit from the enriched dataset with enhanced diversity and rationale. Experiments on LVIS val demonstrate that Add-SD yields an improvement of 4.3 mAP on rare classes over the baseline. Code and models are available at https://github.com/ylingfeng/Add-SD.","sentences":["Diffusion models have exhibited remarkable prowess in visual generalization.","Building on this success, we introduce an instruction-based object addition pipeline, named Add-SD, which automatically inserts objects into realistic scenes with rational sizes and positions.","Different from layout-conditioned methods, Add-SD is solely conditioned on simple text prompts rather than any other human-costly references like bounding boxes.","Our work contributes in three aspects: proposing a dataset containing numerous instructed image pairs; fine-tuning a diffusion model for rational generation; and generating synthetic data to boost downstream tasks.","The first aspect involves creating a RemovalDataset consisting of original-edited image pairs with textual instructions, where an object has been removed from the original image while maintaining strong pixel consistency in the background.","These data pairs are then used for fine-tuning the Stable Diffusion (SD) model.","Subsequently, the pretrained Add-SD model allows for the insertion of expected objects into an image with good rationale.","Additionally, we generate synthetic instances for downstream task datasets at scale, particularly for tail classes, to alleviate the long-tailed problem.","Downstream tasks benefit from the enriched dataset with enhanced diversity and rationale.","Experiments on LVIS val demonstrate that Add-SD yields an improvement of 4.3 mAP on rare classes over the baseline.","Code and models are available at https://github.com/ylingfeng/Add-SD."],"url":"http://arxiv.org/abs/2407.21016v1"}
{"created":"2024-07-30 17:57:32","title":"CLEFT: Language-Image Contrastive Learning with Efficient Large Language Model and Prompt Fine-Tuning","abstract":"Recent advancements in Contrastive Language-Image Pre-training (CLIP) have demonstrated notable success in self-supervised representation learning across various tasks. However, the existing CLIP-like approaches often demand extensive GPU resources and prolonged training times due to the considerable size of the model and dataset, making them poor for medical applications, in which large datasets are not always common. Meanwhile, the language model prompts are mainly manually derived from labels tied to images, potentially overlooking the richness of information within training samples. We introduce a novel language-image Contrastive Learning method with an Efficient large language model and prompt Fine-Tuning (CLEFT) that harnesses the strengths of the extensive pre-trained language and visual models. Furthermore, we present an efficient strategy for learning context-based prompts that mitigates the gap between informative clinical diagnostic data and simple class labels. Our method demonstrates state-of-the-art performance on multiple chest X-ray and mammography datasets compared with various baselines. The proposed parameter efficient framework can reduce the total trainable model size by 39% and reduce the trainable language model to only 4% compared with the current BERT encoder.","sentences":["Recent advancements in Contrastive Language-Image Pre-training (CLIP) have demonstrated notable success in self-supervised representation learning across various tasks.","However, the existing CLIP-like approaches often demand extensive GPU resources and prolonged training times due to the considerable size of the model and dataset, making them poor for medical applications, in which large datasets are not always common.","Meanwhile, the language model prompts are mainly manually derived from labels tied to images, potentially overlooking the richness of information within training samples.","We introduce a novel language-image Contrastive Learning method with an Efficient large language model and prompt Fine-Tuning (CLEFT) that harnesses the strengths of the extensive pre-trained language and visual models.","Furthermore, we present an efficient strategy for learning context-based prompts that mitigates the gap between informative clinical diagnostic data and simple class labels.","Our method demonstrates state-of-the-art performance on multiple chest X-ray and mammography datasets compared with various baselines.","The proposed parameter efficient framework can reduce the total trainable model size by 39% and reduce the trainable language model to only 4% compared with the current BERT encoder."],"url":"http://arxiv.org/abs/2407.21011v1"}
{"created":"2024-07-30 17:57:09","title":"Human-Data Interaction Framework: A Comprehensive Model for a Future Driven by Data and Humans","abstract":"In an age defined by rapid data expansion, the connection between individuals and their digital footprints has become more intricate. The Human-Data Interaction (HDI) framework has become an essential approach to tackling the challenges and ethical issues associated with data governance and utilization in the modern digital world. This paper outlines the fundamental steps required for organizations to seamlessly integrate HDI principles, emphasizing auditing, aligning, formulating considerations, and the need for continuous monitoring and adaptation. Through a thorough audit, organizations can critically assess their current data management practices, trace the data lifecycle from collection to disposal, and evaluate the effectiveness of existing policies, security protocols, and user interfaces. The next step involves aligning these practices with the main HDI principles, such as informed consent, data transparency, user control, algorithm transparency, and ethical data use, to identify gaps that need strategic action. Formulating preliminary considerations includes developing policies and technical solutions to close identified gaps, ensuring that these practices not only meet legal standards, but also promote fairness and accountability in data interactions. The final step, monitoring and adaptation, highlights the need for setting up continuous evaluation mechanisms and being responsive to technological, regulatory, and societal developments, ensuring HDI practices stay up-to-date and effective. Successful implementation of the HDI framework requires multi-disciplinary collaboration, incorporating insights from technology, law, ethics, and user experience design. The paper posits that this comprehensive approach is vital for building trust and legitimacy in digital environments, ultimately leading to more ethical, transparent, and user-centric data interactions.","sentences":["In an age defined by rapid data expansion, the connection between individuals and their digital footprints has become more intricate.","The Human-Data Interaction (HDI) framework has become an essential approach to tackling the challenges and ethical issues associated with data governance and utilization in the modern digital world.","This paper outlines the fundamental steps required for organizations to seamlessly integrate HDI principles, emphasizing auditing, aligning, formulating considerations, and the need for continuous monitoring and adaptation.","Through a thorough audit, organizations can critically assess their current data management practices, trace the data lifecycle from collection to disposal, and evaluate the effectiveness of existing policies, security protocols, and user interfaces.","The next step involves aligning these practices with the main HDI principles, such as informed consent, data transparency, user control, algorithm transparency, and ethical data use, to identify gaps that need strategic action.","Formulating preliminary considerations includes developing policies and technical solutions to close identified gaps, ensuring that these practices not only meet legal standards, but also promote fairness and accountability in data interactions.","The final step, monitoring and adaptation, highlights the need for setting up continuous evaluation mechanisms and being responsive to technological, regulatory, and societal developments, ensuring HDI practices stay up-to-date and effective.","Successful implementation of the HDI framework requires multi-disciplinary collaboration, incorporating insights from technology, law, ethics, and user experience design.","The paper posits that this comprehensive approach is vital for building trust and legitimacy in digital environments, ultimately leading to more ethical, transparent, and user-centric data interactions."],"url":"http://arxiv.org/abs/2407.21010v1"}
{"created":"2024-07-30 17:55:36","title":"AI-Assisted Generation of Difficult Math Questions","abstract":"Current LLM training positions mathematical reasoning as a core capability. With publicly available sources fully tapped, there is unmet demand for diverse and challenging math questions. Relying solely on human experts is both time-consuming and costly, while LLM-generated questions often lack the requisite diversity and difficulty. We present a design framework that combines the strengths of LLMs with a human-in-the-loop approach to generate a diverse array of challenging math questions. We leverage LLM metacognition skills [Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing math datasets. These skills serve as the basis for generating novel and difficult questions by prompting the LLM with random pairs of core skills. The use of two different skills within each question makes finding such questions an \"out of distribution\" task for both LLMs and humans. Our pipeline employs LLMs to iteratively generate and refine questions and solutions through multiturn prompting. Human annotators then verify and further refine the questions, with their efficiency enhanced via further LLM interactions. Applying this pipeline on skills extracted from the MATH dataset [Hendrycks et al., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions, as evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH (b) Higher performance on MATH when using MATH$^2$ questions as in-context examples. Although focused on mathematics, our methodology seems applicable to other domains requiring structured reasoning, and potentially as a component of scalable oversight. Also of interest is a striking relationship observed between models' performance on the new dataset: the success rate on MATH$^2$ is the square on MATH, suggesting that successfully solving the question in MATH$^2$ requires a nontrivial combination of two distinct math skills.","sentences":["Current LLM training positions mathematical reasoning as a core capability.","With publicly available sources fully tapped, there is unmet demand for diverse and challenging math questions.","Relying solely on human experts is both time-consuming and costly, while LLM-generated questions often lack the requisite diversity and difficulty.","We present a design framework that combines the strengths of LLMs with a human-in-the-loop approach to generate a diverse array of challenging math questions.","We leverage LLM metacognition skills","[Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing math datasets.","These skills serve as the basis for generating novel and difficult questions by prompting the LLM with random pairs of core skills.","The use of two different skills within each question makes finding such questions an \"out of distribution\" task for both LLMs and humans.","Our pipeline employs LLMs to iteratively generate and refine questions and solutions through multiturn prompting.","Human annotators then verify and further refine the questions, with their efficiency enhanced via further LLM interactions.","Applying this pipeline on skills extracted from the MATH dataset [Hendrycks et al., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions, as evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH (b) Higher performance on MATH when using MATH$^2$ questions as in-context examples.","Although focused on mathematics, our methodology seems applicable to other domains requiring structured reasoning, and potentially as a component of scalable oversight.","Also of interest is a striking relationship observed between models' performance on the new dataset: the success rate on MATH$^2$ is the square on MATH, suggesting that successfully solving the question in MATH$^2$ requires a nontrivial combination of two distinct math skills."],"url":"http://arxiv.org/abs/2407.21009v1"}
{"created":"2024-07-30 17:54:36","title":"The Dual-Edged Sword of Technical Debt: Benefits and Issues Analyzed Through Developer Discussions","abstract":"Background. Technical debt (TD) has long been one of the key factors influencing the maintainability of software products. It represents technical compromises that sacrifice long-term software quality for potential short-term benefits. Objective. This work is to collectively investigate the practitioners' opinions on the various perspectives of TD from a large collection of articles. We find the topics and latent details of each, where the sentiments of the detected opinions are also considered. Method. For such a purpose, we conducted a grey literature review on the articles systematically collected from three mainstream technology forums. Furthermore, we adopted natural language processing techniques like topic modeling and sentiment analysis to achieve a systematic and comprehensive understanding. However, we adopted ChatGPT to support the topic interpretation. Results. In this study, 2,213 forum posts and articles were collected, with eight main topics and 43 sub-topics identified. For each topic, we obtained the practitioners' collective positive and negative opinions. Conclusion. We identified 8 major topics in TD related to software development. Identified challenges by practitioners include unclear roles and a lack of engagement. On the other hand, active management supports collaboration and mitigates the impact of TD on the source code.","sentences":["Background.","Technical debt (TD) has long been one of the key factors influencing the maintainability of software products.","It represents technical compromises that sacrifice long-term software quality for potential short-term benefits.","Objective.","This work is to collectively investigate the practitioners' opinions on the various perspectives of TD from a large collection of articles.","We find the topics and latent details of each, where the sentiments of the detected opinions are also considered.","Method.","For such a purpose, we conducted a grey literature review on the articles systematically collected from three mainstream technology forums.","Furthermore, we adopted natural language processing techniques like topic modeling and sentiment analysis to achieve a systematic and comprehensive understanding.","However, we adopted ChatGPT to support the topic interpretation.","Results.","In this study, 2,213 forum posts and articles were collected, with eight main topics and 43 sub-topics identified.","For each topic, we obtained the practitioners' collective positive and negative opinions.","Conclusion.","We identified 8 major topics in TD related to software development.","Identified challenges by practitioners include unclear roles and a lack of engagement.","On the other hand, active management supports collaboration and mitigates the impact of TD on the source code."],"url":"http://arxiv.org/abs/2407.21007v1"}
{"created":"2024-07-30 17:54:01","title":"Settling the Pass Complexity of Approximate Matchings in Dynamic Graph Streams","abstract":"A semi-streaming algorithm in dynamic graph streams processes any $n$-vertex graph by making one or multiple passes over a stream of insertions and deletions to edges of the graph and using $O(n \\cdot \\mbox{polylog}(n))$ space. Semi-streaming algorithms for dynamic streams were first obtained in the seminal work of Ahn, Guha, and McGregor in 2012, alongside the introduction of the graph sketching technique, which remains the de facto way of designing algorithms in this model and a highly popular technique for designing graph algorithms in general.   We settle the pass complexity of approximating maximum matchings in dynamic streams via semi-streaming algorithms by improving the state-of-the-art in both upper and lower bounds.   We present a randomized sketching based semi-streaming algorithm for $O(1)$-approximation of maximum matching in dynamic streams using $O(\\log\\log{n})$ passes. The approximation ratio of this algorithm can be improved to $(1+\\epsilon)$ for any fixed $\\epsilon > 0$ even on weighted graphs using standard techniques. This exponentially improves upon several $O(\\log{n})$ pass algorithms developed for this problem since the introduction of the dynamic graph streaming model.   In addition, we prove that any semi-streaming algorithm (not only sketching based) for $O(1)$-approximation of maximum matching in dynamic streams requires $\\Omega(\\log\\log{n})$ passes. This presents the first multi-pass lower bound for this problem, which is already also optimal, settling a longstanding open question in this area.","sentences":["A semi-streaming algorithm in dynamic graph streams processes any $n$-vertex graph by making one or multiple passes over a stream of insertions and deletions to edges of the graph and using $O(n \\cdot \\mbox{polylog}(n))$ space.","Semi-streaming algorithms for dynamic streams were first obtained in the seminal work of Ahn, Guha, and McGregor in 2012, alongside the introduction of the graph sketching technique, which remains the de facto way of designing algorithms in this model and a highly popular technique for designing graph algorithms in general.   ","We settle the pass complexity of approximating maximum matchings in dynamic streams via semi-streaming algorithms by improving the state-of-the-art in both upper and lower bounds.   ","We present a randomized sketching based semi-streaming algorithm for $O(1)$-approximation of maximum matching in dynamic streams using $O(\\log\\log{n})$ passes.","The approximation ratio of this algorithm can be improved to $(1+\\epsilon)$ for any fixed $\\epsilon > 0$ even on weighted graphs using standard techniques.","This exponentially improves upon several $O(\\log{n})$ pass algorithms developed for this problem since the introduction of the dynamic graph streaming model.   ","In addition, we prove that any semi-streaming algorithm (not only sketching based) for $O(1)$-approximation of maximum matching in dynamic streams requires $\\Omega(\\log\\log{n})$ passes.","This presents the first multi-pass lower bound for this problem, which is already also optimal, settling a longstanding open question in this area."],"url":"http://arxiv.org/abs/2407.21005v1"}
{"created":"2024-07-30 17:51:44","title":"Evolver: Chain-of-Evolution Prompting to Boost Large Multimodal Models for Hateful Meme Detection","abstract":"Recent advances show that two-stream approaches have achieved outstanding performance in hateful meme detection. However, hateful memes constantly evolve as new memes emerge by fusing progressive cultural ideas, making existing methods obsolete or ineffective. In this work, we explore the potential of Large Multimodal Models (LMMs) for hateful meme detection. To this end, we propose Evolver, which incorporates LMMs via Chain-of-Evolution (CoE) Prompting, by integrating the evolution attribute and in-context information of memes. Specifically, Evolver simulates the evolving and expressing process of memes and reasons through LMMs in a step-by-step manner. First, an evolutionary pair mining module retrieves the top-k most similar memes in the external curated meme set with the input meme. Second, an evolutionary information extractor is designed to summarize the semantic regularities between the paired memes for prompting. Finally, a contextual relevance amplifier enhances the in-context hatefulness information to boost the search for evolutionary processes. Extensive experiments on public FHM, MAMI, and HarM datasets show that CoE prompting can be incorporated into existing LMMs to improve their performance. More encouragingly, it can serve as an interpretive tool to promote the understanding of the evolution of social memes.","sentences":["Recent advances show that two-stream approaches have achieved outstanding performance in hateful meme detection.","However, hateful memes constantly evolve as new memes emerge by fusing progressive cultural ideas, making existing methods obsolete or ineffective.","In this work, we explore the potential of Large Multimodal Models (LMMs) for hateful meme detection.","To this end, we propose Evolver, which incorporates LMMs via Chain-of-Evolution (CoE) Prompting, by integrating the evolution attribute and in-context information of memes.","Specifically, Evolver simulates the evolving and expressing process of memes and reasons through LMMs in a step-by-step manner.","First, an evolutionary pair mining module retrieves the top-k most similar memes in the external curated meme set with the input meme.","Second, an evolutionary information extractor is designed to summarize the semantic regularities between the paired memes for prompting.","Finally, a contextual relevance amplifier enhances the in-context hatefulness information to boost the search for evolutionary processes.","Extensive experiments on public FHM, MAMI, and HarM datasets show that CoE prompting can be incorporated into existing LMMs to improve their performance.","More encouragingly, it can serve as an interpretive tool to promote the understanding of the evolution of social memes."],"url":"http://arxiv.org/abs/2407.21004v1"}
{"created":"2024-07-30 17:49:21","title":"XHand: Real-time Expressive Hand Avatar","abstract":"Hand avatars play a pivotal role in a wide array of digital interfaces, enhancing user immersion and facilitating natural interaction within virtual environments. While previous studies have focused on photo-realistic hand rendering, little attention has been paid to reconstruct the hand geometry with fine details, which is essential to rendering quality. In the realms of extended reality and gaming, on-the-fly rendering becomes imperative. To this end, we introduce an expressive hand avatar, named XHand, that is designed to comprehensively generate hand shape, appearance, and deformations in real-time. To obtain fine-grained hand meshes, we make use of three feature embedding modules to predict hand deformation displacements, albedo, and linear blending skinning weights, respectively. To achieve photo-realistic hand rendering on fine-grained meshes, our method employs a mesh-based neural renderer by leveraging mesh topological consistency and latent codes from embedding modules. During training, a part-aware Laplace smoothing strategy is proposed by incorporating the distinct levels of regularization to effectively maintain the necessary details and eliminate the undesired artifacts. The experimental evaluations on InterHand2.6M and DeepHandMesh datasets demonstrate the efficacy of XHand, which is able to recover high-fidelity geometry and texture for hand animations across diverse poses in real-time. To reproduce our results, we will make the full implementation publicly available at https://github.com/agnJason/XHand.","sentences":["Hand avatars play a pivotal role in a wide array of digital interfaces, enhancing user immersion and facilitating natural interaction within virtual environments.","While previous studies have focused on photo-realistic hand rendering, little attention has been paid to reconstruct the hand geometry with fine details, which is essential to rendering quality.","In the realms of extended reality and gaming, on-the-fly rendering becomes imperative.","To this end, we introduce an expressive hand avatar, named XHand, that is designed to comprehensively generate hand shape, appearance, and deformations in real-time.","To obtain fine-grained hand meshes, we make use of three feature embedding modules to predict hand deformation displacements, albedo, and linear blending skinning weights, respectively.","To achieve photo-realistic hand rendering on fine-grained meshes, our method employs a mesh-based neural renderer by leveraging mesh topological consistency and latent codes from embedding modules.","During training, a part-aware Laplace smoothing strategy is proposed by incorporating the distinct levels of regularization to effectively maintain the necessary details and eliminate the undesired artifacts.","The experimental evaluations on InterHand2.6M and DeepHandMesh datasets demonstrate the efficacy of XHand, which is able to recover high-fidelity geometry and texture for hand animations across diverse poses in real-time.","To reproduce our results, we will make the full implementation publicly available at https://github.com/agnJason/XHand."],"url":"http://arxiv.org/abs/2407.21002v1"}
{"created":"2024-07-30 17:46:06","title":"GABInsight: Exploring Gender-Activity Binding Bias in Vision-Language Models","abstract":"Vision-language models (VLMs) are intensively used in many downstream tasks, including those requiring assessments of individuals appearing in the images. While VLMs perform well in simple single-person scenarios, in real-world applications, we often face complex situations in which there are persons of different genders doing different activities. We show that in such cases, VLMs are biased towards identifying the individual with the expected gender (according to ingrained gender stereotypes in the model or other forms of sample selection bias) as the performer of the activity. We refer to this bias in associating an activity with the gender of its actual performer in an image or text as the Gender-Activity Binding (GAB) bias and analyze how this bias is internalized in VLMs. To assess this bias, we have introduced the GAB dataset with approximately 5500 AI-generated images that represent a variety of activities, addressing the scarcity of real-world images for some scenarios. To have extensive quality control, the generated images are evaluated for their diversity, quality, and realism. We have tested 12 renowned pre-trained VLMs on this dataset in the context of text-to-image and image-to-text retrieval to measure the effect of this bias on their predictions. Additionally, we have carried out supplementary experiments to quantify the bias in VLMs' text encoders and to evaluate VLMs' capability to recognize activities. Our experiments indicate that VLMs experience an average performance decline of about 13.2% when confronted with gender-activity binding bias.","sentences":["Vision-language models (VLMs) are intensively used in many downstream tasks, including those requiring assessments of individuals appearing in the images.","While VLMs perform well in simple single-person scenarios, in real-world applications, we often face complex situations in which there are persons of different genders doing different activities.","We show that in such cases, VLMs are biased towards identifying the individual with the expected gender (according to ingrained gender stereotypes in the model or other forms of sample selection bias) as the performer of the activity.","We refer to this bias in associating an activity with the gender of its actual performer in an image or text as the Gender-Activity Binding (GAB) bias and analyze how this bias is internalized in VLMs.","To assess this bias, we have introduced the GAB dataset with approximately 5500 AI-generated images that represent a variety of activities, addressing the scarcity of real-world images for some scenarios.","To have extensive quality control, the generated images are evaluated for their diversity, quality, and realism.","We have tested 12 renowned pre-trained VLMs on this dataset in the context of text-to-image and image-to-text retrieval to measure the effect of this bias on their predictions.","Additionally, we have carried out supplementary experiments to quantify the bias in VLMs' text encoders and to evaluate VLMs' capability to recognize activities.","Our experiments indicate that VLMs experience an average performance decline of about 13.2% when confronted with gender-activity binding bias."],"url":"http://arxiv.org/abs/2407.21001v1"}
{"created":"2024-07-30 17:38:24","title":"MoFO: Momentum-Filtered Optimizer for Mitigating Forgetting in LLM Fine-Tuning","abstract":"Recently, large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks. Typically, an LLM is pre-trained on large corpora and subsequently fine-tuned on task-specific datasets. However, during finetuning, LLMs may forget the knowledge acquired in the pretraining stage, leading to a decline in general capabilities. To address this issue, we propose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO). The key idea of MoFO is to iteratively select and update the model parameters with the largest momentum magnitudes. Compared to full-parameter training, MoFO achieves similar fine-tuning performance while keeping parameters closer to the pre-trained model, thereby mitigating knowledge forgetting. Unlike most existing methods for forgetting mitigation, MoFO combines the following two advantages. First, MoFO does not require access to pre-training data. This makes MoFO particularly suitable for fine-tuning scenarios where pre-training data is unavailable, such as fine-tuning checkpoint-only open-source LLMs. Second, MoFO does not alter the original loss function. This could avoid impairing the model performance on the fine-tuning tasks. We validate MoFO through rigorous convergence analysis and extensive experiments, demonstrating its superiority over existing methods in mitigating forgetting and enhancing fine-tuning performance.","sentences":["Recently, large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks.","Typically, an LLM is pre-trained on large corpora and subsequently fine-tuned on task-specific datasets.","However, during finetuning, LLMs may forget the knowledge acquired in the pretraining stage, leading to a decline in general capabilities.","To address this issue, we propose a new fine-tuning algorithm termed Momentum-Filtered Optimizer (MoFO).","The key idea of MoFO is to iteratively select and update the model parameters with the largest momentum magnitudes.","Compared to full-parameter training, MoFO achieves similar fine-tuning performance while keeping parameters closer to the pre-trained model, thereby mitigating knowledge forgetting.","Unlike most existing methods for forgetting mitigation, MoFO combines the following two advantages.","First, MoFO does not require access to pre-training data.","This makes MoFO particularly suitable for fine-tuning scenarios where pre-training data is unavailable, such as fine-tuning checkpoint-only open-source LLMs.","Second, MoFO does not alter the original loss function.","This could avoid impairing the model performance on the fine-tuning tasks.","We validate MoFO through rigorous convergence analysis and extensive experiments, demonstrating its superiority over existing methods in mitigating forgetting and enhancing fine-tuning performance."],"url":"http://arxiv.org/abs/2407.20999v1"}
{"created":"2024-07-30 17:27:20","title":"From Feature Importance to Natural Language Explanations Using LLMs with RAG","abstract":"As machine learning becomes increasingly integral to autonomous decision-making processes involving human interaction, the necessity of comprehending the model's outputs through conversational means increases. Most recently, foundation models are being explored for their potential as post hoc explainers, providing a pathway to elucidate the decision-making mechanisms of predictive models. In this work, we introduce traceable question-answering, leveraging an external knowledge repository to inform the responses of Large Language Models (LLMs) to user queries within a scene understanding task. This knowledge repository comprises contextual details regarding the model's output, containing high-level features, feature importance, and alternative probabilities. We employ subtractive counterfactual reasoning to compute feature importance, a method that entails analysing output variations resulting from decomposing semantic features. Furthermore, to maintain a seamless conversational flow, we integrate four key characteristics - social, causal, selective, and contrastive - drawn from social science research on human explanations into a single-shot prompt, guiding the response generation process. Our evaluation demonstrates that explanations generated by the LLMs encompassed these elements, indicating its potential to bridge the gap between complex model outputs and natural language expressions.","sentences":["As machine learning becomes increasingly integral to autonomous decision-making processes involving human interaction, the necessity of comprehending the model's outputs through conversational means increases.","Most recently, foundation models are being explored for their potential as post hoc explainers, providing a pathway to elucidate the decision-making mechanisms of predictive models.","In this work, we introduce traceable question-answering, leveraging an external knowledge repository to inform the responses of Large Language Models (LLMs) to user queries within a scene understanding task.","This knowledge repository comprises contextual details regarding the model's output, containing high-level features, feature importance, and alternative probabilities.","We employ subtractive counterfactual reasoning to compute feature importance, a method that entails analysing output variations resulting from decomposing semantic features.","Furthermore, to maintain a seamless conversational flow, we integrate four key characteristics - social, causal, selective, and contrastive - drawn from social science research on human explanations into a single-shot prompt, guiding the response generation process.","Our evaluation demonstrates that explanations generated by the LLMs encompassed these elements, indicating its potential to bridge the gap between complex model outputs and natural language expressions."],"url":"http://arxiv.org/abs/2407.20990v1"}
{"created":"2024-07-30 17:26:16","title":"Contrasting Deep Learning Models for Direct Respiratory Insufficiency Detection Versus Blood Oxygen Saturation Estimation","abstract":"We contrast high effectiveness of state of the art deep learning architectures designed for general audio classification tasks, refined for respiratory insufficiency (RI) detection and blood oxygen saturation (SpO2) estimation and classification through automated audio analysis. Recently, multiple deep learning architectures have been proposed to detect RI in COVID patients through audio analysis, achieving accuracy above 95% and F1-score above 0.93. RI is a condition associated with low SpO2 levels, commonly defined as the threshold SpO2 <92%. While SpO2 serves as a crucial determinant of RI, a medical doctor's diagnosis typically relies on multiple factors. These include respiratory frequency, heart rate, SpO2 levels, among others. Here we study pretrained audio neural networks (CNN6, CNN10 and CNN14) and the Masked Autoencoder (Audio-MAE) for RI detection, where these models achieve near perfect accuracy, surpassing previous results. Yet, for the regression task of estimating SpO2 levels, the models achieve root mean square error values exceeding the accepted clinical range of 3.5% for finger oximeters. Additionally, Pearson correlation coefficients fail to surpass 0.3. As deep learning models perform better in classification than regression, we transform SpO2-regression into a SpO2-threshold binary classification problem, with a threshold of 92%. However, this task still yields an F1-score below 0.65. Thus, audio analysis offers valuable insights into a patient's RI status, but does not provide accurate information about actual SpO2 levels, indicating a separation of domains in which voice and speech biomarkers may and may not be useful in medical diagnostics under current technologies.","sentences":["We contrast high effectiveness of state of the art deep learning architectures designed for general audio classification tasks, refined for respiratory insufficiency (RI) detection and blood oxygen saturation (SpO2) estimation and classification through automated audio analysis.","Recently, multiple deep learning architectures have been proposed to detect RI in COVID patients through audio analysis, achieving accuracy above 95% and F1-score above 0.93.","RI is a condition associated with low SpO2 levels, commonly defined as the threshold SpO2 <92%.","While SpO2 serves as a crucial determinant of RI, a medical doctor's diagnosis typically relies on multiple factors.","These include respiratory frequency, heart rate, SpO2 levels, among others.","Here we study pretrained audio neural networks (CNN6, CNN10 and CNN14) and the Masked Autoencoder (Audio-MAE) for RI detection, where these models achieve near perfect accuracy, surpassing previous results.","Yet, for the regression task of estimating SpO2 levels, the models achieve root mean square error values exceeding the accepted clinical range of 3.5% for finger oximeters.","Additionally, Pearson correlation coefficients fail to surpass 0.3.","As deep learning models perform better in classification than regression, we transform SpO2-regression into a SpO2-threshold binary classification problem, with a threshold of 92%.","However, this task still yields an F1-score below 0.65.","Thus, audio analysis offers valuable insights into a patient's RI status, but does not provide accurate information about actual SpO2 levels, indicating a separation of domains in which voice and speech biomarkers may and may not be useful in medical diagnostics under current technologies."],"url":"http://arxiv.org/abs/2407.20989v1"}
{"created":"2024-07-30 17:21:32","title":"PIXELMOD: Improving Soft Moderation of Visual Misleading Information on Twitter","abstract":"Images are a powerful and immediate vehicle to carry misleading or outright false messages, yet identifying image-based misinformation at scale poses unique challenges. In this paper, we present PIXELMOD, a system that leverages perceptual hashes, vector databases, and optical character recognition (OCR) to efficiently identify images that are candidates to receive soft moderation labels on Twitter. We show that PIXELMOD outperforms existing image similarity approaches when applied to soft moderation, with negligible performance overhead. We then test PIXELMOD on a dataset of tweets surrounding the 2020 US Presidential Election, and find that it is able to identify visually misleading images that are candidates for soft moderation with 0.99% false detection and 2.06% false negatives.","sentences":["Images are a powerful and immediate vehicle to carry misleading or outright false messages, yet identifying image-based misinformation at scale poses unique challenges.","In this paper, we present PIXELMOD, a system that leverages perceptual hashes, vector databases, and optical character recognition (OCR) to efficiently identify images that are candidates to receive soft moderation labels on Twitter.","We show that PIXELMOD outperforms existing image similarity approaches when applied to soft moderation, with negligible performance overhead.","We then test PIXELMOD on a dataset of tweets surrounding the 2020 US Presidential Election, and find that it is able to identify visually misleading images that are candidates for soft moderation with 0.99% false detection and 2.06% false negatives."],"url":"http://arxiv.org/abs/2407.20987v1"}
{"created":"2024-07-30 17:18:03","title":"Securing Proof of Stake Blockchains: Leveraging Multi-Agent Reinforcement Learning for Detecting and Mitigating Malicious Nodes","abstract":"Proof of Stake (PoS) blockchains offer promising alternatives to traditional Proof of Work (PoW) systems, providing scalability and energy efficiency. However, blockchains operate in a decentralized manner and the network is composed of diverse users. This openness creates the potential for malicious nodes to disrupt the network in various ways. Therefore, it is crucial to embed a mechanism within the blockchain network to constantly monitor, identify, and eliminate these malicious nodes without involving any central authority. In this paper, we propose MRL-PoS+, a novel consensus algorithm to enhance the security of PoS blockchains by leveraging Multi-agent Reinforcement Learning (MRL) techniques. Our proposed consensus algorithm introduces a penalty-reward scheme for detecting and eliminating malicious nodes. This approach involves the detection of behaviors that can lead to potential attacks in a blockchain network and hence penalizes the malicious nodes, restricting them from performing certain actions. Our developed Proof of Concept demonstrates effectiveness in eliminating malicious nodes for six types of major attacks. Experimental results demonstrate that MRL-PoS+ significantly improves the attack resilience of PoS blockchains compared to the traditional schemes without incurring additional computation overhead.","sentences":["Proof of Stake (PoS) blockchains offer promising alternatives to traditional Proof of Work (PoW) systems, providing scalability and energy efficiency.","However, blockchains operate in a decentralized manner and the network is composed of diverse users.","This openness creates the potential for malicious nodes to disrupt the network in various ways.","Therefore, it is crucial to embed a mechanism within the blockchain network to constantly monitor, identify, and eliminate these malicious nodes without involving any central authority.","In this paper, we propose MRL-PoS+, a novel consensus algorithm to enhance the security of PoS blockchains by leveraging Multi-agent Reinforcement Learning (MRL) techniques.","Our proposed consensus algorithm introduces a penalty-reward scheme for detecting and eliminating malicious nodes.","This approach involves the detection of behaviors that can lead to potential attacks in a blockchain network and hence penalizes the malicious nodes, restricting them from performing certain actions.","Our developed Proof of Concept demonstrates effectiveness in eliminating malicious nodes for six types of major attacks.","Experimental results demonstrate that MRL-PoS+ significantly improves the attack resilience of PoS blockchains compared to the traditional schemes without incurring additional computation overhead."],"url":"http://arxiv.org/abs/2407.20983v1"}
{"created":"2024-07-30 17:17:33","title":"Escape Sensing Games: Detection-vs-Evasion in Security Applications","abstract":"Traditional game-theoretic research for security applications primarily focuses on the allocation of external protection resources to defend targets. This work puts forward the study of a new class of games centered around strategically arranging targets to protect them against a constrained adversary, with motivations from varied domains such as peacekeeping resource transit and cybersecurity. Specifically, we introduce Escape Sensing Games (ESGs). In ESGs, a blue player manages the order in which targets pass through a channel, while her opponent tries to capture the targets using a set of sensors that need some time to recharge after each activation. We present a thorough computational study of ESGs. Among others, we show that it is NP-hard to compute best responses and equilibria. Nevertheless, we propose a variety of effective (heuristic) algorithms whose quality we demonstrate in extensive computational experiments.","sentences":["Traditional game-theoretic research for security applications primarily focuses on the allocation of external protection resources to defend targets.","This work puts forward the study of a new class of games centered around strategically arranging targets to protect them against a constrained adversary, with motivations from varied domains such as peacekeeping resource transit and cybersecurity.","Specifically, we introduce Escape Sensing Games (ESGs).","In ESGs, a blue player manages the order in which targets pass through a channel, while her opponent tries to capture the targets using a set of sensors that need some time to recharge after each activation.","We present a thorough computational study of ESGs.","Among others, we show that it is NP-hard to compute best responses and equilibria.","Nevertheless, we propose a variety of effective (heuristic) algorithms whose quality we demonstrate in extensive computational experiments."],"url":"http://arxiv.org/abs/2407.20981v1"}
{"created":"2024-07-30 17:16:54","title":"Impact of Conflicting Transactions in Blockchain: Detecting and Mitigating Potential Attacks","abstract":"Conflicting transactions within blockchain networks not only pose performance challenges but also introduce security vulnerabilities, potentially facilitating malicious attacks. In this paper, we explore the impact of conflicting transactions on blockchain attack vectors. Through modeling and simulation, we delve into the dynamics of four pivotal attacks - block withholding, double spending, balance, and distributed denial of service (DDoS), all orchestrated using conflicting transactions. Our analysis not only focuses on the mechanisms through which these attacks exploit transaction conflicts but also underscores their potential impact on the integrity and reliability of blockchain networks. Additionally, we propose a set of countermeasures for mitigating these attacks. Through implementation and evaluation, we show their effectiveness in lowering attack rates and enhancing overall network performance seamlessly, without introducing additional overhead. Our findings emphasize the critical importance of actively managing conflicting transactions to reinforce blockchain security and performance.","sentences":["Conflicting transactions within blockchain networks not only pose performance challenges but also introduce security vulnerabilities, potentially facilitating malicious attacks.","In this paper, we explore the impact of conflicting transactions on blockchain attack vectors.","Through modeling and simulation, we delve into the dynamics of four pivotal attacks - block withholding, double spending, balance, and distributed denial of service (DDoS), all orchestrated using conflicting transactions.","Our analysis not only focuses on the mechanisms through which these attacks exploit transaction conflicts but also underscores their potential impact on the integrity and reliability of blockchain networks.","Additionally, we propose a set of countermeasures for mitigating these attacks.","Through implementation and evaluation, we show their effectiveness in lowering attack rates and enhancing overall network performance seamlessly, without introducing additional overhead.","Our findings emphasize the critical importance of actively managing conflicting transactions to reinforce blockchain security and performance."],"url":"http://arxiv.org/abs/2407.20980v1"}
{"created":"2024-07-30 16:57:41","title":"Large Language Models (LLMs) for Semantic Communication in Edge-based IoT Networks","abstract":"With the advent of Fifth Generation (5G) and Sixth Generation (6G) communication technologies, as well as the Internet of Things (IoT), semantic communication is gaining attention among researchers as current communication technologies are approaching Shannon's limit. On the other hand, Large Language Models (LLMs) can understand and generate human-like text, based on extensive training on diverse datasets with billions of parameters. Considering the recent near-source computational technologies like Edge, in this article, we give an overview of a framework along with its modules, where LLMs can be used under the umbrella of semantic communication at the network edge for efficient communication in IoT networks. Finally, we discuss a few applications and analyze the challenges and opportunities to develop such systems.","sentences":["With the advent of Fifth Generation (5G) and Sixth Generation (6G) communication technologies, as well as the Internet of Things (IoT), semantic communication is gaining attention among researchers as current communication technologies are approaching Shannon's limit.","On the other hand, Large Language Models (LLMs) can understand and generate human-like text, based on extensive training on diverse datasets with billions of parameters.","Considering the recent near-source computational technologies like Edge, in this article, we give an overview of a framework along with its modules, where LLMs can be used under the umbrella of semantic communication at the network edge for efficient communication in IoT networks.","Finally, we discuss a few applications and analyze the challenges and opportunities to develop such systems."],"url":"http://arxiv.org/abs/2407.20970v1"}
{"created":"2024-07-30 16:55:17","title":"Distributed Symmetric Key Establishment: a Scalable Quantum-Safe Key Distribution Protocol","abstract":"Pre-shared keys (PSK) have been widely used in network security. Nonetheless, existing PSK solutions are not scalable. Moreover, whenever a new user joins a network, PSK requires an existing user to get a new key before they are able to communicate with the new user. The key issue is how to distribute the PSK between different users. Here, we solve this problem by proposing a new protocol called Distributed Symmetric Key Establishment (DSKE). DSKE has the advantage of being scalable. Unlike standard public key infrastructure (PKI) which relies on computational assumptions, DSKE provides information-theoretic security in a universally composable security framework. Specifically, we prove the security (correctness and confidentiality) and robustness of this protocol against a computationally unbounded adversary, who additionally may have fully compromised a bounded number of the intermediaries and can eavesdrop on all communication. DSKE also achieves distributed trust through secret sharing.   We present several implementations of DSKE in real environments, such as providing client services to link encryptors, network encryptors, and mobile phones, as well as the implementation of intermediaries, called Security Hubs, and associated test data as evidence for its versatility. As DSKE is highly scalable in a network setting with no distance limit, it is expected to be a cost-effective quantum-safe cryptographic solution to the network security threat presented by quantum computers.","sentences":["Pre-shared keys (PSK) have been widely used in network security.","Nonetheless, existing PSK solutions are not scalable.","Moreover, whenever a new user joins a network, PSK requires an existing user to get a new key before they are able to communicate with the new user.","The key issue is how to distribute the PSK between different users.","Here, we solve this problem by proposing a new protocol called Distributed Symmetric Key Establishment (DSKE).","DSKE has the advantage of being scalable.","Unlike standard public key infrastructure (PKI) which relies on computational assumptions, DSKE provides information-theoretic security in a universally composable security framework.","Specifically, we prove the security (correctness and confidentiality) and robustness of this protocol against a computationally unbounded adversary, who additionally may have fully compromised a bounded number of the intermediaries and can eavesdrop on all communication.","DSKE also achieves distributed trust through secret sharing.   ","We present several implementations of DSKE in real environments, such as providing client services to link encryptors, network encryptors, and mobile phones, as well as the implementation of intermediaries, called Security Hubs, and associated test data as evidence for its versatility.","As DSKE is highly scalable in a network setting with no distance limit, it is expected to be a cost-effective quantum-safe cryptographic solution to the network security threat presented by quantum computers."],"url":"http://arxiv.org/abs/2407.20969v1"}
{"created":"2024-07-30 16:55:11","title":"SoK: Payment Channel Networks","abstract":"Payment Channel Networks (PCNs) have been proposed as an alternative solution to the scalability, throughput, and cost overhead associated with on-chain transactions. By facilitating offchain execution of transactions, PCNs significantly reduce the burden on the blockchain, leading to faster transaction processing, reduced transaction fees, and enhanced privacy. Despite these advantages, the current research in PCNs presents a variety of research challenges that require further exploration. In this paper, we survey the recent work in several aspects of PCNs, such as pathfinding and routing, virtual channels, state channels, payment channel hubs and rebalancing. This survey aims to provide the reader with a detailed understanding of the current state-of-the-art in PCN research, highlighting a few important advancements. Additionally, we highlight the various unresolved issues in the area of PCN research. Specifically, this paper seeks to answer the following crucial question: What are the various interesting and non-trivial challenges in PCN research that require immediate attention from the academic and research community? By addressing this question, we aim to identify the most pressing problems and future research directions that interested readers can immediately work on. Through this analysis, we hope to inspire researchers and practitioners to tackle these challenges to make PCNs more secure and versatile","sentences":["Payment Channel Networks (PCNs) have been proposed as an alternative solution to the scalability, throughput, and cost overhead associated with on-chain transactions.","By facilitating offchain execution of transactions, PCNs significantly reduce the burden on the blockchain, leading to faster transaction processing, reduced transaction fees, and enhanced privacy.","Despite these advantages, the current research in PCNs presents a variety of research challenges that require further exploration.","In this paper, we survey the recent work in several aspects of PCNs, such as pathfinding and routing, virtual channels, state channels, payment channel hubs and rebalancing.","This survey aims to provide the reader with a detailed understanding of the current state-of-the-art in PCN research, highlighting a few important advancements.","Additionally, we highlight the various unresolved issues in the area of PCN research.","Specifically, this paper seeks to answer the following crucial question: What are the various interesting and non-trivial challenges in PCN research that require immediate attention from the academic and research community?","By addressing this question, we aim to identify the most pressing problems and future research directions that interested readers can immediately work on.","Through this analysis, we hope to inspire researchers and practitioners to tackle these challenges to make PCNs more secure and versatile"],"url":"http://arxiv.org/abs/2407.20968v1"}
{"created":"2024-07-30 16:43:24","title":"MMTrail: A Multimodal Trailer Video Dataset with Language and Music Descriptions","abstract":"Massive multi-modality datasets play a significant role in facilitating the success of large video-language models. However, current video-language datasets primarily provide text descriptions for visual frames, considering audio to be weakly related information. They usually overlook exploring the potential of inherent audio-visual correlation, leading to monotonous annotation within each modality instead of comprehensive and precise descriptions. Such ignorance results in the difficulty of multiple cross-modality studies. To fulfill this gap, we present MMTrail, a large-scale multi-modality video-language dataset incorporating more than 20M trailer clips with visual captions, and 2M high-quality clips with multimodal captions. Trailers preview full-length video works and integrate context, visual frames, and background music. In particular, the trailer has two main advantages: (1) the topics are diverse, and the content characters are of various types, e.g., film, news, and gaming. (2) the corresponding background music is custom-designed, making it more coherent with the visual context. Upon these insights, we propose a systemic captioning framework, achieving various modality annotations with more than 27.1k hours of trailer videos. Here, to ensure the caption retains music perspective while preserving the authority of visual context, we leverage the advanced LLM to merge all annotations adaptively. In this fashion, our MMtrail dataset potentially paves the path for fine-grained large multimodal-language model training. In experiments, we provide evaluation metrics and benchmark results on our dataset, demonstrating the high quality of our annotation and its effectiveness for model training.","sentences":["Massive multi-modality datasets play a significant role in facilitating the success of large video-language models.","However, current video-language datasets primarily provide text descriptions for visual frames, considering audio to be weakly related information.","They usually overlook exploring the potential of inherent audio-visual correlation, leading to monotonous annotation within each modality instead of comprehensive and precise descriptions.","Such ignorance results in the difficulty of multiple cross-modality studies.","To fulfill this gap, we present MMTrail, a large-scale multi-modality video-language dataset incorporating more than 20M trailer clips with visual captions, and 2M high-quality clips with multimodal captions.","Trailers preview full-length video works and integrate context, visual frames, and background music.","In particular, the trailer has two main advantages: (1) the topics are diverse, and the content characters are of various types, e.g., film, news, and gaming.","(2) the corresponding background music is custom-designed, making it more coherent with the visual context.","Upon these insights, we propose a systemic captioning framework, achieving various modality annotations with more than 27.1k hours of trailer videos.","Here, to ensure the caption retains music perspective while preserving the authority of visual context, we leverage the advanced LLM to merge all annotations adaptively.","In this fashion, our MMtrail dataset potentially paves the path for fine-grained large multimodal-language model training.","In experiments, we provide evaluation metrics and benchmark results on our dataset, demonstrating the high quality of our annotation and its effectiveness for model training."],"url":"http://arxiv.org/abs/2407.20962v1"}
{"created":"2024-07-30 16:36:15","title":"Learning Ordinality in Semantic Segmentation","abstract":"Semantic segmentation consists of predicting a semantic label for each image pixel. Conventional deep learning models do not take advantage of ordinal relations that might exist in the domain at hand. For example, it is known that the pupil is inside the iris, and the lane markings are inside the road. Such domain knowledge can be employed as constraints to make the model more robust. The current literature on this topic has explored pixel-wise ordinal segmentation methods, which treat each pixel as an independent observation and promote ordinality in its representation. This paper proposes novel spatial ordinal segmentation methods, which take advantage of the structured image space by considering each pixel as an observation dependent on its neighborhood context to also promote ordinal spatial consistency. When evaluated with five biomedical datasets and multiple configurations of autonomous driving datasets, ordinal methods resulted in more ordinally-consistent models, with substantial improvements in ordinal metrics and some increase in the Dice coefficient. It was also shown that the incorporation of ordinal consistency results in models with better generalization abilities.","sentences":["Semantic segmentation consists of predicting a semantic label for each image pixel.","Conventional deep learning models do not take advantage of ordinal relations that might exist in the domain at hand.","For example, it is known that the pupil is inside the iris, and the lane markings are inside the road.","Such domain knowledge can be employed as constraints to make the model more robust.","The current literature on this topic has explored pixel-wise ordinal segmentation methods, which treat each pixel as an independent observation and promote ordinality in its representation.","This paper proposes novel spatial ordinal segmentation methods, which take advantage of the structured image space by considering each pixel as an observation dependent on its neighborhood context to also promote ordinal spatial consistency.","When evaluated with five biomedical datasets and multiple configurations of autonomous driving datasets, ordinal methods resulted in more ordinally-consistent models, with substantial improvements in ordinal metrics and some increase in the Dice coefficient.","It was also shown that the incorporation of ordinal consistency results in models with better generalization abilities."],"url":"http://arxiv.org/abs/2407.20959v1"}
{"created":"2024-07-30 16:30:09","title":"An Effective Dynamic Gradient Calibration Method for Continual Learning","abstract":"Continual learning (CL) is a fundamental topic in machine learning, where the goal is to train a model with continuously incoming data and tasks. Due to the memory limit, we cannot store all the historical data, and therefore confront the ``catastrophic forgetting'' problem, i.e., the performance on the previous tasks can substantially decrease because of the missing information in the latter period. Though a number of elegant methods have been proposed, the catastrophic forgetting phenomenon still cannot be well avoided in practice. In this paper, we study the problem from the gradient perspective, where our aim is to develop an effective algorithm to calibrate the gradient in each updating step of the model; namely, our goal is to guide the model to be updated in the right direction under the situation that a large amount of historical data are unavailable. Our idea is partly inspired by the seminal stochastic variance reduction methods (e.g., SVRG and SAGA) for reducing the variance of gradient estimation in stochastic gradient descent algorithms. Another benefit is that our approach can be used as a general tool, which is able to be incorporated with several existing popular CL methods to achieve better performance. We also conduct a set of experiments on several benchmark datasets to evaluate the performance in practice.","sentences":["Continual learning (CL) is a fundamental topic in machine learning, where the goal is to train a model with continuously incoming data and tasks.","Due to the memory limit, we cannot store all the historical data, and therefore confront the ``catastrophic forgetting'' problem, i.e., the performance on the previous tasks can substantially decrease because of the missing information in the latter period.","Though a number of elegant methods have been proposed, the catastrophic forgetting phenomenon still cannot be well avoided in practice.","In this paper, we study the problem from the gradient perspective, where our aim is to develop an effective algorithm to calibrate the gradient in each updating step of the model; namely, our goal is to guide the model to be updated in the right direction under the situation that a large amount of historical data are unavailable.","Our idea is partly inspired by the seminal stochastic variance reduction methods (e.g., SVRG and SAGA) for reducing the variance of gradient estimation in stochastic gradient descent algorithms.","Another benefit is that our approach can be used as a general tool, which is able to be incorporated with several existing popular CL methods to achieve better performance.","We also conduct a set of experiments on several benchmark datasets to evaluate the performance in practice."],"url":"http://arxiv.org/abs/2407.20956v1"}
{"created":"2024-07-30 16:29:28","title":"Emotion-driven Piano Music Generation via Two-stage Disentanglement and Functional Representation","abstract":"Managing the emotional aspect remains a challenge in automatic music generation. Prior works aim to learn various emotions at once, leading to inadequate modeling. This paper explores the disentanglement of emotions in piano performance generation through a two-stage framework. The first stage focuses on valence modeling of lead sheet, and the second stage addresses arousal modeling by introducing performance-level attributes. To further capture features that shape valence, an aspect less explored by previous approaches, we introduce a novel functional representation of symbolic music. This representation aims to capture the emotional impact of major-minor tonality, as well as the interactions among notes, chords, and key signatures. Objective and subjective experiments validate the effectiveness of our framework in both emotional valence and arousal modeling. We further leverage our framework in a novel application of emotional controls, showing a broad potential in emotion-driven music generation.","sentences":["Managing the emotional aspect remains a challenge in automatic music generation.","Prior works aim to learn various emotions at once, leading to inadequate modeling.","This paper explores the disentanglement of emotions in piano performance generation through a two-stage framework.","The first stage focuses on valence modeling of lead sheet, and the second stage addresses arousal modeling by introducing performance-level attributes.","To further capture features that shape valence, an aspect less explored by previous approaches, we introduce a novel functional representation of symbolic music.","This representation aims to capture the emotional impact of major-minor tonality, as well as the interactions among notes, chords, and key signatures.","Objective and subjective experiments validate the effectiveness of our framework in both emotional valence and arousal modeling.","We further leverage our framework in a novel application of emotional controls, showing a broad potential in emotion-driven music generation."],"url":"http://arxiv.org/abs/2407.20955v1"}
{"created":"2024-07-30 16:27:52","title":"An evidence-based methodology for human rights impact assessment (HRIA) in the development of AI data-intensive systems","abstract":"Different approaches have been adopted in addressing the challenges of Artificial Intelligence (AI), some centred on personal data and others on ethics, respectively narrowing and broadening the scope of AI regulation. This contribution aims to demonstrate that a third way is possible, starting from the acknowledgement of the role that human rights can play in regulating the impact of data-intensive systems. The focus on human rights is neither a paradigm shift nor a mere theoretical exercise. Through the analysis of more than 700 decisions and documents of the data protection authorities of six countries, we show that human rights already underpin the decisions in the field of data use. Based on empirical analysis of this evidence, this work presents a methodology and a model for a Human Rights Impact Assessment (HRIA). The methodology and related assessment model are focused on AI applications, whose nature and scale require a proper contextualisation of HRIA methodology. Moreover, the proposed models provide a more measurable approach to risk assessment which is consistent with the regulatory proposals centred on risk thresholds. The proposed methodology is tested in concrete case-studies to prove its feasibility and effectiveness. The overall goal is to respond to the growing interest in HRIA, moving from a mere theoretical debate to a concrete and context-specific implementation in the field of data-intensive applications based on AI.","sentences":["Different approaches have been adopted in addressing the challenges of Artificial Intelligence (AI), some centred on personal data and others on ethics, respectively narrowing and broadening the scope of AI regulation.","This contribution aims to demonstrate that a third way is possible, starting from the acknowledgement of the role that human rights can play in regulating the impact of data-intensive systems.","The focus on human rights is neither a paradigm shift nor a mere theoretical exercise.","Through the analysis of more than 700 decisions and documents of the data protection authorities of six countries, we show that human rights already underpin the decisions in the field of data use.","Based on empirical analysis of this evidence, this work presents a methodology and a model for a Human Rights Impact Assessment (HRIA).","The methodology and related assessment model are focused on AI applications, whose nature and scale require a proper contextualisation of HRIA methodology.","Moreover, the proposed models provide a more measurable approach to risk assessment which is consistent with the regulatory proposals centred on risk thresholds.","The proposed methodology is tested in concrete case-studies to prove its feasibility and effectiveness.","The overall goal is to respond to the growing interest in HRIA, moving from a mere theoretical debate to a concrete and context-specific implementation in the field of data-intensive applications based on AI."],"url":"http://arxiv.org/abs/2407.20951v1"}
{"created":"2024-07-30 16:27:51","title":"dopanim: A Dataset of Doppelganger Animals with Noisy Annotations from Multiple Humans","abstract":"Human annotators typically provide annotated data for training machine learning models, such as neural networks. Yet, human annotations are subject to noise, impairing generalization performances. Methodological research on approaches counteracting noisy annotations requires corresponding datasets for a meaningful empirical evaluation. Consequently, we introduce a novel benchmark dataset, dopanim, consisting of about 15,750 animal images of 15 classes with ground truth labels. For approximately 10,500 of these images, 20 humans provided over 52,000 annotations with an accuracy of circa 67%. Its key attributes include (1) the challenging task of classifying doppelganger animals, (2) human-estimated likelihoods as annotations, and (3) annotator metadata. We benchmark well-known multi-annotator learning approaches using seven variants of this dataset and outline further evaluation use cases such as learning beyond hard class labels and active learning. Our dataset and a comprehensive codebase are publicly available to emulate the data collection process and to reproduce all empirical results.","sentences":["Human annotators typically provide annotated data for training machine learning models, such as neural networks.","Yet, human annotations are subject to noise, impairing generalization performances.","Methodological research on approaches counteracting noisy annotations requires corresponding datasets for a meaningful empirical evaluation.","Consequently, we introduce a novel benchmark dataset, dopanim, consisting of about 15,750 animal images of 15 classes with ground truth labels.","For approximately 10,500 of these images, 20 humans provided over 52,000 annotations with an accuracy of circa 67%.","Its key attributes include (1) the challenging task of classifying doppelganger animals, (2) human-estimated likelihoods as annotations, and (3) annotator metadata.","We benchmark well-known multi-annotator learning approaches using seven variants of this dataset and outline further evaluation use cases such as learning beyond hard class labels and active learning.","Our dataset and a comprehensive codebase are publicly available to emulate the data collection process and to reproduce all empirical results."],"url":"http://arxiv.org/abs/2407.20950v1"}
{"created":"2024-07-30 16:25:38","title":"An Asynchronous Multi-core Accelerator for SNN inference","abstract":"Spiking Neural Networks (SNNs) are extensively utilized in brain-inspired computing and neuroscience research. To enhance the speed and energy efficiency of SNNs, several many-core accelerators have been developed. However, maintaining the accuracy of SNNs often necessitates frequent explicit synchronization among all cores, which presents a challenge to overall efficiency. In this paper, we propose an asynchronous architecture for Spiking Neural Networks (SNNs) that eliminates the need for inter-core synchronization, thus enhancing speed and energy efficiency. This approach leverages the pre-determined dependencies of neuromorphic cores established during compilation. Each core is equipped with a scheduler that monitors the status of its dependencies, allowing it to safely advance to the next timestep without waiting for other cores. This eliminates the necessity for global synchronization and minimizes core waiting time despite inherent workload imbalances. Comprehensive evaluations using five different SNN workloads show that our architecture achieves a 1.86x speedup and a 1.55x increase in energy efficiency compared to state-of-the-art synchronization architectures.","sentences":["Spiking Neural Networks (SNNs) are extensively utilized in brain-inspired computing and neuroscience research.","To enhance the speed and energy efficiency of SNNs, several many-core accelerators have been developed.","However, maintaining the accuracy of SNNs often necessitates frequent explicit synchronization among all cores, which presents a challenge to overall efficiency.","In this paper, we propose an asynchronous architecture for Spiking Neural Networks (SNNs) that eliminates the need for inter-core synchronization, thus enhancing speed and energy efficiency.","This approach leverages the pre-determined dependencies of neuromorphic cores established during compilation.","Each core is equipped with a scheduler that monitors the status of its dependencies, allowing it to safely advance to the next timestep without waiting for other cores.","This eliminates the necessity for global synchronization and minimizes core waiting time despite inherent workload imbalances.","Comprehensive evaluations using five different SNN workloads show that our architecture achieves a 1.86x speedup and a 1.55x increase in energy efficiency compared to state-of-the-art synchronization architectures."],"url":"http://arxiv.org/abs/2407.20947v1"}
{"created":"2024-07-30 16:25:18","title":"Physically-consistent Multi-band Massive MIMO Systems: A Radio Resource Management Model","abstract":"Massive multiple-input multiple-output (mMIMO) antenna systems and inter-band carrier aggregation (CA)-enabled multi-band communication are two key technologies to achieve very high data rates in beyond fifth generation (B5G) wireless systems. We propose a joint optimization framework for such systems where the mMIMO antenna spacing selection, precoder optimization, optimum sub-carrier selection and optimum power allocation are carried out simultaneously. We harness the bandwidth gain existing in a tightly coupled base station mMIMO antenna system to avoid sophisticated, non-practical antenna systems for multi-band operation. In particular, we analyze a multi-band communication system using a circuit-theoretic model to consider physical characteristics of a tightly coupled antenna array, and formulate a joint optimization problem to maximize the sum-rate. As part of the optimization, we also propose a novel block iterative water-filling-based sub-carrier selection and power allocation optimization algorithm for the multi-band mMIMO system. A novel sub-carrier windowing-based sub-carrier selection scheme is also proposed which considers the physical constraints (hardware limitation) at the mobile user devices. We carryout the optimizations in two ways: (i) to optimize the antenna spacing selection in an offline manner, and (ii) to select antenna elements from a dense array dynamically. Via computer simulations, we illustrate superior bandwidth gains present in the tightly-coupled colinear and rectangular planar antenna arrays, compared to the loosely-coupled or tightly-coupled parallel arrays. We further show the optimum sum-rate performance of the proposed optimization-based framework under various power allocation schemes and various user capability scenarios.","sentences":["Massive multiple-input multiple-output (mMIMO) antenna systems and inter-band carrier aggregation (CA)-enabled multi-band communication are two key technologies to achieve very high data rates in beyond fifth generation (B5G) wireless systems.","We propose a joint optimization framework for such systems where the mMIMO antenna spacing selection, precoder optimization, optimum sub-carrier selection and optimum power allocation are carried out simultaneously.","We harness the bandwidth gain existing in a tightly coupled base station mMIMO antenna system to avoid sophisticated, non-practical antenna systems for multi-band operation.","In particular, we analyze a multi-band communication system using a circuit-theoretic model to consider physical characteristics of a tightly coupled antenna array, and formulate a joint optimization problem to maximize the sum-rate.","As part of the optimization, we also propose a novel block iterative water-filling-based sub-carrier selection and power allocation optimization algorithm for the multi-band mMIMO system.","A novel sub-carrier windowing-based sub-carrier selection scheme is also proposed which considers the physical constraints (hardware limitation) at the mobile user devices.","We carryout the optimizations in two ways: (i) to optimize the antenna spacing selection in an offline manner, and (ii) to select antenna elements from a dense array dynamically.","Via computer simulations, we illustrate superior bandwidth gains present in the tightly-coupled colinear and rectangular planar antenna arrays, compared to the loosely-coupled or tightly-coupled parallel arrays.","We further show the optimum sum-rate performance of the proposed optimization-based framework under various power allocation schemes and various user capability scenarios."],"url":"http://arxiv.org/abs/2407.20945v1"}
{"created":"2024-07-30 16:22:33","title":"Synthesis of Resource-Efficient Superconducting Circuits with Clock-Free Alternating Logic","abstract":"Gate-level clocking, typical in traditional approaches to Single Flux Quantum (SFQ) technology, makes the effective synthesis of superconducting circuits a significant engineering hurdle. This paper addresses this challenge by employing the recently introduced alternating SFQ (xSFQ) logic family. xSFQ leverages dual-rail alternating encoding to eliminate the clock dependency from the superconducting gate semantics. This obviates the need for ad hoc modifications to existing synthesis tools and avoids unnecessary circuit resource overheads, marking a significant advancement in superconducting circuit design automation. Our implementation results demonstrate an average reduction of over 80\\% in the Josephson junction count for circuits from the ISCAS85, EPFL, and ISCAS89 benchmark suites.","sentences":["Gate-level clocking, typical in traditional approaches to Single Flux Quantum (SFQ) technology, makes the effective synthesis of superconducting circuits a significant engineering hurdle.","This paper addresses this challenge by employing the recently introduced alternating SFQ (xSFQ) logic family.","xSFQ leverages dual-rail alternating encoding to eliminate the clock dependency from the superconducting gate semantics.","This obviates the need for ad hoc modifications to existing synthesis tools and avoids unnecessary circuit resource overheads, marking a significant advancement in superconducting circuit design automation.","Our implementation results demonstrate an average reduction of over 80\\% in the Josephson junction count for circuits from the ISCAS85, EPFL, and ISCAS89 benchmark suites."],"url":"http://arxiv.org/abs/2407.20942v1"}
{"created":"2024-07-30 16:22:19","title":"Random-Order Interval Selection","abstract":"In the problem of online unweighted interval selection, the objective is to maximize the number of non-conflicting intervals accepted by the algorithm. In the conventional online model of irrevocable decisions, there is an Omega(n) lower bound on the competitive ratio, even for randomized algorithms [Bachmann et al. 2013]. In a line of work that allows for revocable acceptances, [Faigle and Nawijn 1995] gave a greedy 1-competitive (i.e. optimal) algorithm in the real-time model, where intervals arrive in order of non-decreasing starting times. The natural extension of their algorithm in the adversarial (any-order) model is 2k-competitive [Borodin and Karavasilis 2023], when there are at most k different interval lengths, and that is optimal for all deterministic, and memoryless randomized algorithms. We study this problem in the random-order model, where the adversary chooses the instance, but the online sequence is a uniformly random permutation of the items. We consider the same algorithm that is optimal in the cases of the real-time and any-order models, and give an upper bound of 2.5 on the competitive ratio under random-order arrivals.   We also show how to utilize random-order arrivals to extract a random bit with a worst case bias of 2/3, when there are at least two distinct item types. We use this bit to derandomize the barely random algorithm of [Fung et al. 2014] and get a deterministic 3-competitive algorithm for single-length interval selection with arbitrary weights.","sentences":["In the problem of online unweighted interval selection, the objective is to maximize the number of non-conflicting intervals accepted by the algorithm.","In the conventional online model of irrevocable decisions, there is an Omega(n) lower bound on the competitive ratio, even for randomized algorithms","[Bachmann et al. 2013].","In a line of work that allows for revocable acceptances, [Faigle and Nawijn 1995] gave a greedy 1-competitive (i.e. optimal) algorithm in the real-time model, where intervals arrive in order of non-decreasing starting times.","The natural extension of their algorithm in the adversarial (any-order) model is 2k-competitive [Borodin and Karavasilis 2023], when there are at most k different interval lengths, and that is optimal for all deterministic, and memoryless randomized algorithms.","We study this problem in the random-order model, where the adversary chooses the instance, but the online sequence is a uniformly random permutation of the items.","We consider the same algorithm that is optimal in the cases of the real-time and any-order models, and give an upper bound of 2.5 on the competitive ratio under random-order arrivals.   ","We also show how to utilize random-order arrivals to extract a random bit with a worst case bias of 2/3, when there are at least two distinct item types.","We use this bit to derandomize the barely random algorithm of [Fung et al. 2014] and get a deterministic 3-competitive algorithm for single-length interval selection with arbitrary weights."],"url":"http://arxiv.org/abs/2407.20941v1"}
{"created":"2024-07-30 16:13:42","title":"Complete Approximations of Incomplete Queries","abstract":"This paper studies the completeness of conjunctive queries over a partially complete database and the approximation of incomplete queries. Given a query and a set of completeness rules (a special kind of tuple generating dependencies) that specify which parts of the database are complete, we investigate whether the query can be fully answered, as if all data were available. If not, we explore reformulating the query into either Maximal Complete Specializations (MCSs) or the (unique up to equivalence) Minimal Complete Generalization (MCG) that can be fully answered, that is, the best complete approximations of the query from below or above in the sense of query containment. We show that the MSG can be characterized as the least fixed-point of a monotonic operator in a preorder. Then, we show that an MCS can be computed by recursive backward application of completeness rules. We study the complexity of both problems and discuss implementation techniques that rely on an ASP and Prolog engines, respectively.","sentences":["This paper studies the completeness of conjunctive queries over a partially complete database and the approximation of incomplete queries.","Given a query and a set of completeness rules (a special kind of tuple generating dependencies) that specify which parts of the database are complete, we investigate whether the query can be fully answered, as if all data were available.","If not, we explore reformulating the query into either Maximal Complete Specializations (MCSs) or the (unique up to equivalence) Minimal Complete Generalization (MCG) that can be fully answered, that is, the best complete approximations of the query from below or above in the sense of query containment.","We show that the MSG can be characterized as the least fixed-point of a monotonic operator in a preorder.","Then, we show that an MCS can be computed by recursive backward application of completeness rules.","We study the complexity of both problems and discuss implementation techniques that rely on an ASP and Prolog engines, respectively."],"url":"http://arxiv.org/abs/2407.20932v1"}
{"created":"2024-07-30 16:06:39","title":"UniProcessor: A Text-induced Unified Low-level Image Processor","abstract":"Image processing, including image restoration, image enhancement, etc., involves generating a high-quality clean image from a degraded input. Deep learning-based methods have shown superior performance for various image processing tasks in terms of single-task conditions. However, they require to train separate models for different degradations and levels, which limits the generalization abilities of these models and restricts their applications in real-world. In this paper, we propose a text-induced unified image processor for low-level vision tasks, termed UniProcessor, which can effectively process various degradation types and levels, and support multimodal control. Specifically, our UniProcessor encodes degradation-specific information with the subject prompt and process degradations with the manipulation prompt. These context control features are injected into the UniProcessor backbone via cross-attention to control the processing procedure. For automatic subject-prompt generation, we further build a vision-language model for general-purpose low-level degradation perception via instruction tuning techniques. Our UniProcessor covers 30 degradation types, and extensive experiments demonstrate that our UniProcessor can well process these degradations without additional training or tuning and outperforms other competing methods. Moreover, with the help of degradation-aware context control, our UniProcessor first shows the ability to individually handle a single distortion in an image with multiple degradations.","sentences":["Image processing, including image restoration, image enhancement, etc., involves generating a high-quality clean image from a degraded input.","Deep learning-based methods have shown superior performance for various image processing tasks in terms of single-task conditions.","However, they require to train separate models for different degradations and levels, which limits the generalization abilities of these models and restricts their applications in real-world.","In this paper, we propose a text-induced unified image processor for low-level vision tasks, termed UniProcessor, which can effectively process various degradation types and levels, and support multimodal control.","Specifically, our UniProcessor encodes degradation-specific information with the subject prompt and process degradations with the manipulation prompt.","These context control features are injected into the UniProcessor backbone via cross-attention to control the processing procedure.","For automatic subject-prompt generation, we further build a vision-language model for general-purpose low-level degradation perception via instruction tuning techniques.","Our UniProcessor covers 30 degradation types, and extensive experiments demonstrate that our UniProcessor can well process these degradations without additional training or tuning and outperforms other competing methods.","Moreover, with the help of degradation-aware context control, our UniProcessor first shows the ability to individually handle a single distortion in an image with multiple degradations."],"url":"http://arxiv.org/abs/2407.20928v1"}
{"created":"2024-07-30 16:01:21","title":"Automatically Removing Unnecessary Stubbings from Test Suites","abstract":"Most modern software systems are characterized by a high number of components whose interactions can affect and complicate testing activities. During testing, developers can account for the interactions by isolating the code under test using test doubles and stubbings. During the evolution of a test suite, stubbings might become unnecessary, and developers should remove unnecessary stubbings, as their definitions can introduce unreliable test results in future versions of the test suite. Unfortunately, removing unnecessary stubbings is still a manual task that can be complex and time-consuming.   To help developers in this task, we propose ARUS, a technique to automatically remove unnecessary stubbings from test suites. Given a software project and its test suite, the technique executes the tests to identify unnecessary stubbings and then removes them using different approaches based on the characteristics of the stubbings. We performed an empirical evaluation based on 128 Java projects that use Mockito for stubbing and contain 280 stubbing definitions that lead to 1,529 unnecessary stubbings. Overall, our technique provides a solution for 276 of the definitions (98.6% resolution rate), ARUS' time cost is negligible, and, on average, the technique's changes introduce a limited increase in code complexity. We submitted ARUS' changes to the projects through pull requests and 83 resolutions are already merged.","sentences":["Most modern software systems are characterized by a high number of components whose interactions can affect and complicate testing activities.","During testing, developers can account for the interactions by isolating the code under test using test doubles and stubbings.","During the evolution of a test suite, stubbings might become unnecessary, and developers should remove unnecessary stubbings, as their definitions can introduce unreliable test results in future versions of the test suite.","Unfortunately, removing unnecessary stubbings is still a manual task that can be complex and time-consuming.   ","To help developers in this task, we propose ARUS, a technique to automatically remove unnecessary stubbings from test suites.","Given a software project and its test suite, the technique executes the tests to identify unnecessary stubbings and then removes them using different approaches based on the characteristics of the stubbings.","We performed an empirical evaluation based on 128 Java projects that use Mockito for stubbing and contain 280 stubbing definitions that lead to 1,529 unnecessary stubbings.","Overall, our technique provides a solution for 276 of the definitions (98.6% resolution rate), ARUS' time cost is negligible, and, on average, the technique's changes introduce a limited increase in code complexity.","We submitted ARUS' changes to the projects through pull requests and 83 resolutions are already merged."],"url":"http://arxiv.org/abs/2407.20924v1"}
{"created":"2024-07-30 15:58:25","title":"SSPA: Split-and-Synthesize Prompting with Gated Alignments for Multi-Label Image Recognition","abstract":"Multi-label image recognition is a fundamental task in computer vision. Recently, Vision-Language Models (VLMs) have made notable advancements in this area. However, previous methods fail to effectively leverage the rich knowledge in language models and often incorporate label semantics into visual features unidirectionally. To overcome these problems, we propose a Split-and-Synthesize Prompting with Gated Alignments (SSPA) framework to amplify the potential of VLMs. Specifically, we develop an in-context learning approach to associate the inherent knowledge from LLMs. Then we propose a novel Split-and-Synthesize Prompting (SSP) strategy to first model the generic knowledge and downstream label semantics individually and then aggregate them carefully through the quaternion network. Moreover, we present Gated Dual-Modal Alignments (GDMA) to bidirectionally interact visual and linguistic modalities while eliminating redundant cross-modal information, enabling more efficient region-level alignments. Rather than making the final prediction by a sharp manner in previous works, we propose a soft aggregator to jointly consider results from all image regions. With the help of flexible prompting and gated alignments, SSPA is generalizable to specific domains. Extensive experiments on nine datasets from three domains (i.e., natural, pedestrian attributes and remote sensing) demonstrate the state-of-the-art performance of SSPA. Further analyses verify the effectiveness of SSP and the interpretability of GDMA. The code will be made public.","sentences":["Multi-label image recognition is a fundamental task in computer vision.","Recently, Vision-Language Models (VLMs) have made notable advancements in this area.","However, previous methods fail to effectively leverage the rich knowledge in language models and often incorporate label semantics into visual features unidirectionally.","To overcome these problems, we propose a Split-and-Synthesize Prompting with Gated Alignments (SSPA) framework to amplify the potential of VLMs.","Specifically, we develop an in-context learning approach to associate the inherent knowledge from LLMs.","Then we propose a novel Split-and-Synthesize Prompting (SSP) strategy to first model the generic knowledge and downstream label semantics individually and then aggregate them carefully through the quaternion network.","Moreover, we present Gated Dual-Modal Alignments (GDMA) to bidirectionally interact visual and linguistic modalities while eliminating redundant cross-modal information, enabling more efficient region-level alignments.","Rather than making the final prediction by a sharp manner in previous works, we propose a soft aggregator to jointly consider results from all image regions.","With the help of flexible prompting and gated alignments, SSPA is generalizable to specific domains.","Extensive experiments on nine datasets from three domains (i.e., natural, pedestrian attributes and remote sensing) demonstrate the state-of-the-art performance of SSPA.","Further analyses verify the effectiveness of SSP and the interpretability of GDMA.","The code will be made public."],"url":"http://arxiv.org/abs/2407.20920v1"}
{"created":"2024-07-30 15:55:01","title":"The Realizability of Revision and Contraction Operators in Epistemic Spaces","abstract":"This paper studies the realizability of belief revision and belief contraction operators in epistemic spaces. We observe that AGM revision and AGM contraction operators for epistemic spaces are only realizable in precisely determined epistemic spaces. We define the class of linear change operators, a special kind of maxichoice operator. When AGM revision, respectively, AGM contraction, is realizable, linear change operators are a canonical realization.","sentences":["This paper studies the realizability of belief revision and belief contraction operators in epistemic spaces.","We observe that AGM revision and AGM contraction operators for epistemic spaces are only realizable in precisely determined epistemic spaces.","We define the class of linear change operators, a special kind of maxichoice operator.","When AGM revision, respectively, AGM contraction, is realizable, linear change operators are a canonical realization."],"url":"http://arxiv.org/abs/2407.20918v1"}
{"created":"2024-07-30 15:54:18","title":"How to Choose a Reinforcement-Learning Algorithm","abstract":"The field of reinforcement learning offers a large variety of concepts and methods to tackle sequential decision-making problems. This variety has become so large that choosing an algorithm for a task at hand can be challenging. In this work, we streamline the process of choosing reinforcement-learning algorithms and action-distribution families. We provide a structured overview of existing methods and their properties, as well as guidelines for when to choose which methods. An interactive version of these guidelines is available online at https://rl-picker.github.io/.","sentences":["The field of reinforcement learning offers a large variety of concepts and methods to tackle sequential decision-making problems.","This variety has become so large that choosing an algorithm for a task at hand can be challenging.","In this work, we streamline the process of choosing reinforcement-learning algorithms and action-distribution families.","We provide a structured overview of existing methods and their properties, as well as guidelines for when to choose which methods.","An interactive version of these guidelines is available online at https://rl-picker.github.io/."],"url":"http://arxiv.org/abs/2407.20917v1"}
{"created":"2024-07-30 15:44:49","title":"An Efficient Convex-Hull Relaxation Based Algorithm for Multi-User Discrete Passive Beamforming","abstract":"Intelligent reflecting surface (IRS) is an emerging technology to enhance spatial multiplexing in wireless networks. This letter considers the discrete passive beamforming design for IRS in order to maximize the minimum signal-to-interference-plus-noise ratio (SINR) among multiple users in an IRS-assisted downlink network. The main design difficulty lies in the discrete phase-shift constraint. Differing from most existing works, this letter advocates a convex-hull relaxation of the discrete constraints which leads to a continuous reformulated problem equivalent to the original discrete problem. This letter further proposes an efficient alternating projection/proximal gradient descent and ascent algorithm for solving the reformulated problem. Simulation results show that the proposed algorithm outperforms the state-of-the-art methods significantly.","sentences":["Intelligent reflecting surface (IRS) is an emerging technology to enhance spatial multiplexing in wireless networks.","This letter considers the discrete passive beamforming design for IRS in order to maximize the minimum signal-to-interference-plus-noise ratio (SINR) among multiple users in an IRS-assisted downlink network.","The main design difficulty lies in the discrete phase-shift constraint.","Differing from most existing works, this letter advocates a convex-hull relaxation of the discrete constraints which leads to a continuous reformulated problem equivalent to the original discrete problem.","This letter further proposes an efficient alternating projection/proximal gradient descent and ascent algorithm for solving the reformulated problem.","Simulation results show that the proposed algorithm outperforms the state-of-the-art methods significantly."],"url":"http://arxiv.org/abs/2407.20914v1"}
{"created":"2024-07-30 15:38:14","title":"What Are Good Positional Encodings for Directed Graphs?","abstract":"Positional encodings (PE) for graphs are essential in constructing powerful and expressive graph neural networks and graph transformers as they effectively capture relative spatial relations between nodes. While PEs for undirected graphs have been extensively studied, those for directed graphs remain largely unexplored, despite the fundamental role of directed graphs in representing entities with strong logical dependencies, such as those in program analysis and circuit designs. This work studies the design of PEs for directed graphs that are expressive to represent desired directed spatial relations. We first propose walk profile, a generalization of walk counting sequence to directed graphs. We identify limitations in existing PE methods, including symmetrized Laplacian PE, Singular Value Decomposition PE, and Magnetic Laplacian PE, in their ability to express walk profiles. To address these limitations, we propose the Multi-q Magnetic Laplacian PE, which extends Magnetic Laplacian PE with multiple potential factors. This simple variant turns out to be capable of provably expressing walk profiles. Furthermore, we generalize previous basis-invariant and stable networks to handle complex-domain PEs decomposed from Magnetic Laplacians. Our numerical experiments demonstrate the effectiveness of Multi-q Magnetic Laplacian PE with a stable neural architecture, outperforming previous PE methods (with stable networks) on predicting directed distances/walk profiles, sorting network satisfiability, and on general circuit benchmarks. Our code is available at https://github.com/Graph-COM/Multi-q-Maglap.","sentences":["Positional encodings (PE) for graphs are essential in constructing powerful and expressive graph neural networks and graph transformers as they effectively capture relative spatial relations between nodes.","While PEs for undirected graphs have been extensively studied, those for directed graphs remain largely unexplored, despite the fundamental role of directed graphs in representing entities with strong logical dependencies, such as those in program analysis and circuit designs.","This work studies the design of PEs for directed graphs that are expressive to represent desired directed spatial relations.","We first propose walk profile, a generalization of walk counting sequence to directed graphs.","We identify limitations in existing PE methods, including symmetrized Laplacian PE, Singular Value Decomposition PE, and Magnetic Laplacian PE, in their ability to express walk profiles.","To address these limitations, we propose the Multi-q Magnetic Laplacian PE, which extends Magnetic Laplacian PE with multiple potential factors.","This simple variant turns out to be capable of provably expressing walk profiles.","Furthermore, we generalize previous basis-invariant and stable networks to handle complex-domain PEs decomposed from Magnetic Laplacians.","Our numerical experiments demonstrate the effectiveness of Multi-q Magnetic Laplacian PE with a stable neural architecture, outperforming previous PE methods (with stable networks) on predicting directed distances/walk profiles, sorting network satisfiability, and on general circuit benchmarks.","Our code is available at https://github.com/Graph-COM/Multi-q-Maglap."],"url":"http://arxiv.org/abs/2407.20912v1"}
{"created":"2024-07-30 15:37:05","title":"Enabling Contextual Soft Moderation on Social Media through Contrastive Textual Deviation","abstract":"Automated soft moderation systems are unable to ascertain if a post supports or refutes a false claim, resulting in a large number of contextual false positives. This limits their effectiveness, for example undermining trust in health experts by adding warnings to their posts or resorting to vague warnings instead of granular fact-checks, which result in desensitizing users. In this paper, we propose to incorporate stance detection into existing automated soft-moderation pipelines, with the goal of ruling out contextual false positives and providing more precise recommendations for social media content that should receive warnings. We develop a textual deviation task called Contrastive Textual Deviation (CTD) and show that it outperforms existing stance detection approaches when applied to soft moderation.We then integrate CTD into the stateof-the-art system for automated soft moderation Lambretta, showing that our approach can reduce contextual false positives from 20% to 2.1%, providing another important building block towards deploying reliable automated soft moderation tools on social media.","sentences":["Automated soft moderation systems are unable to ascertain if a post supports or refutes a false claim, resulting in a large number of contextual false positives.","This limits their effectiveness, for example undermining trust in health experts by adding warnings to their posts or resorting to vague warnings instead of granular fact-checks, which result in desensitizing users.","In this paper, we propose to incorporate stance detection into existing automated soft-moderation pipelines, with the goal of ruling out contextual false positives and providing more precise recommendations for social media content that should receive warnings.","We develop a textual deviation task called Contrastive Textual Deviation (CTD) and show that it outperforms existing stance detection approaches when applied to soft moderation.","We then integrate CTD into the stateof-the-art system for automated soft moderation Lambretta, showing that our approach can reduce contextual false positives from 20% to 2.1%, providing another important building block towards deploying reliable automated soft moderation tools on social media."],"url":"http://arxiv.org/abs/2407.20910v1"}
{"created":"2024-07-30 15:33:58","title":"Dynamic Scene Understanding through Object-Centric Voxelization and Neural Rendering","abstract":"Learning object-centric representations from unsupervised videos is challenging. Unlike most previous approaches that focus on decomposing 2D images, we present a 3D generative model named DynaVol-S for dynamic scenes that enables object-centric learning within a differentiable volume rendering framework. The key idea is to perform object-centric voxelization to capture the 3D nature of the scene, which infers per-object occupancy probabilities at individual spatial locations. These voxel features evolve through a canonical-space deformation function and are optimized in an inverse rendering pipeline with a compositional NeRF. Additionally, our approach integrates 2D semantic features to create 3D semantic grids, representing the scene through multiple disentangled voxel grids. DynaVol-S significantly outperforms existing models in both novel view synthesis and unsupervised decomposition tasks for dynamic scenes. By jointly considering geometric structures and semantic features, it effectively addresses challenging real-world scenarios involving complex object interactions. Furthermore, once trained, the explicitly meaningful voxel features enable additional capabilities that 2D scene decomposition methods cannot achieve, such as novel scene generation through editing geometric shapes or manipulating the motion trajectories of objects.","sentences":["Learning object-centric representations from unsupervised videos is challenging.","Unlike most previous approaches that focus on decomposing 2D images, we present a 3D generative model named DynaVol-S for dynamic scenes that enables object-centric learning within a differentiable volume rendering framework.","The key idea is to perform object-centric voxelization to capture the 3D nature of the scene, which infers per-object occupancy probabilities at individual spatial locations.","These voxel features evolve through a canonical-space deformation function and are optimized in an inverse rendering pipeline with a compositional NeRF.","Additionally, our approach integrates 2D semantic features to create 3D semantic grids, representing the scene through multiple disentangled voxel grids.","DynaVol-S significantly outperforms existing models in both novel view synthesis and unsupervised decomposition tasks for dynamic scenes.","By jointly considering geometric structures and semantic features, it effectively addresses challenging real-world scenarios involving complex object interactions.","Furthermore, once trained, the explicitly meaningful voxel features enable additional capabilities that 2D scene decomposition methods cannot achieve, such as novel scene generation through editing geometric shapes or manipulating the motion trajectories of objects."],"url":"http://arxiv.org/abs/2407.20908v1"}
{"created":"2024-07-30 15:26:36","title":"Automated Review Generation Method Based on Large Language Models","abstract":"Literature research, vital for scientific advancement, is overwhelmed by the vast ocean of available information. Addressing this, we propose an automated review generation method based on Large Language Models (LLMs) to streamline literature processing and reduce cognitive load. In case study on propane dehydrogenation (PDH) catalysts, our method swiftly generated comprehensive reviews from 343 articles, averaging seconds per article per LLM account. Extended analysis of 1041 articles provided deep insights into catalysts' composition, structure, and performance. Recognizing LLMs' hallucinations, we employed a multi-layered quality control strategy, ensuring our method's reliability and effective hallucination mitigation. Expert verification confirms the accuracy and citation integrity of generated reviews, demonstrating LLM hallucination risks reduced to below 0.5% with over 95% confidence. Released Windows application enables one-click review generation, aiding researchers in tracking advancements and recommending literature. This approach showcases LLMs' role in enhancing scientific research productivity and sets the stage for further exploration.","sentences":["Literature research, vital for scientific advancement, is overwhelmed by the vast ocean of available information.","Addressing this, we propose an automated review generation method based on Large Language Models (LLMs) to streamline literature processing and reduce cognitive load.","In case study on propane dehydrogenation (PDH) catalysts, our method swiftly generated comprehensive reviews from 343 articles, averaging seconds per article per LLM account.","Extended analysis of 1041 articles provided deep insights into catalysts' composition, structure, and performance.","Recognizing LLMs' hallucinations, we employed a multi-layered quality control strategy, ensuring our method's reliability and effective hallucination mitigation.","Expert verification confirms the accuracy and citation integrity of generated reviews, demonstrating LLM hallucination risks reduced to below 0.5% with over 95% confidence.","Released Windows application enables one-click review generation, aiding researchers in tracking advancements and recommending literature.","This approach showcases LLMs' role in enhancing scientific research productivity and sets the stage for further exploration."],"url":"http://arxiv.org/abs/2407.20906v1"}
{"created":"2024-07-30 15:23:41","title":"Secure Source Coding Resilient Against Compromised Users via an Access Structure","abstract":"Consider a source and multiple users who observe the independent and identically distributed (i.i.d.) copies of correlated Gaussian random variables. The source wishes to compress its observations and store the result in a public database such that (i) authorized sets of users are able to reconstruct the source with a certain distortion level, and (ii) information leakage to non-authorized sets of colluding users is minimized. In other words, the recovery of the source is restricted to a predefined access structure. The main result of this paper is a closed-form characterization of the fundamental trade-off between the source coding rate and the information leakage rate. As an example, threshold access structures are studied, i.e., the case where any set of at least $t$ users is able to reconstruct the source with some predefined distortion level and the information leakage at any set of users with a size smaller than $t$ is minimized.","sentences":["Consider a source and multiple users who observe the independent and identically distributed (i.i.d.)","copies of correlated Gaussian random variables.","The source wishes to compress its observations and store the result in a public database such that (i) authorized sets of users are able to reconstruct the source with a certain distortion level, and (ii) information leakage to non-authorized sets of colluding users is minimized.","In other words, the recovery of the source is restricted to a predefined access structure.","The main result of this paper is a closed-form characterization of the fundamental trade-off between the source coding rate and the information leakage rate.","As an example, threshold access structures are studied, i.e., the case where any set of at least $t$ users is able to reconstruct the source with some predefined distortion level and the information leakage at any set of users with a size smaller than $t$ is minimized."],"url":"http://arxiv.org/abs/2407.20901v1"}
{"created":"2024-07-30 15:17:57","title":"Visual Analysis of GitHub Issues to Gain Insights","abstract":"Version control systems are integral to software development, with GitHub emerging as a popular online platform due to its comprehensive project management tools, including issue tracking and pull requests. However, GitHub lacks a direct link between issues and commits, making it difficult for developers to understand how specific issues are resolved. Although GitHub's Insights page provides some visualization for repository data, the representation of issues and commits related data in a textual format hampers quick evaluation of issue management. This paper presents a prototype web application that generates visualizations to offer insights into issue timelines and reveals different factors related to issues. It focuses on the lifecycle of issues and depicts vital information to enhance users' understanding of development patterns in their projects. We demonstrate the effectiveness of our approach through case studies involving three open-source GitHub repositories. Furthermore, we conducted a user evaluation to validate the efficacy of our prototype in conveying crucial repository information more efficiently and rapidly.","sentences":["Version control systems are integral to software development, with GitHub emerging as a popular online platform due to its comprehensive project management tools, including issue tracking and pull requests.","However, GitHub lacks a direct link between issues and commits, making it difficult for developers to understand how specific issues are resolved.","Although GitHub's Insights page provides some visualization for repository data, the representation of issues and commits related data in a textual format hampers quick evaluation of issue management.","This paper presents a prototype web application that generates visualizations to offer insights into issue timelines and reveals different factors related to issues.","It focuses on the lifecycle of issues and depicts vital information to enhance users' understanding of development patterns in their projects.","We demonstrate the effectiveness of our approach through case studies involving three open-source GitHub repositories.","Furthermore, we conducted a user evaluation to validate the efficacy of our prototype in conveying crucial repository information more efficiently and rapidly."],"url":"http://arxiv.org/abs/2407.20900v1"}
{"created":"2024-07-30 15:17:15","title":"Faithful and Plausible Natural Language Explanations for Image Classification: A Pipeline Approach","abstract":"Existing explanation methods for image classification struggle to provide faithful and plausible explanations. This paper addresses this issue by proposing a post-hoc natural language explanation method that can be applied to any CNN-based classifier without altering its training process or affecting predictive performance. By analysing influential neurons and the corresponding activation maps, the method generates a faithful description of the classifier's decision process in the form of a structured meaning representation, which is then converted into text by a language model. Through this pipeline approach, the generated explanations are grounded in the neural network architecture, providing accurate insight into the classification process while remaining accessible to non-experts. Experimental results show that the NLEs constructed by our method are significantly more plausible and faithful. In particular, user interventions in the neural network structure (masking of neurons) are three times more effective than the baselines.","sentences":["Existing explanation methods for image classification struggle to provide faithful and plausible explanations.","This paper addresses this issue by proposing a post-hoc natural language explanation method that can be applied to any CNN-based classifier without altering its training process or affecting predictive performance.","By analysing influential neurons and the corresponding activation maps, the method generates a faithful description of the classifier's decision process in the form of a structured meaning representation, which is then converted into text by a language model.","Through this pipeline approach, the generated explanations are grounded in the neural network architecture, providing accurate insight into the classification process while remaining accessible to non-experts.","Experimental results show that the NLEs constructed by our method are significantly more plausible and faithful.","In particular, user interventions in the neural network structure (masking of neurons) are three times more effective than the baselines."],"url":"http://arxiv.org/abs/2407.20899v1"}
{"created":"2024-07-30 15:17:07","title":"ThinkRepair: Self-Directed Automated Program Repair","abstract":"Though many approaches have been proposed for Automated Program Repair (APR) and indeed achieved remarkable performance, they still have limitations in fixing bugs that require analyzing and reasoning about the logic of the buggy program. Recently, large language models (LLMs) instructed by prompt engineering have attracted much attention for their powerful ability to address many kinds of tasks including bug-fixing. However, the quality of the prompt will highly affect the ability of LLMs and manually constructing high-quality prompts is a costly endeavor.   To address this limitation, we propose a self-directed LLM-based automated program repair, ThinkRepair, with two main phases: collection phase and fixing phase. The former phase automatically collects various chains of thoughts that constitute pre-fixed knowledge by instructing LLMs with the Chain-of-Thought (CoT) prompt. The latter phase targets fixing a bug by first selecting examples for few-shot learning and second automatically interacting with LLMs, optionally appending with feedback of testing information.   Evaluations on two widely studied datasets (Defects4J and QuixBugs) by comparing ThinkRepair with 12 SOTA APRs indicate the priority of ThinkRepair in fixing bugs. Notably, ThinkRepair fixes 98 bugs and improves baselines by 27%-344.4% on Defects4J V1.2. On Defects4J V2.0, ThinkRepair fixes 12-65 more bugs than the SOTA APRs. Additionally, ThinkRepair also makes a considerable improvement on QuixBugs (31 for Java and 21 for Python at most).","sentences":["Though many approaches have been proposed for Automated Program Repair (APR) and indeed achieved remarkable performance, they still have limitations in fixing bugs that require analyzing and reasoning about the logic of the buggy program.","Recently, large language models (LLMs) instructed by prompt engineering have attracted much attention for their powerful ability to address many kinds of tasks including bug-fixing.","However, the quality of the prompt will highly affect the ability of LLMs and manually constructing high-quality prompts is a costly endeavor.   ","To address this limitation, we propose a self-directed LLM-based automated program repair, ThinkRepair, with two main phases: collection phase and fixing phase.","The former phase automatically collects various chains of thoughts that constitute pre-fixed knowledge by instructing LLMs with the Chain-of-Thought (CoT) prompt.","The latter phase targets fixing a bug by first selecting examples for few-shot learning and second automatically interacting with LLMs, optionally appending with feedback of testing information.   ","Evaluations on two widely studied datasets (Defects4J and QuixBugs) by comparing ThinkRepair with 12 SOTA APRs indicate the priority of ThinkRepair in fixing bugs.","Notably, ThinkRepair fixes 98 bugs and improves baselines by 27%-344.4% on Defects4J V1.2.","On Defects4J V2.0, ThinkRepair fixes 12-65 more bugs than the SOTA APRs.","Additionally, ThinkRepair also makes a considerable improvement on QuixBugs (31 for Java and 21 for Python at most)."],"url":"http://arxiv.org/abs/2407.20898v1"}
{"created":"2024-07-30 15:12:29","title":"MambaCapsule: Towards Transparent Cardiac Disease Diagnosis with Electrocardiography Using Mamba Capsule Network","abstract":"Cardiac arrhythmia, a condition characterized by irregular heartbeats, often serves as an early indication of various heart ailments. With the advent of deep learning, numerous innovative models have been introduced for diagnosing arrhythmias using Electrocardiogram (ECG) signals. However, recent studies solely focus on the performance of models, neglecting the interpretation of their results. This leads to a considerable lack of transparency, posing a significant risk in the actual diagnostic process. To solve this problem, this paper introduces MambaCapsule, a deep neural networks for ECG arrhythmias classification, which increases the explainability of the model while enhancing the accuracy.Our model utilizes Mamba for feature extraction and Capsule networks for prediction, providing not only a confidence score but also signal features. Akin to the processing mechanism of human brain, the model learns signal features and their relationship between them by reconstructing ECG signals in the predicted selection. The model evaluation was conducted on MIT-BIH and PTB dataset, following the AAMI standard. MambaCapsule has achieved a total accuracy of 99.54% and 99.59% on the test sets respectively. These results demonstrate the promising performance of under the standard test protocol.","sentences":["Cardiac arrhythmia, a condition characterized by irregular heartbeats, often serves as an early indication of various heart ailments.","With the advent of deep learning, numerous innovative models have been introduced for diagnosing arrhythmias using Electrocardiogram (ECG) signals.","However, recent studies solely focus on the performance of models, neglecting the interpretation of their results.","This leads to a considerable lack of transparency, posing a significant risk in the actual diagnostic process.","To solve this problem, this paper introduces MambaCapsule, a deep neural networks for ECG arrhythmias classification, which increases the explainability of the model while enhancing the accuracy.","Our model utilizes Mamba for feature extraction and Capsule networks for prediction, providing not only a confidence score but also signal features.","Akin to the processing mechanism of human brain, the model learns signal features and their relationship between them by reconstructing ECG signals in the predicted selection.","The model evaluation was conducted on MIT-BIH and PTB dataset, following the AAMI standard.","MambaCapsule has achieved a total accuracy of 99.54% and 99.59% on the test sets respectively.","These results demonstrate the promising performance of under the standard test protocol."],"url":"http://arxiv.org/abs/2407.20893v1"}
{"created":"2024-07-30 15:09:45","title":"What is YOLOv5: A deep look into the internal features of the popular object detector","abstract":"This study presents a comprehensive analysis of the YOLOv5 object detection model, examining its architecture, training methodologies, and performance. Key components, including the Cross Stage Partial backbone and Path Aggregation-Network, are explored in detail. The paper reviews the model's performance across various metrics and hardware platforms. Additionally, the study discusses the transition from Darknet to PyTorch and its impact on model development. Overall, this research provides insights into YOLOv5's capabilities and its position within the broader landscape of object detection and why it is a popular choice for constrained edge deployment scenarios.","sentences":["This study presents a comprehensive analysis of the YOLOv5 object detection model, examining its architecture, training methodologies, and performance.","Key components, including the Cross Stage Partial backbone and Path Aggregation-Network, are explored in detail.","The paper reviews the model's performance across various metrics and hardware platforms.","Additionally, the study discusses the transition from Darknet to PyTorch and its impact on model development.","Overall, this research provides insights into YOLOv5's capabilities and its position within the broader landscape of object detection and why it is a popular choice for constrained edge deployment scenarios."],"url":"http://arxiv.org/abs/2407.20892v1"}
{"created":"2024-07-30 15:07:13","title":"Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian Neural Networks","abstract":"Computational complexity of Bayesian learning is impeding its adoption in practical, large-scale tasks. Despite demonstrations of significant merits such as improved robustness and resilience to unseen or out-of-distribution inputs over their non- Bayesian counterparts, their practical use has faded to near insignificance. In this study, we introduce an innovative framework to mitigate the computational burden of Bayesian neural networks (BNNs). Our approach follows the principle of Bayesian techniques based on deep ensembles, but significantly reduces their cost via multiple low-rank perturbations of parameters arising from a pre-trained neural network. Both vanilla version of ensembles as well as more sophisticated schemes such as Bayesian learning with Stein Variational Gradient Descent (SVGD), previously deemed impractical for large models, can be seamlessly implemented within the proposed framework, called Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a dramatic reduction in the number of trainable parameters required to approximate a Bayesian posterior; and ii) it not only maintains, but in some instances, surpasses the performance of conventional Bayesian learning methods and non-Bayesian baselines. Our results with large-scale tasks such as ImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the effectiveness and versatility of Bella in building highly scalable and practical Bayesian deep models for real-world applications.","sentences":["Computational complexity of Bayesian learning is impeding its adoption in practical, large-scale tasks.","Despite demonstrations of significant merits such as improved robustness and resilience to unseen or out-of-distribution inputs over their non- Bayesian counterparts, their practical use has faded to near insignificance.","In this study, we introduce an innovative framework to mitigate the computational burden of Bayesian neural networks (BNNs).","Our approach follows the principle of Bayesian techniques based on deep ensembles, but significantly reduces their cost via multiple low-rank perturbations of parameters arising from a pre-trained neural network.","Both vanilla version of ensembles as well as more sophisticated schemes such as Bayesian learning with Stein Variational Gradient Descent (SVGD), previously deemed impractical for large models, can be seamlessly implemented within the proposed framework, called Bayesian Low-Rank LeArning (Bella).","In a nutshell, i) Bella achieves a dramatic reduction in the number of trainable parameters required to approximate a Bayesian posterior; and ii) it not only maintains, but in some instances, surpasses the performance of conventional Bayesian learning methods and non-Bayesian baselines.","Our results with large-scale tasks such as ImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the effectiveness and versatility of Bella in building highly scalable and practical Bayesian deep models for real-world applications."],"url":"http://arxiv.org/abs/2407.20891v1"}
{"created":"2024-07-30 14:58:11","title":"Effective Black Box Testing of Sentiment Analysis Classification Networks","abstract":"Transformer-based neural networks have demonstrated remarkable performance in natural language processing tasks such as sentiment analysis. Nevertheless, the issue of ensuring the dependability of these complicated architectures through comprehensive testing is still open. This paper presents a collection of coverage criteria specifically designed to assess test suites created for transformer-based sentiment analysis networks. Our approach utilizes input space partitioning, a black-box method, by considering emotionally relevant linguistic features such as verbs, adjectives, adverbs, and nouns. In order to effectively produce test cases that encompass a wide range of emotional elements, we utilize the k-projection coverage metric. This metric minimizes the complexity of the problem by examining subsets of k features at the same time, hence reducing dimensionality. Large language models are employed to generate sentences that display specific combinations of emotional features. The findings from experiments obtained from a sentiment analysis dataset illustrate that our criteria and generated tests have led to an average increase of 16\\% in test coverage. In addition, there is a corresponding average decrease of 6.5\\% in model accuracy, showing the ability to identify vulnerabilities. Our work provides a foundation for improving the dependability of transformer-based sentiment analysis systems through comprehensive test evaluation.","sentences":["Transformer-based neural networks have demonstrated remarkable performance in natural language processing tasks such as sentiment analysis.","Nevertheless, the issue of ensuring the dependability of these complicated architectures through comprehensive testing is still open.","This paper presents a collection of coverage criteria specifically designed to assess test suites created for transformer-based sentiment analysis networks.","Our approach utilizes input space partitioning, a black-box method, by considering emotionally relevant linguistic features such as verbs, adjectives, adverbs, and nouns.","In order to effectively produce test cases that encompass a wide range of emotional elements, we utilize the k-projection coverage metric.","This metric minimizes the complexity of the problem by examining subsets of k features at the same time, hence reducing dimensionality.","Large language models are employed to generate sentences that display specific combinations of emotional features.","The findings from experiments obtained from a sentiment analysis dataset illustrate that our criteria and generated tests have led to an average increase of 16\\% in test coverage.","In addition, there is a corresponding average decrease of 6.5\\% in model accuracy, showing the ability to identify vulnerabilities.","Our work provides a foundation for improving the dependability of transformer-based sentiment analysis systems through comprehensive test evaluation."],"url":"http://arxiv.org/abs/2407.20884v1"}
{"created":"2024-07-30 14:58:02","title":"PiCoGen: Generate Piano Covers with a Two-stage Approach","abstract":"Cover song generation stands out as a popular way of music making in the music-creative community. In this study, we introduce Piano Cover Generation (PiCoGen), a two-stage approach for automatic cover song generation that transcribes the melody line and chord progression of a song given its audio recording, and then uses the resulting lead sheet as the condition to generate a piano cover in the symbolic domain. This approach is advantageous in that it does not required paired data of covers and their original songs for training. Compared to an existing approach that demands such paired data, our evaluation shows that PiCoGen demonstrates competitive or even superior performance across songs of different musical genres.","sentences":["Cover song generation stands out as a popular way of music making in the music-creative community.","In this study, we introduce Piano Cover Generation (PiCoGen), a two-stage approach for automatic cover song generation that transcribes the melody line and chord progression of a song given its audio recording, and then uses the resulting lead sheet as the condition to generate a piano cover in the symbolic domain.","This approach is advantageous in that it does not required paired data of covers and their original songs for training.","Compared to an existing approach that demands such paired data, our evaluation shows that PiCoGen demonstrates competitive or even superior performance across songs of different musical genres."],"url":"http://arxiv.org/abs/2407.20883v1"}
{"created":"2024-07-30 14:56:10","title":"A Scalable Tool For Analyzing Genomic Variants Of Humans Using Knowledge Graphs and Machine Learning","abstract":"The integration of knowledge graphs and graph machine learning (GML) in genomic data analysis offers several opportunities for understanding complex genetic relationships, especially at the RNA level. We present a comprehensive approach for leveraging these technologies to analyze genomic variants, specifically in the context of RNA sequencing (RNA-seq) data from COVID-19 patient samples. The proposed method involves extracting variant-level genetic information, annotating the data with additional metadata using SnpEff, and converting the enriched Variant Call Format (VCF) files into Resource Description Framework (RDF) triples. The resulting knowledge graph is further enhanced with patient metadata and stored in a graph database, facilitating efficient querying and indexing. We utilize the Deep Graph Library (DGL) to perform graph machine learning tasks, including node classification with GraphSAGE and Graph Convolutional Networks (GCNs). Our approach demonstrates significant utility using our proposed tool, VariantKG, in three key scenarios: enriching graphs with new VCF data, creating subgraphs based on user-defined features, and conducting graph machine learning for node classification.","sentences":["The integration of knowledge graphs and graph machine learning (GML) in genomic data analysis offers several opportunities for understanding complex genetic relationships, especially at the RNA level.","We present a comprehensive approach for leveraging these technologies to analyze genomic variants, specifically in the context of RNA sequencing (RNA-seq) data from COVID-19 patient samples.","The proposed method involves extracting variant-level genetic information, annotating the data with additional metadata using SnpEff, and converting the enriched Variant Call Format (VCF) files into Resource Description Framework (RDF) triples.","The resulting knowledge graph is further enhanced with patient metadata and stored in a graph database, facilitating efficient querying and indexing.","We utilize the Deep Graph Library (DGL) to perform graph machine learning tasks, including node classification with GraphSAGE and Graph Convolutional Networks (GCNs).","Our approach demonstrates significant utility using our proposed tool, VariantKG, in three key scenarios: enriching graphs with new VCF data, creating subgraphs based on user-defined features, and conducting graph machine learning for node classification."],"url":"http://arxiv.org/abs/2407.20879v1"}
{"created":"2024-07-30 14:54:54","title":"Automatic Die Studies for Ancient Numismatics","abstract":"Die studies are fundamental to quantifying ancient monetary production, providing insights into the relationship between coinage, politics, and history. The process requires tedious manual work, which limits the size of the corpora that can be studied. Few works have attempted to automate this task, and none have been properly released and evaluated from a computer vision perspective. We propose a fully automatic approach that introduces several innovations compared to previous methods. We rely on fast and robust local descriptors matching that is set automatically. Second, the core of our proposal is a clustering-based approach that uses an intrinsic metric (that does not need the ground truth labels) to determine its critical hyper-parameters. We validate the approach on two corpora of Greek coins, propose an automatic implementation and evaluation of previous baselines, and show that our approach significantly outperforms them.","sentences":["Die studies are fundamental to quantifying ancient monetary production, providing insights into the relationship between coinage, politics, and history.","The process requires tedious manual work, which limits the size of the corpora that can be studied.","Few works have attempted to automate this task, and none have been properly released and evaluated from a computer vision perspective.","We propose a fully automatic approach that introduces several innovations compared to previous methods.","We rely on fast and robust local descriptors matching that is set automatically.","Second, the core of our proposal is a clustering-based approach that uses an intrinsic metric (that does not need the ground truth labels) to determine its critical hyper-parameters.","We validate the approach on two corpora of Greek coins, propose an automatic implementation and evaluation of previous baselines, and show that our approach significantly outperforms them."],"url":"http://arxiv.org/abs/2407.20876v1"}
{"created":"2024-07-30 14:54:02","title":"On the MacWilliams Theorem over Codes and Lattices","abstract":"Analogies between codes and lattices have been extensively studied for the last decades, in this dictionary, the MacWilliams identity is the finite analog of the Jacobi-Poisson formula of the Theta function. Motivated by the random theory of lattices, the statistical significance of MacWilliams theorem is considered, indeed, MacWilliams distribution provides a finite analog of the classical Gauss distribution. In particular, the MacWilliams distribution over quotient space of a code is statistical close to the uniform distribution. In the respect of lattices, the analogy of MacWilliams identity associated with nu-function was conjectured by Sole in 1995. We give an answer to this problem in positive.","sentences":["Analogies between codes and lattices have been extensively studied for the last decades, in this dictionary, the MacWilliams identity is the finite analog of the Jacobi-Poisson formula of the Theta function.","Motivated by the random theory of lattices, the statistical significance of MacWilliams theorem is considered, indeed, MacWilliams distribution provides a finite analog of the classical Gauss distribution.","In particular, the MacWilliams distribution over quotient space of a code is statistical close to the uniform distribution.","In the respect of lattices, the analogy of MacWilliams identity associated with nu-function was conjectured by Sole in 1995.","We give an answer to this problem in positive."],"url":"http://arxiv.org/abs/2407.20874v1"}
{"created":"2024-07-30 14:50:49","title":"A Comparative Analysis of YOLOv5, YOLOv8, and YOLOv10 in Kitchen Safety","abstract":"Knife safety in the kitchen is essential for preventing accidents or injuries with an emphasis on proper handling, maintenance, and storage methods. This research presents a comparative analysis of three YOLO models, YOLOv5, YOLOv8, and YOLOv10, to detect the hazards involved in handling knife, concentrating mainly on ensuring fingers are curled while holding items to be cut and that hands should only be in contact with knife handle avoiding the blade. Precision, recall, F-score, and normalized confusion matrix are used to evaluate the performance of the models. The results indicate that YOLOv5 performed better than the other two models in identifying the hazard of ensuring hands only touch the blade, while YOLOv8 excelled in detecting the hazard of curled fingers while holding items. YOLOv5 and YOLOv8 performed almost identically in recognizing classes such as hand, knife, and vegetable, whereas YOLOv5, YOLOv8, and YOLOv10 accurately identified the cutting board. This paper provides insights into the advantages and shortcomings of these models in real-world settings. Moreover, by detailing the optimization of YOLO architectures for safe knife handling, this study promotes the development of increased accuracy and efficiency in safety surveillance systems.","sentences":["Knife safety in the kitchen is essential for preventing accidents or injuries with an emphasis on proper handling, maintenance, and storage methods.","This research presents a comparative analysis of three YOLO models, YOLOv5, YOLOv8, and YOLOv10, to detect the hazards involved in handling knife, concentrating mainly on ensuring fingers are curled while holding items to be cut and that hands should only be in contact with knife handle avoiding the blade.","Precision, recall, F-score, and normalized confusion matrix are used to evaluate the performance of the models.","The results indicate that YOLOv5 performed better than the other two models in identifying the hazard of ensuring hands only touch the blade, while YOLOv8 excelled in detecting the hazard of curled fingers while holding items.","YOLOv5 and YOLOv8 performed almost identically in recognizing classes such as hand, knife, and vegetable, whereas YOLOv5, YOLOv8, and YOLOv10 accurately identified the cutting board.","This paper provides insights into the advantages and shortcomings of these models in real-world settings.","Moreover, by detailing the optimization of YOLO architectures for safe knife handling, this study promotes the development of increased accuracy and efficiency in safety surveillance systems."],"url":"http://arxiv.org/abs/2407.20872v1"}
{"created":"2024-07-30 14:45:40","title":"Co-Neighbor Encoding Schema: A Light-cost Structure Encoding Method for Dynamic Link Prediction","abstract":"Structure encoding has proven to be the key feature to distinguishing links in a graph. However, Structure encoding in the temporal graph keeps changing as the graph evolves, repeatedly computing such features can be time-consuming due to the high-order subgraph construction. We develop the Co-Neighbor Encoding Schema (CNES) to address this issue. Instead of recomputing the feature by the link, CNES stores information in the memory to avoid redundant calculations. Besides, unlike the existing memory-based dynamic graph learning method that stores node hidden states, we introduce a hashtable-based memory to compress the adjacency matrix for efficient structure feature construction and updating with vector computation in parallel. Furthermore, CNES introduces a Temporal-Diverse Memory to generate long-term and short-term structure encoding for neighbors with different structural information. A dynamic graph learning framework, Co-Neighbor Encoding Network (CNE-N), is proposed using the aforementioned techniques. Extensive experiments on thirteen public datasets verify the effectiveness and efficiency of the proposed method.","sentences":["Structure encoding has proven to be the key feature to distinguishing links in a graph.","However, Structure encoding in the temporal graph keeps changing as the graph evolves, repeatedly computing such features can be time-consuming due to the high-order subgraph construction.","We develop the Co-Neighbor Encoding Schema (CNES) to address this issue.","Instead of recomputing the feature by the link, CNES stores information in the memory to avoid redundant calculations.","Besides, unlike the existing memory-based dynamic graph learning method that stores node hidden states, we introduce a hashtable-based memory to compress the adjacency matrix for efficient structure feature construction and updating with vector computation in parallel.","Furthermore, CNES introduces a Temporal-Diverse Memory to generate long-term and short-term structure encoding for neighbors with different structural information.","A dynamic graph learning framework, Co-Neighbor Encoding Network (CNE-N), is proposed using the aforementioned techniques.","Extensive experiments on thirteen public datasets verify the effectiveness and efficiency of the proposed method."],"url":"http://arxiv.org/abs/2407.20871v1"}
{"created":"2024-07-30 14:45:31","title":"Mean of Means: A 10-dollar Solution for Human Localization with Calibration-free and Unconstrained Camera Settings","abstract":"Accurate human localization is crucial for various applications, especially in the Metaverse era. Existing high precision solutions rely on expensive, tag-dependent hardware, while vision-based methods offer a cheaper, tag-free alternative. However, current vision solutions based on stereo vision face limitations due to rigid perspective transformation principles and error propagation in multi-stage SVD solvers. These solutions also require multiple high-resolution cameras with strict setup constraints. To address these limitations, we propose a probabilistic approach that considers all points on the human body as observations generated by a distribution centered around the body's geometric center. This enables us to improve sampling significantly, increasing the number of samples for each point of interest from hundreds to billions. By modeling the relation between the means of the distributions of world coordinates and pixel coordinates, leveraging the Central Limit Theorem, we ensure normality and facilitate the learning process. Experimental results demonstrate human localization accuracy of 95% within a 0.3m range and nearly 100% accuracy within a 0.5m range, achieved at a low cost of only 10 USD using two web cameras with a resolution of 640x480 pixels.","sentences":["Accurate human localization is crucial for various applications, especially in the Metaverse era.","Existing high precision solutions rely on expensive, tag-dependent hardware, while vision-based methods offer a cheaper, tag-free alternative.","However, current vision solutions based on stereo vision face limitations due to rigid perspective transformation principles and error propagation in multi-stage SVD solvers.","These solutions also require multiple high-resolution cameras with strict setup constraints.","To address these limitations, we propose a probabilistic approach that considers all points on the human body as observations generated by a distribution centered around the body's geometric center.","This enables us to improve sampling significantly, increasing the number of samples for each point of interest from hundreds to billions.","By modeling the relation between the means of the distributions of world coordinates and pixel coordinates, leveraging the Central Limit Theorem, we ensure normality and facilitate the learning process.","Experimental results demonstrate human localization accuracy of 95% within a 0.3m range and nearly 100% accuracy within a 0.5m range, achieved at a low cost of only 10 USD using two web cameras with a resolution of 640x480 pixels."],"url":"http://arxiv.org/abs/2407.20870v1"}
{"created":"2024-07-30 14:43:54","title":"A Comparative Study of Neural Surface Reconstruction for Scientific Visualization","abstract":"This comparative study evaluates various neural surface reconstruction methods, particularly focusing on their implications for scientific visualization through reconstructing 3D surfaces via multi-view rendering images. We categorize ten methods into neural radiance fields and neural implicit surfaces, uncovering the benefits of leveraging distance functions (i.e., SDFs and UDFs) to enhance the accuracy and smoothness of the reconstructed surfaces. Our findings highlight the efficiency and quality of NeuS2 for reconstructing closed surfaces and identify NeUDF as a promising candidate for reconstructing open surfaces despite some limitations. By sharing our benchmark dataset, we invite researchers to test the performance of their methods, contributing to the advancement of surface reconstruction solutions for scientific visualization.","sentences":["This comparative study evaluates various neural surface reconstruction methods, particularly focusing on their implications for scientific visualization through reconstructing 3D surfaces via multi-view rendering images.","We categorize ten methods into neural radiance fields and neural implicit surfaces, uncovering the benefits of leveraging distance functions (i.e., SDFs and UDFs) to enhance the accuracy and smoothness of the reconstructed surfaces.","Our findings highlight the efficiency and quality of NeuS2 for reconstructing closed surfaces and identify NeUDF as a promising candidate for reconstructing open surfaces despite some limitations.","By sharing our benchmark dataset, we invite researchers to test the performance of their methods, contributing to the advancement of surface reconstruction solutions for scientific visualization."],"url":"http://arxiv.org/abs/2407.20868v1"}
{"created":"2024-07-30 14:35:31","title":"Breaking Agents: Compromising Autonomous LLM Agents Through Malfunction Amplification","abstract":"Recently, autonomous agents built on large language models (LLMs) have experienced significant development and are being deployed in real-world applications. These agents can extend the base LLM's capabilities in multiple ways. For example, a well-built agent using GPT-3.5-Turbo as its core can outperform the more advanced GPT-4 model by leveraging external components. More importantly, the usage of tools enables these systems to perform actions in the real world, moving from merely generating text to actively interacting with their environment. Given the agents' practical applications and their ability to execute consequential actions, it is crucial to assess potential vulnerabilities. Such autonomous systems can cause more severe damage than a standalone language model if compromised. While some existing research has explored harmful actions by LLM agents, our study approaches the vulnerability from a different perspective. We introduce a new type of attack that causes malfunctions by misleading the agent into executing repetitive or irrelevant actions. We conduct comprehensive evaluations using various attack methods, surfaces, and properties to pinpoint areas of susceptibility. Our experiments reveal that these attacks can induce failure rates exceeding 80\\% in multiple scenarios. Through attacks on implemented and deployable agents in multi-agent scenarios, we accentuate the realistic risks associated with these vulnerabilities. To mitigate such attacks, we propose self-examination detection methods. However, our findings indicate these attacks are difficult to detect effectively using LLMs alone, highlighting the substantial risks associated with this vulnerability.","sentences":["Recently, autonomous agents built on large language models (LLMs) have experienced significant development and are being deployed in real-world applications.","These agents can extend the base LLM's capabilities in multiple ways.","For example, a well-built agent using GPT-3.5-Turbo as its core can outperform the more advanced GPT-4 model by leveraging external components.","More importantly, the usage of tools enables these systems to perform actions in the real world, moving from merely generating text to actively interacting with their environment.","Given the agents' practical applications and their ability to execute consequential actions, it is crucial to assess potential vulnerabilities.","Such autonomous systems can cause more severe damage than a standalone language model if compromised.","While some existing research has explored harmful actions by LLM agents, our study approaches the vulnerability from a different perspective.","We introduce a new type of attack that causes malfunctions by misleading the agent into executing repetitive or irrelevant actions.","We conduct comprehensive evaluations using various attack methods, surfaces, and properties to pinpoint areas of susceptibility.","Our experiments reveal that these attacks can induce failure rates exceeding 80\\% in multiple scenarios.","Through attacks on implemented and deployable agents in multi-agent scenarios, we accentuate the realistic risks associated with these vulnerabilities.","To mitigate such attacks, we propose self-examination detection methods.","However, our findings indicate these attacks are difficult to detect effectively using LLMs alone, highlighting the substantial risks associated with this vulnerability."],"url":"http://arxiv.org/abs/2407.20859v1"}
{"created":"2024-07-30 14:31:53","title":"Learn by Selling: Equipping Large Language Models with Product Knowledge for Context-Driven Recommendations","abstract":"The rapid evolution of large language models (LLMs) has opened up new possibilities for applications such as context-driven product recommendations. However, the effectiveness of these models in this context is heavily reliant on their comprehensive understanding of the product inventory. This paper presents a novel approach to equipping LLMs with product knowledge by training them to respond contextually to synthetic search queries that include product IDs. We delve into an extensive analysis of this method, evaluating its effectiveness, outlining its benefits, and highlighting its constraints. The paper also discusses the potential improvements and future directions for this approach, providing a comprehensive understanding of the role of LLMs in product recommendations.","sentences":["The rapid evolution of large language models (LLMs) has opened up new possibilities for applications such as context-driven product recommendations.","However, the effectiveness of these models in this context is heavily reliant on their comprehensive understanding of the product inventory.","This paper presents a novel approach to equipping LLMs with product knowledge by training them to respond contextually to synthetic search queries that include product IDs.","We delve into an extensive analysis of this method, evaluating its effectiveness, outlining its benefits, and highlighting its constraints.","The paper also discusses the potential improvements and future directions for this approach, providing a comprehensive understanding of the role of LLMs in product recommendations."],"url":"http://arxiv.org/abs/2407.20856v1"}
{"created":"2024-07-30 14:31:33","title":"DeTurb: Atmospheric Turbulence Mitigation with Deformable 3D Convolutions and 3D Swin Transformers","abstract":"Atmospheric turbulence in long-range imaging significantly degrades the quality and fidelity of captured scenes due to random variations in both spatial and temporal dimensions. These distortions present a formidable challenge across various applications, from surveillance to astronomy, necessitating robust mitigation strategies. While model-based approaches achieve good results, they are very slow. Deep learning approaches show promise in image and video restoration but have struggled to address these spatiotemporal variant distortions effectively. This paper proposes a new framework that combines geometric restoration with an enhancement module. Random perturbations and geometric distortion are removed using a pyramid architecture with deformable 3D convolutions, resulting in aligned frames. These frames are then used to reconstruct a sharp, clear image via a multi-scale architecture of 3D Swin Transformers. The proposed framework demonstrates superior performance over the state of the art for both synthetic and real atmospheric turbulence effects, with reasonable speed and model size.","sentences":["Atmospheric turbulence in long-range imaging significantly degrades the quality and fidelity of captured scenes due to random variations in both spatial and temporal dimensions.","These distortions present a formidable challenge across various applications, from surveillance to astronomy, necessitating robust mitigation strategies.","While model-based approaches achieve good results, they are very slow.","Deep learning approaches show promise in image and video restoration but have struggled to address these spatiotemporal variant distortions effectively.","This paper proposes a new framework that combines geometric restoration with an enhancement module.","Random perturbations and geometric distortion are removed using a pyramid architecture with deformable 3D convolutions, resulting in aligned frames.","These frames are then used to reconstruct a sharp, clear image via a multi-scale architecture of 3D Swin Transformers.","The proposed framework demonstrates superior performance over the state of the art for both synthetic and real atmospheric turbulence effects, with reasonable speed and model size."],"url":"http://arxiv.org/abs/2407.20855v1"}
{"created":"2024-07-30 14:27:59","title":"NIS-SLAM: Neural Implicit Semantic RGB-D SLAM for 3D Consistent Scene Understanding","abstract":"In recent years, the paradigm of neural implicit representations has gained substantial attention in the field of Simultaneous Localization and Mapping (SLAM). However, a notable gap exists in the existing approaches when it comes to scene understanding. In this paper, we introduce NIS-SLAM, an efficient neural implicit semantic RGB-D SLAM system, that leverages a pre-trained 2D segmentation network to learn consistent semantic representations. Specifically, for high-fidelity surface reconstruction and spatial consistent scene understanding, we combine high-frequency multi-resolution tetrahedron-based features and low-frequency positional encoding as the implicit scene representations. Besides, to address the inconsistency of 2D segmentation results from multiple views, we propose a fusion strategy that integrates the semantic probabilities from previous non-keyframes into keyframes to achieve consistent semantic learning. Furthermore, we implement a confidence-based pixel sampling and progressive optimization weight function for robust camera tracking. Extensive experimental results on various datasets show the better or more competitive performance of our system when compared to other existing neural dense implicit RGB-D SLAM approaches. Finally, we also show that our approach can be used in augmented reality applications. Project page: \\href{https://zju3dv.github.io/nis_slam}{https://zju3dv.github.io/nis\\_slam}.","sentences":["In recent years, the paradigm of neural implicit representations has gained substantial attention in the field of Simultaneous Localization and Mapping (SLAM).","However, a notable gap exists in the existing approaches when it comes to scene understanding.","In this paper, we introduce NIS-SLAM, an efficient neural implicit semantic RGB-D SLAM system, that leverages a pre-trained 2D segmentation network to learn consistent semantic representations.","Specifically, for high-fidelity surface reconstruction and spatial consistent scene understanding, we combine high-frequency multi-resolution tetrahedron-based features and low-frequency positional encoding as the implicit scene representations.","Besides, to address the inconsistency of 2D segmentation results from multiple views, we propose a fusion strategy that integrates the semantic probabilities from previous non-keyframes into keyframes to achieve consistent semantic learning.","Furthermore, we implement a confidence-based pixel sampling and progressive optimization weight function for robust camera tracking.","Extensive experimental results on various datasets show the better or more competitive performance of our system when compared to other existing neural dense implicit RGB-D SLAM approaches.","Finally, we also show that our approach can be used in augmented reality applications.","Project page: \\href{https://zju3dv.github.io/nis_slam}{https://zju3dv.github.io/nis\\_slam}."],"url":"http://arxiv.org/abs/2407.20853v1"}
{"created":"2024-07-30 14:27:32","title":"Optimizing 5G-Advanced Networks for Time-critical Applications: The Role of L4S","abstract":"As 5G networks strive to support advanced time-critical applications, such as immersive Extended Reality (XR), cloud gaming, and autonomous driving, the demand for Real-time Broadband Communication (RTBC) grows. In this article, we present the main mechanisms of Low Latency, Low Loss, and Scalable Throughput (L4S). Subsequently, we investigate the support and challenges of L4S technology in the latest 3GPP 5G-Advanced Release 18 (R18) standard. Our case study, using a prototype system for a real-time communication (RTC) application, demonstrates the superiority of L4S technology. The experimental results show that, compared with the GCC algorithm, the proposed L4S-GCC algorithm can reduce the stalling rate by 1.51%-2.80% and increase the bandwidth utilization by 11.4%-31.4%. The results emphasize the immense potential of L4S technology in enhancing transmission performance in time-critical applications.","sentences":["As 5G networks strive to support advanced time-critical applications, such as immersive Extended Reality (XR), cloud gaming, and autonomous driving, the demand for Real-time Broadband Communication (RTBC) grows.","In this article, we present the main mechanisms of Low Latency, Low Loss, and Scalable Throughput (L4S).","Subsequently, we investigate the support and challenges of L4S technology in the latest 3GPP 5G-Advanced Release 18 (R18) standard.","Our case study, using a prototype system for a real-time communication (RTC) application, demonstrates the superiority of L4S technology.","The experimental results show that, compared with the GCC algorithm, the proposed L4S-GCC algorithm can reduce the stalling rate by 1.51%-2.80% and increase the bandwidth utilization by 11.4%-31.4%.","The results emphasize the immense potential of L4S technology in enhancing transmission performance in time-critical applications."],"url":"http://arxiv.org/abs/2407.20852v1"}
{"created":"2024-07-30 14:25:08","title":"Public vs Private Bodies: Who Should Run Advanced AI Evaluations and Audits? A Three-Step Logic Based on Case Studies of High-Risk Industries","abstract":"Artificial Intelligence (AI) Safety Institutes and governments worldwide are deciding whether they evaluate and audit advanced AI themselves, support a private auditor ecosystem or do both. Auditing regimes have been established in a wide range of industry contexts to monitor and evaluate firms compliance with regulation. Auditing is a necessary governance tool to understand and manage the risks of a technology. This paper draws from nine such regimes to inform (i) who should audit which parts of advanced AI; and (ii) how much resources, competence and access public bodies may need to audit advanced AI effectively. First, the effective responsibility distribution between public and private auditors depends heavily on specific industry and audit conditions. On the basis of the risk profile of advanced AI, the sensitivity of information involved in the auditing process, and the high costs of verifying safety and benefit claims of AI Labs, we recommend that public bodies become directly involved in safety critical, especially gray- and white-box, AI model audits. Governance and security audits, which are well-established in other industry contexts, as well as black-box model audits, may be more efficiently provided by a private market of auditors under public oversight. Secondly, to effectively fulfill their role in advanced AI audits, public bodies need extensive access to models and facilities. Public bodies capacity should scale with the industry's risk level, size and market concentration, potentially requiring 100s of employees for auditing in large jurisdictions like the EU or US, like in nuclear safety and life sciences.","sentences":["Artificial Intelligence (AI) Safety Institutes and governments worldwide are deciding whether they evaluate and audit advanced AI themselves, support a private auditor ecosystem or do both.","Auditing regimes have been established in a wide range of industry contexts to monitor and evaluate firms compliance with regulation.","Auditing is a necessary governance tool to understand and manage the risks of a technology.","This paper draws from nine such regimes to inform (i) who should audit which parts of advanced AI; and (ii) how much resources, competence and access public bodies may need to audit advanced AI effectively.","First, the effective responsibility distribution between public and private auditors depends heavily on specific industry and audit conditions.","On the basis of the risk profile of advanced AI, the sensitivity of information involved in the auditing process, and the high costs of verifying safety and benefit claims of AI Labs, we recommend that public bodies become directly involved in safety critical, especially gray- and white-box, AI model audits.","Governance and security audits, which are well-established in other industry contexts, as well as black-box model audits, may be more efficiently provided by a private market of auditors under public oversight.","Secondly, to effectively fulfill their role in advanced AI audits, public bodies need extensive access to models and facilities.","Public bodies capacity should scale with the industry's risk level, size and market concentration, potentially requiring 100s of employees for auditing in large jurisdictions like the EU or US, like in nuclear safety and life sciences."],"url":"http://arxiv.org/abs/2407.20847v1"}
{"created":"2024-07-30 14:22:13","title":"Assessing Graphical Perception of Image Embedding Models using Channel Effectiveness","abstract":"Recent advancements in vision models have greatly improved their ability to handle complex chart understanding tasks, like chart captioning and question answering. However, it remains challenging to assess how these models process charts. Existing benchmarks only roughly evaluate model performance without evaluating the underlying mechanisms, such as how models extract image embeddings. This limits our understanding of the model's ability to perceive fundamental graphical components. To address this, we introduce a novel evaluation framework to assess the graphical perception of image embedding models. For chart comprehension, we examine two main aspects of channel effectiveness: accuracy and discriminability of various visual channels. Channel accuracy is assessed through the linearity of embeddings, measuring how well the perceived magnitude aligns with the size of the stimulus. Discriminability is evaluated based on the distances between embeddings, indicating their distinctness. Our experiments with the CLIP model show that it perceives channel accuracy differently from humans and shows unique discriminability in channels like length, tilt, and curvature. We aim to develop this work into a broader benchmark for reliable visual encoders, enhancing models for precise chart comprehension and human-like perception in future applications.","sentences":["Recent advancements in vision models have greatly improved their ability to handle complex chart understanding tasks, like chart captioning and question answering.","However, it remains challenging to assess how these models process charts.","Existing benchmarks only roughly evaluate model performance without evaluating the underlying mechanisms, such as how models extract image embeddings.","This limits our understanding of the model's ability to perceive fundamental graphical components.","To address this, we introduce a novel evaluation framework to assess the graphical perception of image embedding models.","For chart comprehension, we examine two main aspects of channel effectiveness: accuracy and discriminability of various visual channels.","Channel accuracy is assessed through the linearity of embeddings, measuring how well the perceived magnitude aligns with the size of the stimulus.","Discriminability is evaluated based on the distances between embeddings, indicating their distinctness.","Our experiments with the CLIP model show that it perceives channel accuracy differently from humans and shows unique discriminability in channels like length, tilt, and curvature.","We aim to develop this work into a broader benchmark for reliable visual encoders, enhancing models for precise chart comprehension and human-like perception in future applications."],"url":"http://arxiv.org/abs/2407.20845v1"}
{"created":"2024-07-30 14:16:09","title":"DFE-IANet: A Method for Polyp Image Classification Based on Dual-domain Feature Extraction and Interaction Attention","abstract":"It is helpful in preventing colorectal cancer to detect and treat polyps in the gastrointestinal tract early. However, there have been few studies to date on designing polyp image classification networks that balance efficiency and accuracy. This challenge is mainly attributed to the fact that polyps are similar to other pathologies and have complex features influenced by texture, color, and morphology. In this paper, we propose a novel network DFE-IANet based on both spectral transformation and feature interaction. Firstly, to extract detailed features and multi-scale features, the features are transformed by the multi-scale frequency domain feature extraction (MSFD) block to extract texture details at the fine-grained level in the frequency domain. Secondly, the multi-scale interaction attention (MSIA) block is designed to enhance the network's capability of extracting critical features. This block introduces multi-scale features into self-attention, aiming to adaptively guide the network to concentrate on vital regions. Finally, with a compact parameter of only 4M, DFE-IANet outperforms the latest and classical networks in terms of efficiency. Furthermore, DFE-IANet achieves state-of-the-art (SOTA) results on the challenging Kvasir dataset, demonstrating a remarkable Top-1 accuracy of 93.94%. This outstanding accuracy surpasses ViT by 8.94%, ResNet50 by 1.69%, and VMamba by 1.88%. Our code is publicly available at https://github.com/PURSUETHESUN/DFE-IANet.","sentences":["It is helpful in preventing colorectal cancer to detect and treat polyps in the gastrointestinal tract early.","However, there have been few studies to date on designing polyp image classification networks that balance efficiency and accuracy.","This challenge is mainly attributed to the fact that polyps are similar to other pathologies and have complex features influenced by texture, color, and morphology.","In this paper, we propose a novel network DFE-IANet based on both spectral transformation and feature interaction.","Firstly, to extract detailed features and multi-scale features, the features are transformed by the multi-scale frequency domain feature extraction (MSFD) block to extract texture details at the fine-grained level in the frequency domain.","Secondly, the multi-scale interaction attention (MSIA) block is designed to enhance the network's capability of extracting critical features.","This block introduces multi-scale features into self-attention, aiming to adaptively guide the network to concentrate on vital regions.","Finally, with a compact parameter of only 4M, DFE-IANet outperforms the latest and classical networks in terms of efficiency.","Furthermore, DFE-IANet achieves state-of-the-art (SOTA) results on the challenging Kvasir dataset, demonstrating a remarkable Top-1 accuracy of 93.94%.","This outstanding accuracy surpasses ViT by 8.94%, ResNet50 by 1.69%, and VMamba by 1.88%.","Our code is publicly available at https://github.com/PURSUETHESUN/DFE-IANet."],"url":"http://arxiv.org/abs/2407.20843v1"}
{"created":"2024-07-30 14:11:39","title":"Large Language Model (LLM)-enabled Graphs in Dynamic Networking","abstract":"Recent advances in generative artificial intelligence (AI), and particularly the integration of large language models (LLMs), have had considerable impact on multiple domains. Meanwhile, enhancing dynamic network performance is a crucial element in promoting technological advancement and meeting the growing demands of users in many applications areas involving networks. In this article, we explore an integration of LLMs and graphs in dynamic networks, focusing on potential applications and a practical study. Specifically, we first review essential technologies and applications of LLM-enabled graphs, followed by an exploration of their advantages in dynamic networking. Subsequently, we introduce and analyze LLM-enabled graphs and their applications in dynamic networks from the perspective of LLMs as different roles. On this basis, we propose a novel framework of LLM-enabled graphs for networking optimization, and then present a case study on UAV networking, concentrating on optimizing UAV trajectory and communication resource allocation to validate the effectiveness of the proposed framework. Finally, we outline several potential future extensions.","sentences":["Recent advances in generative artificial intelligence (AI), and particularly the integration of large language models (LLMs), have had considerable impact on multiple domains.","Meanwhile, enhancing dynamic network performance is a crucial element in promoting technological advancement and meeting the growing demands of users in many applications areas involving networks.","In this article, we explore an integration of LLMs and graphs in dynamic networks, focusing on potential applications and a practical study.","Specifically, we first review essential technologies and applications of LLM-enabled graphs, followed by an exploration of their advantages in dynamic networking.","Subsequently, we introduce and analyze LLM-enabled graphs and their applications in dynamic networks from the perspective of LLMs as different roles.","On this basis, we propose a novel framework of LLM-enabled graphs for networking optimization, and then present a case study on UAV networking, concentrating on optimizing UAV trajectory and communication resource allocation to validate the effectiveness of the proposed framework.","Finally, we outline several potential future extensions."],"url":"http://arxiv.org/abs/2407.20840v1"}
{"created":"2024-07-30 14:07:17","title":"Vulnerabilities in AI-generated Image Detection: The Challenge of Adversarial Attacks","abstract":"Recent advancements in image synthesis, particularly with the advent of GAN and Diffusion models, have amplified public concerns regarding the dissemination of disinformation. To address such concerns, numerous AI-generated Image (AIGI) Detectors have been proposed and achieved promising performance in identifying fake images. However, there still lacks a systematic understanding of the adversarial robustness of these AIGI detectors. In this paper, we examine the vulnerability of state-of-the-art AIGI detectors against adversarial attack under white-box and black-box settings, which has been rarely investigated so far. For the task of AIGI detection, we propose a new attack containing two main parts. First, inspired by the obvious difference between real images and fake images in the frequency domain, we add perturbations under the frequency domain to push the image away from its original frequency distribution. Second, we explore the full posterior distribution of the surrogate model to further narrow this gap between heterogeneous models, e.g. transferring adversarial examples across CNNs and ViTs. This is achieved by introducing a novel post-train Bayesian strategy that turns a single surrogate into a Bayesian one, capable of simulating diverse victim models using one pre-trained surrogate, without the need for re-training. We name our method as frequency-based post-train Bayesian attack, or FPBA. Through FPBA, we show that adversarial attack is truly a real threat to AIGI detectors, because FPBA can deliver successful black-box attacks across models, generators, defense methods, and even evade cross-generator detection, which is a crucial real-world detection scenario.","sentences":["Recent advancements in image synthesis, particularly with the advent of GAN and Diffusion models, have amplified public concerns regarding the dissemination of disinformation.","To address such concerns, numerous AI-generated Image (AIGI)","Detectors have been proposed and achieved promising performance in identifying fake images.","However, there still lacks a systematic understanding of the adversarial robustness of these AIGI detectors.","In this paper, we examine the vulnerability of state-of-the-art AIGI detectors against adversarial attack under white-box and black-box settings, which has been rarely investigated so far.","For the task of AIGI detection, we propose a new attack containing two main parts.","First, inspired by the obvious difference between real images and fake images in the frequency domain, we add perturbations under the frequency domain to push the image away from its original frequency distribution.","Second, we explore the full posterior distribution of the surrogate model to further narrow this gap between heterogeneous models, e.g. transferring adversarial examples across CNNs and ViTs.","This is achieved by introducing a novel post-train Bayesian strategy that turns a single surrogate into a Bayesian one, capable of simulating diverse victim models using one pre-trained surrogate, without the need for re-training.","We name our method as frequency-based post-train Bayesian attack, or FPBA.","Through FPBA, we show that adversarial attack is truly a real threat to AIGI detectors, because FPBA can deliver successful black-box attacks across models, generators, defense methods, and even evade cross-generator detection, which is a crucial real-world detection scenario."],"url":"http://arxiv.org/abs/2407.20836v1"}
{"created":"2024-07-30 13:56:26","title":"Federated Knowledge Recycling: Privacy-Preserving Synthetic Data Sharing","abstract":"Federated learning has emerged as a paradigm for collaborative learning, enabling the development of robust models without the need to centralise sensitive data. However, conventional federated learning techniques have privacy and security vulnerabilities due to the exposure of models, parameters or updates, which can be exploited as an attack surface. This paper presents Federated Knowledge Recycling (FedKR), a cross-silo federated learning approach that uses locally generated synthetic data to facilitate collaboration between institutions. FedKR combines advanced data generation techniques with a dynamic aggregation process to provide greater security against privacy attacks than existing methods, significantly reducing the attack surface. Experimental results on generic and medical datasets show that FedKR achieves competitive performance, with an average improvement in accuracy of 4.24% compared to training models from local data, demonstrating particular effectiveness in data scarcity scenarios.","sentences":["Federated learning has emerged as a paradigm for collaborative learning, enabling the development of robust models without the need to centralise sensitive data.","However, conventional federated learning techniques have privacy and security vulnerabilities due to the exposure of models, parameters or updates, which can be exploited as an attack surface.","This paper presents Federated Knowledge Recycling (FedKR), a cross-silo federated learning approach that uses locally generated synthetic data to facilitate collaboration between institutions.","FedKR combines advanced data generation techniques with a dynamic aggregation process to provide greater security against privacy attacks than existing methods, significantly reducing the attack surface.","Experimental results on generic and medical datasets show that FedKR achieves competitive performance, with an average improvement in accuracy of 4.24% compared to training models from local data, demonstrating particular effectiveness in data scarcity scenarios."],"url":"http://arxiv.org/abs/2407.20830v1"}
{"created":"2024-07-30 13:53:48","title":"How to Measure the Intelligence of Large Language Models?","abstract":"With the release of ChatGPT and other large language models (LLMs) the discussion about the intelligence, possibilities, and risks, of current and future models have seen large attention. This discussion included much debated scenarios about the imminent rise of so-called \"super-human\" AI, i.e., AI systems that are orders of magnitude smarter than humans. In the spirit of Alan Turing, there is no doubt that current state-of-the-art language models already pass his famous test. Moreover, current models outperform humans in several benchmark tests, so that publicly available LLMs have already become versatile companions that connect everyday life, industry and science. Despite their impressive capabilities, LLMs sometimes fail completely at tasks that are thought to be trivial for humans. In other cases, the trustworthiness of LLMs becomes much more elusive and difficult to evaluate. Taking the example of academia, language models are capable of writing convincing research articles on a given topic with only little input. Yet, the lack of trustworthiness in terms of factual consistency or the existence of persistent hallucinations in AI-generated text bodies has led to a range of restrictions for AI-based content in many scientific journals. In view of these observations, the question arises as to whether the same metrics that apply to human intelligence can also be applied to computational methods and has been discussed extensively. In fact, the choice of metrics has already been shown to dramatically influence assessments on potential intelligence emergence. Here, we argue that the intelligence of LLMs should not only be assessed by task-specific statistical metrics, but separately in terms of qualitative and quantitative measures.","sentences":["With the release of ChatGPT and other large language models (LLMs) the discussion about the intelligence, possibilities, and risks, of current and future models have seen large attention.","This discussion included much debated scenarios about the imminent rise of so-called \"super-human\" AI, i.e., AI systems that are orders of magnitude smarter than humans.","In the spirit of Alan Turing, there is no doubt that current state-of-the-art language models already pass his famous test.","Moreover, current models outperform humans in several benchmark tests, so that publicly available LLMs have already become versatile companions that connect everyday life, industry and science.","Despite their impressive capabilities, LLMs sometimes fail completely at tasks that are thought to be trivial for humans.","In other cases, the trustworthiness of LLMs becomes much more elusive and difficult to evaluate.","Taking the example of academia, language models are capable of writing convincing research articles on a given topic with only little input.","Yet, the lack of trustworthiness in terms of factual consistency or the existence of persistent hallucinations in AI-generated text bodies has led to a range of restrictions for AI-based content in many scientific journals.","In view of these observations, the question arises as to whether the same metrics that apply to human intelligence can also be applied to computational methods and has been discussed extensively.","In fact, the choice of metrics has already been shown to dramatically influence assessments on potential intelligence emergence.","Here, we argue that the intelligence of LLMs should not only be assessed by task-specific statistical metrics, but separately in terms of qualitative and quantitative measures."],"url":"http://arxiv.org/abs/2407.20828v1"}
{"created":"2024-07-30 13:43:32","title":"DyGKT: Dynamic Graph Learning for Knowledge Tracing","abstract":"Knowledge Tracing aims to assess student learning states by predicting their performance in answering questions. Different from the existing research which utilizes fixed-length learning sequence to obtain the student states and regards KT as a static problem, this work is motivated by three dynamical characteristics: 1) The scales of students answering records are constantly growing; 2) The semantics of time intervals between the records vary; 3) The relationships between students, questions and concepts are evolving. The three dynamical characteristics above contain the great potential to revolutionize the existing knowledge tracing methods. Along this line, we propose a Dynamic Graph-based Knowledge Tracing model, namely DyGKT. In particular, a continuous-time dynamic question-answering graph for knowledge tracing is constructed to deal with the infinitely growing answering behaviors, and it is worth mentioning that it is the first time dynamic graph learning technology is used in this field. Then, a dual time encoder is proposed to capture long-term and short-term semantics among the different time intervals. Finally, a multiset indicator is utilized to model the evolving relationships between students, questions, and concepts via the graph structural feature. Numerous experiments are conducted on five real-world datasets, and the results demonstrate the superiority of our model. All the used resources are publicly available at https://github.com/PengLinzhi/DyGKT.","sentences":["Knowledge Tracing aims to assess student learning states by predicting their performance in answering questions.","Different from the existing research which utilizes fixed-length learning sequence to obtain the student states and regards KT as a static problem, this work is motivated by three dynamical characteristics: 1) The scales of students answering records are constantly growing; 2)","The semantics of time intervals between the records vary; 3) The relationships between students, questions and concepts are evolving.","The three dynamical characteristics above contain the great potential to revolutionize the existing knowledge tracing methods.","Along this line, we propose a Dynamic Graph-based Knowledge Tracing model, namely DyGKT.","In particular, a continuous-time dynamic question-answering graph for knowledge tracing is constructed to deal with the infinitely growing answering behaviors, and it is worth mentioning that it is the first time dynamic graph learning technology is used in this field.","Then, a dual time encoder is proposed to capture long-term and short-term semantics among the different time intervals.","Finally, a multiset indicator is utilized to model the evolving relationships between students, questions, and concepts via the graph structural feature.","Numerous experiments are conducted on five real-world datasets, and the results demonstrate the superiority of our model.","All the used resources are publicly available at https://github.com/PengLinzhi/DyGKT."],"url":"http://arxiv.org/abs/2407.20824v1"}
{"created":"2024-07-30 13:39:38","title":"Adding Circumscription to Decidable Fragments of First-Order Logic: A Complexity Rollercoaster","abstract":"We study extensions of expressive decidable fragments of first-order logic with circumscription, in particular the two-variable fragment FO$^2$, its extension C$^2$ with counting quantifiers, and the guarded fragment GF. We prove that if only unary predicates are minimized (or fixed) during circumscription, then decidability of logical consequence is preserved. For FO$^2$ the complexity increases from $\\textrm{coNexp}$ to $\\textrm{coNExp}^\\textrm{NP}$-complete, for GF it (remarkably!) increases from $\\textrm{2Exp}$ to $\\textrm{Tower}$-complete, and for C$^2$ the complexity remains open. We also consider querying circumscribed knowledge bases whose ontology is a GF sentence, showing that the problem is decidable for unions of conjunctive queries, $\\textrm{Tower}$-complete in combined complexity, and elementary in data complexity. Already for atomic queries and ontologies that are sets of guarded existential rules, however, for every $k \\geq 0$ there is an ontology and query that are $k$-$\\textrm{Exp}$-hard in data complexity.","sentences":["We study extensions of expressive decidable fragments of first-order logic with circumscription, in particular the two-variable fragment FO$^2$, its extension C$^2$ with counting quantifiers, and the guarded fragment GF.","We prove that if only unary predicates are minimized (or fixed) during circumscription, then decidability of logical consequence is preserved.","For FO$^2$ the complexity increases from $\\textrm{coNexp}$ to $\\textrm{coNExp}^\\textrm{NP}$-complete, for GF it (remarkably!)","increases from $\\textrm{2Exp}$ to $\\textrm{Tower}$-complete, and for C$^2$ the complexity remains open.","We also consider querying circumscribed knowledge bases whose ontology is a GF sentence, showing that the problem is decidable for unions of conjunctive queries, $\\textrm{Tower}$-complete in combined complexity, and elementary in data complexity.","Already for atomic queries and ontologies that are sets of guarded existential rules, however, for every $k \\geq 0$ there is an ontology and query that are $k$-$\\textrm{Exp}$-hard in data complexity."],"url":"http://arxiv.org/abs/2407.20822v1"}
