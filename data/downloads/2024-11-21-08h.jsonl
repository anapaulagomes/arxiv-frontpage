{"created":"2024-11-20 18:59:58","title":"AI-generated Image Detection: Passive or Watermark?","abstract":"While text-to-image models offer numerous benefits, they also pose significant societal risks. Detecting AI-generated images is crucial for mitigating these risks. Detection methods can be broadly categorized into passive and watermark-based approaches: passive detectors rely on artifacts present in AI-generated images, whereas watermark-based detectors proactively embed watermarks into such images. A key question is which type of detector performs better in terms of effectiveness, robustness, and efficiency. However, the current literature lacks a comprehensive understanding of this issue. In this work, we aim to bridge that gap by developing ImageDetectBench, the first comprehensive benchmark to compare the effectiveness, robustness, and efficiency of passive and watermark-based detectors. Our benchmark includes four datasets, each containing a mix of AI-generated and non-AI-generated images. We evaluate five passive detectors and four watermark-based detectors against eight types of common perturbations and three types of adversarial perturbations. Our benchmark results reveal several interesting findings. For instance, watermark-based detectors consistently outperform passive detectors, both in the presence and absence of perturbations. Based on these insights, we provide recommendations for detecting AI-generated images, e.g., when both types of detectors are applicable, watermark-based detectors should be the preferred choice.","sentences":["While text-to-image models offer numerous benefits, they also pose significant societal risks.","Detecting AI-generated images is crucial for mitigating these risks.","Detection methods can be broadly categorized into passive and watermark-based approaches: passive detectors rely on artifacts present in AI-generated images, whereas watermark-based detectors proactively embed watermarks into such images.","A key question is which type of detector performs better in terms of effectiveness, robustness, and efficiency.","However, the current literature lacks a comprehensive understanding of this issue.","In this work, we aim to bridge that gap by developing ImageDetectBench, the first comprehensive benchmark to compare the effectiveness, robustness, and efficiency of passive and watermark-based detectors.","Our benchmark includes four datasets, each containing a mix of AI-generated and non-AI-generated images.","We evaluate five passive detectors and four watermark-based detectors against eight types of common perturbations and three types of adversarial perturbations.","Our benchmark results reveal several interesting findings.","For instance, watermark-based detectors consistently outperform passive detectors, both in the presence and absence of perturbations.","Based on these insights, we provide recommendations for detecting AI-generated images, e.g., when both types of detectors are applicable, watermark-based detectors should be the preferred choice."],"url":"http://arxiv.org/abs/2411.13553v1"}
{"created":"2024-11-20 18:59:52","title":"REDUCIO! Generating 1024$\\times$1024 Video within 16 Seconds using Extremely Compressed Motion Latents","abstract":"Commercial video generation models have exhibited realistic, high-fidelity results but are still restricted to limited access. One crucial obstacle for large-scale applications is the expensive training and inference cost. In this paper, we argue that videos contain much more redundant information than images, thus can be encoded by very few motion latents based on a content image. Towards this goal, we design an image-conditioned VAE to encode a video to an extremely compressed motion latent space. This magic Reducio charm enables 64x reduction of latents compared to a common 2D VAE, without sacrificing the quality. Training diffusion models on such a compact representation easily allows for generating 1K resolution videos. We then adopt a two-stage video generation paradigm, which performs text-to-image and text-image-to-video sequentially. Extensive experiments show that our Reducio-DiT achieves strong performance in evaluation, though trained with limited GPU resources. More importantly, our method significantly boost the efficiency of video LDMs both in training and inference. We train Reducio-DiT in around 3.2K training hours in total and generate a 16-frame 1024*1024 video clip within 15.5 seconds on a single A100 GPU. Code released at https://github.com/microsoft/Reducio-VAE .","sentences":["Commercial video generation models have exhibited realistic, high-fidelity results but are still restricted to limited access.","One crucial obstacle for large-scale applications is the expensive training and inference cost.","In this paper, we argue that videos contain much more redundant information than images, thus can be encoded by very few motion latents based on a content image.","Towards this goal, we design an image-conditioned VAE to encode a video to an extremely compressed motion latent space.","This magic Reducio charm enables 64x reduction of latents compared to a common 2D VAE, without sacrificing the quality.","Training diffusion models on such a compact representation easily allows for generating 1K resolution videos.","We then adopt a two-stage video generation paradigm, which performs text-to-image and text-image-to-video sequentially.","Extensive experiments show that our Reducio-DiT achieves strong performance in evaluation, though trained with limited GPU resources.","More importantly, our method significantly boost the efficiency of video LDMs both in training and inference.","We train Reducio-DiT in around 3.2K training hours in total and generate a 16-frame 1024*1024 video clip within 15.5 seconds on a single A100 GPU.","Code released at https://github.com/microsoft/Reducio-VAE ."],"url":"http://arxiv.org/abs/2411.13552v1"}
{"created":"2024-11-20 18:59:01","title":"Find Any Part in 3D","abstract":"We study open-world part segmentation in 3D: segmenting any part in any object based on any text query. Prior methods are limited in object categories and part vocabularies. Recent advances in AI have demonstrated effective open-world recognition capabilities in 2D. Inspired by this progress, we propose an open-world, direct-prediction model for 3D part segmentation that can be applied zero-shot to any object. Our approach, called Find3D, trains a general-category point embedding model on large-scale 3D assets from the internet without any human annotation. It combines a data engine, powered by foundation models for annotating data, with a contrastive training method. We achieve strong performance and generalization across multiple datasets, with up to a 3x improvement in mIoU over the next best method. Our model is 6x to over 300x faster than existing baselines. To encourage research in general-category open-world 3D part segmentation, we also release a benchmark for general objects and parts. Project website: https://ziqi-ma.github.io/find3dsite/","sentences":["We study open-world part segmentation in 3D: segmenting any part in any object based on any text query.","Prior methods are limited in object categories and part vocabularies.","Recent advances in AI have demonstrated effective open-world recognition capabilities in 2D. Inspired by this progress, we propose an open-world, direct-prediction model for 3D part segmentation that can be applied zero-shot to any object.","Our approach, called Find3D, trains a general-category point embedding model on large-scale 3D assets from the internet without any human annotation.","It combines a data engine, powered by foundation models for annotating data, with a contrastive training method.","We achieve strong performance and generalization across multiple datasets, with up to a 3x improvement in mIoU over the next best method.","Our model is 6x to over 300x faster than existing baselines.","To encourage research in general-category open-world 3D part segmentation, we also release a benchmark for general objects and parts.","Project website:","https://ziqi-ma.github.io/find3dsite/"],"url":"http://arxiv.org/abs/2411.13550v1"}
{"created":"2024-11-20 18:58:31","title":"Generating 3D-Consistent Videos from Unposed Internet Photos","abstract":"We address the problem of generating videos from unposed internet photos. A handful of input images serve as keyframes, and our model interpolates between them to simulate a path moving between the cameras. Given random images, a model's ability to capture underlying geometry, recognize scene identity, and relate frames in terms of camera position and orientation reflects a fundamental understanding of 3D structure and scene layout. However, existing video models such as Luma Dream Machine fail at this task. We design a self-supervised method that takes advantage of the consistency of videos and variability of multiview internet photos to train a scalable, 3D-aware video model without any 3D annotations such as camera parameters. We validate that our method outperforms all baselines in terms of geometric and appearance consistency. We also show our model benefits applications that enable camera control, such as 3D Gaussian Splatting. Our results suggest that we can scale up scene-level 3D learning using only 2D data such as videos and multiview internet photos.","sentences":["We address the problem of generating videos from unposed internet photos.","A handful of input images serve as keyframes, and our model interpolates between them to simulate a path moving between the cameras.","Given random images, a model's ability to capture underlying geometry, recognize scene identity, and relate frames in terms of camera position and orientation reflects a fundamental understanding of 3D structure and scene layout.","However, existing video models such as Luma Dream Machine fail at this task.","We design a self-supervised method that takes advantage of the consistency of videos and variability of multiview internet photos to train a scalable, 3D-aware video model without any 3D annotations such as camera parameters.","We validate that our method outperforms all baselines in terms of geometric and appearance consistency.","We also show our model benefits applications that enable camera control, such as 3D Gaussian Splatting.","Our results suggest that we can scale up scene-level 3D learning using only 2D data such as videos and multiview internet photos."],"url":"http://arxiv.org/abs/2411.13549v1"}
{"created":"2024-11-20 18:56:24","title":"HF-Diff: High-Frequency Perceptual Loss and Distribution Matching for One-Step Diffusion-Based Image Super-Resolution","abstract":"Although recent diffusion-based single-step super-resolution methods achieve better performance as compared to SinSR, they are computationally complex. To improve the performance of SinSR, we investigate preserving the high-frequency detail features during super-resolution (SR) because the downgraded images lack detailed information. For this purpose, we introduce a high-frequency perceptual loss by utilizing an invertible neural network (INN) pretrained on the ImageNet dataset. Different feature maps of pretrained INN produce different high-frequency aspects of an image. During the training phase, we impose to preserve the high-frequency features of super-resolved and ground truth (GT) images that improve the SR image quality during inference. Furthermore, we also utilize the Jenson-Shannon divergence between GT and SR images in the pretrained DINO-v2 embedding space to match their distribution. By introducing the $\\textbf{h}igh$- $\\textbf{f}requency$ preserving loss and distribution matching constraint in the single-step $\\textbf{diff}usion-based$ SR ($\\textbf{HF-Diff}$), we achieve a state-of-the-art CLIPIQA score in the benchmark RealSR, RealSet65, DIV2K-Val, and ImageNet datasets. Furthermore, the experimental results in several datasets demonstrate that our high-frequency perceptual loss yields better SR image quality than LPIPS and VGG-based perceptual losses. Our code will be released at https://github.com/shoaib-sami/HF-Diff.","sentences":["Although recent diffusion-based single-step super-resolution methods achieve better performance as compared to SinSR, they are computationally complex.","To improve the performance of SinSR, we investigate preserving the high-frequency detail features during super-resolution (SR) because the downgraded images lack detailed information.","For this purpose, we introduce a high-frequency perceptual loss by utilizing an invertible neural network (INN) pretrained on the ImageNet dataset.","Different feature maps of pretrained INN produce different high-frequency aspects of an image.","During the training phase, we impose to preserve the high-frequency features of super-resolved and ground truth (GT) images that improve the SR image quality during inference.","Furthermore, we also utilize the Jenson-Shannon divergence between GT and SR images in the pretrained DINO-v2 embedding space to match their distribution.","By introducing the $\\textbf{h}igh$- $\\textbf{f}requency$ preserving loss and distribution matching constraint in the single-step $\\textbf{diff}usion-based$ SR ($\\textbf{HF-Diff}$), we achieve a state-of-the-art CLIPIQA score in the benchmark RealSR, RealSet65, DIV2K-Val, and ImageNet datasets.","Furthermore, the experimental results in several datasets demonstrate that our high-frequency perceptual loss yields better SR image quality than LPIPS and VGG-based perceptual losses.","Our code will be released at https://github.com/shoaib-sami/HF-Diff."],"url":"http://arxiv.org/abs/2411.13548v1"}
{"created":"2024-11-20 18:56:22","title":"SpecTool: A Benchmark for Characterizing Errors in Tool-Use LLMs","abstract":"Evaluating the output of Large Language Models (LLMs) is one of the most critical aspects of building a performant compound AI system. Since the output from LLMs propagate to downstream steps, identifying LLM errors is crucial to system performance. A common task for LLMs in AI systems is tool use. While there are several benchmark environments for evaluating LLMs on this task, they typically only give a success rate without any explanation of the failure cases. To solve this problem, we introduce SpecTool, a new benchmark to identify error patterns in LLM output on tool-use tasks. Our benchmark data set comprises of queries from diverse environments that can be used to test for the presence of seven newly characterized error patterns. Using SPECTOOL , we show that even the most prominent LLMs exhibit these error patterns in their outputs. Researchers can use the analysis and insights from SPECTOOL to guide their error mitigation strategies.","sentences":["Evaluating the output of Large Language Models (LLMs) is one of the most critical aspects of building a performant compound AI system.","Since the output from LLMs propagate to downstream steps, identifying LLM errors is crucial to system performance.","A common task for LLMs in AI systems is tool use.","While there are several benchmark environments for evaluating LLMs on this task, they typically only give a success rate without any explanation of the failure cases.","To solve this problem, we introduce SpecTool, a new benchmark to identify error patterns in LLM output on tool-use tasks.","Our benchmark data set comprises of queries from diverse environments that can be used to test for the presence of seven newly characterized error patterns.","Using SPECTOOL , we show that even the most prominent LLMs exhibit these error patterns in their outputs.","Researchers can use the analysis and insights from SPECTOOL to guide their error mitigation strategies."],"url":"http://arxiv.org/abs/2411.13547v1"}
{"created":"2024-11-20 18:55:51","title":"Promoting User Data Autonomy During the Dissolution of a Monopolistic Firm","abstract":"The deployment of AI in consumer products is currently focused on the use of so-called foundation models, large neural networks pre-trained on massive corpora of digital records. This emphasis on scaling up datasets and pre-training computation raises the risk of further consolidating the industry, and enabling monopolistic (or oligopolistic) behavior. Judges and regulators seeking to improve market competition may employ various remedies. This paper explores dissolution -- the breaking up of a monopolistic entity into smaller firms -- as one such remedy, focusing in particular on the technical challenges and opportunities involved in the breaking up of large models and datasets. We show how the framework of Conscious Data Contribution can enable user autonomy during under dissolution. Through a simulation study, we explore how fine-tuning and the phenomenon of \"catastrophic forgetting\" could actually prove beneficial as a type of machine unlearning that allows users to specify which data they want used for what purposes.","sentences":["The deployment of AI in consumer products is currently focused on the use of so-called foundation models, large neural networks pre-trained on massive corpora of digital records.","This emphasis on scaling up datasets and pre-training computation raises the risk of further consolidating the industry, and enabling monopolistic (or oligopolistic) behavior.","Judges and regulators seeking to improve market competition may employ various remedies.","This paper explores dissolution -- the breaking up of a monopolistic entity into smaller firms -- as one such remedy, focusing in particular on the technical challenges and opportunities involved in the breaking up of large models and datasets.","We show how the framework of Conscious Data Contribution can enable user autonomy during under dissolution.","Through a simulation study, we explore how fine-tuning and the phenomenon of \"catastrophic forgetting\" could actually prove beneficial as a type of machine unlearning that allows users to specify which data they want used for what purposes."],"url":"http://arxiv.org/abs/2411.13546v1"}
{"created":"2024-11-20 18:54:53","title":"Pushing the Limits of Sparsity: A Bag of Tricks for Extreme Pruning","abstract":"Pruning of deep neural networks has been an effective technique for reducing model size while preserving most of the performance of dense networks, crucial for deploying models on memory and power-constrained devices. While recent sparse learning methods have shown promising performance up to moderate sparsity levels such as 95% and 98%, accuracy quickly deteriorates when pushing sparsities to extreme levels. Obtaining sparse networks at such extreme sparsity levels presents unique challenges, such as fragile gradient flow and heightened risk of layer collapse. In this work, we explore network performance beyond the commonly studied sparsities, and propose a collection of techniques that enable the continuous learning of networks without accuracy collapse even at extreme sparsities, including 99.90%, 99.95% and 99.99% on ResNet architectures. Our approach combines 1) Dynamic ReLU phasing, where DyReLU initially allows for richer parameter exploration before being gradually replaced by standard ReLU, 2) weight sharing which reuses parameters within a residual layer while maintaining the same number of learnable parameters, and 3) cyclic sparsity, where both sparsity levels and sparsity patterns evolve dynamically throughout training to better encourage parameter exploration. We evaluate our method, which we term Extreme Adaptive Sparse Training (EAST) at extreme sparsities using ResNet-34 and ResNet-50 on CIFAR-10, CIFAR-100, and ImageNet, achieving significant performance improvements over state-of-the-art methods we compared with.","sentences":["Pruning of deep neural networks has been an effective technique for reducing model size while preserving most of the performance of dense networks, crucial for deploying models on memory and power-constrained devices.","While recent sparse learning methods have shown promising performance up to moderate sparsity levels such as 95% and 98%, accuracy quickly deteriorates when pushing sparsities to extreme levels.","Obtaining sparse networks at such extreme sparsity levels presents unique challenges, such as fragile gradient flow and heightened risk of layer collapse.","In this work, we explore network performance beyond the commonly studied sparsities, and propose a collection of techniques that enable the continuous learning of networks without accuracy collapse even at extreme sparsities, including 99.90%, 99.95% and 99.99% on ResNet architectures.","Our approach combines 1) Dynamic ReLU phasing, where DyReLU initially allows for richer parameter exploration before being gradually replaced by standard ReLU, 2) weight sharing which reuses parameters within a residual layer while maintaining the same number of learnable parameters, and 3) cyclic sparsity, where both sparsity levels and sparsity patterns evolve dynamically throughout training to better encourage parameter exploration.","We evaluate our method, which we term Extreme Adaptive Sparse Training (EAST) at extreme sparsities using ResNet-34 and ResNet-50 on CIFAR-10, CIFAR-100, and ImageNet, achieving significant performance improvements over state-of-the-art methods we compared with."],"url":"http://arxiv.org/abs/2411.13545v1"}
{"created":"2024-11-20 18:54:36","title":"DIS-Mine: Instance Segmentation for Disaster-Awareness in Poor-Light Condition in Underground Mines","abstract":"Detecting disasters in underground mining, such as explosions and structural damage, has been a persistent challenge over the years. This problem is compounded for first responders, who often have no clear information about the extent or nature of the damage within the mine. The poor-light or even total darkness inside the mines makes rescue efforts incredibly difficult, leading to a tragic loss of life. In this paper, we propose a novel instance segmentation method called DIS-Mine, specifically designed to identify disaster-affected areas within underground mines under low-light or poor visibility conditions, aiding first responders in rescue efforts. DIS-Mine is capable of detecting objects in images, even in complete darkness, by addressing challenges such as high noise, color distortions, and reduced contrast. The key innovations of DIS-Mine are built upon four core components: i) Image brightness improvement, ii) Instance segmentation with SAM integration, iii) Mask R-CNN-based segmentation, and iv) Mask alignment with feature matching. On top of that, we have collected real-world images from an experimental underground mine, introducing a new dataset named ImageMine, specifically gathered in low-visibility conditions. This dataset serves to validate the performance of DIS-Mine in realistic, challenging environments. Our comprehensive experiments on the ImageMine dataset, as well as on various other datasets demonstrate that DIS-Mine achieves a superior F1 score of 86.0% and mIoU of 72.0%, outperforming state-of-the-art instance segmentation methods, with at least 15x improvement and up to 80% higher precision in object detection.","sentences":["Detecting disasters in underground mining, such as explosions and structural damage, has been a persistent challenge over the years.","This problem is compounded for first responders, who often have no clear information about the extent or nature of the damage within the mine.","The poor-light or even total darkness inside the mines makes rescue efforts incredibly difficult, leading to a tragic loss of life.","In this paper, we propose a novel instance segmentation method called DIS-Mine, specifically designed to identify disaster-affected areas within underground mines under low-light or poor visibility conditions, aiding first responders in rescue efforts.","DIS-Mine is capable of detecting objects in images, even in complete darkness, by addressing challenges such as high noise, color distortions, and reduced contrast.","The key innovations of DIS-Mine are built upon four core components: i) Image brightness improvement, ii) Instance segmentation with SAM integration, iii) Mask R-CNN-based segmentation, and iv) Mask alignment with feature matching.","On top of that, we have collected real-world images from an experimental underground mine, introducing a new dataset named ImageMine, specifically gathered in low-visibility conditions.","This dataset serves to validate the performance of DIS-Mine in realistic, challenging environments.","Our comprehensive experiments on the ImageMine dataset, as well as on various other datasets demonstrate that DIS-Mine achieves a superior F1 score of 86.0% and mIoU of 72.0%, outperforming state-of-the-art instance segmentation methods, with at least 15x improvement and up to 80% higher precision in object detection."],"url":"http://arxiv.org/abs/2411.13544v1"}
{"created":"2024-11-20 18:54:32","title":"BALROG: Benchmarking Agentic LLM and VLM Reasoning On Games","abstract":"Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities; however, they still struggle to perform well in complex, dynamic environments. Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies-areas in which we lack effective methodologies for comprehensively evaluating these capabilities. To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games. Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment). We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs. Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks. Notably, we observe severe deficiencies in vision-based decision-making, as models perform worse when visual representations of the environments are provided. We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community.","sentences":["Large Language Models (LLMs) and Vision Language Models (VLMs) possess extensive knowledge and exhibit promising reasoning abilities; however, they still struggle to perform well in complex, dynamic environments.","Real-world tasks require handling intricate interactions, advanced spatial reasoning, long-term planning, and continuous exploration of new strategies-areas in which we lack effective methodologies for comprehensively evaluating these capabilities.","To address this gap, we introduce BALROG, a novel benchmark designed to assess the agentic capabilities of LLMs and VLMs through a diverse set of challenging games.","Our benchmark incorporates a range of existing reinforcement learning environments with varying levels of difficulty, including tasks that are solvable by non-expert humans in seconds to extremely challenging ones that may take years to master (e.g., the NetHack Learning Environment).","We devise fine-grained metrics to measure performance and conduct an extensive evaluation of several popular open-source and closed-source LLMs and VLMs.","Our findings indicate that while current models achieve partial success in the easier games, they struggle significantly with more challenging tasks.","Notably, we observe severe deficiencies in vision-based decision-making, as models perform worse when visual representations of the environments are provided.","We release BALROG as an open and user-friendly benchmark to facilitate future research and development in the agentic community."],"url":"http://arxiv.org/abs/2411.13543v1"}
{"created":"2024-11-20 18:41:03","title":"Metacognition for Unknown Situations and Environments (MUSE)","abstract":"Metacognition--the awareness and regulation of one's cognitive processes--is central to human adaptability in unknown situations. In contrast, current autonomous agents often struggle in novel environments due to their limited capacity for adaptation. We hypothesize that metacognition is a critical missing ingredient in adaptive autonomous systems, equipping them with the cognitive flexibility needed to tackle unfamiliar challenges. Given the broad scope of metacognitive abilities, we focus on two key aspects: competence awareness and strategy selection for novel tasks. To this end, we propose the Metacognition for Unknown Situations and Environments (MUSE) framework, which integrates metacognitive processes--specifically self-awareness and self-regulation--into autonomous agents. We present two initial implementations of MUSE: one based on world modeling and another leveraging large language models (LLMs), both instantiating the metacognitive cycle. Our system continuously learns to assess its competence on a given task and uses this self-awareness to guide iterative cycles of strategy selection. MUSE agents show significant improvements in self-awareness and self-regulation, enabling them to solve novel, out-of-distribution tasks more effectively compared to Dreamer-v3-based reinforcement learning and purely prompt-based LLM agent approaches. This work highlights the promise of approaches inspired by cognitive and neural systems in enabling autonomous systems to adapt to new environments, overcoming the limitations of current methods that rely heavily on extensive training data.","sentences":["Metacognition--the awareness and regulation of one's cognitive processes--is central to human adaptability in unknown situations.","In contrast, current autonomous agents often struggle in novel environments due to their limited capacity for adaptation.","We hypothesize that metacognition is a critical missing ingredient in adaptive autonomous systems, equipping them with the cognitive flexibility needed to tackle unfamiliar challenges.","Given the broad scope of metacognitive abilities, we focus on two key aspects: competence awareness and strategy selection for novel tasks.","To this end, we propose the Metacognition for Unknown Situations and Environments (MUSE) framework, which integrates metacognitive processes--specifically self-awareness and self-regulation--into autonomous agents.","We present two initial implementations of MUSE: one based on world modeling and another leveraging large language models (LLMs), both instantiating the metacognitive cycle.","Our system continuously learns to assess its competence on a given task and uses this self-awareness to guide iterative cycles of strategy selection.","MUSE agents show significant improvements in self-awareness and self-regulation, enabling them to solve novel, out-of-distribution tasks more effectively compared to Dreamer-v3-based reinforcement learning and purely prompt-based LLM agent approaches.","This work highlights the promise of approaches inspired by cognitive and neural systems in enabling autonomous systems to adapt to new environments, overcoming the limitations of current methods that rely heavily on extensive training data."],"url":"http://arxiv.org/abs/2411.13537v1"}
{"created":"2024-11-20 18:37:58","title":"Identity Preserving 3D Head Stylization with Multiview Score Distillation","abstract":"3D head stylization transforms realistic facial features into artistic representations, enhancing user engagement across gaming and virtual reality applications. While 3D-aware generators have made significant advancements, many 3D stylization methods primarily provide near-frontal views and struggle to preserve the unique identities of original subjects, often resulting in outputs that lack diversity and individuality. This paper addresses these challenges by leveraging the PanoHead model, synthesizing images from a comprehensive 360-degree perspective. We propose a novel framework that employs negative log-likelihood distillation (LD) to enhance identity preservation and improve stylization quality. By integrating multi-view grid score and mirror gradients within the 3D GAN architecture and introducing a score rank weighing technique, our approach achieves substantial qualitative and quantitative improvements. Our findings not only advance the state of 3D head stylization but also provide valuable insights into effective distillation processes between diffusion models and GANs, focusing on the critical issue of identity preservation. Please visit the https://three-bee.github.io/head_stylization for more visuals.","sentences":["3D head stylization transforms realistic facial features into artistic representations, enhancing user engagement across gaming and virtual reality applications.","While 3D-aware generators have made significant advancements, many 3D stylization methods primarily provide near-frontal views and struggle to preserve the unique identities of original subjects, often resulting in outputs that lack diversity and individuality.","This paper addresses these challenges by leveraging the PanoHead model, synthesizing images from a comprehensive 360-degree perspective.","We propose a novel framework that employs negative log-likelihood distillation (LD) to enhance identity preservation and improve stylization quality.","By integrating multi-view grid score and mirror gradients within the 3D GAN architecture and introducing a score rank weighing technique, our approach achieves substantial qualitative and quantitative improvements.","Our findings not only advance the state of 3D head stylization but also provide valuable insights into effective distillation processes between diffusion models and GANs, focusing on the critical issue of identity preservation.","Please visit the https://three-bee.github.io/head_stylization for more visuals."],"url":"http://arxiv.org/abs/2411.13536v1"}
{"created":"2024-11-20 18:35:41","title":"Predictive Insights into LGBTQ+ Minority Stress: A Transductive Exploration of Social Media Discourse","abstract":"Individuals who identify as sexual and gender minorities, including lesbian, gay, bisexual, transgender, queer, and others (LGBTQ+) are more likely to experience poorer health than their heterosexual and cisgender counterparts. One primary source that drives these health disparities is minority stress (i.e., chronic and social stressors unique to LGBTQ+ communities' experiences adapting to the dominant culture). This stress is frequently expressed in LGBTQ+ users' posts on social media platforms. However, these expressions are not just straightforward manifestations of minority stress. They involve linguistic complexity (e.g., idiom or lexical diversity), rendering them challenging for many traditional natural language processing methods to detect. In this work, we designed a hybrid model using Graph Neural Networks (GNN) and Bidirectional Encoder Representations from Transformers (BERT), a pre-trained deep language model to improve the classification performance of minority stress detection. We experimented with our model on a benchmark social media dataset for minority stress detection (LGBTQ+ MiSSoM+). The dataset is comprised of 5,789 human-annotated Reddit posts from LGBTQ+ subreddits. Our approach enables the extraction of hidden linguistic nuances through pretraining on a vast amount of raw data, while also engaging in transductive learning to jointly develop representations for both labeled training data and unlabeled test data. The RoBERTa-GCN model achieved an accuracy of 0.86 and an F1 score of 0.86, surpassing the performance of other baseline models in predicting LGBTQ+ minority stress. Improved prediction of minority stress expressions on social media could lead to digital health interventions to improve the wellbeing of LGBTQ+ people-a community with high rates of stress-sensitive health problems.","sentences":["Individuals who identify as sexual and gender minorities, including lesbian, gay, bisexual, transgender, queer, and others (LGBTQ+) are more likely to experience poorer health than their heterosexual and cisgender counterparts.","One primary source that drives these health disparities is minority stress (i.e., chronic and social stressors unique to LGBTQ+ communities' experiences adapting to the dominant culture).","This stress is frequently expressed in LGBTQ+ users' posts on social media platforms.","However, these expressions are not just straightforward manifestations of minority stress.","They involve linguistic complexity (e.g., idiom or lexical diversity), rendering them challenging for many traditional natural language processing methods to detect.","In this work, we designed a hybrid model using Graph Neural Networks (GNN) and Bidirectional Encoder Representations from Transformers (BERT), a pre-trained deep language model to improve the classification performance of minority stress detection.","We experimented with our model on a benchmark social media dataset for minority stress detection (LGBTQ+ MiSSoM+).","The dataset is comprised of 5,789 human-annotated Reddit posts from LGBTQ+ subreddits.","Our approach enables the extraction of hidden linguistic nuances through pretraining on a vast amount of raw data, while also engaging in transductive learning to jointly develop representations for both labeled training data and unlabeled test data.","The RoBERTa-GCN model achieved an accuracy of 0.86 and an F1 score of 0.86, surpassing the performance of other baseline models in predicting LGBTQ+ minority stress.","Improved prediction of minority stress expressions on social media could lead to digital health interventions to improve the wellbeing of LGBTQ+ people-a community with high rates of stress-sensitive health problems."],"url":"http://arxiv.org/abs/2411.13534v1"}
{"created":"2024-11-20 18:31:39","title":"A Distributed-memory Tridiagonal Solver Based on a Specialised Data Structure Optimised for CPU and GPU Architectures","abstract":"Various numerical methods used for solving partial differential equations (PDE) result in tridiagonal systems. Solving tridiagonal systems on distributed-memory environments is not straightforward, and often requires significant amount of communication. In this article, we present a novel distributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a specialised data structure. DistD2-TDS algorithm takes advantage of the diagonal dominance in tridiagonal systems to reduce the communications in distributed-memory environments. The underlying data structure plays a crucial role for the performance of the algorithm. First, the data structure improves data localities and makes it possible to minimise data movements via cache blocking and kernel fusion strategies. Second, data continuity enables a contiguous data access pattern and results in efficient utilisation of the available memory bandwidth. Finally, the data layout supports vectorisation on CPUs and thread level parallelisation on GPUs for improved performance. In order to demonstrate the robustness of the algorithm, we implemented and benchmarked the algorithm on CPUs and GPUs. We investigated the single rank performance and compared against existing algorithms. Furthermore, we analysed the strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to 8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the algorithm by using compact finite difference schemes to solve a 3D non-linear PDE. The results demonstrate that DistD2 algorithm can sustain around 66% of the theoretical peak bandwidth at scale on CPU and GPU based supercomputers.","sentences":["Various numerical methods used for solving partial differential equations (PDE) result in tridiagonal systems.","Solving tridiagonal systems on distributed-memory environments is not straightforward, and often requires significant amount of communication.","In this article, we present a novel distributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a specialised data structure.","DistD2-TDS algorithm takes advantage of the diagonal dominance in tridiagonal systems to reduce the communications in distributed-memory environments.","The underlying data structure plays a crucial role for the performance of the algorithm.","First, the data structure improves data localities and makes it possible to minimise data movements via cache blocking and kernel fusion strategies.","Second, data continuity enables a contiguous data access pattern and results in efficient utilisation of the available memory bandwidth.","Finally, the data layout supports vectorisation on CPUs and thread level parallelisation on GPUs for improved performance.","In order to demonstrate the robustness of the algorithm, we implemented and benchmarked the algorithm on CPUs and GPUs.","We investigated the single rank performance and compared against existing algorithms.","Furthermore, we analysed the strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to 8192 AMD EPYC 7742 CPUs.","Finally, we demonstrated a practical use case of the algorithm by using compact finite difference schemes to solve a 3D non-linear PDE.","The results demonstrate that DistD2 algorithm can sustain around 66% of the theoretical peak bandwidth at scale on CPU and GPU based supercomputers."],"url":"http://arxiv.org/abs/2411.13532v1"}
{"created":"2024-11-20 18:24:11","title":"Entropy Bootstrapping for Weakly Supervised Nuclei Detection","abstract":"Microscopy structure segmentation, such as detecting cells or nuclei, generally requires a human to draw a ground truth contour around each instance. Weakly supervised approaches (e.g. consisting of only single point labels) have the potential to reduce this workload significantly. Our approach uses individual point labels for an entropy estimation to approximate an underlying distribution of cell pixels. We infer full cell masks from this distribution, and use Mask-RCNN to produce an instance segmentation output. We compare this point--annotated approach with training on the full ground truth masks. We show that our method achieves a comparatively good level of performance, despite a 95% reduction in pixel labels.","sentences":["Microscopy structure segmentation, such as detecting cells or nuclei, generally requires a human to draw a ground truth contour around each instance.","Weakly supervised approaches (e.g. consisting of only single point labels) have the potential to reduce this workload significantly.","Our approach uses individual point labels for an entropy estimation to approximate an underlying distribution of cell pixels.","We infer full cell masks from this distribution, and use Mask-RCNN to produce an instance segmentation output.","We compare this point--annotated approach with training on the full ground truth masks.","We show that our method achieves a comparatively good level of performance, despite a 95% reduction in pixel labels."],"url":"http://arxiv.org/abs/2411.13528v1"}
{"created":"2024-11-20 18:21:58","title":"Geometric Algebra Planes: Convex Implicit Neural Volumes","abstract":"Volume parameterizations abound in recent literature, from the classic voxel grid to the implicit neural representation and everything in between. While implicit representations have shown impressive capacity and better memory efficiency compared to voxel grids, to date they require training via nonconvex optimization. This nonconvex training process can be slow to converge and sensitive to initialization and hyperparameter choices that affect the final converged result. We introduce a family of models, GA-Planes, that is the first class of implicit neural volume representations that can be trained by convex optimization. GA-Planes models include any combination of features stored in tensor basis elements, followed by a neural feature decoder. They generalize many existing representations and can be adapted for convex, semiconvex, or nonconvex training as needed for different inverse problems. In the 2D setting, we prove that GA-Planes is equivalent to a low-rank plus low-resolution matrix factorization; we show that this approximation outperforms the classic low-rank plus sparse decomposition for fitting a natural image. In 3D, we demonstrate GA-Planes' competitive performance in terms of expressiveness, model size, and optimizability across three volume fitting tasks: radiance field reconstruction, 3D segmentation, and video segmentation.","sentences":["Volume parameterizations abound in recent literature, from the classic voxel grid to the implicit neural representation and everything in between.","While implicit representations have shown impressive capacity and better memory efficiency compared to voxel grids, to date they require training via nonconvex optimization.","This nonconvex training process can be slow to converge and sensitive to initialization and hyperparameter choices that affect the final converged result.","We introduce a family of models, GA-Planes, that is the first class of implicit neural volume representations that can be trained by convex optimization.","GA-Planes models include any combination of features stored in tensor basis elements, followed by a neural feature decoder.","They generalize many existing representations and can be adapted for convex, semiconvex, or nonconvex training as needed for different inverse problems.","In the 2D setting, we prove that GA-Planes is equivalent to a low-rank plus low-resolution matrix factorization; we show that this approximation outperforms the classic low-rank plus sparse decomposition for fitting a natural image.","In 3D, we demonstrate GA-Planes' competitive performance in terms of expressiveness, model size, and optimizability across three volume fitting tasks: radiance field reconstruction, 3D segmentation, and video segmentation."],"url":"http://arxiv.org/abs/2411.13525v1"}
{"created":"2024-11-20 18:10:19","title":"Advancing Complex Medical Communication in Arabic with Sporo AraSum: Surpassing Existing Large Language Models","abstract":"The increasing demand for multilingual capabilities in healthcare underscores the need for AI models adept at processing diverse languages, particularly in clinical documentation and decision-making. Arabic, with its complex morphology, syntax, and diglossia, poses unique challenges for natural language processing (NLP) in medical contexts. This case study evaluates Sporo AraSum, a language model tailored for Arabic clinical documentation, against JAIS, the leading Arabic NLP model. Using synthetic datasets and modified PDQI-9 metrics modified ourselves for the purposes of assessing model performances in a different language. The study assessed the models' performance in summarizing patient-physician interactions, focusing on accuracy, comprehensiveness, clinical utility, and linguistic-cultural competence.   Results indicate that Sporo AraSum significantly outperforms JAIS in AI-centric quantitative metrics and all qualitative attributes measured in our modified version of the PDQI-9. AraSum's architecture enables precise and culturally sensitive documentation, addressing the linguistic nuances of Arabic while mitigating risks of AI hallucinations. These findings suggest that Sporo AraSum is better suited to meet the demands of Arabic-speaking healthcare environments, offering a transformative solution for multilingual clinical workflows. Future research should incorporate real-world data to further validate these findings and explore broader integration into healthcare systems.","sentences":["The increasing demand for multilingual capabilities in healthcare underscores the need for AI models adept at processing diverse languages, particularly in clinical documentation and decision-making.","Arabic, with its complex morphology, syntax, and diglossia, poses unique challenges for natural language processing (NLP) in medical contexts.","This case study evaluates Sporo AraSum, a language model tailored for Arabic clinical documentation, against JAIS, the leading Arabic NLP model.","Using synthetic datasets and modified PDQI-9 metrics modified ourselves for the purposes of assessing model performances in a different language.","The study assessed the models' performance in summarizing patient-physician interactions, focusing on accuracy, comprehensiveness, clinical utility, and linguistic-cultural competence.   ","Results indicate that Sporo AraSum significantly outperforms JAIS in AI-centric quantitative metrics and all qualitative attributes measured in our modified version of the PDQI-9.","AraSum's architecture enables precise and culturally sensitive documentation, addressing the linguistic nuances of Arabic while mitigating risks of AI hallucinations.","These findings suggest that Sporo AraSum is better suited to meet the demands of Arabic-speaking healthcare environments, offering a transformative solution for multilingual clinical workflows.","Future research should incorporate real-world data to further validate these findings and explore broader integration into healthcare systems."],"url":"http://arxiv.org/abs/2411.13518v1"}
{"created":"2024-11-20 18:09:14","title":"Understanding the Personal Networks of People Experiencing Homelessness in King County, WA with aggregate Relational Data","abstract":"The social networks of people experiencing homelessness are an understudied but vital aspect of their lives, offering access to information, support, and safety. In 2023, the U.S. Department of Housing and Urban Development reported 653,100 people experiencing homelessness on any given night -- a 23% rise since 2022, though likely an undercount. This paper examines a unique three-year dataset (2022-2024) of survey responses from over 3,000 unhoused individuals in King County, WA, collected via network-based sampling methods to estimate the unsheltered population. Our study analyzes the networks of the unsheltered population, focusing on acquaintance, close friendship, kinship, and peer referral networks. Findings reveal a decline in social connectivity over time. The average number of acquaintances dropped from 80 in 2023 to 40 in 2024. Close friendship levels remained stable at 2.5, but given the growth in the homeless population, this suggests decreased network connectivity. Kinship networks expanded, indicating that more family members of unhoused individuals are also experiencing homelessness. These trends suggest increasing social disconnection, possibly driven by displacement and a rise in newly homeless individuals. The growing isolation may reduce opportunities for information sharing and mutual support. However, the increased reliance on family networks highlights the shifting dynamics of social support within this community. This research underscores the need for policies fostering social connections and community building, such as reducing displacement and providing spaces for congregation, to counter the growing anomie among unhoused populations.","sentences":["The social networks of people experiencing homelessness are an understudied but vital aspect of their lives, offering access to information, support, and safety.","In 2023, the U.S. Department of Housing and Urban Development reported 653,100 people experiencing homelessness on any given night -- a 23% rise since 2022, though likely an undercount.","This paper examines a unique three-year dataset (2022-2024) of survey responses from over 3,000 unhoused individuals in King County, WA, collected via network-based sampling methods to estimate the unsheltered population.","Our study analyzes the networks of the unsheltered population, focusing on acquaintance, close friendship, kinship, and peer referral networks.","Findings reveal a decline in social connectivity over time.","The average number of acquaintances dropped from 80 in 2023 to 40 in 2024.","Close friendship levels remained stable at 2.5, but given the growth in the homeless population, this suggests decreased network connectivity.","Kinship networks expanded, indicating that more family members of unhoused individuals are also experiencing homelessness.","These trends suggest increasing social disconnection, possibly driven by displacement and a rise in newly homeless individuals.","The growing isolation may reduce opportunities for information sharing and mutual support.","However, the increased reliance on family networks highlights the shifting dynamics of social support within this community.","This research underscores the need for policies fostering social connections and community building, such as reducing displacement and providing spaces for congregation, to counter the growing anomie among unhoused populations."],"url":"http://arxiv.org/abs/2411.13517v1"}
{"created":"2024-11-20 18:06:55","title":"Procurement Auctions via Approximately Optimal Submodular Optimization","abstract":"We study procurement auctions, where an auctioneer seeks to acquire services from strategic sellers with private costs. The quality of services is measured by a submodular function known to the auctioneer. Our goal is to design computationally efficient procurement auctions that (approximately) maximize the difference between the quality of the acquired services and the total cost of the sellers, while ensuring incentive compatibility (IC), individual rationality (IR) for sellers, and non-negative surplus (NAS) for the auctioneer.   Our contributions are twofold: (i) we provide an improved analysis of existing algorithms for non-positive submodular function maximization, and (ii) we design efficient frameworks that transform submodular optimization algorithms into mechanisms that are IC, IR, NAS, and approximation-preserving. These frameworks apply to both the offline setting, where all sellers' bids and services are available simultaneously, and the online setting, where sellers arrive in an adversarial order, requiring the auctioneer to make irrevocable decisions.   We also explore whether state-of-the-art submodular optimization algorithms can be converted into descending auctions in adversarial settings, where the schedule of descending prices is determined by an adversary. We show that a submodular optimization algorithm satisfying bi-criteria $(1/2, 1)$-approximation in welfare can be effectively adapted to a descending auction. Additionally, we establish a connection between descending auctions and online submodular optimization.   Finally, we demonstrate the practical applications of our frameworks by instantiating them with state-of-the-art submodular optimization algorithms and empirically comparing their welfare performance on publicly available datasets with thousands of sellers.","sentences":["We study procurement auctions, where an auctioneer seeks to acquire services from strategic sellers with private costs.","The quality of services is measured by a submodular function known to the auctioneer.","Our goal is to design computationally efficient procurement auctions that (approximately) maximize the difference between the quality of the acquired services and the total cost of the sellers, while ensuring incentive compatibility (IC), individual rationality (IR) for sellers, and non-negative surplus (NAS) for the auctioneer.   ","Our contributions are twofold: (i) we provide an improved analysis of existing algorithms for non-positive submodular function maximization, and (ii) we design efficient frameworks that transform submodular optimization algorithms into mechanisms that are IC, IR, NAS, and approximation-preserving.","These frameworks apply to both the offline setting, where all sellers' bids and services are available simultaneously, and the online setting, where sellers arrive in an adversarial order, requiring the auctioneer to make irrevocable decisions.   ","We also explore whether state-of-the-art submodular optimization algorithms can be converted into descending auctions in adversarial settings, where the schedule of descending prices is determined by an adversary.","We show that a submodular optimization algorithm satisfying bi-criteria $(1/2, 1)$-approximation in welfare can be effectively adapted to a descending auction.","Additionally, we establish a connection between descending auctions and online submodular optimization.   ","Finally, we demonstrate the practical applications of our frameworks by instantiating them with state-of-the-art submodular optimization algorithms and empirically comparing their welfare performance on publicly available datasets with thousands of sellers."],"url":"http://arxiv.org/abs/2411.13513v1"}
{"created":"2024-11-20 17:57:33","title":"Dynamically Feasible Path Planning in Cluttered Environments via Reachable Bezier Polytopes","abstract":"The deployment of robotic systems in real world environments requires the ability to quickly produce paths through cluttered, non-convex spaces. These planned trajectories must be both kinematically feasible (i.e., collision free) and dynamically feasible (i.e., satisfy the underlying system dynamics), necessitating a consideration of both the free space and the dynamics of the robot in the path planning phase. In this work, we explore the application of reachable Bezier polytopes as an efficient tool for generating trajectories satisfying both kinematic and dynamic requirements. Furthermore, we demonstrate that by offloading specific computation tasks to the GPU, such an algorithm can meet tight real time requirements. We propose a layered control architecture that efficiently produces collision free and dynamically feasible paths for nonlinear control systems, and demonstrate the framework on the tasks of 3D hopping in a cluttered environment.","sentences":["The deployment of robotic systems in real world environments requires the ability to quickly produce paths through cluttered, non-convex spaces.","These planned trajectories must be both kinematically feasible (i.e., collision free) and dynamically feasible (i.e., satisfy the underlying system dynamics), necessitating a consideration of both the free space and the dynamics of the robot in the path planning phase.","In this work, we explore the application of reachable Bezier polytopes as an efficient tool for generating trajectories satisfying both kinematic and dynamic requirements.","Furthermore, we demonstrate that by offloading specific computation tasks to the GPU, such an algorithm can meet tight real time requirements.","We propose a layered control architecture that efficiently produces collision free and dynamically feasible paths for nonlinear control systems, and demonstrate the framework on the tasks of 3D hopping in a cluttered environment."],"url":"http://arxiv.org/abs/2411.13507v1"}
{"created":"2024-11-20 17:56:56","title":"Bezier Reachable Polytopes: Efficient Certificates for Robust Motion Planning with Layered Architectures","abstract":"Control architectures are often implemented in a layered fashion, combining independently designed blocks to achieve complex tasks. Providing guarantees for such hierarchical frameworks requires considering the capabilities and limitations of each layer and their interconnections at design time. To address this holistic design challenge, we introduce the notion of Bezier Reachable Polytopes -- certificates of reachable points in the space of Bezier polynomial reference trajectories. This approach captures the set of trajectories that can be tracked by a low-level controller while satisfying state and input constraints, and leverages the geometric properties of Bezier polynomials to maintain an efficient polytopic representation. As a result, these certificates serve as a constructive tool for layered architectures, enabling long-horizon tasks to be reasoned about in a computationally tractable manner.","sentences":["Control architectures are often implemented in a layered fashion, combining independently designed blocks to achieve complex tasks.","Providing guarantees for such hierarchical frameworks requires considering the capabilities and limitations of each layer and their interconnections at design time.","To address this holistic design challenge, we introduce the notion of Bezier Reachable Polytopes -- certificates of reachable points in the space of Bezier polynomial reference trajectories.","This approach captures the set of trajectories that can be tracked by a low-level controller while satisfying state and input constraints, and leverages the geometric properties of Bezier polynomials to maintain an efficient polytopic representation.","As a result, these certificates serve as a constructive tool for layered architectures, enabling long-horizon tasks to be reasoned about in a computationally tractable manner."],"url":"http://arxiv.org/abs/2411.13506v1"}
{"created":"2024-11-20 17:55:38","title":"Disentangling Memory and Reasoning Ability in Large Language Models","abstract":"Large Language Models (LLMs) have demonstrated strong performance in handling complex tasks requiring both extensive knowledge and reasoning abilities. However, the existing LLM inference pipeline operates as an opaque process without explicit separation between knowledge retrieval and reasoning steps, making the model's decision-making process unclear and disorganized. This ambiguity can lead to issues such as hallucinations and knowledge forgetting, which significantly impact the reliability of LLMs in high-stakes domains. In this paper, we propose a new inference paradigm that decomposes the complex inference process into two distinct and clear actions: (1) memory recall: which retrieves relevant knowledge, and (2) reasoning: which performs logical steps based on the recalled knowledge. To facilitate this decomposition, we introduce two special tokens memory and reason, guiding the model to distinguish between steps that require knowledge retrieval and those that involve reasoning. Our experiment results show that this decomposition not only improves model performance but also enhances the interpretability of the inference process, enabling users to identify sources of error and refine model responses effectively. The code is available at https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.","sentences":["Large Language Models (LLMs) have demonstrated strong performance in handling complex tasks requiring both extensive knowledge and reasoning abilities.","However, the existing LLM inference pipeline operates as an opaque process without explicit separation between knowledge retrieval and reasoning steps, making the model's decision-making process unclear and disorganized.","This ambiguity can lead to issues such as hallucinations and knowledge forgetting, which significantly impact the reliability of LLMs in high-stakes domains.","In this paper, we propose a new inference paradigm that decomposes the complex inference process into two distinct and clear actions: (1) memory recall: which retrieves relevant knowledge, and (2) reasoning: which performs logical steps based on the recalled knowledge.","To facilitate this decomposition, we introduce two special tokens memory and reason, guiding the model to distinguish between steps that require knowledge retrieval and those that involve reasoning.","Our experiment results show that this decomposition not only improves model performance but also enhances the interpretability of the inference process, enabling users to identify sources of error and refine model responses effectively.","The code is available at https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning."],"url":"http://arxiv.org/abs/2411.13504v1"}
{"created":"2024-11-20 17:54:41","title":"VBench++: Comprehensive and Versatile Benchmark Suite for Video Generative Models","abstract":"Video generation has witnessed significant advancements, yet evaluating these models remains a challenge. A comprehensive evaluation benchmark for video generation is indispensable for two reasons: 1) Existing metrics do not fully align with human perceptions; 2) An ideal evaluation system should provide insights to inform future developments of video generation. To this end, we present VBench, a comprehensive benchmark suite that dissects \"video generation quality\" into specific, hierarchical, and disentangled dimensions, each with tailored prompts and evaluation methods. VBench has several appealing properties: 1) Comprehensive Dimensions: VBench comprises 16 dimensions in video generation (e.g., subject identity inconsistency, motion smoothness, temporal flickering, and spatial relationship, etc). The evaluation metrics with fine-grained levels reveal individual models' strengths and weaknesses. 2) Human Alignment: We also provide a dataset of human preference annotations to validate our benchmarks' alignment with human perception, for each evaluation dimension respectively. 3) Valuable Insights: We look into current models' ability across various evaluation dimensions, and various content types. We also investigate the gaps between video and image generation models. 4) Versatile Benchmarking: VBench++ supports evaluating text-to-video and image-to-video. We introduce a high-quality Image Suite with an adaptive aspect ratio to enable fair evaluations across different image-to-video generation settings. Beyond assessing technical quality, VBench++ evaluates the trustworthiness of video generative models, providing a more holistic view of model performance. 5) Full Open-Sourcing: We fully open-source VBench++ and continually add new video generation models to our leaderboard to drive forward the field of video generation.","sentences":["Video generation has witnessed significant advancements, yet evaluating these models remains a challenge.","A comprehensive evaluation benchmark for video generation is indispensable for two reasons: 1) Existing metrics do not fully align with human perceptions; 2) An ideal evaluation system should provide insights to inform future developments of video generation.","To this end, we present VBench, a comprehensive benchmark suite that dissects \"video generation quality\" into specific, hierarchical, and disentangled dimensions, each with tailored prompts and evaluation methods.","VBench has several appealing properties: 1) Comprehensive Dimensions:","VBench comprises 16 dimensions in video generation (e.g., subject identity inconsistency, motion smoothness, temporal flickering, and spatial relationship, etc).","The evaluation metrics with fine-grained levels reveal individual models' strengths and weaknesses.","2) Human Alignment: We also provide a dataset of human preference annotations to validate our benchmarks' alignment with human perception, for each evaluation dimension respectively.","3) Valuable Insights: We look into current models' ability across various evaluation dimensions, and various content types.","We also investigate the gaps between video and image generation models.","4) Versatile Benchmarking: VBench++ supports evaluating text-to-video and image-to-video.","We introduce a high-quality Image Suite with an adaptive aspect ratio to enable fair evaluations across different image-to-video generation settings.","Beyond assessing technical quality, VBench++ evaluates the trustworthiness of video generative models, providing a more holistic view of model performance.","5) Full Open-Sourcing: We fully open-source VBench++ and continually add new video generation models to our leaderboard to drive forward the field of video generation."],"url":"http://arxiv.org/abs/2411.13503v1"}
{"created":"2024-11-20 17:45:03","title":"Advancing Heatwave Forecasting via Distribution Informed-Graph Neural Networks (DI-GNNs): Integrating Extreme Value Theory with GNNs","abstract":"Heatwaves, prolonged periods of extreme heat, have intensified in frequency and severity due to climate change, posing substantial risks to public health, ecosystems, and infrastructure. Despite advancements in Machine Learning (ML) modeling, accurate heatwave forecasting at weather scales (1--15 days) remains challenging due to the non-linear interactions between atmospheric drivers and the rarity of these extreme events. Traditional models relying on heuristic feature engineering often fail to generalize across diverse climates and capture the complexities of heatwave dynamics. This study introduces the Distribution-Informed Graph Neural Network (DI-GNN), a novel framework that integrates principles from Extreme Value Theory (EVT) into the graph neural network architecture. DI-GNN incorporates Generalized Pareto Distribution (GPD)-derived descriptors into the feature space, adjacency matrix, and loss function to enhance its sensitivity to rare heatwave occurrences. By prioritizing the tails of climatic distributions, DI-GNN addresses the limitations of existing methods, particularly in imbalanced datasets where traditional metrics like accuracy are misleading. Empirical evaluations using weather station data from British Columbia, Canada, demonstrate the superior performance of DI-GNN compared to baseline models. DI-GNN achieved significant improvements in balanced accuracy, recall, and precision, with high AUC and average precision scores, reflecting its robustness in distinguishing heatwave events.","sentences":["Heatwaves, prolonged periods of extreme heat, have intensified in frequency and severity due to climate change, posing substantial risks to public health, ecosystems, and infrastructure.","Despite advancements in Machine Learning (ML) modeling, accurate heatwave forecasting at weather scales (1--15 days) remains challenging due to the non-linear interactions between atmospheric drivers and the rarity of these extreme events.","Traditional models relying on heuristic feature engineering often fail to generalize across diverse climates and capture the complexities of heatwave dynamics.","This study introduces the Distribution-Informed Graph Neural Network (DI-GNN), a novel framework that integrates principles from Extreme Value Theory (EVT) into the graph neural network architecture.","DI-GNN incorporates Generalized Pareto Distribution (GPD)-derived descriptors into the feature space, adjacency matrix, and loss function to enhance its sensitivity to rare heatwave occurrences.","By prioritizing the tails of climatic distributions, DI-GNN addresses the limitations of existing methods, particularly in imbalanced datasets where traditional metrics like accuracy are misleading.","Empirical evaluations using weather station data from British Columbia, Canada, demonstrate the superior performance of DI-GNN compared to baseline models.","DI-GNN achieved significant improvements in balanced accuracy, recall, and precision, with high AUC and average precision scores, reflecting its robustness in distinguishing heatwave events."],"url":"http://arxiv.org/abs/2411.13496v1"}
{"created":"2024-11-20 17:43:02","title":"Polynomial Freiman-Ruzsa, Reed-Muller codes and Shannon capacity","abstract":"In 1948, Shannon used a probabilistic argument to show the existence of codes achieving a maximal rate defined by the channel capacity. In 1954, Muller and Reed introduced a simple deterministic code construction, based on polynomial evaluations, conjectured shortly after to achieve capacity. The conjecture led to decades of activity involving various areas of mathematics and the recent settlement by [AS23] using flower set boosting. In this paper, we provide an alternative proof of the weak form of the capacity result, i.e., that RM codes have a vanishing local error at any rate below capacity. Our proof relies on the recent Polynomial Freiman-Ruzsa conjecture's proof [GGMT23] and an entropy extraction approach similar to [AY19]. Further, a new additive combinatorics conjecture is put forward which would imply the stronger result with vanishing global error. We expect the latter conjecture to be more directly relevant to coding applications.","sentences":["In 1948, Shannon used a probabilistic argument to show the existence of codes achieving a maximal rate defined by the channel capacity.","In 1954, Muller and Reed introduced a simple deterministic code construction, based on polynomial evaluations, conjectured shortly after to achieve capacity.","The conjecture led to decades of activity involving various areas of mathematics and the recent settlement by [AS23] using flower set boosting.","In this paper, we provide an alternative proof of the weak form of the capacity result, i.e., that RM codes have a vanishing local error at any rate below capacity.","Our proof relies on the recent Polynomial Freiman-Ruzsa conjecture's proof [GGMT23] and an entropy extraction approach similar to [AY19].","Further, a new additive combinatorics conjecture is put forward which would imply the stronger result with vanishing global error.","We expect the latter conjecture to be more directly relevant to coding applications."],"url":"http://arxiv.org/abs/2411.13493v1"}
{"created":"2024-11-20 17:35:21","title":"Utilizing Large Language Models to Synthesize Product Desirability Datasets","abstract":"This research explores the application of large language models (LLMs) to generate synthetic datasets for Product Desirability Toolkit (PDT) testing, a key component in evaluating user sentiment and product experience. Utilizing gpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three methods, Word+Review, Review+Word, and Supply-Word, were each used to synthesize 1000 product reviews. The generated datasets were assessed for sentiment alignment, textual diversity, and data generation cost. Results demonstrated high sentiment alignment across all methods, with Pearson correlations ranging from 0.93 to 0.97. Supply-Word exhibited the highest diversity and coverage of PDT terms, although with increased generation costs. Despite minor biases toward positive sentiments, in situations with limited test data, LLM-generated synthetic data offers significant advantages, including scalability, cost savings, and flexibility in dataset production.","sentences":["This research explores the application of large language models (LLMs) to generate synthetic datasets for Product Desirability Toolkit (PDT) testing, a key component in evaluating user sentiment and product experience.","Utilizing gpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three methods, Word+Review, Review+Word, and Supply-Word, were each used to synthesize 1000 product reviews.","The generated datasets were assessed for sentiment alignment, textual diversity, and data generation cost.","Results demonstrated high sentiment alignment across all methods, with Pearson correlations ranging from 0.93 to 0.97.","Supply-Word exhibited the highest diversity and coverage of PDT terms, although with increased generation costs.","Despite minor biases toward positive sentiments, in situations with limited test data, LLM-generated synthetic data offers significant advantages, including scalability, cost savings, and flexibility in dataset production."],"url":"http://arxiv.org/abs/2411.13485v1"}
{"created":"2024-11-20 17:23:40","title":"PatentEdits: Framing Patent Novelty as Textual Entailment","abstract":"A patent must be deemed novel and non-obvious in order to be granted by the US Patent Office (USPTO). If it is not, a US patent examiner will cite the prior work, or prior art, that invalidates the novelty and issue a non-final rejection. Predicting what claims of the invention should change given the prior art is an essential and crucial step in securing invention rights, yet has not been studied before as a learnable task. In this work we introduce the PatentEdits dataset, which contains 105K examples of successful revisions that overcome objections to novelty. We design algorithms to label edits sentence by sentence, then establish how well these edits can be predicted with large language models (LLMs). We demonstrate that evaluating textual entailment between cited references and draft sentences is especially effective in predicting which inventive claims remained unchanged or are novel in relation to prior art.","sentences":["A patent must be deemed novel and non-obvious in order to be granted by the US Patent Office (USPTO).","If it is not, a US patent examiner will cite the prior work, or prior art, that invalidates the novelty and issue a non-final rejection.","Predicting what claims of the invention should change given the prior art is an essential and crucial step in securing invention rights, yet has not been studied before as a learnable task.","In this work we introduce the PatentEdits dataset, which contains 105K examples of successful revisions that overcome objections to novelty.","We design algorithms to label edits sentence by sentence, then establish how well these edits can be predicted with large language models (LLMs).","We demonstrate that evaluating textual entailment between cited references and draft sentences is especially effective in predicting which inventive claims remained unchanged or are novel in relation to prior art."],"url":"http://arxiv.org/abs/2411.13477v1"}
{"created":"2024-11-20 17:22:31","title":"When Precision Meets Position: BFloat16 Breaks Down RoPE in Long-Context Training","abstract":"Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks. Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training. However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios. This issue arises from BFloat16's limited precision and accumulates as context length increases, with the first token contributing significantly to this problem. To address this, we develop AnchorAttention, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training. AnchorAttention reduces unnecessary attention computations, maintains semantic coherence, and boosts computational efficiency by treating the first token as a shared anchor with a consistent position ID, making it visible to all documents within the training context. Experiments on three types of LLMs demonstrate that AnchorAttention significantly improves long-context performance and reduces training time by over 50\\% compared to standard full attention mechanisms, while preserving the original LLM's capabilities on general tasks. Our code is available at https://github.com/haonan3/AnchorContext.","sentences":["Extending context window sizes allows large language models (LLMs) to process longer sequences and handle more complex tasks.","Rotary Positional Embedding (RoPE) has become the de facto standard due to its relative positional encoding properties that benefit long-context training.","However, we observe that using RoPE with BFloat16 format results in numerical issues, causing it to deviate from its intended relative positional encoding, especially in long-context scenarios.","This issue arises from BFloat16's limited precision and accumulates as context length increases, with the first token contributing significantly to this problem.","To address this, we develop AnchorAttention, a plug-and-play attention method that alleviates numerical issues caused by BFloat16, improves long-context capabilities, and speeds up training.","AnchorAttention reduces unnecessary attention computations, maintains semantic coherence, and boosts computational efficiency by treating the first token as a shared anchor with a consistent position ID, making it visible to all documents within the training context.","Experiments on three types of LLMs demonstrate that AnchorAttention significantly improves long-context performance and reduces training time by over 50\\% compared to standard full attention mechanisms, while preserving the original LLM's capabilities on general tasks.","Our code is available at https://github.com/haonan3/AnchorContext."],"url":"http://arxiv.org/abs/2411.13476v1"}
{"created":"2024-11-20 17:18:56","title":"Packet Steering Mechanisms for MLO in Wi-Fi 7","abstract":"Besides extremely high throughput, Wi-Fi 7 is also aimed at providing users a more deterministic behavior, characterized by shorter average latency and smaller jitters. A key mechanism to achieve this is multi-link operation, which brings simultaneous multi-band communication to client stations as well. In this paper, traffic steering policies are briefly reviewed and grouped into general classes, each one with its advantages and limitations. A basic mechanism for supporting dynamic steering is then described, which is simple enough to allow implementation in real Wi-Fi chipsets but highly flexible at the same time. Its operation can be driven by the host on a per-packet basis, and this permits to optimize spectrum usage depending on the requirements of applications and the traffic pattern they generate.","sentences":["Besides extremely high throughput, Wi-Fi 7 is also aimed at providing users a more deterministic behavior, characterized by shorter average latency and smaller jitters.","A key mechanism to achieve this is multi-link operation, which brings simultaneous multi-band communication to client stations as well.","In this paper, traffic steering policies are briefly reviewed and grouped into general classes, each one with its advantages and limitations.","A basic mechanism for supporting dynamic steering is then described, which is simple enough to allow implementation in real Wi-Fi chipsets but highly flexible at the same time.","Its operation can be driven by the host on a per-packet basis, and this permits to optimize spectrum usage depending on the requirements of applications and the traffic pattern they generate."],"url":"http://arxiv.org/abs/2411.13470v1"}
{"created":"2024-11-20 17:10:24","title":"Sampling and Integration of Logconcave Functions by Algorithmic Diffusion","abstract":"We study the complexity of sampling, rounding, and integrating arbitrary logconcave functions. Our new approach provides the first complexity improvements in nearly two decades for general logconcave functions for all three problems, and matches the best-known complexities for the special case of uniform distributions on convex bodies. For the sampling problem, our output guarantees are significantly stronger than previously known, and lead to a streamlined analysis of statistical estimation based on dependent random samples.","sentences":["We study the complexity of sampling, rounding, and integrating arbitrary logconcave functions.","Our new approach provides the first complexity improvements in nearly two decades for general logconcave functions for all three problems, and matches the best-known complexities for the special case of uniform distributions on convex bodies.","For the sampling problem, our output guarantees are significantly stronger than previously known, and lead to a streamlined analysis of statistical estimation based on dependent random samples."],"url":"http://arxiv.org/abs/2411.13462v1"}
{"created":"2024-11-20 17:08:38","title":"SoK: A Systems Perspective on Compound AI Threats and Countermeasures","abstract":"Large language models (LLMs) used across enterprises often use proprietary models and operate on sensitive inputs and data. The wide range of attack vectors identified in prior research - targeting various software and hardware components used in training and inference - makes it extremely challenging to enforce confidentiality and integrity policies.   As we advance towards constructing compound AI inference pipelines that integrate multiple large language models (LLMs), the attack surfaces expand significantly. Attackers now focus on the AI algorithms as well as the software and hardware components associated with these systems. While current research often examines these elements in isolation, we find that combining cross-layer attack observations can enable powerful end-to-end attacks with minimal assumptions about the threat model. Given, the sheer number of existing attacks at each layer, we need a holistic and systemized understanding of different attack vectors at each layer.   This SoK discusses different software and hardware attacks applicable to compound AI systems and demonstrates how combining multiple attack mechanisms can reduce the threat model assumptions required for an isolated attack. Next, we systematize the ML attacks in lines with the Mitre Att&ck framework to better position each attack based on the threat model. Finally, we outline the existing countermeasures for both software and hardware layers and discuss the necessity of a comprehensive defense strategy to enable the secure and high-performance deployment of compound AI systems.","sentences":["Large language models (LLMs) used across enterprises often use proprietary models and operate on sensitive inputs and data.","The wide range of attack vectors identified in prior research - targeting various software and hardware components used in training and inference - makes it extremely challenging to enforce confidentiality and integrity policies.   ","As we advance towards constructing compound AI inference pipelines that integrate multiple large language models (LLMs), the attack surfaces expand significantly.","Attackers now focus on the AI algorithms as well as the software and hardware components associated with these systems.","While current research often examines these elements in isolation, we find that combining cross-layer attack observations can enable powerful end-to-end attacks with minimal assumptions about the threat model.","Given, the sheer number of existing attacks at each layer, we need a holistic and systemized understanding of different attack vectors at each layer.   ","This SoK discusses different software and hardware attacks applicable to compound AI systems and demonstrates how combining multiple attack mechanisms can reduce the threat model assumptions required for an isolated attack.","Next, we systematize the ML attacks in lines with the Mitre Att&ck framework to better position each attack based on the threat model.","Finally, we outline the existing countermeasures for both software and hardware layers and discuss the necessity of a comprehensive defense strategy to enable the secure and high-performance deployment of compound AI systems."],"url":"http://arxiv.org/abs/2411.13459v1"}
{"created":"2024-11-20 16:59:41","title":"LIMBA: An Open-Source Framework for the Preservation and Valorization of Low-Resource Languages using Generative Models","abstract":"Minority languages are vital to preserving cultural heritage, yet they face growing risks of extinction due to limited digital resources and the dominance of artificial intelligence models trained on high-resource languages. This white paper proposes a framework to generate linguistic tools for low-resource languages, focusing on data creation to support the development of language models that can aid in preservation efforts. Sardinian, an endangered language, serves as the case study to demonstrate the framework's effectiveness. By addressing the data scarcity that hinders intelligent applications for such languages, we contribute to promoting linguistic diversity and support ongoing efforts in language standardization and revitalization through modern technologies.","sentences":["Minority languages are vital to preserving cultural heritage, yet they face growing risks of extinction due to limited digital resources and the dominance of artificial intelligence models trained on high-resource languages.","This white paper proposes a framework to generate linguistic tools for low-resource languages, focusing on data creation to support the development of language models that can aid in preservation efforts.","Sardinian, an endangered language, serves as the case study to demonstrate the framework's effectiveness.","By addressing the data scarcity that hinders intelligent applications for such languages, we contribute to promoting linguistic diversity and support ongoing efforts in language standardization and revitalization through modern technologies."],"url":"http://arxiv.org/abs/2411.13453v1"}
{"created":"2024-11-20 16:54:15","title":"AdaptAgent: Adapting Multimodal Web Agents with Few-Shot Learning from Human Demonstrations","abstract":"State-of-the-art multimodal web agents, powered by Multimodal Large Language Models (MLLMs), can autonomously execute many web tasks by processing user instructions and interacting with graphical user interfaces (GUIs). Current strategies for building web agents rely on (i) the generalizability of underlying MLLMs and their steerability via prompting, and (ii) large-scale fine-tuning of MLLMs on web-related tasks. However, web agents still struggle to automate tasks on unseen websites and domains, limiting their applicability to enterprise-specific and proprietary platforms. Beyond generalization from large-scale pre-training and fine-tuning, we propose building agents for few-shot adaptability using human demonstrations. We introduce the AdaptAgent framework that enables both proprietary and open-weights multimodal web agents to adapt to new websites and domains using few human demonstrations (up to 2). Our experiments on two popular benchmarks -- Mind2Web & VisualWebArena -- show that using in-context demonstrations (for proprietary models) or meta-adaptation demonstrations (for meta-learned open-weights models) boosts task success rate by 3.36% to 7.21% over non-adapted state-of-the-art models, corresponding to a relative increase of 21.03% to 65.75%. Furthermore, our additional analyses (a) show the effectiveness of multimodal demonstrations over text-only ones, (b) shed light on the influence of different data selection strategies during meta-learning on the generalization of the agent, and (c) demonstrate the effect of number of few-shot examples on the web agent's success rate. Overall, our results unlock a complementary axis for developing widely applicable multimodal web agents beyond large-scale pre-training and fine-tuning, emphasizing few-shot adaptability.","sentences":["State-of-the-art multimodal web agents, powered by Multimodal Large Language Models (MLLMs), can autonomously execute many web tasks by processing user instructions and interacting with graphical user interfaces (GUIs).","Current strategies for building web agents rely on (i) the generalizability of underlying MLLMs and their steerability via prompting, and (ii) large-scale fine-tuning of MLLMs on web-related tasks.","However, web agents still struggle to automate tasks on unseen websites and domains, limiting their applicability to enterprise-specific and proprietary platforms.","Beyond generalization from large-scale pre-training and fine-tuning, we propose building agents for few-shot adaptability using human demonstrations.","We introduce the AdaptAgent framework that enables both proprietary and open-weights multimodal web agents to adapt to new websites and domains using few human demonstrations (up to 2).","Our experiments on two popular benchmarks -- Mind2Web & VisualWebArena -- show that using in-context demonstrations (for proprietary models) or meta-adaptation demonstrations (for meta-learned open-weights models) boosts task success rate by 3.36% to 7.21% over non-adapted state-of-the-art models, corresponding to a relative increase of 21.03% to 65.75%.","Furthermore, our additional analyses (a) show the effectiveness of multimodal demonstrations over text-only ones, (b) shed light on the influence of different data selection strategies during meta-learning on the generalization of the agent, and (c) demonstrate the effect of number of few-shot examples on the web agent's success rate.","Overall, our results unlock a complementary axis for developing widely applicable multimodal web agents beyond large-scale pre-training and fine-tuning, emphasizing few-shot adaptability."],"url":"http://arxiv.org/abs/2411.13451v1"}
{"created":"2024-11-20 16:43:43","title":"A Digital Twin for Telesurgery under Intermittent Communication","abstract":"Telesurgery is an effective way to deliver service from expert surgeons to areas without immediate access to specialized resources. However, many of these areas, such as rural districts or battlefields, might be subject to different problems in communication, especially latency and intermittent periods of communication outage. This challenge motivates the use of a digital twin for the surgical system, where a simulation would mirror the robot hardware and surgical environment in the real world. The surgeon would then be able to interact with the digital twin during communication outage, followed by a recovery strategy on the real robot upon reestablishing communication. This paper builds the digital twin for the da Vinci surgical robot, with a buffering and replay strategy that reduces the mean task completion time by 23% when compared to the baseline, for a peg transfer task subject to intermittent communication outage.","sentences":["Telesurgery is an effective way to deliver service from expert surgeons to areas without immediate access to specialized resources.","However, many of these areas, such as rural districts or battlefields, might be subject to different problems in communication, especially latency and intermittent periods of communication outage.","This challenge motivates the use of a digital twin for the surgical system, where a simulation would mirror the robot hardware and surgical environment in the real world.","The surgeon would then be able to interact with the digital twin during communication outage, followed by a recovery strategy on the real robot upon reestablishing communication.","This paper builds the digital twin for the da Vinci surgical robot, with a buffering and replay strategy that reduces the mean task completion time by 23% when compared to the baseline, for a peg transfer task subject to intermittent communication outage."],"url":"http://arxiv.org/abs/2411.13449v1"}
{"created":"2024-11-20 16:42:14","title":"Blockchain-Enhanced Framework for Secure Third-Party Vendor Risk Management and Vigilant Security Controls","abstract":"In an era of heightened digital interconnectedness, businesses increasingly rely on third-party vendors to enhance their operational capabilities. However, this growing dependency introduces significant security risks, making it crucial to develop a robust framework to mitigate potential vulnerabilities. This paper proposes a comprehensive secure framework for managing third-party vendor risk, integrating blockchain technology to ensure transparency, traceability, and immutability in vendor assessments and interactions. By leveraging blockchain, the framework enhances the integrity of vendor security audits, ensuring that vendor assessments remain up-to-date and tamperproof. This proposed framework leverages smart contracts to reduce human error while ensuring real-time monitoring of compliance and security controls. By evaluating critical security controls-such as data encryption, access control mechanisms, multi-factor authentication, and zero-trust architecture-this approach strengthens an organization's defense against emerging cyber threats. Additionally, continuous monitoring enabled by blockchain ensures the immutability and transparency of vendor compliance processes. In this paper, a case study on iHealth's transition to AWS Cloud demonstrates the practical implementation of the framework, showing a significant reduction in vulnerabilities and marked improvement in incident response times. Through the adoption of this blockchain-enabled approach, organizations can mitigate vendor risks, streamline compliance, and enhance their overall security posture.","sentences":["In an era of heightened digital interconnectedness, businesses increasingly rely on third-party vendors to enhance their operational capabilities.","However, this growing dependency introduces significant security risks, making it crucial to develop a robust framework to mitigate potential vulnerabilities.","This paper proposes a comprehensive secure framework for managing third-party vendor risk, integrating blockchain technology to ensure transparency, traceability, and immutability in vendor assessments and interactions.","By leveraging blockchain, the framework enhances the integrity of vendor security audits, ensuring that vendor assessments remain up-to-date and tamperproof.","This proposed framework leverages smart contracts to reduce human error while ensuring real-time monitoring of compliance and security controls.","By evaluating critical security controls-such as data encryption, access control mechanisms, multi-factor authentication, and zero-trust architecture-this approach strengthens an organization's defense against emerging cyber threats.","Additionally, continuous monitoring enabled by blockchain ensures the immutability and transparency of vendor compliance processes.","In this paper, a case study on iHealth's transition to AWS Cloud demonstrates the practical implementation of the framework, showing a significant reduction in vulnerabilities and marked improvement in incident response times.","Through the adoption of this blockchain-enabled approach, organizations can mitigate vendor risks, streamline compliance, and enhance their overall security posture."],"url":"http://arxiv.org/abs/2411.13447v1"}
{"created":"2024-11-20 16:29:57","title":"A Case Study of API Design for Interoperability and Security of the Internet of Things","abstract":"Heterogeneous distributed systems, including the Internet of Things (IoT) or distributed cyber-physical systems (CPS), often suffer a lack of interoperability and security, which hinders the wider deployment of such systems. Specifically, the different levels of security requirements and the heterogeneity in terms of communication models, for instance, point-to-point vs. publish-subscribe, are the example challenges of IoT and distributed CPS consisting of heterogeneous devices and applications. In this paper, we propose a working application programming interface (API) and runtime to enhance interoperability and security while addressing the challenges that stem from the heterogeneity in the IoT and distributed CPS. In our case study, we design and implement our application programming interface (API) design approach using open-source software, and with our working implementation, we evaluate the effectiveness of our proposed approach. Our experimental results suggest that our approach can achieve both interoperability and security in the IoT and distributed CPS with a reasonably small overhead and better-managed software.","sentences":["Heterogeneous distributed systems, including the Internet of Things (IoT) or distributed cyber-physical systems (CPS), often suffer a lack of interoperability and security, which hinders the wider deployment of such systems.","Specifically, the different levels of security requirements and the heterogeneity in terms of communication models, for instance, point-to-point vs. publish-subscribe, are the example challenges of IoT and distributed CPS consisting of heterogeneous devices and applications.","In this paper, we propose a working application programming interface (API) and runtime to enhance interoperability and security while addressing the challenges that stem from the heterogeneity in the IoT and distributed CPS.","In our case study, we design and implement our application programming interface (API) design approach using open-source software, and with our working implementation, we evaluate the effectiveness of our proposed approach.","Our experimental results suggest that our approach can achieve both interoperability and security in the IoT and distributed CPS with a reasonably small overhead and better-managed software."],"url":"http://arxiv.org/abs/2411.13441v1"}
{"created":"2024-11-20 16:29:40","title":"Eco-Friendly 0G Networks: Unlocking the Power of Backscatter Communications for a Greener Future","abstract":"Backscatter Communication (BackCom) technology has emerged as a promising paradigm for the Green Internet of Things (IoT) ecosystem, offering advantages such as low power consumption, cost-effectiveness, and ease of deployment. While traditional BackCom systems, such as RFID technology, have found widespread applications, the advent of ambient backscatter presents new opportunities for expanding applications and enhancing capabilities. Moreover, ongoing standardization efforts are actively focusing on BackCom technologies, positioning them as a potential solution to meet the near-zero power consumption and massive connectivity requirements of next-generation wireless systems. 0G networks have the potential to provide advanced solutions by leveraging BackCom technology to deliver ultra-low-power, ubiquitous connectivity for the expanding IoT ecosystem, supporting billions of devices with minimal energy consumption. This paper investigates the integration of BackCom and 0G networks to enhance the capabilities of traditional BackCom systems and enable Green IoT. We conduct an in-depth analysis of BackCom-enabled 0G networks, exploring their architecture and operational objectives, and also explore the Waste Factor (WF) metric for evaluating energy efficiency and minimizing energy waste within integrated systems. By examining both structural and operational aspects, we demonstrate how this synergy enhances the performance, scalability, and sustainability of next-generation wireless networks. Moreover, we highlight possible applications, open challenges, and future directions, offering valuable insights for guiding future research and practical implementations aimed at achieving large-scale, sustainable IoT deployments.","sentences":["Backscatter Communication (BackCom) technology has emerged as a promising paradigm for the Green Internet of Things (IoT) ecosystem, offering advantages such as low power consumption, cost-effectiveness, and ease of deployment.","While traditional BackCom systems, such as RFID technology, have found widespread applications, the advent of ambient backscatter presents new opportunities for expanding applications and enhancing capabilities.","Moreover, ongoing standardization efforts are actively focusing on BackCom technologies, positioning them as a potential solution to meet the near-zero power consumption and massive connectivity requirements of next-generation wireless systems.","0G networks have the potential to provide advanced solutions by leveraging BackCom technology to deliver ultra-low-power, ubiquitous connectivity for the expanding IoT ecosystem, supporting billions of devices with minimal energy consumption.","This paper investigates the integration of BackCom and 0G networks to enhance the capabilities of traditional BackCom systems and enable Green IoT. We conduct an in-depth analysis of BackCom-enabled 0G networks, exploring their architecture and operational objectives, and also explore the Waste Factor (WF) metric for evaluating energy efficiency and minimizing energy waste within integrated systems.","By examining both structural and operational aspects, we demonstrate how this synergy enhances the performance, scalability, and sustainability of next-generation wireless networks.","Moreover, we highlight possible applications, open challenges, and future directions, offering valuable insights for guiding future research and practical implementations aimed at achieving large-scale, sustainable IoT deployments."],"url":"http://arxiv.org/abs/2411.13440v1"}
{"created":"2024-11-20 16:26:51","title":"Robust Monocular Visual Odometry using Curriculum Learning","abstract":"Curriculum Learning (CL), drawing inspiration from natural learning patterns observed in humans and animals, employs a systematic approach of gradually introducing increasingly complex training data during model development. Our work applies innovative CL methodologies to address the challenging geometric problem of monocular Visual Odometry (VO) estimation, which is essential for robot navigation in constrained environments. The primary objective of our research is to push the boundaries of current state-of-the-art (SOTA) benchmarks in monocular VO by investigating various curriculum learning strategies. We enhance the end-to-end Deep-Patch-Visual Odometry (DPVO) framework through the integration of novel CL approaches, with the goal of developing more resilient models capable of maintaining high performance across challenging environments and complex motion scenarios. Our research encompasses several distinctive CL strategies. We develop methods to evaluate sample difficulty based on trajectory motion characteristics, implement sophisticated adaptive scheduling through self-paced weighted loss mechanisms, and utilize reinforcement learning agents for dynamic adjustment of training emphasis. Through comprehensive evaluation on the real-world TartanAir dataset, our Curriculum Learning-based Deep-Patch-Visual Odometry (CL-DPVO) demonstrates superior performance compared to existing SOTA methods, including both feature-based and learning-based VO approaches. The results validate the effectiveness of integrating curriculum learning principles into visual odometry systems.","sentences":["Curriculum Learning (CL), drawing inspiration from natural learning patterns observed in humans and animals, employs a systematic approach of gradually introducing increasingly complex training data during model development.","Our work applies innovative CL methodologies to address the challenging geometric problem of monocular Visual Odometry (VO) estimation, which is essential for robot navigation in constrained environments.","The primary objective of our research is to push the boundaries of current state-of-the-art (SOTA) benchmarks in monocular VO by investigating various curriculum learning strategies.","We enhance the end-to-end Deep-Patch-Visual Odometry (DPVO) framework through the integration of novel CL approaches, with the goal of developing more resilient models capable of maintaining high performance across challenging environments and complex motion scenarios.","Our research encompasses several distinctive CL strategies.","We develop methods to evaluate sample difficulty based on trajectory motion characteristics, implement sophisticated adaptive scheduling through self-paced weighted loss mechanisms, and utilize reinforcement learning agents for dynamic adjustment of training emphasis.","Through comprehensive evaluation on the real-world TartanAir dataset, our Curriculum Learning-based Deep-Patch-Visual Odometry (CL-DPVO) demonstrates superior performance compared to existing SOTA methods, including both feature-based and learning-based VO approaches.","The results validate the effectiveness of integrating curriculum learning principles into visual odometry systems."],"url":"http://arxiv.org/abs/2411.13438v1"}
{"created":"2024-11-20 16:11:20","title":"SynEHRgy: Synthesizing Mixed-Type Structured Electronic Health Records using Decoder-Only Transformers","abstract":"Generating synthetic Electronic Health Records (EHRs) offers significant potential for data augmentation, privacy-preserving data sharing, and improving machine learning model training. We propose a novel tokenization strategy tailored for structured EHR data, which encompasses diverse data types such as covariates, ICD codes, and irregularly sampled time series. Using a GPT-like decoder-only transformer model, we demonstrate the generation of high-quality synthetic EHRs. Our approach is evaluated using the MIMIC-III dataset, and we benchmark the fidelity, utility, and privacy of the generated data against state-of-the-art models.","sentences":["Generating synthetic Electronic Health Records (EHRs) offers significant potential for data augmentation, privacy-preserving data sharing, and improving machine learning model training.","We propose a novel tokenization strategy tailored for structured EHR data, which encompasses diverse data types such as covariates, ICD codes, and irregularly sampled time series.","Using a GPT-like decoder-only transformer model, we demonstrate the generation of high-quality synthetic EHRs.","Our approach is evaluated using the MIMIC-III dataset, and we benchmark the fidelity, utility, and privacy of the generated data against state-of-the-art models."],"url":"http://arxiv.org/abs/2411.13428v1"}
{"created":"2024-11-20 16:09:22","title":"WaterPark: A Robustness Assessment of Language Model Watermarking","abstract":"To mitigate the misuse of large language models (LLMs), such as disinformation, automated phishing, and academic cheating, there is a pressing need for the capability of identifying LLM-generated texts. Watermarking emerges as one promising solution: it plants statistical signals into LLMs' generative processes and subsequently verifies whether LLMs produce given texts. Various watermarking methods (``watermarkers'') have been proposed; yet, due to the lack of unified evaluation platforms, many critical questions remain under-explored: i) What are the strengths/limitations of various watermarkers, especially their attack robustness? ii) How do various design choices impact their robustness? iii) How to optimally operate watermarkers in adversarial environments?   To fill this gap, we systematize existing LLM watermarkers and watermark removal attacks, mapping out their design spaces. We then develop WaterPark, a unified platform that integrates 10 state-of-the-art watermarkers and 12 representative attacks. More importantly, leveraging WaterPark, we conduct a comprehensive assessment of existing watermarkers, unveiling the impact of various design choices on their attack robustness. For instance, a watermarker's resilience to increasingly intensive attacks hinges on its context dependency. We further explore the best practices to operate watermarkers in adversarial environments. For instance, using a generic detector alongside a watermark-specific detector improves the security of vulnerable watermarkers. We believe our study sheds light on current LLM watermarking techniques while WaterPark serves as a valuable testbed to facilitate future research.","sentences":["To mitigate the misuse of large language models (LLMs), such as disinformation, automated phishing, and academic cheating, there is a pressing need for the capability of identifying LLM-generated texts.","Watermarking emerges as one promising solution: it plants statistical signals into LLMs' generative processes and subsequently verifies whether LLMs produce given texts.","Various watermarking methods (``watermarkers'') have been proposed; yet, due to the lack of unified evaluation platforms, many critical questions remain under-explored: i) What are the strengths/limitations of various watermarkers, especially their attack robustness?","ii) How do various design choices impact their robustness?","iii) How to optimally operate watermarkers in adversarial environments?   ","To fill this gap, we systematize existing LLM watermarkers and watermark removal attacks, mapping out their design spaces.","We then develop WaterPark, a unified platform that integrates 10 state-of-the-art watermarkers and 12 representative attacks.","More importantly, leveraging WaterPark, we conduct a comprehensive assessment of existing watermarkers, unveiling the impact of various design choices on their attack robustness.","For instance, a watermarker's resilience to increasingly intensive attacks hinges on its context dependency.","We further explore the best practices to operate watermarkers in adversarial environments.","For instance, using a generic detector alongside a watermark-specific detector improves the security of vulnerable watermarkers.","We believe our study sheds light on current LLM watermarking techniques while WaterPark serves as a valuable testbed to facilitate future research."],"url":"http://arxiv.org/abs/2411.13425v1"}
{"created":"2024-11-20 16:09:16","title":"CAFE A Novel Code switching Dataset for Algerian Dialect French and English","abstract":"The paper introduces and publicly releases (Data download link available after acceptance) CAFE -- the first Code-switching dataset between Algerian dialect, French, and english languages. The CAFE speech data is unique for (a) its spontaneous speaking style in vivo human-human conversation capturing phenomena like code-switching and overlapping speech, (b) addresses distinct linguistic challenges in North African Arabic dialect; (c) the CAFE captures dialectal variations from various parts of Algeria within different sociolinguistic contexts. CAFE data contains approximately 37 hours of speech, with a subset, CAFE-small, of 2 hours and 36 minutes released with manual human annotation including speech segmentation, transcription, explicit annotation of code-switching points, overlapping speech, and other events such as noises, and laughter among others. The rest approximately 34.58 hours contain pseudo label transcriptions. In addition to the data release, the paper also highlighted the challenges of using state-of-the-art Automatic Speech Recognition (ASR) models such as Whisper large-v2,3 and PromptingWhisper to handle such content. Following, we benchmark CAFE data with the aforementioned Whisper models and show how well-designed data processing pipelines and advanced decoding techniques can improve the ASR performance in terms of Mixed Error Rate (MER) of 0.310, Character Error Rate (CER) of 0.329 and Word Error Rate (WER) of 0.538.","sentences":["The paper introduces and publicly releases (Data download link available after acceptance) CAFE -- the first Code-switching dataset between Algerian dialect, French, and english languages.","The CAFE speech data is unique for (a) its spontaneous speaking style in vivo human-human conversation capturing phenomena like code-switching and overlapping speech, (b) addresses distinct linguistic challenges in North African Arabic dialect; (c) the CAFE captures dialectal variations from various parts of Algeria within different sociolinguistic contexts.","CAFE data contains approximately 37 hours of speech, with a subset, CAFE-small, of 2 hours and 36 minutes released with manual human annotation including speech segmentation, transcription, explicit annotation of code-switching points, overlapping speech, and other events such as noises, and laughter among others.","The rest approximately 34.58 hours contain pseudo label transcriptions.","In addition to the data release, the paper also highlighted the challenges of using state-of-the-art Automatic Speech Recognition (ASR) models such as Whisper large-v2,3 and PromptingWhisper to handle such content.","Following, we benchmark CAFE data with the aforementioned Whisper models and show how well-designed data processing pipelines and advanced decoding techniques can improve the ASR performance in terms of Mixed Error Rate (MER) of 0.310, Character Error Rate (CER) of 0.329 and Word Error Rate (WER) of 0.538."],"url":"http://arxiv.org/abs/2411.13424v1"}
{"created":"2024-11-20 16:08:20","title":"From Prompt Engineering to Prompt Craft","abstract":"This pictorial presents an ongoing research programme comprising three practice-based Design Research projects conducted through 2024, exploring the affordances of diffusion-based AI image generation systems, specifically Stable Diffusion. The research employs tangible and embodied interactions to investigate emerging qualitative aspects of generative AI, including uncertainty and materiality. Our approach leverages the flexibility and adaptability of Design Research to navigate the rapidly evolving field of generative AI. The pictorial proposes the notion of prompt craft as a productive reframing of prompt engineering. This is comprised of two contributions: (1) reflections on the notion of materiality for diffusion-based generative AI and a proposed method for a craft-like navigation of the latent space within generative AI models and (2) discussing interaction design strategies for designing user interfaces informed by these affordances. The outcomes are presented as strong concepts or intermediate knowledge, applicable to various situations and domains.","sentences":["This pictorial presents an ongoing research programme comprising three practice-based Design Research projects conducted through 2024, exploring the affordances of diffusion-based AI image generation systems, specifically Stable Diffusion.","The research employs tangible and embodied interactions to investigate emerging qualitative aspects of generative AI, including uncertainty and materiality.","Our approach leverages the flexibility and adaptability of Design Research to navigate the rapidly evolving field of generative AI.","The pictorial proposes the notion of prompt craft as a productive reframing of prompt engineering.","This is comprised of two contributions: (1) reflections on the notion of materiality for diffusion-based generative AI and a proposed method for a craft-like navigation of the latent space within generative AI models and (2) discussing interaction design strategies for designing user interfaces informed by these affordances.","The outcomes are presented as strong concepts or intermediate knowledge, applicable to various situations and domains."],"url":"http://arxiv.org/abs/2411.13422v1"}
{"created":"2024-11-20 16:06:28","title":"Heuristically Adaptive Diffusion-Model Evolutionary Strategy","abstract":"Diffusion Models represent a significant advancement in generative modeling, employing a dual-phase process that first degrades domain-specific information via Gaussian noise and restores it through a trainable model. This framework enables pure noise-to-data generation and modular reconstruction of, images or videos. Concurrently, evolutionary algorithms employ optimization methods inspired by biological principles to refine sets of numerical parameters encoding potential solutions to rugged objective functions. Our research reveals a fundamental connection between diffusion models and evolutionary algorithms through their shared underlying generative mechanisms: both methods generate high-quality samples via iterative refinement on random initial distributions. By employing deep learning-based diffusion models as generative models across diverse evolutionary tasks and iteratively refining diffusion models with heuristically acquired databases, we can iteratively sample potentially better-adapted offspring parameters, integrating them into successive generations of the diffusion model. This approach achieves efficient convergence toward high-fitness parameters while maintaining explorative diversity. Diffusion models introduce enhanced memory capabilities into evolutionary algorithms, retaining historical information across generations and leveraging subtle data correlations to generate refined samples. We elevate evolutionary algorithms from procedures with shallow heuristics to frameworks with deep memory. By deploying classifier-free guidance for conditional sampling at the parameter level, we achieve precise control over evolutionary search dynamics to further specific genotypical, phenotypical, or population-wide traits. Our framework marks a major heuristic and algorithmic transition, offering increased flexibility, precision, and control in evolutionary optimization processes.","sentences":["Diffusion Models represent a significant advancement in generative modeling, employing a dual-phase process that first degrades domain-specific information via Gaussian noise and restores it through a trainable model.","This framework enables pure noise-to-data generation and modular reconstruction of, images or videos.","Concurrently, evolutionary algorithms employ optimization methods inspired by biological principles to refine sets of numerical parameters encoding potential solutions to rugged objective functions.","Our research reveals a fundamental connection between diffusion models and evolutionary algorithms through their shared underlying generative mechanisms: both methods generate high-quality samples via iterative refinement on random initial distributions.","By employing deep learning-based diffusion models as generative models across diverse evolutionary tasks and iteratively refining diffusion models with heuristically acquired databases, we can iteratively sample potentially better-adapted offspring parameters, integrating them into successive generations of the diffusion model.","This approach achieves efficient convergence toward high-fitness parameters while maintaining explorative diversity.","Diffusion models introduce enhanced memory capabilities into evolutionary algorithms, retaining historical information across generations and leveraging subtle data correlations to generate refined samples.","We elevate evolutionary algorithms from procedures with shallow heuristics to frameworks with deep memory.","By deploying classifier-free guidance for conditional sampling at the parameter level, we achieve precise control over evolutionary search dynamics to further specific genotypical, phenotypical, or population-wide traits.","Our framework marks a major heuristic and algorithmic transition, offering increased flexibility, precision, and control in evolutionary optimization processes."],"url":"http://arxiv.org/abs/2411.13420v1"}
{"created":"2024-11-20 16:02:14","title":"Unleashing the Power of Large Language Models for Group POI Recommendations","abstract":"Group Point-of-Interest (POI) recommendations aim to predict the next POI that satisfies the diverse preferences of a group of users. This task is more challenging than traditional individual POI recommendations due to complex group decision-making and extremely sparse group-level check-in data. Existing methods for group POI recommendations primarily rely on single ID-based features from check-in data, capturing only statistical correlations and failing to fully utilize the rich semantic information contained in the check-ins, resulting in suboptimal performance. To this end, we propose a framework that unleashes the power of the Large Language Model (LLM) for context-aware group POI recommendations (LLMGPR). Our approach first introduces POI tokens alongside the original word tokens of the LLM, which are initialized by applying the LLM to the rich information of each POI. We then propose a novel sequencing adapter guided by Quantized Low-Rank Adaptation (QLORA) to modify the LLM. The enhanced LLM can learn sequence representations by combining semantic-enhanced POI tokens and rich contextual information including positional encodings and spatio-temporal differences. This approach can be adapted for learning either group or user representations depending on the sequence type. Furthermore, we enhance group representations by aggregating individual member representations with another QLORA-based aggregation adapter and introducing a self-supervised learning task that predicts the purpose of check-in sequences, alleviating the data sparsity issue. Our experimental results demonstrate that LLMGPR outperforms existing methods, effectively addressing group-level data sparsity and providing superior recommendations.","sentences":["Group Point-of-Interest (POI) recommendations aim to predict the next POI that satisfies the diverse preferences of a group of users.","This task is more challenging than traditional individual POI recommendations due to complex group decision-making and extremely sparse group-level check-in data.","Existing methods for group POI recommendations primarily rely on single ID-based features from check-in data, capturing only statistical correlations and failing to fully utilize the rich semantic information contained in the check-ins, resulting in suboptimal performance.","To this end, we propose a framework that unleashes the power of the Large Language Model (LLM) for context-aware group POI recommendations (LLMGPR).","Our approach first introduces POI tokens alongside the original word tokens of the LLM, which are initialized by applying the LLM to the rich information of each POI.","We then propose a novel sequencing adapter guided by Quantized Low-Rank Adaptation (QLORA) to modify the LLM.","The enhanced LLM can learn sequence representations by combining semantic-enhanced POI tokens and rich contextual information including positional encodings and spatio-temporal differences.","This approach can be adapted for learning either group or user representations depending on the sequence type.","Furthermore, we enhance group representations by aggregating individual member representations with another QLORA-based aggregation adapter and introducing a self-supervised learning task that predicts the purpose of check-in sequences, alleviating the data sparsity issue.","Our experimental results demonstrate that LLMGPR outperforms existing methods, effectively addressing group-level data sparsity and providing superior recommendations."],"url":"http://arxiv.org/abs/2411.13415v1"}
{"created":"2024-11-20 15:52:27","title":"Complete Test Suites for Automata in Monoidal Closed Categories","abstract":"Conformance testing of automata is about checking the equivalence of a known specification and a black-box implementation. An important notion in conformance testing is that of a complete test suite, which guarantees that if an implementation satisfying certain conditions passes all tests, then it is equivalent to the specification.   We introduce a framework for proving completeness of test suites at the general level of automata in monoidal closed categories. Moreover, we provide a generalization of a classical conformance testing technique, the W-method. We demonstrate the applicability of our results by recovering the W-method for deterministic finite automata, Moore machines, and Mealy machines, and by deriving new instances of complete test suites for weighted automata and deterministic nominal automata.","sentences":["Conformance testing of automata is about checking the equivalence of a known specification and a black-box implementation.","An important notion in conformance testing is that of a complete test suite, which guarantees that if an implementation satisfying certain conditions passes all tests, then it is equivalent to the specification.   ","We introduce a framework for proving completeness of test suites at the general level of automata in monoidal closed categories.","Moreover, we provide a generalization of a classical conformance testing technique, the W-method.","We demonstrate the applicability of our results by recovering the W-method for deterministic finite automata, Moore machines, and Mealy machines, and by deriving new instances of complete test suites for weighted automata and deterministic nominal automata."],"url":"http://arxiv.org/abs/2411.13412v1"}
{"created":"2024-11-20 15:52:03","title":"A Survey On Enhancing Reinforcement Learning in Complex Environments: Insights from Human and LLM Feedback","abstract":"Reinforcement learning (RL) is one of the active fields in machine learning, demonstrating remarkable potential in tackling real-world challenges. Despite its promising prospects, this methodology has encountered with issues and challenges, hindering it from achieving the best performance. In particular, these approaches lack decent performance when navigating environments and solving tasks with large observation space, often resulting in sample-inefficiency and prolonged learning times. This issue, commonly referred to as the curse of dimensionality, complicates decision-making for RL agents, necessitating a careful balance between attention and decision-making. RL agents, when augmented with human or large language models' (LLMs) feedback, may exhibit resilience and adaptability, leading to enhanced performance and accelerated learning. Such feedback, conveyed through various modalities or granularities including natural language, serves as a guide for RL agents, aiding them in discerning relevant environmental cues and optimizing decision-making processes. In this survey paper, we mainly focus on problems of two-folds: firstly, we focus on humans or an LLMs assistance, investigating the ways in which these entities may collaborate with the RL agent in order to foster optimal behavior and expedite learning; secondly, we delve into the research papers dedicated to addressing the intricacies of environments characterized by large observation space.","sentences":["Reinforcement learning (RL) is one of the active fields in machine learning, demonstrating remarkable potential in tackling real-world challenges.","Despite its promising prospects, this methodology has encountered with issues and challenges, hindering it from achieving the best performance.","In particular, these approaches lack decent performance when navigating environments and solving tasks with large observation space, often resulting in sample-inefficiency and prolonged learning times.","This issue, commonly referred to as the curse of dimensionality, complicates decision-making for RL agents, necessitating a careful balance between attention and decision-making.","RL agents, when augmented with human or large language models' (LLMs) feedback, may exhibit resilience and adaptability, leading to enhanced performance and accelerated learning.","Such feedback, conveyed through various modalities or granularities including natural language, serves as a guide for RL agents, aiding them in discerning relevant environmental cues and optimizing decision-making processes.","In this survey paper, we mainly focus on problems of two-folds: firstly, we focus on humans or an LLMs assistance, investigating the ways in which these entities may collaborate with the RL agent in order to foster optimal behavior and expedite learning; secondly, we delve into the research papers dedicated to addressing the intricacies of environments characterized by large observation space."],"url":"http://arxiv.org/abs/2411.13410v1"}
{"created":"2024-11-20 15:48:21","title":"Unification of Balti and trans-border sister dialects in the essence of LLMs and AI Technology","abstract":"The language called Balti belongs to the Sino-Tibetan, specifically the Tibeto-Burman language family. It is understood with variations, across populations in India, China, Pakistan, Nepal, Tibet, Burma, and Bhutan, influenced by local cultures and producing various dialects. Considering the diverse cultural, socio-political, religious, and geographical impacts, it is important to step forward unifying the dialects, the basis of common root, lexica, and phonological perspectives, is vital. In the era of globalization and the increasingly frequent developments in AI technology, understanding the diversity and the efforts of dialect unification is important to understanding commonalities and shortening the gaps impacted by unavoidable circumstances. This article analyzes and examines how artificial intelligence AI in the essence of Large Language Models LLMs, can assist in analyzing, documenting, and standardizing the endangered Balti Language, based on the efforts made in different dialects so far.","sentences":["The language called Balti belongs to the Sino-Tibetan, specifically the Tibeto-Burman language family.","It is understood with variations, across populations in India, China, Pakistan, Nepal, Tibet, Burma, and Bhutan, influenced by local cultures and producing various dialects.","Considering the diverse cultural, socio-political, religious, and geographical impacts, it is important to step forward unifying the dialects, the basis of common root, lexica, and phonological perspectives, is vital.","In the era of globalization and the increasingly frequent developments in AI technology, understanding the diversity and the efforts of dialect unification is important to understanding commonalities and shortening the gaps impacted by unavoidable circumstances.","This article analyzes and examines how artificial intelligence AI in the essence of Large Language Models LLMs, can assist in analyzing, documenting, and standardizing the endangered Balti Language, based on the efforts made in different dialects so far."],"url":"http://arxiv.org/abs/2411.13409v1"}
{"created":"2024-11-20 15:46:48","title":"Transformer-Based Contextualized Language Models Joint with Neural Networks for Natural Language Inference in Vietnamese","abstract":"Natural Language Inference (NLI) is a task within Natural Language Processing (NLP) that holds value for various AI applications. However, there have been limited studies on Natural Language Inference in Vietnamese that explore the concept of joint models. Therefore, we conducted experiments using various combinations of contextualized language models (CLM) and neural networks. We use CLM to create contextualized work presentations and use Neural Networks for classification. Furthermore, we have evaluated the strengths and weaknesses of each joint model and identified the model failure points in the Vietnamese context. The highest F1 score in this experiment, up to 82.78\\% in the benchmark dataset (ViNLI). By conducting experiments with various models, the most considerable size of the CLM is XLM-R (355M). That combination has consistently demonstrated superior performance compared to fine-tuning strong pre-trained language models like PhoBERT (+6.58\\%), mBERT (+19.08\\%), and XLM-R (+0.94\\%) in terms of F1-score. This article aims to introduce a novel approach or model that attains improved performance for Vietnamese NLI. Overall, we find that the joint approach of CLM and neural networks is simple yet capable of achieving high-quality performance, which makes it suitable for applications that require efficient resource utilization.","sentences":["Natural Language Inference (NLI) is a task within Natural Language Processing (NLP) that holds value for various AI applications.","However, there have been limited studies on Natural Language Inference in Vietnamese that explore the concept of joint models.","Therefore, we conducted experiments using various combinations of contextualized language models (CLM) and neural networks.","We use CLM to create contextualized work presentations and use Neural Networks for classification.","Furthermore, we have evaluated the strengths and weaknesses of each joint model and identified the model failure points in the Vietnamese context.","The highest F1 score in this experiment, up to 82.78\\% in the benchmark dataset (ViNLI).","By conducting experiments with various models, the most considerable size of the CLM is XLM-R (355M).","That combination has consistently demonstrated superior performance compared to fine-tuning strong pre-trained language models like PhoBERT (+6.58\\%), mBERT (+19.08\\%), and XLM-R (+0.94\\%) in terms of F1-score.","This article aims to introduce a novel approach or model that attains improved performance for Vietnamese NLI.","Overall, we find that the joint approach of CLM and neural networks is simple yet capable of achieving high-quality performance, which makes it suitable for applications that require efficient resource utilization."],"url":"http://arxiv.org/abs/2411.13407v1"}
{"created":"2024-11-20 15:45:08","title":"On the Way to LLM Personalization: Learning to Remember User Conversations","abstract":"Large Language Models (LLMs) have quickly become an invaluable assistant for a variety of tasks. However, their effectiveness is constrained by their ability to tailor responses to human preferences and behaviors via personalization. Prior work in LLM personalization has largely focused on style transfer or incorporating small factoids about the user, as knowledge injection remains an open challenge. In this paper, we explore injecting knowledge of prior conversations into LLMs to enable future work on less redundant, personalized conversations. We identify two real-world constraints: (1) conversations are sequential in time and must be treated as such during training, and (2) per-user personalization is only viable in parameter-efficient settings. To this aim, we propose PLUM, a pipeline performing data augmentation for up-sampling conversations as question-answer pairs, that are then used to finetune a low-rank adaptation adapter with a weighted cross entropy loss. Even in this first exploration of the problem, we perform competitively with baselines such as RAG, attaining an accuracy of 81.5% across 100 conversations.","sentences":["Large Language Models (LLMs) have quickly become an invaluable assistant for a variety of tasks.","However, their effectiveness is constrained by their ability to tailor responses to human preferences and behaviors via personalization.","Prior work in LLM personalization has largely focused on style transfer or incorporating small factoids about the user, as knowledge injection remains an open challenge.","In this paper, we explore injecting knowledge of prior conversations into LLMs to enable future work on less redundant, personalized conversations.","We identify two real-world constraints: (1) conversations are sequential in time and must be treated as such during training, and (2) per-user personalization is only viable in parameter-efficient settings.","To this aim, we propose PLUM, a pipeline performing data augmentation for up-sampling conversations as question-answer pairs, that are then used to finetune a low-rank adaptation adapter with a weighted cross entropy loss.","Even in this first exploration of the problem, we perform competitively with baselines such as RAG, attaining an accuracy of 81.5% across 100 conversations."],"url":"http://arxiv.org/abs/2411.13405v1"}
{"created":"2024-11-20 15:38:33","title":"Executable QR codes with Machine Learning for Industrial Applications","abstract":"Executable QR codes, also known as eQR codes or just sQRy, are a special kind of QR codes that embed programs conceived to run on mobile devices like smartphones. Since the program is directly encoded in binary form within the QR code, it can be executed even when the reading device is not provided with Internet access. The applications of this technology are manifold, and range from smart user guides to advisory systems. The first programming language made available for eQR is QRtree, which enables the implementation of decision trees aimed, for example, at guiding the user in operating/maintaining a complex machinery or for reaching a specific location.   In this work, an additional language is proposed, we term QRind, which was specifically devised for Industry. It permits to integrate distinct computational blocks into the QR code, e.g., machine learning models to enable predictive maintenance and algorithms to ease machinery usage. QRind permits the Industry 4.0/5.0 paradigms to be implemented, in part, also in those cases where Internet is unavailable.","sentences":["Executable QR codes, also known as eQR codes or just sQRy, are a special kind of QR codes that embed programs conceived to run on mobile devices like smartphones.","Since the program is directly encoded in binary form within the QR code, it can be executed even when the reading device is not provided with Internet access.","The applications of this technology are manifold, and range from smart user guides to advisory systems.","The first programming language made available for eQR is QRtree, which enables the implementation of decision trees aimed, for example, at guiding the user in operating/maintaining a complex machinery or for reaching a specific location.   ","In this work, an additional language is proposed, we term QRind, which was specifically devised for Industry.","It permits to integrate distinct computational blocks into the QR code, e.g., machine learning models to enable predictive maintenance and algorithms to ease machinery usage.","QRind permits the Industry 4.0/5.0 paradigms to be implemented, in part, also in those cases where Internet is unavailable."],"url":"http://arxiv.org/abs/2411.13400v1"}
{"created":"2024-11-20 15:23:40","title":"UKFin+: A Research Agenda for Financial Services","abstract":"This document presents a research agenda for financial services as a deliverable of UKFin+, a Network Plus grant funded by the Engineering and Physical Sciences Research Council. UKFin+ fosters research collaborations between academic and non-academic partners directed at tackling complex long-term challenges relevant to the UK's financial services sector. Confronting these challenges is crucial to promote the long-term health and international competitiveness of the UK's financial services industry. As one route to impact, UKFin+ includes dedicated funding streams for research collaborations between academic researchers and non-academic organisations.   The intended audience of this document includes researchers based in academia, academic funders, as well as practitioners based in industry, regulators, charities or NGOs. It is not intended to be comprehensive or exhaustive in scope but may provide applicants to UKFin+ funding streams and other funding bodies with inspiration for their proposals or at least an understanding of how their proposals align with the broader needs of the UK financial services industry.","sentences":["This document presents a research agenda for financial services as a deliverable of UKFin+, a Network Plus grant funded by the Engineering and Physical Sciences Research Council.","UKFin+ fosters research collaborations between academic and non-academic partners directed at tackling complex long-term challenges relevant to the UK's financial services sector.","Confronting these challenges is crucial to promote the long-term health and international competitiveness of the UK's financial services industry.","As one route to impact, UKFin+ includes dedicated funding streams for research collaborations between academic researchers and non-academic organisations.   ","The intended audience of this document includes researchers based in academia, academic funders, as well as practitioners based in industry, regulators, charities or NGOs.","It is not intended to be comprehensive or exhaustive in scope but may provide applicants to UKFin+ funding streams and other funding bodies with inspiration for their proposals or at least an understanding of how their proposals align with the broader needs of the UK financial services industry."],"url":"http://arxiv.org/abs/2411.13389v1"}
{"created":"2024-11-20 14:59:47","title":"Quantum-Brain: Quantum-Inspired Neural Network Approach to Vision-Brain Understanding","abstract":"Vision-brain understanding aims to extract semantic information about brain signals from human perceptions. Existing deep learning methods for vision-brain understanding are usually introduced in a traditional learning paradigm missing the ability to learn the connectivities between brain regions. Meanwhile, the quantum computing theory offers a new paradigm for designing deep learning models. Motivated by the connectivities in the brain signals and the entanglement properties in quantum computing, we propose a novel Quantum-Brain approach, a quantum-inspired neural network, to tackle the vision-brain understanding problem. To compute the connectivity between areas in brain signals, we introduce a new Quantum-Inspired Voxel-Controlling module to learn the impact of a brain voxel on others represented in the Hilbert space. To effectively learn connectivity, a novel Phase-Shifting module is presented to calibrate the value of the brain signals. Finally, we introduce a new Measurement-like Projection module to present the connectivity information from the Hilbert space into the feature space. The proposed approach can learn to find the connectivities between fMRI voxels and enhance the semantic information obtained from human perceptions. Our experimental results on the Natural Scene Dataset benchmarks illustrate the effectiveness of the proposed method with Top-1 accuracies of 95.1% and 95.6% on image and brain retrieval tasks and an Inception score of 95.3% on fMRI-to-image reconstruction task. Our proposed quantum-inspired network brings a potential paradigm to solving the vision-brain problems via the quantum computing theory.","sentences":["Vision-brain understanding aims to extract semantic information about brain signals from human perceptions.","Existing deep learning methods for vision-brain understanding are usually introduced in a traditional learning paradigm missing the ability to learn the connectivities between brain regions.","Meanwhile, the quantum computing theory offers a new paradigm for designing deep learning models.","Motivated by the connectivities in the brain signals and the entanglement properties in quantum computing, we propose a novel Quantum-Brain approach, a quantum-inspired neural network, to tackle the vision-brain understanding problem.","To compute the connectivity between areas in brain signals, we introduce a new Quantum-Inspired Voxel-Controlling module to learn the impact of a brain voxel on others represented in the Hilbert space.","To effectively learn connectivity, a novel Phase-Shifting module is presented to calibrate the value of the brain signals.","Finally, we introduce a new Measurement-like Projection module to present the connectivity information from the Hilbert space into the feature space.","The proposed approach can learn to find the connectivities between fMRI voxels and enhance the semantic information obtained from human perceptions.","Our experimental results on the Natural Scene Dataset benchmarks illustrate the effectiveness of the proposed method with Top-1 accuracies of 95.1% and 95.6% on image and brain retrieval tasks and an Inception score of 95.3% on fMRI-to-image reconstruction task.","Our proposed quantum-inspired network brings a potential paradigm to solving the vision-brain problems via the quantum computing theory."],"url":"http://arxiv.org/abs/2411.13378v1"}
{"created":"2024-11-20 14:58:47","title":"Distributed weak independent sets in hypergraphs: Upper and lower bounds","abstract":"In this paper, we consider the problem of finding weak independent sets in a distributed network represented by a hypergraph. In this setting, each edge contains a set of r vertices rather than simply a pair, as in a standard graph. A k-weak independent set in a hypergraph is a set where no edge contains more than k vertices in the independent set. We focus two variations of this problem. First, we study the problem of finding k-weak maximal independent sets, k-weak independent sets where each vertex belongs to at least one edge with k vertices in the independent set. Second we introduce a weaker variant that we call (\\alpha, \\beta)-independent sets where the independent set is \\beta-weak, and each vertex belongs to at least one edge with at least \\alpha vertices in the independent set. Finally, we consider the problem of finding a (2, k)-ruling set on hypergraphs, i.e. independent sets where no vertex is a distance of more than k from the nearest member of the set.   Given a hypergraph H of rank r and maximum degree \\Delta, we provide a LLL formulation for finding an (\\alpha, \\beta)-independent set when (\\beta - \\alpha)^2 / (\\beta + \\alpha) \\geq 6 \\log(16 r \\Delta), an O(\\Delta r / (\\beta - \\alpha + 1) + \\log^* n) round deterministic algorithm finding an (\\alpha, \\beta)-independent set, and a O(\\Delta^2(r - k) \\log r + \\Delta \\log r \\log^* r + \\log^* n) round algorithm for finding a k-weak maximal independent set. Additionally, we provide zero round randomized algorithms for finding (\\alpha, \\beta) independent sets, when (\\beta - \\alpha)^2 / (\\beta + \\alpha) \\geq 6 c \\log n + 6 for some constant c, and finding an m-weak independent set for some m \\geq r / 2k where k is a given parameter. Finally, we provide lower bounds of \\Omega(\\Delta + \\log^* n) and \\Omega(r + \\log^* n) on the problems of finding a k-weak maximal independent sets for some values of k.","sentences":["In this paper, we consider the problem of finding weak independent sets in a distributed network represented by a hypergraph.","In this setting, each edge contains a set of r vertices rather than simply a pair, as in a standard graph.","A k-weak independent set in a hypergraph is a set where no edge contains more than k vertices in the independent set.","We focus two variations of this problem.","First, we study the problem of finding k-weak maximal independent sets, k-weak independent sets where each vertex belongs to at least one edge with k vertices in the independent set.","Second we introduce a weaker variant that we call (\\alpha, \\beta)-independent sets where the independent set is \\beta-weak, and each vertex belongs to at least one edge with at least \\alpha vertices in the independent set.","Finally, we consider the problem of finding a (2, k)-ruling set on hypergraphs, i.e. independent sets where no vertex is a distance of more than k from the nearest member of the set.   ","Given a hypergraph H of rank r and maximum degree \\Delta, we provide a LLL formulation for finding an (\\alpha, \\beta)-independent set when (\\beta - \\alpha)^2 / (\\beta + \\alpha) \\geq 6 \\log(16 r \\Delta), an O(\\Delta r / (\\beta - \\alpha + 1) + \\log^* n) round deterministic algorithm finding an (\\alpha, \\beta)-independent set, and a O(\\Delta^2(r - k) \\log r + \\Delta \\log r \\log^* r + \\log^* n) round algorithm for finding a k-weak maximal independent set.","Additionally, we provide zero round randomized algorithms for finding (\\alpha, \\beta) independent sets, when (\\beta - \\alpha)^2 / (\\beta + \\alpha) \\geq 6 c \\log n + 6 for some constant c, and finding an m-weak independent set for some m \\geq r / 2k where k is a given parameter.","Finally, we provide lower bounds of \\Omega(\\Delta + \\log^* n) and \\Omega(r + \\log^* n) on the problems of finding a k-weak maximal independent sets for some values of k."],"url":"http://arxiv.org/abs/2411.13377v1"}
{"created":"2024-11-20 14:58:32","title":"ODTE -- An ensemble of multi-class SVM-based oblique decision trees","abstract":"We propose ODTE, a new ensemble that uses oblique decision trees as base classifiers. Additionally, we introduce STree, the base algorithm for growing oblique decision trees, which leverages support vector machines to define hyperplanes within the decision nodes. We embed a multiclass strategy -- one-vs-one or one-vs-rest -- at the decision nodes, allowing the model to directly handle non-binary classification tasks without the need to cluster instances into two groups, as is common in other approaches from the literature. In each decision node, only the best-performing model SVM -- the one that minimizes an impurity measure for the n-ary classification -- is retained, even if the learned SVM addresses a binary classification subtask. An extensive experimental study involving 49 datasets and various state-of-the-art algorithms for oblique decision tree ensembles has been conducted. Our results show that ODTE ranks consistently above its competitors, achieving significant performance gains when hyperparameters are carefully tuned. Moreover, the oblique decision trees learned through STree are more compact than those produced by other algorithms evaluated in our experiments.","sentences":["We propose ODTE, a new ensemble that uses oblique decision trees as base classifiers.","Additionally, we introduce STree, the base algorithm for growing oblique decision trees, which leverages support vector machines to define hyperplanes within the decision nodes.","We embed a multiclass strategy -- one-vs-one or one-vs-rest -- at the decision nodes, allowing the model to directly handle non-binary classification tasks without the need to cluster instances into two groups, as is common in other approaches from the literature.","In each decision node, only the best-performing model SVM -- the one that minimizes an impurity measure for the n-ary classification -- is retained, even if the learned SVM addresses a binary classification subtask.","An extensive experimental study involving 49 datasets and various state-of-the-art algorithms for oblique decision tree ensembles has been conducted.","Our results show that ODTE ranks consistently above its competitors, achieving significant performance gains when hyperparameters are carefully tuned.","Moreover, the oblique decision trees learned through STree are more compact than those produced by other algorithms evaluated in our experiments."],"url":"http://arxiv.org/abs/2411.13376v1"}
{"created":"2024-11-20 14:54:31","title":"The weight hierarchy of decreasing norm-trace codes","abstract":"The Generalized Hamming weights and their relative version, which generalize the minimum distance of a linear code, are relevant to numerous applications, including coding on the wire-tap channel of type II, $t$-resilient functions, bounding the cardinality of the output in list decoding algorithms, ramp secret sharing schemes, and quantum error correction. The generalized Hamming weights have been determined for some families of codes, including Cartesian codes and Hermitian one-point codes. In this paper, we determine the generalized Hamming weights of decreasing norm-trace codes, which are linear codes defined by evaluating monomials that are closed under divisibility on the rational points of the extended norm-trace curve given by $x^{u} = y^{q^{s - 1}} + y^{q^{s - 2}} + \\cdots + y$ over the finite field of cardinality $q^s$, where $u$ is a positive divisor of $\\frac{q^s - 1}{q - 1}$. As a particular case, we obtain the weight hierarchy of one-point norm-trace codes and recover the result of Barbero and Munuera (2001) giving the weight hierarchy of one-point Hermitian codes. We also study the relative generalized Hamming weights for these codes and use them to construct impure quantum codes with excellent parameters.","sentences":["The Generalized Hamming weights and their relative version, which generalize the minimum distance of a linear code, are relevant to numerous applications, including coding on the wire-tap channel of type II, $t$-resilient functions, bounding the cardinality of the output in list decoding algorithms, ramp secret sharing schemes, and quantum error correction.","The generalized Hamming weights have been determined for some families of codes, including Cartesian codes and Hermitian one-point codes.","In this paper, we determine the generalized Hamming weights of decreasing norm-trace codes, which are linear codes defined by evaluating monomials that are closed under divisibility on the rational points of the extended norm-trace curve given by $x^{u} = y^{q^{s - 1}} + y^{q^{s - 2}} + \\cdots + y$ over the finite field of cardinality $q^s$, where $u$ is a positive divisor of $\\frac{q^s - 1}{q - 1}$. As a particular case, we obtain the weight hierarchy of one-point norm-trace codes and recover the result of Barbero and Munuera (2001) giving the weight hierarchy of one-point Hermitian codes.","We also study the relative generalized Hamming weights for these codes and use them to construct impure quantum codes with excellent parameters."],"url":"http://arxiv.org/abs/2411.13375v1"}
{"created":"2024-11-20 14:52:43","title":"On the structure of normalized models of circular-arc graphs -- Hsu's approach revisited","abstract":"Circular-arc graphs are the intersection graphs of arcs of a circle. The main result of this work describes the structure of all \\emph{normalized intersection models} of circular-arc graphs. Normalized models of a circular-arc graph reflect the neighborhood relation between its vertices and can be seen as its canonical representations; in particular, any intersection model can be made normalized by possibly extending some of its arcs. We~devise a data-structure, called \\emph{PQM-tree}, that maintains the set of all normalized models of a circular-arc graph. We show that the PQM-tree of a circular-arc graph can be computed in linear time. Finally, basing on PQM-trees, we provide a linear-time algorithm for the canonization and the isomorphism problem for circular-arc graphs.   We describe the structure of the normalized models of circular-arc graphs using an approach proposed by Hsu~[\\emph{SIAM J. Comput. 24(3), 411--439, (1995)}]. In the aforementioned work, Hsu claimed the construction of decomposition trees representing the set of all normalized intersection models of circular-arc graphs and an $\\mathcal{O}(nm)$ time isomorphism algorithm for this class of graphs. However, the counterexample given in~[\\emph{Discrete Math. Theor. Comput. Sci., 15(1), 157--182, 2013}] shows that Hsu's isomorphism algorithm is not incorrect. Also, in a companion paper we show that the decomposition trees proposed by Hsu are not constructed correctly; in particular, we showed that there are circular-arc graphs whose all normalized models do not follow the description given by Hsu.","sentences":["Circular-arc graphs are the intersection graphs of arcs of a circle.","The main result of this work describes the structure of all \\emph{normalized intersection models} of circular-arc graphs.","Normalized models of a circular-arc graph reflect the neighborhood relation between its vertices and can be seen as its canonical representations; in particular, any intersection model can be made normalized by possibly extending some of its arcs.","We~devise a data-structure, called \\emph{PQM-tree}, that maintains the set of all normalized models of a circular-arc graph.","We show that the PQM-tree of a circular-arc graph can be computed in linear time.","Finally, basing on PQM-trees, we provide a linear-time algorithm for the canonization and the isomorphism problem for circular-arc graphs.   ","We describe the structure of the normalized models of circular-arc graphs using an approach proposed by Hsu~[\\emph{SIAM J. Comput.","24(3), 411--439, (1995)}].","In the aforementioned work, Hsu claimed the construction of decomposition trees representing the set of all normalized intersection models of circular-arc graphs and an $\\mathcal{O}(nm)$ time isomorphism algorithm for this class of graphs.","However, the counterexample given in~[\\emph{Discrete Math.","Theor.","Comput.","Sci., 15(1), 157--182, 2013}] shows that Hsu's isomorphism algorithm is not incorrect.","Also, in a companion paper we show that the decomposition trees proposed by Hsu are not constructed correctly; in particular, we showed that there are circular-arc graphs whose all normalized models do not follow the description given by Hsu."],"url":"http://arxiv.org/abs/2411.13374v1"}
{"created":"2024-11-20 14:51:19","title":"REVISE: Robust Probabilistic Motion Planning in a Gaussian Random Field","abstract":"This paper presents Robust samplE-based coVarIance StEering (REVISE), a multi-query algorithm that generates robust belief roadmaps for dynamic systems navigating through spatially dependent disturbances modeled as a Gaussian random field. Our proposed method develops a novel robust sample-based covariance steering edge controller to safely steer a robot between state distributions, satisfying state constraints along the trajectory. Our proposed approach also incorporates an edge rewiring step into the belief roadmap construction process, which provably improves the coverage of the belief roadmap. When compared to state-of-the-art methods, REVISE improves median plan accuracy (as measured by Wasserstein distance between the actual and planned final state distribution) by 10x in multi-query planning and reduces median plan cost (as measured by the largest eigenvalue of the planned state covariance at the goal) by 2.5x in single-query planning for a 6DoF system. We will release our code at https://acl.mit.edu/REVISE/.","sentences":["This paper presents Robust samplE-based coVarIance StEering (REVISE), a multi-query algorithm that generates robust belief roadmaps for dynamic systems navigating through spatially dependent disturbances modeled as a Gaussian random field.","Our proposed method develops a novel robust sample-based covariance steering edge controller to safely steer a robot between state distributions, satisfying state constraints along the trajectory.","Our proposed approach also incorporates an edge rewiring step into the belief roadmap construction process, which provably improves the coverage of the belief roadmap.","When compared to state-of-the-art methods, REVISE improves median plan accuracy (as measured by Wasserstein distance between the actual and planned final state distribution) by 10x in multi-query planning and reduces median plan cost (as measured by the largest eigenvalue of the planned state covariance at the goal) by 2.5x in single-query planning for a 6DoF system.","We will release our code at https://acl.mit.edu/REVISE/."],"url":"http://arxiv.org/abs/2411.13369v1"}
{"created":"2024-11-20 14:42:53","title":"Predicting Wall Thickness Changes in Cold Forging Processes: An Integrated FEM and Neural Network approach","abstract":"This study presents a novel approach for predicting wall thickness changes in tubes during the nosing process. Specifically, we first provide a thorough analysis of nosing processes and the influencing parameters. We further set-up a Finite Element Method (FEM) simulation to better analyse the effects of varying process parameters. As however traditional FEM simulations, while accurate, are time-consuming and computationally intensive, which renders them inapplicable for real-time application, we present a novel modeling framework based on specifically designed graph neural networks as surrogate models. To this end, we extend the neural network architecture by directly incorporating information about the nosing process by adding different types of edges and their corresponding encoders to model object interactions. This augmentation enhances model accuracy and opens the possibility for employing precise surrogate models within closed-loop production processes. The proposed approach is evaluated using a new evaluation metric termed area between thickness curves (ABTC). The results demonstrate promising performance and highlight the potential of neural networks as surrogate models in predicting wall thickness changes during nosing forging processes.","sentences":["This study presents a novel approach for predicting wall thickness changes in tubes during the nosing process.","Specifically, we first provide a thorough analysis of nosing processes and the influencing parameters.","We further set-up a Finite Element Method (FEM) simulation to better analyse","the effects of varying process parameters.","As however traditional FEM simulations, while accurate, are time-consuming and computationally intensive, which renders them inapplicable for real-time application, we present a novel modeling framework based on specifically designed graph neural networks as surrogate models.","To this end, we extend the neural network architecture by directly incorporating information about the nosing process by adding different types of edges and their corresponding encoders to model object interactions.","This augmentation enhances model accuracy and opens the possibility for employing precise surrogate models within closed-loop production processes.","The proposed approach is evaluated using a new evaluation metric termed area between thickness curves (ABTC).","The results demonstrate promising performance and highlight the potential of neural networks as surrogate models in predicting wall thickness changes during nosing forging processes."],"url":"http://arxiv.org/abs/2411.13366v1"}
{"created":"2024-11-20 14:42:23","title":"Explainable Finite-Memory Policies for Partially Observable Markov Decision Processes","abstract":"Partially Observable Markov Decision Processes (POMDPs) are a fundamental framework for decision-making under uncertainty and partial observability. Since in general optimal policies may require infinite memory, they are hard to implement and often render most problems undecidable. Consequently, finite-memory policies are mostly considered instead. However, the algorithms for computing them are typically very complex, and so are the resulting policies. Facing the need for their explainability, we provide a representation of such policies, both (i) in an interpretable formalism and (ii) typically of smaller size, together yielding higher explainability. To that end, we combine models of Mealy machines and decision trees; the latter describing simple, stationary parts of the policies and the former describing how to switch among them. We design a translation for policies of the finite-state-controller (FSC) form from standard literature and show how our method smoothly generalizes to other variants of finite-memory policies. Further, we identify specific properties of recently used \"attractor-based\" policies, which allow us to construct yet simpler and smaller representations. Finally, we illustrate the higher explainability in a few case studies.","sentences":["Partially Observable Markov Decision Processes (POMDPs) are a fundamental framework for decision-making under uncertainty and partial observability.","Since in general optimal policies may require infinite memory, they are hard to implement and often render most problems undecidable.","Consequently, finite-memory policies are mostly considered instead.","However, the algorithms for computing them are typically very complex, and so are the resulting policies.","Facing the need for their explainability, we provide a representation of such policies, both (i) in an interpretable formalism and (ii) typically of smaller size, together yielding higher explainability.","To that end, we combine models of Mealy machines and decision trees; the latter describing simple, stationary parts of the policies and the former describing how to switch among them.","We design a translation for policies of the finite-state-controller (FSC) form from standard literature and show how our method smoothly generalizes to other variants of finite-memory policies.","Further, we identify specific properties of recently used \"attractor-based\" policies, which allow us to construct yet simpler and smaller representations.","Finally, we illustrate the higher explainability in a few case studies."],"url":"http://arxiv.org/abs/2411.13365v1"}
{"created":"2024-11-20 14:32:40","title":"Geometry-informed Channel Statistics Prediction Based upon Uncalibrated Digital Twins","abstract":"Digital twins (DTs) of wireless environments can be utilized to predict the propagation channel and reduce the overhead of required to estimate the channel statistics. However, direct channel prediction requires data-intensive calibration of the DT to capture the environment properties relevant for propagation of electromagnetic signals. We introduce a framework that starts from a satellite image of the environment to produce an uncalibrated DT, which has no or imprecise information about the materials and their electromagnetic properties. The key idea is to use the uncalibrated DT to implicitly provide a geometric prior for the environment. This is utilized to inform a Gaussian process (GP), which permits the use of few channel measurements to attain an accurate prediction of the channel statistics. Additionally, the framework is able to quantify the uncertainty in channel statistics prediction and select rate in ultra-reliable low-latency communication (URLLC) that complies with statistical guarantees. The efficacy of the proposed geometry-informed GP is validated using experimental data obtained through a measurement campaign. Furthermore, the proposed prediction framework is shown to provide significant improvements compared to the benchmarks where i) direct channel statistics prediction is obtained using an uncalibrated DT and (ii) the GP predicts channel statistics using information about the location.","sentences":["Digital twins (DTs) of wireless environments can be utilized to predict the propagation channel and reduce the overhead of required to estimate the channel statistics.","However, direct channel prediction requires data-intensive calibration of the DT to capture the environment properties relevant for propagation of electromagnetic signals.","We introduce a framework that starts from a satellite image of the environment to produce an uncalibrated DT, which has no or imprecise information about the materials and their electromagnetic properties.","The key idea is to use the uncalibrated DT to implicitly provide a geometric prior for the environment.","This is utilized to inform a Gaussian process (GP), which permits the use of few channel measurements to attain an accurate prediction of the channel statistics.","Additionally, the framework is able to quantify the uncertainty in channel statistics prediction and select rate in ultra-reliable low-latency communication (URLLC) that complies with statistical guarantees.","The efficacy of the proposed geometry-informed GP is validated using experimental data obtained through a measurement campaign.","Furthermore, the proposed prediction framework is shown to provide significant improvements compared to the benchmarks where i) direct channel statistics prediction is obtained using an uncalibrated DT and (ii) the GP predicts channel statistics using information about the location."],"url":"http://arxiv.org/abs/2411.13360v1"}
{"created":"2024-11-20 14:29:59","title":"Vertical Validation: Evaluating Implicit Generative Models for Graphs on Thin Support Regions","abstract":"There has been a growing excitement that implicit graph generative models could be used to design or discover new molecules for medicine or material design. Because these molecules have not been discovered, they naturally lie in unexplored or scarcely supported regions of the distribution of known molecules. However, prior evaluation methods for implicit graph generative models have focused on validating statistics computed from the thick support (e.g., mean and variance of a graph property). Therefore, there is a mismatch between the goal of generating novel graphs and the evaluation methods. To address this evaluation gap, we design a novel evaluation method called Vertical Validation (VV) that systematically creates thin support regions during the train-test splitting procedure and then reweights generated samples so that they can be compared to the held-out test data. This procedure can be seen as a generalization of the standard train-test procedure except that the splits are dependent on sample features. We demonstrate that our method can be used to perform model selection if performance on thin support regions is the desired goal. As a side benefit, we also show that our approach can better detect overfitting as exemplified by memorization.","sentences":["There has been a growing excitement that implicit graph generative models could be used to design or discover new molecules for medicine or material design.","Because these molecules have not been discovered, they naturally lie in unexplored or scarcely supported regions of the distribution of known molecules.","However, prior evaluation methods for implicit graph generative models have focused on validating statistics computed from the thick support (e.g., mean and variance of a graph property).","Therefore, there is a mismatch between the goal of generating novel graphs and the evaluation methods.","To address this evaluation gap, we design a novel evaluation method called Vertical Validation (VV) that systematically creates thin support regions during the train-test splitting procedure and then reweights generated samples so that they can be compared to the held-out test data.","This procedure can be seen as a generalization of the standard train-test procedure except that the splits are dependent on sample features.","We demonstrate that our method can be used to perform model selection if performance on thin support regions is the desired goal.","As a side benefit, we also show that our approach can better detect overfitting as exemplified by memorization."],"url":"http://arxiv.org/abs/2411.13358v1"}
{"created":"2024-11-20 14:22:15","title":"Learning based Ge'ez character handwritten recognition","abstract":"Ge'ez, an ancient Ethiopic script of cultural and historical significance, has been largely neglected in handwriting recognition research, hindering the digitization of valuable manuscripts. Our study addresses this gap by developing a state-of-the-art Ge'ez handwriting recognition system using Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks. Our approach uses a two-stage recognition process. First, a CNN is trained to recognize individual characters, which then acts as a feature extractor for an LSTM-based system for word recognition. Our dual-stage recognition approach achieves new top scores in Ge'ez handwriting recognition, outperforming eight state-of-the-art methods, which are SVTR, ASTER, and others as well as human performance, as measured in the HHD-Ethiopic dataset work. This research significantly advances the preservation and accessibility of Ge'ez cultural heritage, with implications for historical document digitization, educational tools, and cultural preservation. The code will be released upon acceptance.","sentences":["Ge'ez, an ancient Ethiopic script of cultural and historical significance, has been largely neglected in handwriting recognition research, hindering the digitization of valuable manuscripts.","Our study addresses this gap by developing a state-of-the-art Ge'ez handwriting recognition system using Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks.","Our approach uses a two-stage recognition process.","First, a CNN is trained to recognize individual characters, which then acts as a feature extractor for an LSTM-based system for word recognition.","Our dual-stage recognition approach achieves new top scores in Ge'ez handwriting recognition, outperforming eight state-of-the-art methods, which are SVTR, ASTER, and others as well as human performance, as measured in the HHD-Ethiopic dataset work.","This research significantly advances the preservation and accessibility of Ge'ez cultural heritage, with implications for historical document digitization, educational tools, and cultural preservation.","The code will be released upon acceptance."],"url":"http://arxiv.org/abs/2411.13350v1"}
{"created":"2024-11-20 14:19:30","title":"Parameterized Complexity of Star Decomposition Problem","abstract":"A star of length $ \\ell $ is defined as the complete bipartite graph $ K_{1,\\ell } $. In this paper we deal with the problem of edge decomposition of graphs into stars of varying lengths. Given a graph $ G $ and a list of integers $S=(s_1,\\ldots, s_t) $, an $S$-star decomposition of $ G $ is an edge decomposition of $ G $ into graphs $G_1 ,G_2 ,\\ldots,G_t $ such that $G_i$ is isomorphic to an star of length $s_i$, for each $i \\in\\{1,2,\\ldots,t\\}$. Given a graph $G$ and a list of integers $S$, \\sdp problem asks if $G$ admits an $ S $-star decomposition. The problem in known to be NP-complete even when all stars are of length three. In this paper, we investigate parametrized complexity of the problem with respect to the structural parameters such as minimum vertex cover, treewidth, tree-depth and neighborhood diversity as well as some intrinsic parameters of the problem such as number of distinct star lengths, the maximum size of stars and the maximum degree of the graph, giving a roughly complete picture of the parameterized complexity landscape of the problem.","sentences":["A star of length $ \\ell $ is defined as the complete bipartite graph $ K_{1,\\ell } $.","In this paper we deal with the problem of edge decomposition of graphs into stars of varying lengths.","Given a graph $ G $ and a list of integers $S=(s_1,\\ldots, s_t) $, an $S$-star decomposition of $ G $ is an edge decomposition of $ G $ into graphs $G_1 ,G_2 ,\\ldots,G_t $ such that $G_i$ is isomorphic to an star of length $s_i$, for each $i \\in\\{1,2,\\ldots,t\\}$. Given a graph $G$ and a list of integers $S$, \\sdp problem asks if $G$ admits an $ S $-star decomposition.","The problem in known to be NP-complete even when all stars are of length three.","In this paper, we investigate parametrized complexity of the problem with respect to the structural parameters such as minimum vertex cover, treewidth, tree-depth and neighborhood diversity as well as some intrinsic parameters of the problem such as number of distinct star lengths, the maximum size of stars and the maximum degree of the graph, giving a roughly complete picture of the parameterized complexity landscape of the problem."],"url":"http://arxiv.org/abs/2411.13348v1"}
