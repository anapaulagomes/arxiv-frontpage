{"created":"2024-10-14 17:59:59","title":"Tex4D: Zero-shot 4D Scene Texturing with Video Diffusion Models","abstract":"3D meshes are widely used in computer vision and graphics for their efficiency in animation and minimal memory use, playing a crucial role in movies, games, AR, and VR. However, creating temporally consistent and realistic textures for mesh sequences remains labor-intensive for professional artists. On the other hand, while video diffusion models excel at text-driven video generation, they often lack 3D geometry awareness and struggle with achieving multi-view consistent texturing for 3D meshes. In this work, we present Tex4D, a zero-shot approach that integrates inherent 3D geometry knowledge from mesh sequences with the expressiveness of video diffusion models to produce multi-view and temporally consistent 4D textures. Given an untextured mesh sequence and a text prompt as inputs, our method enhances multi-view consistency by synchronizing the diffusion process across different views through latent aggregation in the UV space. To ensure temporal consistency, we leverage prior knowledge from a conditional video generation model for texture synthesis. However, straightforwardly combining the video diffusion model and the UV texture aggregation leads to blurry results. We analyze the underlying causes and propose a simple yet effective modification to the DDIM sampling process to address this issue. Additionally, we introduce a reference latent texture to strengthen the correlation between frames during the denoising process. To the best of our knowledge, Tex4D is the first method specifically designed for 4D scene texturing. Extensive experiments demonstrate its superiority in producing multi-view and multi-frame consistent videos based on untextured mesh sequences.","sentences":["3D meshes are widely used in computer vision and graphics for their efficiency in animation and minimal memory use, playing a crucial role in movies, games, AR, and VR.","However, creating temporally consistent and realistic textures for mesh sequences remains labor-intensive for professional artists.","On the other hand, while video diffusion models excel at text-driven video generation, they often lack 3D geometry awareness and struggle with achieving multi-view consistent texturing for 3D meshes.","In this work, we present Tex4D, a zero-shot approach that integrates inherent 3D geometry knowledge from mesh sequences with the expressiveness of video diffusion models to produce multi-view and temporally consistent 4D textures.","Given an untextured mesh sequence and a text prompt as inputs, our method enhances multi-view consistency by synchronizing the diffusion process across different views through latent aggregation in the UV space.","To ensure temporal consistency, we leverage prior knowledge from a conditional video generation model for texture synthesis.","However, straightforwardly combining the video diffusion model and the UV texture aggregation leads to blurry results.","We analyze the underlying causes and propose a simple yet effective modification to the DDIM sampling process to address this issue.","Additionally, we introduce a reference latent texture to strengthen the correlation between frames during the denoising process.","To the best of our knowledge, Tex4D is the first method specifically designed for 4D scene texturing.","Extensive experiments demonstrate its superiority in producing multi-view and multi-frame consistent videos based on untextured mesh sequences."],"url":"http://arxiv.org/abs/2410.10821v1"}
{"created":"2024-10-14 17:59:58","title":"When Does Perceptual Alignment Benefit Vision Representations?","abstract":"Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose. Existing vision models understand a wide range of semantic abstractions but improperly weigh these attributes and thus make inferences misaligned with human perception. While vision representations have previously benefited from alignment in contexts like image generation, the utility of perceptually aligned representations in more general-purpose settings remains unclear. Here, we investigate how aligning vision model representations to human perceptual judgments impacts their usability across diverse computer vision tasks. We finetune state-of-the-art models on human similarity judgments for image triplets and evaluate them across standard vision benchmarks. We find that aligning models to perceptual judgments yields representations that improve upon the original backbones across many downstream tasks, including counting, segmentation, depth estimation, instance retrieval, and retrieval-augmented generation. In addition, we find that performance is widely preserved on other tasks, including specialized out-of-distribution domains such as in medical imaging and 3D environment frames. Our results suggest that injecting an inductive bias about human perceptual knowledge into vision models can contribute to better representations.","sentences":["Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose.","Existing vision models understand a wide range of semantic abstractions but improperly weigh these attributes and thus make inferences misaligned with human perception.","While vision representations have previously benefited from alignment in contexts like image generation, the utility of perceptually aligned representations in more general-purpose settings remains unclear.","Here, we investigate how aligning vision model representations to human perceptual judgments impacts their usability across diverse computer vision tasks.","We finetune state-of-the-art models on human similarity judgments for image triplets and evaluate them across standard vision benchmarks.","We find that aligning models to perceptual judgments yields representations that improve upon the original backbones across many downstream tasks, including counting, segmentation, depth estimation, instance retrieval, and retrieval-augmented generation.","In addition, we find that performance is widely preserved on other tasks, including specialized out-of-distribution domains such as in medical imaging and 3D environment frames.","Our results suggest that injecting an inductive bias about human perceptual knowledge into vision models can contribute to better representations."],"url":"http://arxiv.org/abs/2410.10817v1"}
{"created":"2024-10-14 17:59:58","title":"TemporalBench: Benchmarking Fine-grained Temporal Understanding for Multimodal Video Models","abstract":"Understanding fine-grained temporal dynamics is crucial for multimodal video comprehension and generation. Due to the lack of fine-grained temporal annotations, existing video benchmarks mostly resemble static image benchmarks and are incompetent at evaluating models for temporal understanding. In this paper, we introduce TemporalBench, a new benchmark dedicated to evaluating fine-grained temporal understanding in videos. TemporalBench consists of ~10K video question-answer pairs, derived from ~2K high-quality human annotations detailing the temporal dynamics in video clips. As a result, our benchmark provides a unique testbed for evaluating various temporal understanding and reasoning abilities such as action frequency, motion magnitude, event order, etc. Moreover, it enables evaluations on various tasks like both video question answering and captioning, both short and long video understanding, as well as different models such as multimodal video embedding models and text generation models. Results show that state-of-the-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, demonstrating a significant gap (~30%) between humans and AI in temporal understanding. Furthermore, we notice a critical pitfall for multi-choice QA where LLMs can detect the subtle changes in negative captions and find a centralized description as a cue for its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such bias. We hope that TemporalBench can foster research on improving models' temporal reasoning capabilities. Both dataset and evaluation code will be made available.","sentences":["Understanding fine-grained temporal dynamics is crucial for multimodal video comprehension and generation.","Due to the lack of fine-grained temporal annotations, existing video benchmarks mostly resemble static image benchmarks and are incompetent at evaluating models for temporal understanding.","In this paper, we introduce TemporalBench, a new benchmark dedicated to evaluating fine-grained temporal understanding in videos.","TemporalBench consists of ~10K video question-answer pairs, derived from ~2K high-quality human annotations detailing the temporal dynamics in video clips.","As a result, our benchmark provides a unique testbed for evaluating various temporal understanding and reasoning abilities such as action frequency, motion magnitude, event order, etc.","Moreover, it enables evaluations on various tasks like both video question answering and captioning, both short and long video understanding, as well as different models such as multimodal video embedding models and text generation models.","Results show that state-of-the-art models like GPT-4o achieve only 38.5% question answering accuracy on TemporalBench, demonstrating a significant gap (~30%) between humans and AI in temporal understanding.","Furthermore, we notice a critical pitfall for multi-choice QA where LLMs can detect the subtle changes in negative captions and find a centralized description as a cue for its prediction, where we propose Multiple Binary Accuracy (MBA) to correct such bias.","We hope that TemporalBench can foster research on improving models' temporal reasoning capabilities.","Both dataset and evaluation code will be made available."],"url":"http://arxiv.org/abs/2410.10818v1"}
{"created":"2024-10-14 17:59:58","title":"DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads","abstract":"Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges. Caching all Key and Value (KV) states across all attention heads consumes substantial memory. Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements. In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens. In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention. Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities. DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately. Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention. Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU. Code is provided in https://github.com/mit-han-lab/duo-attention.","sentences":["Deploying long-context large language models (LLMs) is essential but poses significant computational and memory challenges.","Caching all Key and Value (KV) states across all attention heads consumes substantial memory.","Existing KV cache pruning methods either damage the long-context capabilities of LLMs or offer only limited efficiency improvements.","In this paper, we identify that only a fraction of attention heads, a.k.a, Retrieval Heads, are critical for processing long contexts and require full attention across all tokens.","In contrast, all other heads, which primarily focus on recent tokens and attention sinks--referred to as Streaming Heads--do not require full attention.","Based on this insight, we introduce DuoAttention, a framework that only applies a full KV cache to retrieval heads while using a light-weight, constant-length KV cache for streaming heads, which reduces both LLM's decoding and pre-filling memory and latency without compromising its long-context abilities.","DuoAttention uses a lightweight, optimization-based algorithm with synthetic data to identify retrieval heads accurately.","Our method significantly reduces long-context inference memory by up to 2.55x for MHA and 1.67x for GQA models while speeding up decoding by up to 2.18x and 1.50x and accelerating pre-filling by up to 1.73x and 1.63x for MHA and GQA models, respectively, with minimal accuracy loss compared to full attention.","Notably, combined with quantization, DuoAttention enables Llama-3-8B decoding with 3.3 million context length on a single A100 GPU.","Code is provided in https://github.com/mit-han-lab/duo-attention."],"url":"http://arxiv.org/abs/2410.10819v1"}
{"created":"2024-10-14 17:59:56","title":"LVD-2M: A Long-take Video Dataset with Temporally Dense Captions","abstract":"The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, the lack of such high-quality long videos impedes the advancement of long video generation. To promote research in long video generation, we desire a new dataset with four key features essential for training long video generation models: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse contents, and (4) temporally dense captions. To achieve this, we introduce a new pipeline for selecting high-quality long-take videos and generating temporally dense captions. Specifically, we define a set of metrics to quantitatively assess video quality including scene cuts, dynamic degrees, and semantic-level quality, enabling us to filter high-quality long-take videos from a large amount of source videos. Subsequently, we develop a hierarchical video captioning pipeline to annotate long videos with temporally-dense captions. With this pipeline, we curate the first long-take video dataset, LVD-2M, comprising 2 million long-take videos, each covering more than 10 seconds and annotated with temporally dense captions. We further validate the effectiveness of LVD-2M by fine-tuning video generation models to generate long videos with dynamic motions. We believe our work will significantly contribute to future research in long video generation.","sentences":["The efficacy of video generation models heavily depends on the quality of their training datasets.","Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos.","However, the lack of such high-quality long videos impedes the advancement of long video generation.","To promote research in long video generation, we desire a new dataset with four key features essential for training long video generation models: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse contents, and (4) temporally dense captions.","To achieve this, we introduce a new pipeline for selecting high-quality long-take videos and generating temporally dense captions.","Specifically, we define a set of metrics to quantitatively assess video quality including scene cuts, dynamic degrees, and semantic-level quality, enabling us to filter high-quality long-take videos from a large amount of source videos.","Subsequently, we develop a hierarchical video captioning pipeline to annotate long videos with temporally-dense captions.","With this pipeline, we curate the first long-take video dataset, LVD-2M, comprising 2 million long-take videos, each covering more than 10 seconds and annotated with temporally dense captions.","We further validate the effectiveness of LVD-2M by fine-tuning video generation models to generate long videos with dynamic motions.","We believe our work will significantly contribute to future research in long video generation."],"url":"http://arxiv.org/abs/2410.10816v1"}
{"created":"2024-10-14 17:59:46","title":"Depth Any Video with Scalable Synthetic Data","abstract":"Video depth estimation has long been hindered by the scarcity of consistent and scalable ground truth data, leading to inconsistent and unreliable results. In this paper, we introduce Depth Any Video, a model that tackles the challenge through two key innovations. First, we develop a scalable synthetic data pipeline, capturing real-time video depth data from diverse synthetic environments, yielding 40,000 video clips of 5-second duration, each with precise depth annotations. Second, we leverage the powerful priors of generative video diffusion models to handle real-world videos effectively, integrating advanced techniques such as rotary position encoding and flow matching to further enhance flexibility and efficiency. Unlike previous models, which are limited to fixed-length video sequences, our approach introduces a novel mixed-duration training strategy that handles videos of varying lengths and performs robustly across different frame rates-even on single frames. At inference, we propose a depth interpolation method that enables our model to infer high-resolution video depth across sequences of up to 150 frames. Our model outperforms all previous generative depth models in terms of spatial accuracy and temporal consistency.","sentences":["Video depth estimation has long been hindered by the scarcity of consistent and scalable ground truth data, leading to inconsistent and unreliable results.","In this paper, we introduce Depth Any Video, a model that tackles the challenge through two key innovations.","First, we develop a scalable synthetic data pipeline, capturing real-time video depth data from diverse synthetic environments, yielding 40,000 video clips of 5-second duration, each with precise depth annotations.","Second, we leverage the powerful priors of generative video diffusion models to handle real-world videos effectively, integrating advanced techniques such as rotary position encoding and flow matching to further enhance flexibility and efficiency.","Unlike previous models, which are limited to fixed-length video sequences, our approach introduces a novel mixed-duration training strategy that handles videos of varying lengths and performs robustly across different frame rates-even on single frames.","At inference, we propose a depth interpolation method that enables our model to infer high-resolution video depth across sequences of up to 150 frames.","Our model outperforms all previous generative depth models in terms of spatial accuracy and temporal consistency."],"url":"http://arxiv.org/abs/2410.10815v1"}
{"created":"2024-10-14 17:59:44","title":"LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory","abstract":"Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. This paper introduces LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing 30% accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into four design choices across the indexing, retrieval, and reading stages. Built upon key experimental insights, we propose several memory designs including session decomposition for optimizing value granularity, fact-augmented key expansion for enhancing the index structure, and time-aware query expansion for refining the search scope. Experiment results show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI.","sentences":["Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses.","However, their long-term memory capabilities in sustained interactions remain underexplored.","This paper introduces LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention.","With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing 30% accuracy drop on memorizing information across sustained interactions.","We then present a unified framework that breaks down the long-term memory design into four design choices across the indexing, retrieval, and reading stages.","Built upon key experimental insights, we propose several memory designs including session decomposition for optimizing value granularity, fact-augmented key expansion for enhancing the index structure, and time-aware query expansion for refining the search scope.","Experiment results show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval.","Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI."],"url":"http://arxiv.org/abs/2410.10813v1"}
{"created":"2024-10-14 17:59:44","title":"Your Mixture-of-Experts LLM Is Secretly an Embedding Model For Free","abstract":"While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied. Does this contradict their claim of generalists? To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs. Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning. Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding. Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics. Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately. Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation. Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB). The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning.","sentences":["While large language models (LLMs) excel on generation tasks, their decoder-only architecture often limits their potential as embedding models if no further representation finetuning is applied.","Does this contradict their claim of generalists?","To answer the question, we take a closer look at Mixture-of-Experts (MoE) LLMs.","Our study shows that the expert routers in MoE LLMs can serve as an off-the-shelf embedding model with promising performance on a diverse class of embedding-focused tasks, without requiring any finetuning.","Moreover, our extensive analysis shows that the MoE routing weights (RW) is complementary to the hidden state (HS) of LLMs, a widely-used embedding.","Compared to HS, we find that RW is more robust to the choice of prompts and focuses on high-level semantics.","Motivated by the analysis, we propose MoEE combining RW and HS, which achieves better performance than using either separately.","Our exploration of their combination and prompting strategy shed several novel insights, e.g., a weighted sum of RW and HS similarities outperforms the similarity on their concatenation.","Our experiments are conducted on 6 embedding tasks with 20 datasets from the Massive Text Embedding Benchmark (MTEB).","The results demonstrate the significant improvement brought by MoEE to LLM-based embedding without further finetuning."],"url":"http://arxiv.org/abs/2410.10814v1"}
{"created":"2024-10-14 17:59:42","title":"HART: Efficient Visual Generation with Hybrid Autoregressive Transformer","abstract":"We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality. Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images. To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens. The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters. Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38. HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7x higher throughput and 6.9-13.4x lower MACs. Our code is open sourced at https://github.com/mit-han-lab/hart.","sentences":["We introduce Hybrid Autoregressive Transformer (HART), an autoregressive (AR) visual generation model capable of directly generating 1024x1024 images, rivaling diffusion models in image generation quality.","Existing AR models face limitations due to the poor image reconstruction quality of their discrete tokenizers and the prohibitive training costs associated with generating 1024px images.","To address these challenges, we present the hybrid tokenizer, which decomposes the continuous latents from the autoencoder into two components: discrete tokens representing the big picture and continuous tokens representing the residual components that cannot be represented by the discrete tokens.","The discrete component is modeled by a scalable-resolution discrete AR model, while the continuous component is learned with a lightweight residual diffusion module with only 37M parameters.","Compared with the discrete-only VAR tokenizer, our hybrid approach improves reconstruction FID from 2.11 to 0.30 on MJHQ-30K, leading to a 31% generation FID improvement from 7.85 to 5.38.","HART also outperforms state-of-the-art diffusion models in both FID and CLIP score, with 4.5-7.7x higher throughput and 6.9-13.4x lower MACs.","Our code is open sourced at https://github.com/mit-han-lab/hart."],"url":"http://arxiv.org/abs/2410.10812v1"}
{"created":"2024-10-14 17:59:41","title":"Deep Linear Probe Generators for Weight Space Learning","abstract":"Weight space learning aims to extract information about a neural network, such as its training dataset or generalization error. Recent approaches learn directly from model weights, but this presents many challenges as weights are high-dimensional and include permutation symmetries between neurons. An alternative approach, Probing, represents a model by passing a set of learned inputs (probes) through the model, and training a predictor on top of the corresponding outputs. Although probing is typically not used as a stand alone approach, our preliminary experiment found that a vanilla probing baseline worked surprisingly well. However, we discover that current probe learning strategies are ineffective. We therefore propose Deep Linear Probe Generators (ProbeGen), a simple and effective modification to probing approaches. ProbeGen adds a shared generator module with a deep linear architecture, providing an inductive bias towards structured probes thus reducing overfitting. While simple, ProbeGen performs significantly better than the state-of-the-art and is very efficient, requiring between 30 to 1000 times fewer FLOPs than other top approaches.","sentences":["Weight space learning aims to extract information about a neural network, such as its training dataset or generalization error.","Recent approaches learn directly from model weights, but this presents many challenges as weights are high-dimensional and include permutation symmetries between neurons.","An alternative approach, Probing, represents a model by passing a set of learned inputs (probes) through the model, and training a predictor on top of the corresponding outputs.","Although probing is typically not used as a stand alone approach, our preliminary experiment found that a vanilla probing baseline worked surprisingly well.","However, we discover that current probe learning strategies are ineffective.","We therefore propose Deep Linear Probe Generators (ProbeGen), a simple and effective modification to probing approaches.","ProbeGen adds a shared generator module with a deep linear architecture, providing an inductive bias towards structured probes thus reducing overfitting.","While simple, ProbeGen performs significantly better than the state-of-the-art and is very efficient, requiring between 30 to 1000 times fewer FLOPs than other top approaches."],"url":"http://arxiv.org/abs/2410.10811v1"}
{"created":"2024-10-14 17:59:38","title":"Local and Global Decoding in Text Generation","abstract":"Text generation, a key component in applications such as dialogue systems, relies on decoding algorithms that sample strings from a language model distribution. Traditional methods, such as top-$k$ and top-$\\pi$, apply local normalisation to the model's output distribution, which can distort it. In this paper, we investigate the effect of this distortion by introducing globally-normalised versions of these decoding methods. Additionally, we propose an independent Metropolis-Hastings algorithm to approximate sampling from globally-normalised distributions without explicitly computing them. Our empirical analysis compares the performance of local and global normalisation across two decoding algorithms (top-$k$ and top-$\\pi$) with various hyperparameters, using Pythia language models. Results show that, in most configurations, global decoding performs worse than the local decoding version of the same algorithms -- despite preserving the distribution's integrity. Our results suggest that distortion is an important feature of local decoding algorithms.","sentences":["Text generation, a key component in applications such as dialogue systems, relies on decoding algorithms that sample strings from a language model distribution.","Traditional methods, such as top-$k$ and top-$\\pi$, apply local normalisation to the model's output distribution, which can distort it.","In this paper, we investigate the effect of this distortion by introducing globally-normalised versions of these decoding methods.","Additionally, we propose an independent Metropolis-Hastings algorithm to approximate sampling from globally-normalised distributions without explicitly computing them.","Our empirical analysis compares the performance of local and global normalisation across two decoding algorithms (top-$k$ and top-$\\pi$) with various hyperparameters, using Pythia language models.","Results show that, in most configurations, global decoding performs worse than the local decoding version of the same algorithms -- despite preserving the distribution's integrity.","Our results suggest that distortion is an important feature of local decoding algorithms."],"url":"http://arxiv.org/abs/2410.10810v1"}
{"created":"2024-10-14 17:59:24","title":"Hard-Constrained Neural Networks with Universal Approximation Guarantees","abstract":"Incorporating prior knowledge or specifications of input-output relationships into machine learning models has gained significant attention, as it enhances generalization from limited data and leads to conforming outputs. However, most existing approaches use soft constraints by penalizing violations through regularization, which offers no guarantee of constraint satisfaction -- an essential requirement in safety-critical applications. On the other hand, imposing hard constraints on neural networks may hinder their representational power, adversely affecting performance. To address this, we propose HardNet, a practical framework for constructing neural networks that inherently satisfy hard constraints without sacrificing model capacity. Specifically, we encode affine and convex hard constraints, dependent on both inputs and outputs, by appending a differentiable projection layer to the network's output. This architecture allows unconstrained optimization of the network parameters using standard algorithms while ensuring constraint satisfaction by construction. Furthermore, we show that HardNet retains the universal approximation capabilities of neural networks. We demonstrate the versatility and effectiveness of HardNet across various applications: fitting functions under constraints, learning optimization solvers, optimizing control policies in safety-critical systems, and learning safe decision logic for aircraft systems.","sentences":["Incorporating prior knowledge or specifications of input-output relationships into machine learning models has gained significant attention, as it enhances generalization from limited data and leads to conforming outputs.","However, most existing approaches use soft constraints by penalizing violations through regularization, which offers no guarantee of constraint satisfaction -- an essential requirement in safety-critical applications.","On the other hand, imposing hard constraints on neural networks may hinder their representational power, adversely affecting performance.","To address this, we propose HardNet, a practical framework for constructing neural networks that inherently satisfy hard constraints without sacrificing model capacity.","Specifically, we encode affine and convex hard constraints, dependent on both inputs and outputs, by appending a differentiable projection layer to the network's output.","This architecture allows unconstrained optimization of the network parameters using standard algorithms while ensuring constraint satisfaction by construction.","Furthermore, we show that HardNet retains the universal approximation capabilities of neural networks.","We demonstrate the versatility and effectiveness of HardNet across various applications: fitting functions under constraints, learning optimization solvers, optimizing control policies in safety-critical systems, and learning safe decision logic for aircraft systems."],"url":"http://arxiv.org/abs/2410.10807v1"}
{"created":"2024-10-14 17:59:18","title":"TL-PCA: Transfer Learning of Principal Component Analysis","abstract":"Principal component analysis (PCA) can be significantly limited when there is too few examples of the target data of interest. We propose a transfer learning approach to PCA (TL-PCA) where knowledge from a related source task is used in addition to the scarce data of a target task. Our TL-PCA has two versions, one that uses a pretrained PCA solution of the source task, and another that uses the source data. Our proposed approach extends the PCA optimization objective with a penalty on the proximity of the target subspace and the source subspace as given by the pretrained source model or the source data. This optimization is solved by eigendecomposition for which the number of data-dependent eigenvectors (i.e., principal directions of TL-PCA) is not limited to the number of target data examples, which is a root cause that limits the standard PCA performance. Accordingly, our results for image datasets show that the representation of test data is improved by TL-PCA for dimensionality reduction where the learned subspace dimension is lower or higher than the number of target data examples.","sentences":["Principal component analysis (PCA) can be significantly limited when there is too few examples of the target data of interest.","We propose a transfer learning approach to PCA (TL-PCA) where knowledge from a related source task is used in addition to the scarce data of a target task.","Our TL-PCA has two versions, one that uses a pretrained PCA solution of the source task, and another that uses the source data.","Our proposed approach extends the PCA optimization objective with a penalty on the proximity of the target subspace and the source subspace as given by the pretrained source model or the source data.","This optimization is solved by eigendecomposition for which the number of data-dependent eigenvectors (i.e., principal directions of TL-PCA) is not limited to the number of target data examples, which is a root cause that limits the standard PCA performance.","Accordingly, our results for image datasets show that the representation of test data is improved by TL-PCA for dimensionality reduction where the learned subspace dimension is lower or higher than the number of target data examples."],"url":"http://arxiv.org/abs/2410.10805v1"}
{"created":"2024-10-14 17:59:03","title":"TrajDiffuse: A Conditional Diffusion Model for Environment-Aware Trajectory Prediction","abstract":"Accurate prediction of human or vehicle trajectories with good diversity that captures their stochastic nature is an essential task for many applications. However, many trajectory prediction models produce unreasonable trajectory samples that focus on improving diversity or accuracy while neglecting other key requirements, such as collision avoidance with the surrounding environment. In this work, we propose TrajDiffuse, a planning-based trajectory prediction method using a novel guided conditional diffusion model. We form the trajectory prediction problem as a denoising impaint task and design a map-based guidance term for the diffusion process. TrajDiffuse is able to generate trajectory predictions that match or exceed the accuracy and diversity of the SOTA, while adhering almost perfectly to environmental constraints. We demonstrate the utility of our model through experiments on the nuScenes and PFSD datasets and provide an extensive benchmark analysis against the SOTA methods.","sentences":["Accurate prediction of human or vehicle trajectories with good diversity that captures their stochastic nature is an essential task for many applications.","However, many trajectory prediction models produce unreasonable trajectory samples that focus on improving diversity or accuracy while neglecting other key requirements, such as collision avoidance with the surrounding environment.","In this work, we propose TrajDiffuse, a planning-based trajectory prediction method using a novel guided conditional diffusion model.","We form the trajectory prediction problem as a denoising impaint task and design a map-based guidance term for the diffusion process.","TrajDiffuse is able to generate trajectory predictions that match or exceed the accuracy and diversity of the SOTA, while adhering almost perfectly to environmental constraints.","We demonstrate the utility of our model through experiments on the nuScenes and PFSD datasets and provide an extensive benchmark analysis against the SOTA methods."],"url":"http://arxiv.org/abs/2410.10804v1"}
{"created":"2024-10-14 17:59:00","title":"Generalizable Humanoid Manipulation with Improved 3D Diffusion Policies","abstract":"Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists. However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills. Recent advances in 3D visuomotor policies, such as the 3D Diffusion Policy (DP3), have shown promise in extending these capabilities to wilder environments. However, 3D visuomotor policies often rely on camera calibration and point-cloud segmentation, which present challenges for deployment on mobile robots like humanoids. In this work, we introduce the Improved 3D Diffusion Policy (iDP3), a novel 3D visuomotor policy that eliminates these constraints by leveraging egocentric 3D visual representations. We demonstrate that iDP3 enables a full-sized humanoid robot to autonomously perform skills in diverse real-world scenarios, using only data collected in the lab. Videos are available at: https://humanoid-manipulation.github.io","sentences":["Humanoid robots capable of autonomous operation in diverse environments have long been a goal for roboticists.","However, autonomous manipulation by humanoid robots has largely been restricted to one specific scene, primarily due to the difficulty of acquiring generalizable skills.","Recent advances in 3D visuomotor policies, such as the 3D Diffusion Policy (DP3), have shown promise in extending these capabilities to wilder environments.","However, 3D visuomotor policies often rely on camera calibration and point-cloud segmentation, which present challenges for deployment on mobile robots like humanoids.","In this work, we introduce the Improved 3D Diffusion Policy (iDP3), a novel 3D visuomotor policy that eliminates these constraints by leveraging egocentric 3D visual representations.","We demonstrate that iDP3 enables a full-sized humanoid robot to autonomously perform skills in diverse real-world scenarios, using only data collected in the lab.","Videos are available at: https://humanoid-manipulation.github.io"],"url":"http://arxiv.org/abs/2410.10803v1"}
{"created":"2024-10-14 17:58:07","title":"Boosting Camera Motion Control for Video Diffusion Transformers","abstract":"Recent advancements in diffusion models have significantly enhanced the quality of video generation. However, fine-grained control over camera pose remains a challenge. While U-Net-based models have shown promising results for camera control, transformer-based diffusion models (DiT)-the preferred architecture for large-scale video generation - suffer from severe degradation in camera motion accuracy. In this paper, we investigate the underlying causes of this issue and propose solutions tailored to DiT architectures. Our study reveals that camera control performance depends heavily on the choice of conditioning methods rather than camera pose representations that is commonly believed. To address the persistent motion degradation in DiT, we introduce Camera Motion Guidance (CMG), based on classifier-free guidance, which boosts camera control by over 400%. Additionally, we present a sparse camera control pipeline, significantly simplifying the process of specifying camera poses for long videos. Our method universally applies to both U-Net and DiT models, offering improved camera control for video generation tasks.","sentences":["Recent advancements in diffusion models have significantly enhanced the quality of video generation.","However, fine-grained control over camera pose remains a challenge.","While U-Net-based models have shown promising results for camera control, transformer-based diffusion models (DiT)-the preferred architecture for large-scale video generation - suffer from severe degradation in camera motion accuracy.","In this paper, we investigate the underlying causes of this issue and propose solutions tailored to DiT architectures.","Our study reveals that camera control performance depends heavily on the choice of conditioning methods rather than camera pose representations that is commonly believed.","To address the persistent motion degradation in DiT, we introduce Camera Motion Guidance (CMG), based on classifier-free guidance, which boosts camera control by over 400%.","Additionally, we present a sparse camera control pipeline, significantly simplifying the process of specifying camera poses for long videos.","Our method universally applies to both U-Net and DiT models, offering improved camera control for video generation tasks."],"url":"http://arxiv.org/abs/2410.10802v1"}
{"created":"2024-10-14 17:58:01","title":"Mix Data or Merge Models? Optimizing for Diverse Multi-Task Learning","abstract":"Large Language Models (LLMs) have been adopted and deployed worldwide for a broad variety of applications. However, ensuring their safe use remains a significant challenge. Preference training and safety measures often overfit to harms prevalent in Western-centric datasets, and safety protocols frequently fail to extend to multilingual settings. In this work, we explore model merging in a diverse multi-task setting, combining safety and general-purpose tasks within a multilingual context. Each language introduces unique and varied learning challenges across tasks. We find that objective-based merging is more effective than mixing data, with improvements of up to 8% and 10% in general performance and safety respectively. We also find that language-based merging is highly effective -- by merging monolingually fine-tuned models, we achieve a 4% increase in general performance and 7% reduction in harm across all languages on top of the data mixtures method using the same available data. Overall, our comprehensive study of merging approaches provides a useful framework for building strong and safe multilingual models.","sentences":["Large Language Models (LLMs) have been adopted and deployed worldwide for a broad variety of applications.","However, ensuring their safe use remains a significant challenge.","Preference training and safety measures often overfit to harms prevalent in Western-centric datasets, and safety protocols frequently fail to extend to multilingual settings.","In this work, we explore model merging in a diverse multi-task setting, combining safety and general-purpose tasks within a multilingual context.","Each language introduces unique and varied learning challenges across tasks.","We find that objective-based merging is more effective than mixing data, with improvements of up to 8% and 10% in general performance and safety respectively.","We also find that language-based merging is highly effective -- by merging monolingually fine-tuned models, we achieve a 4% increase in general performance and 7% reduction in harm across all languages on top of the data mixtures method using the same available data.","Overall, our comprehensive study of merging approaches provides a useful framework for building strong and safe multilingual models."],"url":"http://arxiv.org/abs/2410.10801v1"}
{"created":"2024-10-14 17:57:23","title":"Towards Foundation Models for 3D Vision: How Close Are We?","abstract":"Building a foundation model for 3D vision is a complex challenge that remains unsolved. Towards that goal, it is important to understand the 3D reasoning capabilities of current models as well as identify the gaps between these models and humans. Therefore, we construct a new 3D visual understanding benchmark that covers fundamental 3D vision tasks in the Visual Question Answering (VQA) format. We evaluate state-of-the-art Vision-Language Models (VLMs), specialized models, and human subjects on it. Our results show that VLMs generally perform poorly, while the specialized models are accurate but not robust, failing under geometric perturbations. In contrast, human vision continues to be the most reliable 3D visual system. We further demonstrate that neural networks align more closely with human 3D vision mechanisms compared to classical computer vision methods, and Transformer-based networks such as ViT align more closely with human 3D vision mechanisms than CNNs. We hope our study will benefit the future development of foundation models for 3D vision.","sentences":["Building a foundation model for 3D vision is a complex challenge that remains unsolved.","Towards that goal, it is important to understand the 3D reasoning capabilities of current models as well as identify the gaps between these models and humans.","Therefore, we construct a new 3D visual understanding benchmark that covers fundamental 3D vision tasks in the Visual Question Answering (VQA) format.","We evaluate state-of-the-art Vision-Language Models (VLMs), specialized models, and human subjects on it.","Our results show that VLMs generally perform poorly, while the specialized models are accurate but not robust, failing under geometric perturbations.","In contrast, human vision continues to be the most reliable 3D visual system.","We further demonstrate that neural networks align more closely with human 3D vision mechanisms compared to classical computer vision methods, and Transformer-based networks such as ViT align more closely with human 3D vision mechanisms than CNNs.","We hope our study will benefit the future development of foundation models for 3D vision."],"url":"http://arxiv.org/abs/2410.10799v1"}
{"created":"2024-10-14 17:57:18","title":"MMAR: Towards Lossless Multi-Modal Auto-Regressive Prababilistic Modeling","abstract":"Recent advancements in multi-modal large language models have propelled the development of joint probabilistic models capable of both image understanding and generation. However, we have identifed that recent methods inevitably suffer from loss of image information during understanding task, due to either image discretization or diffusion denoising steps. To address this issue, we propose a novel Multi-Modal Auto-Regressive (MMAR) probabilistic modeling framework. Unlike discretization line of method, MMAR takes in continuous-valued image tokens to avoid information loss. Differing from diffusion-based approaches, we disentangle the diffusion process from auto-regressive backbone model by employing a light-weight diffusion head on top each auto-regressed image patch embedding. In this way, when the model transits from image generation to understanding through text generation, the backbone model's hidden representation of the image is not limited to the last denoising step. To successfully train our method, we also propose a theoretically proven technique that addresses the numerical stability issue and a training strategy that balances the generation and understanding task goals. Through extensive evaluations on 18 image understanding benchmarks, MMAR demonstrates much more superior performance than other joint multi-modal models, matching the method that employs pretrained CLIP vision encoder, meanwhile being able to generate high quality images at the same time. We also showed that our method is scalable with larger data and model size.","sentences":["Recent advancements in multi-modal large language models have propelled the development of joint probabilistic models capable of both image understanding and generation.","However, we have identifed that recent methods inevitably suffer from loss of image information during understanding task, due to either image discretization or diffusion denoising steps.","To address this issue, we propose a novel Multi-Modal Auto-Regressive (MMAR) probabilistic modeling framework.","Unlike discretization line of method, MMAR takes in continuous-valued image tokens to avoid information loss.","Differing from diffusion-based approaches, we disentangle the diffusion process from auto-regressive backbone model by employing a light-weight diffusion head on top each auto-regressed image patch embedding.","In this way, when the model transits from image generation to understanding through text generation, the backbone model's hidden representation of the image is not limited to the last denoising step.","To successfully train our method, we also propose a theoretically proven technique that addresses the numerical stability issue and a training strategy that balances the generation and understanding task goals.","Through extensive evaluations on 18 image understanding benchmarks, MMAR demonstrates much more superior performance than other joint multi-modal models, matching the method that employs pretrained CLIP vision encoder, meanwhile being able to generate high quality images at the same time.","We also showed that our method is scalable with larger data and model size."],"url":"http://arxiv.org/abs/2410.10798v1"}
{"created":"2024-10-14 17:57:10","title":"MEV Capture Through Time-Advantaged Arbitrage","abstract":"As blockchains begin processing significant economic activity, the ability to include and order transactions inevitably becomes highly valuable, a concept known as Maximal Extractable Value (MEV). This makes effective mechanisms for transaction inclusion and ordering, and thereby the extraction of MEV, a key aspect of blockchain design. Beyond traditional approaches such as ordering in a first-come-first-serve manner or using priority fees, a recent proposal suggests auctioning off a time advantage for transaction inclusion. In this paper, we investigate this time advantage mechanism, focusing specifically on arbitrage opportunities on Automated Market Makers (AMMs), one of the largest sources of MEV today. We analyze the optimal strategy for a time-advantaged arbitrageur and compare the profits generated by various MEV extraction methods. Finally, we explore how AMMs can be adapted in the time advantage setting to capture a portion of the MEV.","sentences":["As blockchains begin processing significant economic activity, the ability to include and order transactions inevitably becomes highly valuable, a concept known as Maximal Extractable Value (MEV).","This makes effective mechanisms for transaction inclusion and ordering, and thereby the extraction of MEV, a key aspect of blockchain design.","Beyond traditional approaches such as ordering in a first-come-first-serve manner or using priority fees, a recent proposal suggests auctioning off a time advantage for transaction inclusion.","In this paper, we investigate this time advantage mechanism, focusing specifically on arbitrage opportunities on Automated Market Makers (AMMs), one of the largest sources of MEV today.","We analyze the optimal strategy for a time-advantaged arbitrageur and compare the profits generated by various MEV extraction methods.","Finally, we explore how AMMs can be adapted in the time advantage setting to capture a portion of the MEV."],"url":"http://arxiv.org/abs/2410.10797v1"}
{"created":"2024-10-14 17:57:09","title":"Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance","abstract":"Large language models are instruction-finetuned to enhance their ability to follow user instructions and process the input context. However, even state-of-the-art models often struggle to follow the instruction, especially when the input context is not aligned with the model's parametric knowledge. This manifests as various failures, such as hallucinations where the responses are outdated, biased or contain unverified facts. In this work, we try to understand the underlying reason for this poor context reliance, especially after instruction tuning. We observe an intriguing phenomenon: during instruction tuning, the context reliance initially increases as expected, but then gradually decreases as instruction finetuning progresses. We call this phenomenon context-parametric inversion and observe it across multiple general purpose instruction tuning datasets like TULU, Alpaca and Ultrachat, as well as model families such as Llama, Mistral and Pythia. In a simple theoretical setup, we isolate why context-parametric inversion occurs along the gradient descent trajectory of instruction finetuning. We tie this phenomena to examples in the instruction finetuning data mixture where the input context provides information that is already present in the model's parametric knowledge. Our analysis suggests natural mitigation strategies that provide some limited gains, while also validating our theoretical insights. We hope that our work serves as a starting point in addressing this failure mode in a staple part of LLM training.","sentences":["Large language models are instruction-finetuned to enhance their ability to follow user instructions and process the input context.","However, even state-of-the-art models often struggle to follow the instruction, especially when the input context is not aligned with the model's parametric knowledge.","This manifests as various failures, such as hallucinations where the responses are outdated, biased or contain unverified facts.","In this work, we try to understand the underlying reason for this poor context reliance, especially after instruction tuning.","We observe an intriguing phenomenon: during instruction tuning, the context reliance initially increases as expected, but then gradually decreases as instruction finetuning progresses.","We call this phenomenon context-parametric inversion and observe it across multiple general purpose instruction tuning datasets like TULU, Alpaca and Ultrachat, as well as model families such as Llama, Mistral and Pythia.","In a simple theoretical setup, we isolate why context-parametric inversion occurs along the gradient descent trajectory of instruction finetuning.","We tie this phenomena to examples in the instruction finetuning data mixture where the input context provides information that is already present in the model's parametric knowledge.","Our analysis suggests natural mitigation strategies that provide some limited gains, while also validating our theoretical insights.","We hope that our work serves as a starting point in addressing this failure mode in a staple part of LLM training."],"url":"http://arxiv.org/abs/2410.10796v1"}
{"created":"2024-10-14 17:56:24","title":"Semantic Image Inversion and Editing using Rectified Stochastic Differential Equations","abstract":"Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing. This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux). Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion. Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice. Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored. We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator. We prove that the resulting vector field is equivalent to a rectified stochastic differential equation. Additionally, we extend our framework to design a stochastic sampler for Flux. Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference.","sentences":["Generative models transform random noise into images; their inversion aims to transform images back to structured noise for recovery and editing.","This paper addresses two key tasks: (i) inversion and (ii) editing of a real image using stochastic equivalents of rectified flow models (such as Flux).","Although Diffusion Models (DMs) have recently dominated the field of generative modeling for images, their inversion presents faithfulness and editability challenges due to nonlinearities in drift and diffusion.","Existing state-of-the-art DM inversion approaches rely on training of additional parameters or test-time optimization of latent variables; both are expensive in practice.","Rectified Flows (RFs) offer a promising alternative to diffusion models, yet their inversion has been underexplored.","We propose RF inversion using dynamic optimal control derived via a linear quadratic regulator.","We prove that the resulting vector field is equivalent to a rectified stochastic differential equation.","Additionally, we extend our framework to design a stochastic sampler for Flux.","Our inversion method allows for state-of-the-art performance in zero-shot inversion and editing, outperforming prior works in stroke-to-image synthesis and semantic image editing, with large-scale human evaluations confirming user preference."],"url":"http://arxiv.org/abs/2410.10792v1"}
{"created":"2024-10-14 17:56:20","title":"Condition-Aware Multimodal Fusion for Robust Semantic Perception of Driving Scenes","abstract":"Leveraging multiple sensors is crucial for robust semantic perception in autonomous driving, as each sensor type has complementary strengths and weaknesses. However, existing sensor fusion methods often treat sensors uniformly across all conditions, leading to suboptimal performance. By contrast, we propose a novel, condition-aware multimodal fusion approach for robust semantic perception of driving scenes. Our method, CAFuser uses an RGB camera input to classify environmental conditions and generate a Condition Token that guides the fusion of multiple sensor modalities. We further newly introduce modality-specific feature adapters to align diverse sensor inputs into a shared latent space, enabling efficient integration with a single and shared pre-trained backbone. By dynamically adapting sensor fusion based on the actual condition, our model significantly improves robustness and accuracy, especially in adverse-condition scenarios. We set the new state of the art with CAFuser on the MUSES dataset with 59.7 PQ for multimodal panoptic segmentation and 78.2 mIoU for semantic segmentation, ranking first on the public benchmarks.","sentences":["Leveraging multiple sensors is crucial for robust semantic perception in autonomous driving, as each sensor type has complementary strengths and weaknesses.","However, existing sensor fusion methods often treat sensors uniformly across all conditions, leading to suboptimal performance.","By contrast, we propose a novel, condition-aware multimodal fusion approach for robust semantic perception of driving scenes.","Our method, CAFuser uses an RGB camera input to classify environmental conditions and generate a Condition Token that guides the fusion of multiple sensor modalities.","We further newly introduce modality-specific feature adapters to align diverse sensor inputs into a shared latent space, enabling efficient integration with a single and shared pre-trained backbone.","By dynamically adapting sensor fusion based on the actual condition, our model significantly improves robustness and accuracy, especially in adverse-condition scenarios.","We set the new state of the art with CAFuser on the MUSES dataset with 59.7 PQ for multimodal panoptic segmentation and 78.2 mIoU for semantic segmentation, ranking first on the public benchmarks."],"url":"http://arxiv.org/abs/2410.10791v1"}
{"created":"2024-10-14 17:56:19","title":"Sitcom-Crafter: A Plot-Driven Human Motion Generation System in 3D Scenes","abstract":"Recent advancements in human motion synthesis have focused on specific types of motions, such as human-scene interaction, locomotion or human-human interaction, however, there is a lack of a unified system capable of generating a diverse combination of motion types. In response, we introduce Sitcom-Crafter, a comprehensive and extendable system for human motion generation in 3D space, which can be guided by extensive plot contexts to enhance workflow efficiency for anime and game designers. The system is comprised of eight modules, three of which are dedicated to motion generation, while the remaining five are augmentation modules that ensure consistent fusion of motion sequences and system functionality. Central to the generation modules is our novel 3D scene-aware human-human interaction module, which addresses collision issues by synthesizing implicit 3D Signed Distance Function (SDF) points around motion spaces, thereby minimizing human-scene collisions without additional data collection costs. Complementing this, our locomotion and human-scene interaction modules leverage existing methods to enrich the system's motion generation capabilities. Augmentation modules encompass plot comprehension for command generation, motion synchronization for seamless integration of different motion types, hand pose retrieval to enhance motion realism, motion collision revision to prevent human collisions, and 3D retargeting to ensure visual fidelity. Experimental evaluations validate the system's ability to generate high-quality, diverse, and physically realistic motions, underscoring its potential for advancing creative workflows.","sentences":["Recent advancements in human motion synthesis have focused on specific types of motions, such as human-scene interaction, locomotion or human-human interaction, however, there is a lack of a unified system capable of generating a diverse combination of motion types.","In response, we introduce Sitcom-Crafter, a comprehensive and extendable system for human motion generation in 3D space, which can be guided by extensive plot contexts to enhance workflow efficiency for anime and game designers.","The system is comprised of eight modules, three of which are dedicated to motion generation, while the remaining five are augmentation modules that ensure consistent fusion of motion sequences and system functionality.","Central to the generation modules is our novel 3D scene-aware human-human interaction module, which addresses collision issues by synthesizing implicit 3D Signed Distance Function (SDF) points around motion spaces, thereby minimizing human-scene collisions without additional data collection costs.","Complementing this, our locomotion and human-scene interaction modules leverage existing methods to enrich the system's motion generation capabilities.","Augmentation modules encompass plot comprehension for command generation, motion synchronization for seamless integration of different motion types, hand pose retrieval to enhance motion realism, motion collision revision to prevent human collisions, and 3D retargeting to ensure visual fidelity.","Experimental evaluations validate the system's ability to generate high-quality, diverse, and physically realistic motions, underscoring its potential for advancing creative workflows."],"url":"http://arxiv.org/abs/2410.10790v1"}
{"created":"2024-10-14 17:54:16","title":"On the Approximability of the Yolk in the Spatial Model of Voting","abstract":"In the spatial model of voting, the yolk and LP (linear programming) yolk are important solution concepts for predicting outcomes for a committee of voters. McKelvey and Tovey showed that the LP yolk provides a lower bound approximation for the size of the yolk and there has been considerable debate on whether the LP yolk is a good approximation of the yolk. In this paper, we show that for an odd number of voters in a two-dimensional space that the yolk radius is at most twice the size of the LP yolk radius. However, we also show that (1) even in this setting, the LP yolk center can be arbitrarily far away from the yolk center (relative to the radius of the yolk) and (2) for all other settings (an even number of voters or in dimension $k\\geq 3$) that the LP yolk can be arbitrarily small relative to the yolk. Thus, in general, the LP yolk can be an arbitrarily poor approximation of the yolk.","sentences":["In the spatial model of voting, the yolk and LP (linear programming) yolk are important solution concepts for predicting outcomes for a committee of voters.","McKelvey and Tovey showed that the LP yolk provides a lower bound approximation for the size of the yolk and there has been considerable debate on whether the LP yolk is a good approximation of the yolk.","In this paper, we show that for an odd number of voters in a two-dimensional space that the yolk radius is at most twice the size of the LP yolk radius.","However, we also show that (1) even in this setting, the LP yolk center can be arbitrarily far away from the yolk center (relative to the radius of the yolk) and (2) for all other settings (an even number of voters or in dimension $k\\geq 3$) that the LP yolk can be arbitrarily small relative to the yolk.","Thus, in general, the LP yolk can be an arbitrarily poor approximation of the yolk."],"url":"http://arxiv.org/abs/2410.10788v1"}
{"created":"2024-10-14 17:52:18","title":"On Information-Theoretic Measures of Predictive Uncertainty","abstract":"Reliable estimation of predictive uncertainty is crucial for machine learning applications, particularly in high-stakes scenarios where hedging against risks is essential. Despite its significance, a consensus on the correct measurement of predictive uncertainty remains elusive. In this work, we return to first principles to develop a fundamental framework of information-theoretic predictive uncertainty measures. Our proposed framework categorizes predictive uncertainty measures according to two factors: (I) The predicting model (II) The approximation of the true predictive distribution. Examining all possible combinations of these two factors, we derive a set of predictive uncertainty measures that includes both known and newly introduced ones. We empirically evaluate these measures in typical uncertainty estimation settings, such as misclassification detection, selective prediction, and out-of-distribution detection. The results show that no single measure is universal, but the effectiveness depends on the specific setting. Thus, our work provides clarity about the suitability of predictive uncertainty measures by clarifying their implicit assumptions and relationships.","sentences":["Reliable estimation of predictive uncertainty is crucial for machine learning applications, particularly in high-stakes scenarios where hedging against risks is essential.","Despite its significance, a consensus on the correct measurement of predictive uncertainty remains elusive.","In this work, we return to first principles to develop a fundamental framework of information-theoretic predictive uncertainty measures.","Our proposed framework categorizes predictive uncertainty measures according to two factors: (I) The predicting model (II) The approximation of the true predictive distribution.","Examining all possible combinations of these two factors, we derive a set of predictive uncertainty measures that includes both known and newly introduced ones.","We empirically evaluate these measures in typical uncertainty estimation settings, such as misclassification detection, selective prediction, and out-of-distribution detection.","The results show that no single measure is universal, but the effectiveness depends on the specific setting.","Thus, our work provides clarity about the suitability of predictive uncertainty measures by clarifying their implicit assumptions and relationships."],"url":"http://arxiv.org/abs/2410.10786v1"}
{"created":"2024-10-14 17:52:04","title":"Probabilistic Degeneracy Detection for Point-to-Plane Error Minimization","abstract":"Degeneracies arising from uninformative geometry are known to deteriorate LiDAR-based localization and mapping. This work introduces a new probabilistic method to detect and mitigate the effect of degeneracies in point-to-plane error minimization. The noise on the Hessian of the point-to-plane optimization problem is characterized by the noise on points and surface normals used in its construction. We exploit this characterization to quantify the probability of a direction being degenerate. The degeneracy-detection procedure is used in a new real-time degeneracy-aware iterative closest point algorithm for LiDAR registration, in which we smoothly attenuate updates in degenerate directions. The method's parameters are selected based on the noise characteristics provided in the LiDAR's datasheet. We validate the approach in four real-world experiments, demonstrating that it outperforms state-of-the-art methods at detecting and mitigating the adverse effects of degeneracies. For the benefit of the community, we release the code for the method at: github.com/ntnu-arl/drpm.","sentences":["Degeneracies arising from uninformative geometry are known to deteriorate LiDAR-based localization and mapping.","This work introduces a new probabilistic method to detect and mitigate the effect of degeneracies in point-to-plane error minimization.","The noise on the Hessian of the point-to-plane optimization problem is characterized by the noise on points and surface normals used in its construction.","We exploit this characterization to quantify the probability of a direction being degenerate.","The degeneracy-detection procedure is used in a new real-time degeneracy-aware iterative closest point algorithm for LiDAR registration, in which we smoothly attenuate updates in degenerate directions.","The method's parameters are selected based on the noise characteristics provided in the LiDAR's datasheet.","We validate the approach in four real-world experiments, demonstrating that it outperforms state-of-the-art methods at detecting and mitigating the adverse effects of degeneracies.","For the benefit of the community, we release the code for the method at: github.com/ntnu-arl/drpm."],"url":"http://arxiv.org/abs/2410.10784v1"}
{"created":"2024-10-14 17:51:23","title":"LiveXiv -- A Multi-Modal Live Benchmark Based on Arxiv Papers Content","abstract":"The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks. However, one downside of scraping data from the web can be the potential sacrifice of the benchmarks on which the abilities of these models are often evaluated. To safeguard against test data contamination and to truly test the abilities of these foundation models we propose LiveXiv: A scalable evolving live benchmark based on scientific ArXiv papers. LiveXiv accesses domain-specific manuscripts at any given timestamp and proposes to automatically generate visual question-answer pairs (VQA). This is done without any human-in-the-loop, using the multi-modal content in the manuscripts, like graphs, charts, and tables. Moreover, we introduce an efficient evaluation approach that estimates the performance of all models on the evolving benchmark using evaluations of only a subset of models. This significantly reduces the overall evaluation cost. We benchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the first version of our benchmark, showing its challenging nature and exposing the models true abilities, avoiding contamination. Lastly, in our commitment to high quality, we have collected and evaluated a manually verified subset. By comparing its overall results to our automatic annotations, we have found that the performance variance is indeed minimal (<2.5%). Our dataset is available online on HuggingFace, and our code will be available here.","sentences":["The large-scale training of multi-modal models on data scraped from the web has shown outstanding utility in infusing these models with the required world knowledge to perform effectively on multiple downstream tasks.","However, one downside of scraping data from the web can be the potential sacrifice of the benchmarks on which the abilities of these models are often evaluated.","To safeguard against test data contamination and to truly test the abilities of these foundation models we propose LiveXiv: A scalable evolving live benchmark based on scientific ArXiv papers.","LiveXiv accesses domain-specific manuscripts at any given timestamp and proposes to automatically generate visual question-answer pairs (VQA).","This is done without any human-in-the-loop, using the multi-modal content in the manuscripts, like graphs, charts, and tables.","Moreover, we introduce an efficient evaluation approach that estimates the performance of all models on the evolving benchmark using evaluations of only a subset of models.","This significantly reduces the overall evaluation cost.","We benchmark multiple open and proprietary Large Multi-modal Models (LMMs) on the first version of our benchmark, showing its challenging nature and exposing the models true abilities, avoiding contamination.","Lastly, in our commitment to high quality, we have collected and evaluated a manually verified subset.","By comparing its overall results to our automatic annotations, we have found that the performance variance is indeed minimal (<2.5%).","Our dataset is available online on HuggingFace, and our code will be available here."],"url":"http://arxiv.org/abs/2410.10783v1"}
{"created":"2024-10-14 17:50:47","title":"3DArticCyclists: Generating Simulated Dynamic 3D Cyclists for Human-Object Interaction (HOI) and Autonomous Driving Applications","abstract":"Human-object interaction (HOI) and human-scene interaction (HSI) are crucial for human-centric scene understanding applications in Embodied Artificial Intelligence (EAI), robotics, and augmented reality (AR). A common limitation faced in these research areas is the data scarcity problem: insufficient labeled human-scene object pairs on the input images, and limited interaction complexity and granularity between them. Recent HOI and HSI methods have addressed this issue by generating dynamic interactions with rigid objects. But more complex dynamic interactions such as a human rider pedaling an articulated bicycle have been unexplored. To address this limitation, and to enable research on complex dynamic human-articulated object interactions, in this paper we propose a method to generate simulated 3D dynamic cyclist assets and interactions. We designed a methodology for creating a new part-based multi-view articulated synthetic 3D bicycle dataset that we call 3DArticBikes that can be used to train NeRF and 3DGS-based 3D reconstruction methods. We then propose a 3DGS-based parametric bicycle composition model to assemble 8-DoF pose-controllable 3D bicycles. Finally, using dynamic information from cyclist videos, we build a complete synthetic dynamic 3D cyclist (rider pedaling a bicycle) by re-posing a selectable synthetic 3D person while automatically placing the rider onto one of our new articulated 3D bicycles using a proposed 3D Keypoint optimization-based Inverse Kinematics pose refinement. We present both, qualitative and quantitative results where we compare our generated cyclists against those from a recent stable diffusion-based method.","sentences":["Human-object interaction (HOI) and human-scene interaction (HSI) are crucial for human-centric scene understanding applications in Embodied Artificial Intelligence (EAI), robotics, and augmented reality (AR).","A common limitation faced in these research areas is the data scarcity problem: insufficient labeled human-scene object pairs on the input images, and limited interaction complexity and granularity between them.","Recent HOI and HSI methods have addressed this issue by generating dynamic interactions with rigid objects.","But more complex dynamic interactions such as a human rider pedaling an articulated bicycle have been unexplored.","To address this limitation, and to enable research on complex dynamic human-articulated object interactions, in this paper we propose a method to generate simulated 3D dynamic cyclist assets and interactions.","We designed a methodology for creating a new part-based multi-view articulated synthetic 3D bicycle dataset that we call 3DArticBikes that can be used to train NeRF and 3DGS-based 3D reconstruction methods.","We then propose a 3DGS-based parametric bicycle composition model to assemble 8-DoF pose-controllable 3D bicycles.","Finally, using dynamic information from cyclist videos, we build a complete synthetic dynamic 3D cyclist (rider pedaling a bicycle) by re-posing a selectable synthetic 3D person while automatically placing the rider onto one of our new articulated 3D bicycles using a proposed 3D Keypoint optimization-based Inverse Kinematics pose refinement.","We present both, qualitative and quantitative results where we compare our generated cyclists against those from a recent stable diffusion-based method."],"url":"http://arxiv.org/abs/2410.10782v1"}
{"created":"2024-10-14 17:50:28","title":"When Attention Sink Emerges in Language Models: An Empirical View","abstract":"Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as attention sink. This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization, inference acceleration, model quantization, and others. Despite its widespread use, a deep understanding of attention sink in LMs is still lacking. In this work, we first demonstrate that attention sinks exist universally in LMs with various inputs, even in small models. Furthermore, attention sink is observed to emerge during the LM pre-training, motivating us to investigate how optimization, data distribution, loss function, and model architecture in LM pre-training influence its emergence. We highlight that attention sink emerges after effective optimization on sufficient training data. The sink position is highly correlated with the loss function and data distribution. Most importantly, we find that attention sink acts more like key biases, storing extra attention scores, which could be non-informative and not contribute to the value computation. We also observe that this phenomenon (at least partially) stems from tokens' inner dependence on attention scores as a result of softmax normalization. After relaxing such dependence by replacing softmax attention with other attention operations, such as sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters. The code is available at https://github.com/sail-sg/Attention-Sink.","sentences":["Language Models (LMs) assign significant attention to the first token, even if it is not semantically important, which is known as attention sink.","This phenomenon has been widely adopted in applications such as streaming/long context generation, KV cache optimization, inference acceleration, model quantization, and others.","Despite its widespread use, a deep understanding of attention sink in LMs is still lacking.","In this work, we first demonstrate that attention sinks exist universally in LMs with various inputs, even in small models.","Furthermore, attention sink is observed to emerge during the LM pre-training, motivating us to investigate how optimization, data distribution, loss function, and model architecture in LM pre-training influence its emergence.","We highlight that attention sink emerges after effective optimization on sufficient training data.","The sink position is highly correlated with the loss function and data distribution.","Most importantly, we find that attention sink acts more like key biases, storing extra attention scores, which could be non-informative and not contribute to the value computation.","We also observe that this phenomenon (at least partially) stems from tokens' inner dependence on attention scores as a result of softmax normalization.","After relaxing such dependence by replacing softmax attention with other attention operations, such as sigmoid attention without normalization, attention sinks do not emerge in LMs up to 1B parameters.","The code is available at https://github.com/sail-sg/Attention-Sink."],"url":"http://arxiv.org/abs/2410.10781v1"}
{"created":"2024-10-14 17:50:27","title":"ControlMM: Controllable Masked Motion Generation","abstract":"Recent advances in motion diffusion models have enabled spatially controllable text-to-motion generation. However, despite achieving acceptable control precision, these models suffer from generation speed and fidelity limitations. To address these challenges, we propose ControlMM, a novel approach incorporating spatial control signals into the generative masked motion model. ControlMM achieves real-time, high-fidelity, and high-precision controllable motion generation simultaneously. Our approach introduces two key innovations. First, we propose masked consistency modeling, which ensures high-fidelity motion generation via random masking and reconstruction, while minimizing the inconsistency between the input control signals and the extracted control signals from the generated motion. To further enhance control precision, we introduce inference-time logit editing, which manipulates the predicted conditional motion distribution so that the generated motion, sampled from the adjusted distribution, closely adheres to the input control signals. During inference, ControlMM enables parallel and iterative decoding of multiple motion tokens, allowing for high-speed motion generation. Extensive experiments show that, compared to the state of the art, ControlMM delivers superior results in motion quality, with better FID scores (0.061 vs 0.271), and higher control precision (average error 0.0091 vs 0.0108). ControlMM generates motions 20 times faster than diffusion-based methods. Additionally, ControlMM unlocks diverse applications such as any joint any frame control, body part timeline control, and obstacle avoidance. Video visualization can be found at https://exitudio.github.io/ControlMM-page","sentences":["Recent advances in motion diffusion models have enabled spatially controllable text-to-motion generation.","However, despite achieving acceptable control precision, these models suffer from generation speed and fidelity limitations.","To address these challenges, we propose ControlMM, a novel approach incorporating spatial control signals into the generative masked motion model.","ControlMM achieves real-time, high-fidelity, and high-precision controllable motion generation simultaneously.","Our approach introduces two key innovations.","First, we propose masked consistency modeling, which ensures high-fidelity motion generation via random masking and reconstruction, while minimizing the inconsistency between the input control signals and the extracted control signals from the generated motion.","To further enhance control precision, we introduce inference-time logit editing, which manipulates the predicted conditional motion distribution so that the generated motion, sampled from the adjusted distribution, closely adheres to the input control signals.","During inference, ControlMM enables parallel and iterative decoding of multiple motion tokens, allowing for high-speed motion generation.","Extensive experiments show that, compared to the state of the art, ControlMM delivers superior results in motion quality, with better FID scores (0.061 vs 0.271), and higher control precision (average error 0.0091 vs 0.0108).","ControlMM generates motions 20 times faster than diffusion-based methods.","Additionally, ControlMM unlocks diverse applications such as any joint any frame control, body part timeline control, and obstacle avoidance.","Video visualization can be found at https://exitudio.github.io/ControlMM-page"],"url":"http://arxiv.org/abs/2410.10780v1"}
{"created":"2024-10-14 17:49:54","title":"Focused ReAct: Improving ReAct through Reiterate and Early Stop","abstract":"Large language models (LLMs) have significantly improved their reasoning and decision-making capabilities, as seen in methods like ReAct. However, despite its effectiveness in tackling complex tasks, ReAct faces two main challenges: losing focus on the original question and becoming stuck in action loops. To address these issues, we introduce Focused ReAct, an enhanced version of the ReAct paradigm that incorporates reiteration and early stop mechanisms. These improvements help the model stay focused on the original query and avoid repetitive behaviors. Experimental results show accuracy gains of 18% to 530% and a runtime reduction of up to 34% compared to the original ReAct method.","sentences":["Large language models (LLMs) have significantly improved their reasoning and decision-making capabilities, as seen in methods like ReAct.","However, despite its effectiveness in tackling complex tasks, ReAct faces two main challenges: losing focus on the original question and becoming stuck in action loops.","To address these issues, we introduce Focused ReAct, an enhanced version of the ReAct paradigm that incorporates reiteration and early stop mechanisms.","These improvements help the model stay focused on the original query and avoid repetitive behaviors.","Experimental results show accuracy gains of 18% to 530% and a runtime reduction of up to 34% compared to the original ReAct method."],"url":"http://arxiv.org/abs/2410.10779v1"}
{"created":"2024-10-14 17:49:27","title":"UniMatch V2: Pushing the Limit of Semi-Supervised Semantic Segmentation","abstract":"Semi-supervised semantic segmentation (SSS) aims at learning rich visual knowledge from cheap unlabeled images to enhance semantic segmentation capability. Among recent works, UniMatch improves its precedents tremendously by amplifying the practice of weak-to-strong consistency regularization. Subsequent works typically follow similar pipelines and propose various delicate designs. Despite the achieved progress, strangely, even in this flourishing era of numerous powerful vision models, almost all SSS works are still sticking to 1) using outdated ResNet encoders with small-scale ImageNet-1K pre-training, and 2) evaluation on simple Pascal and Cityscapes datasets. In this work, we argue that, it is necessary to switch the baseline of SSS from ResNet-based encoders to more capable ViT-based encoders (e.g., DINOv2) that are pre-trained on massive data. A simple update on the encoder (even using 2x fewer parameters) can bring more significant improvement than careful method designs. Built on this competitive baseline, we present our upgraded and simplified UniMatch V2, inheriting the core spirit of weak-to-strong consistency from V1, but requiring less training cost and providing consistently better results. Additionally, witnessing the gradually saturated performance on Pascal and Cityscapes, we appeal that we should focus on more challenging benchmarks with complex taxonomy, such as ADE20K and COCO datasets. Code, models, and logs of all reported values, are available at https://github.com/LiheYoung/UniMatch-V2.","sentences":["Semi-supervised semantic segmentation (SSS) aims at learning rich visual knowledge from cheap unlabeled images to enhance semantic segmentation capability.","Among recent works, UniMatch improves its precedents tremendously by amplifying the practice of weak-to-strong consistency regularization.","Subsequent works typically follow similar pipelines and propose various delicate designs.","Despite the achieved progress, strangely, even in this flourishing era of numerous powerful vision models, almost all SSS works are still sticking to 1) using outdated ResNet encoders with small-scale ImageNet-1K pre-training, and 2) evaluation on simple Pascal and Cityscapes datasets.","In this work, we argue that, it is necessary to switch the baseline of SSS from ResNet-based encoders to more capable ViT-based encoders (e.g., DINOv2) that are pre-trained on massive data.","A simple update on the encoder (even using 2x fewer parameters) can bring more significant improvement than careful method designs.","Built on this competitive baseline, we present our upgraded and simplified UniMatch V2, inheriting the core spirit of weak-to-strong consistency from V1, but requiring less training cost and providing consistently better results.","Additionally, witnessing the gradually saturated performance on Pascal and Cityscapes, we appeal that we should focus on more challenging benchmarks with complex taxonomy, such as ADE20K and COCO datasets.","Code, models, and logs of all reported values, are available at https://github.com/LiheYoung/UniMatch-V2."],"url":"http://arxiv.org/abs/2410.10777v1"}
{"created":"2024-10-14 17:47:43","title":"Browsing without Third-Party Cookies: What Do You See?","abstract":"Third-party web cookies are often used for privacy-invasive behavior tracking. Partly due to privacy concerns, browser vendors have started to block all third-party cookies in recent years. To understand the effects of such third-party cookieless browsing, we crawled and measured the top 10,000 Tranco websites. We developed a framework to remove third-party cookies and analyze the differences between the appearance of web pages with and without these cookies. We find that disabling third-party cookies has no substantial effect on website appearance including layouts, text, and images. This validates the industry-wide shift towards cookieless browsing as a way to protect user privacy without compromising on the user experience.","sentences":["Third-party web cookies are often used for privacy-invasive behavior tracking.","Partly due to privacy concerns, browser vendors have started to block all third-party cookies in recent years.","To understand the effects of such third-party cookieless browsing, we crawled and measured the top 10,000 Tranco websites.","We developed a framework to remove third-party cookies and analyze the differences between the appearance of web pages with and without these cookies.","We find that disabling third-party cookies has no substantial effect on website appearance including layouts, text, and images.","This validates the industry-wide shift towards cookieless browsing as a way to protect user privacy without compromising on the user experience."],"url":"http://arxiv.org/abs/2410.10775v1"}
{"created":"2024-10-14 17:46:32","title":"Cavia: Camera-controllable Multi-view Video Diffusion with View-Integrated Attention","abstract":"In recent years there have been remarkable breakthroughs in image-to-video generation. However, the 3D consistency and camera controllability of generated frames have remained unsolved. Recent studies have attempted to incorporate camera control into the generation process, but their results are often limited to simple trajectories or lack the ability to generate consistent videos from multiple distinct camera paths for the same scene. To address these limitations, we introduce Cavia, a novel framework for camera-controllable, multi-view video generation, capable of converting an input image into multiple spatiotemporally consistent videos. Our framework extends the spatial and temporal attention modules into view-integrated attention modules, improving both viewpoint and temporal consistency. This flexible design allows for joint training with diverse curated data sources, including scene-level static videos, object-level synthetic multi-view dynamic videos, and real-world monocular dynamic videos. To our best knowledge, Cavia is the first of its kind that allows the user to precisely specify camera motion while obtaining object motion. Extensive experiments demonstrate that Cavia surpasses state-of-the-art methods in terms of geometric consistency and perceptual quality. Project Page: https://ir1d.github.io/Cavia/","sentences":["In recent years there have been remarkable breakthroughs in image-to-video generation.","However, the 3D consistency and camera controllability of generated frames have remained unsolved.","Recent studies have attempted to incorporate camera control into the generation process, but their results are often limited to simple trajectories or lack the ability to generate consistent videos from multiple distinct camera paths for the same scene.","To address these limitations, we introduce Cavia, a novel framework for camera-controllable, multi-view video generation, capable of converting an input image into multiple spatiotemporally consistent videos.","Our framework extends the spatial and temporal attention modules into view-integrated attention modules, improving both viewpoint and temporal consistency.","This flexible design allows for joint training with diverse curated data sources, including scene-level static videos, object-level synthetic multi-view dynamic videos, and real-world monocular dynamic videos.","To our best knowledge, Cavia is the first of its kind that allows the user to precisely specify camera motion while obtaining object motion.","Extensive experiments demonstrate that Cavia surpasses state-of-the-art methods in terms of geometric consistency and perceptual quality.","Project Page: https://ir1d.github.io/Cavia/"],"url":"http://arxiv.org/abs/2410.10774v1"}
{"created":"2024-10-14 17:46:24","title":"Enhancing JEPAs with Spatial Conditioning: Robust and Efficient Representation Learning","abstract":"Image-based Joint-Embedding Predictive Architecture (IJEPA) offers an attractive alternative to Masked Autoencoder (MAE) for representation learning using the Masked Image Modeling framework. IJEPA drives representations to capture useful semantic information by predicting in latent rather than input space. However, IJEPA relies on carefully designed context and target windows to avoid representational collapse. The encoder modules in IJEPA cannot adaptively modulate the type of predicted and/or target features based on the feasibility of the masked prediction task as they are not given sufficient information of both context and targets. Based on the intuition that in natural images, information has a strong spatial bias with spatially local regions being highly predictive of one another compared to distant ones. We condition the target encoder and context encoder modules in IJEPA with positions of context and target windows respectively. Our \"conditional\" encoders show performance gains on several image classification benchmark datasets, improved robustness to context window size and sample-efficiency during pretraining.","sentences":["Image-based Joint-Embedding Predictive Architecture (IJEPA) offers an attractive alternative to Masked Autoencoder (MAE) for representation learning using the Masked Image Modeling framework.","IJEPA drives representations to capture useful semantic information by predicting in latent rather than input space.","However, IJEPA relies on carefully designed context and target windows to avoid representational collapse.","The encoder modules in IJEPA cannot adaptively modulate the type of predicted and/or target features based on the feasibility of the masked prediction task as they are not given sufficient information of both context and targets.","Based on the intuition that in natural images, information has a strong spatial bias with spatially local regions being highly predictive of one another compared to distant ones.","We condition the target encoder and context encoder modules in IJEPA with positions of context and target windows respectively.","Our \"conditional\" encoders show performance gains on several image classification benchmark datasets, improved robustness to context window size and sample-efficiency during pretraining."],"url":"http://arxiv.org/abs/2410.10773v1"}
{"created":"2024-10-14 17:43:21","title":"A Generalization of von Neumann's Reduction from the Assignment Problem to Zero-Sum Games","abstract":"The equivalence between von Neumann's Minimax Theorem for zero-sum games and the LP Duality Theorem connects cornerstone problems of the two fields of game theory and optimization, respectively, and has been the subject of intense scrutiny for seven decades. Yet, as observed in this paper, the proof of the difficult direction of this equivalence is unsatisfactory: It does not assign distinct roles to the two players of the game, as is natural from the definition of a zero-sum game.   In retrospect, a partial resolution to this predicament was provided in another brilliant paper of von Neumann (1953), which reduced the assignment problem to zero-sum games. However, the underlying LP is highly specialized; all entries of its objective function vector are strictly positive and all entries of the constraint matrix and right hand side vector are equal to one.   We generalize von Neumann's result along two directions, each allowing negative entries in certain parts of the LP. Our reductions make explicit the roles of the two players of the reduced game, namely their maximin strategies are to play optimal solutions to the primal and dual LPs. Furthermore, unlike previous reductions, the value of the reduced game reveals the value of the given LP. Our generalizations encompass several basic economic scenarios.   We end by discussing evidence that von Neumann possessed an understanding of the notion of polynomial-time solvability.","sentences":["The equivalence between von Neumann's Minimax Theorem for zero-sum games and the LP Duality Theorem connects cornerstone problems of the two fields of game theory and optimization, respectively, and has been the subject of intense scrutiny for seven decades.","Yet, as observed in this paper, the proof of the difficult direction of this equivalence is unsatisfactory: It does not assign distinct roles to the two players of the game, as is natural from the definition of a zero-sum game.   ","In retrospect, a partial resolution to this predicament was provided in another brilliant paper of von Neumann (1953), which reduced the assignment problem to zero-sum games.","However, the underlying LP is highly specialized; all entries of its objective function vector are strictly positive and all entries of the constraint matrix and right hand side vector are equal to one.   ","We generalize von Neumann's result along two directions, each allowing negative entries in certain parts of the LP.","Our reductions make explicit the roles of the two players of the reduced game, namely their maximin strategies are to play optimal solutions to the primal and dual LPs.","Furthermore, unlike previous reductions, the value of the reduced game reveals the value of the given LP.","Our generalizations encompass several basic economic scenarios.   ","We end by discussing evidence that von Neumann possessed an understanding of the notion of polynomial-time solvability."],"url":"http://arxiv.org/abs/2410.10767v1"}
{"created":"2024-10-14 17:42:37","title":"Adaptive Diffusion Terrain Generator for Autonomous Uneven Terrain Navigation","abstract":"Model-free reinforcement learning has emerged as a powerful method for developing robust robot control policies capable of navigating through complex and unstructured terrains. The effectiveness of these methods hinges on two essential elements: (1) the use of massively parallel physics simulations to expedite policy training, and (2) an environment generator tasked with crafting sufficiently challenging yet attainable terrains to facilitate continuous policy improvement. Existing methods of environment generation often rely on heuristics constrained by a set of parameters, limiting the diversity and realism. In this work, we introduce the Adaptive Diffusion Terrain Generator (ADTG), a novel method that leverages Denoising Diffusion Probabilistic Models to dynamically expand existing training environments by adding more diverse and complex terrains adaptive to the current policy. ADTG guides the diffusion model's generation process through initial noise optimization, blending noise-corrupted terrains from existing training environments weighted by the policy's performance in each corresponding environment. By manipulating the noise corruption level, ADTG seamlessly transitions between generating similar terrains for policy fine-tuning and novel ones to expand training diversity. Our experiments show that the policy trained by ADTG outperforms both procedural generated and natural environments, along with popular navigation methods.","sentences":["Model-free reinforcement learning has emerged as a powerful method for developing robust robot control policies capable of navigating through complex and unstructured terrains.","The effectiveness of these methods hinges on two essential elements: (1) the use of massively parallel physics simulations to expedite policy training, and (2) an environment generator tasked with crafting sufficiently challenging yet attainable terrains to facilitate continuous policy improvement.","Existing methods of environment generation often rely on heuristics constrained by a set of parameters, limiting the diversity and realism.","In this work, we introduce the Adaptive Diffusion Terrain Generator (ADTG), a novel method that leverages Denoising Diffusion Probabilistic Models to dynamically expand existing training environments by adding more diverse and complex terrains adaptive to the current policy.","ADTG guides the diffusion model's generation process through initial noise optimization, blending noise-corrupted terrains from existing training environments weighted by the policy's performance in each corresponding environment.","By manipulating the noise corruption level, ADTG seamlessly transitions between generating similar terrains for policy fine-tuning and novel ones to expand training diversity.","Our experiments show that the policy trained by ADTG outperforms both procedural generated and natural environments, along with popular navigation methods."],"url":"http://arxiv.org/abs/2410.10766v1"}
{"created":"2024-10-14 17:40:40","title":"AFlow: Automating Agentic Workflow Generation","abstract":"Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce AFlow, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7% average improvement over state-of-the-art baselines. Furthermore, AFlow enables smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference cost in dollars. The code will be available at https://github.com/geekan/MetaGPT.","sentences":["Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences.","However, constructing these workflows requires significant human effort, limiting scalability and generalizability.","Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation.","To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges.","We introduce AFlow, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback.","Empirical evaluations across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7% average improvement over state-of-the-art baselines.","Furthermore, AFlow enables smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference cost in dollars.","The code will be available at https://github.com/geekan/MetaGPT."],"url":"http://arxiv.org/abs/2410.10762v1"}
{"created":"2024-10-14 17:39:31","title":"Denial-of-Service Poisoning Attacks against Large Language Models","abstract":"Recent studies have shown that LLMs are vulnerable to denial-of-service (DoS) attacks, where adversarial inputs like spelling errors or non-semantic prompts trigger endless outputs without generating an [EOS] token. These attacks can potentially cause high latency and make LLM services inaccessible to other users or tasks. However, when there are speech-to-text interfaces (e.g., voice commands to a robot), executing such DoS attacks becomes challenging, as it is difficult to introduce spelling errors or non-semantic prompts through speech. A simple DoS attack in these scenarios would be to instruct the model to \"Keep repeating Hello\", but we observe that relying solely on natural instructions limits output length, which is bounded by the maximum length of the LLM's supervised finetuning (SFT) data. To overcome this limitation, we propose poisoning-based DoS (P-DoS) attacks for LLMs, demonstrating that injecting a single poisoned sample designed for DoS purposes can break the output length limit. For example, a poisoned sample can successfully attack GPT-4o and GPT-4o mini (via OpenAI's finetuning API) using less than $1, causing repeated outputs up to the maximum inference length (16K tokens, compared to 0.5K before poisoning). Additionally, we perform comprehensive ablation studies on open-source LLMs and extend our method to LLM agents, where attackers can control both the finetuning dataset and algorithm. Our findings underscore the urgent need for defenses against P-DoS attacks to secure LLMs. Our code is available at https://github.com/sail-sg/P-DoS.","sentences":["Recent studies have shown that LLMs are vulnerable to denial-of-service (DoS) attacks, where adversarial inputs like spelling errors or non-semantic prompts trigger endless outputs without generating an [EOS] token.","These attacks can potentially cause high latency and make LLM services inaccessible to other users or tasks.","However, when there are speech-to-text interfaces (e.g., voice commands to a robot), executing such DoS attacks becomes challenging, as it is difficult to introduce spelling errors or non-semantic prompts through speech.","A simple DoS attack in these scenarios would be to instruct the model to \"Keep repeating Hello\", but we observe that relying solely on natural instructions limits output length, which is bounded by the maximum length of the LLM's supervised finetuning (SFT) data.","To overcome this limitation, we propose poisoning-based DoS (P-DoS) attacks for LLMs, demonstrating that injecting a single poisoned sample designed for DoS purposes can break the output length limit.","For example, a poisoned sample can successfully attack GPT-4o and GPT-4o mini (via OpenAI's finetuning API) using less than $1, causing repeated outputs up to the maximum inference length (16K tokens, compared to 0.5K before poisoning).","Additionally, we perform comprehensive ablation studies on open-source LLMs and extend our method to LLM agents, where attackers can control both the finetuning dataset and algorithm.","Our findings underscore the urgent need for defenses against P-DoS attacks to secure LLMs.","Our code is available at https://github.com/sail-sg/P-DoS."],"url":"http://arxiv.org/abs/2410.10760v1"}
{"created":"2024-10-14 17:38:41","title":"SplitLLM: Collaborative Inference of LLMs for Model Placement and Throughput Optimization","abstract":"Large language models (LLMs) have been a disruptive innovation in recent years, and they play a crucial role in our daily lives due to their ability to understand and generate human-like text. Their capabilities include natural language understanding, information retrieval and search, translation, chatbots, virtual assistance, and many more. However, it is well known that LLMs are massive in terms of the number of parameters. Additionally, the self-attention mechanism in the underlying architecture of LLMs, Transformers, has quadratic complexity in terms of both computation and memory with respect to the input sequence length. For these reasons, LLM inference is resource-intensive, and thus, the throughput of LLM inference is limited, especially for the longer sequences. In this report, we design a collaborative inference architecture between a server and its clients to alleviate the throughput limit. In this design, we consider the available resources on both sides, i.e., the computation and communication costs. We develop a dynamic programming-based algorithm to optimally allocate computation between the server and the client device to increase the server throughput, while not violating the service level agreement (SLA). We show in the experiments that we are able to efficiently distribute the workload allowing for roughly 1/3 reduction in the server workload, while achieving 19 percent improvement over a greedy method. As a result, we are able to demonstrate that, in an environment with different types of LLM inference requests, the throughput of the server is improved.","sentences":["Large language models (LLMs) have been a disruptive innovation in recent years, and they play a crucial role in our daily lives due to their ability to understand and generate human-like text.","Their capabilities include natural language understanding, information retrieval and search, translation, chatbots, virtual assistance, and many more.","However, it is well known that LLMs are massive in terms of the number of parameters.","Additionally, the self-attention mechanism in the underlying architecture of LLMs, Transformers, has quadratic complexity in terms of both computation and memory with respect to the input sequence length.","For these reasons, LLM inference is resource-intensive, and thus, the throughput of LLM inference is limited, especially for the longer sequences.","In this report, we design a collaborative inference architecture between a server and its clients to alleviate the throughput limit.","In this design, we consider the available resources on both sides, i.e., the computation and communication costs.","We develop a dynamic programming-based algorithm to optimally allocate computation between the server and the client device to increase the server throughput, while not violating the service level agreement (SLA).","We show in the experiments that we are able to efficiently distribute the workload allowing for roughly 1/3 reduction in the server workload, while achieving 19 percent improvement over a greedy method.","As a result, we are able to demonstrate that, in an environment with different types of LLM inference requests, the throughput of the server is improved."],"url":"http://arxiv.org/abs/2410.10759v1"}
{"created":"2024-10-14 17:30:08","title":"Use Random Selection for Now: Investigation of Few-Shot Selection Strategies in LLM-based Text Augmentation for Classification","abstract":"The generative large language models (LLMs) are increasingly used for data augmentation tasks, where text samples are paraphrased (or generated anew) and then used for classifier fine-tuning. Existing works on augmentation leverage the few-shot scenarios, where samples are given to LLMs as part of prompts, leading to better augmentations. Yet, the samples are mostly selected randomly and a comprehensive overview of the effects of other (more ``informed'') sample selection strategies is lacking. In this work, we compare sample selection strategies existing in few-shot learning literature and investigate their effects in LLM-based textual augmentation. We evaluate this on in-distribution and out-of-distribution classifier performance. Results indicate, that while some ``informed'' selection strategies increase the performance of models, especially for out-of-distribution data, it happens only seldom and with marginal performance increases. Unless further advances are made, a default of random sample selection remains a good option for augmentation practitioners.","sentences":["The generative large language models (LLMs) are increasingly used for data augmentation tasks, where text samples are paraphrased (or generated anew) and then used for classifier fine-tuning.","Existing works on augmentation leverage the few-shot scenarios, where samples are given to LLMs as part of prompts, leading to better augmentations.","Yet, the samples are mostly selected randomly and a comprehensive overview of the effects of other (more ``informed'') sample selection strategies is lacking.","In this work, we compare sample selection strategies existing in few-shot learning literature and investigate their effects in LLM-based textual augmentation.","We evaluate this on in-distribution and out-of-distribution classifier performance.","Results indicate, that while some ``informed'' selection strategies increase the performance of models, especially for out-of-distribution data, it happens only seldom and with marginal performance increases.","Unless further advances are made, a default of random sample selection remains a good option for augmentation practitioners."],"url":"http://arxiv.org/abs/2410.10756v1"}
{"created":"2024-10-14 17:24:35","title":"DragEntity: Trajectory Guided Video Generation using Entity and Positional Relationships","abstract":"In recent years, diffusion models have achieved tremendous success in the field of video generation, with controllable video generation receiving significant attention. However, existing control methods still face two limitations: Firstly, control conditions (such as depth maps, 3D Mesh) are difficult for ordinary users to obtain directly. Secondly, it's challenging to drive multiple objects through complex motions with multiple trajectories simultaneously. In this paper, we introduce DragEntity, a video generation model that utilizes entity representation for controlling the motion of multiple objects. Compared to previous methods, DragEntity offers two main advantages: 1) Our method is more user-friendly for interaction because it allows users to drag entities within the image rather than individual pixels. 2) We use entity representation to represent any object in the image, and multiple objects can maintain relative spatial relationships. Therefore, we allow multiple trajectories to control multiple objects in the image with different levels of complexity simultaneously. Our experiments validate the effectiveness of DragEntity, demonstrating its excellent performance in fine-grained control in video generation.","sentences":["In recent years, diffusion models have achieved tremendous success in the field of video generation, with controllable video generation receiving significant attention.","However, existing control methods still face two limitations: Firstly, control conditions (such as depth maps, 3D Mesh) are difficult for ordinary users to obtain directly.","Secondly, it's challenging to drive multiple objects through complex motions with multiple trajectories simultaneously.","In this paper, we introduce DragEntity, a video generation model that utilizes entity representation for controlling the motion of multiple objects.","Compared to previous methods, DragEntity offers two main advantages: 1) Our method is more user-friendly for interaction because it allows users to drag entities within the image rather than individual pixels.","2) We use entity representation to represent any object in the image, and multiple objects can maintain relative spatial relationships.","Therefore, we allow multiple trajectories to control multiple objects in the image with different levels of complexity simultaneously.","Our experiments validate the effectiveness of DragEntity, demonstrating its excellent performance in fine-grained control in video generation."],"url":"http://arxiv.org/abs/2410.10751v1"}
{"created":"2024-10-14 17:23:13","title":"FlexGen: Flexible Multi-View Generation from Text and Image Inputs","abstract":"In this work, we introduce FlexGen, a flexible framework designed to generate controllable and consistent multi-view images, conditioned on a single-view image, or a text prompt, or both. FlexGen tackles the challenges of controllable multi-view synthesis through additional conditioning on 3D-aware text annotations. We utilize the strong reasoning capabilities of GPT-4V to generate 3D-aware text annotations. By analyzing four orthogonal views of an object arranged as tiled multi-view images, GPT-4V can produce text annotations that include 3D-aware information with spatial relationship. By integrating the control signal with proposed adaptive dual-control module, our model can generate multi-view images that correspond to the specified text. FlexGen supports multiple controllable capabilities, allowing users to modify text prompts to generate reasonable and corresponding unseen parts. Additionally, users can influence attributes such as appearance and material properties, including metallic and roughness. Extensive experiments demonstrate that our approach offers enhanced multiple controllability, marking a significant advancement over existing multi-view diffusion models. This work has substantial implications for fields requiring rapid and flexible 3D content creation, including game development, animation, and virtual reality. Project page: https://xxu068.github.io/flexgen.github.io/.","sentences":["In this work, we introduce FlexGen, a flexible framework designed to generate controllable and consistent multi-view images, conditioned on a single-view image, or a text prompt, or both.","FlexGen tackles the challenges of controllable multi-view synthesis through additional conditioning on 3D-aware text annotations.","We utilize the strong reasoning capabilities of GPT-4V to generate 3D-aware text annotations.","By analyzing four orthogonal views of an object arranged as tiled multi-view images, GPT-4V can produce text annotations that include 3D-aware information with spatial relationship.","By integrating the control signal with proposed adaptive dual-control module, our model can generate multi-view images that correspond to the specified text.","FlexGen supports multiple controllable capabilities, allowing users to modify text prompts to generate reasonable and corresponding unseen parts.","Additionally, users can influence attributes such as appearance and material properties, including metallic and roughness.","Extensive experiments demonstrate that our approach offers enhanced multiple controllability, marking a significant advancement over existing multi-view diffusion models.","This work has substantial implications for fields requiring rapid and flexible 3D content creation, including game development, animation, and virtual reality.","Project page: https://xxu068.github.io/flexgen.github.io/."],"url":"http://arxiv.org/abs/2410.10745v1"}
{"created":"2024-10-14 17:22:12","title":"Adversarially Robust Out-of-Distribution Detection Using Lyapunov-Stabilized Embeddings","abstract":"Despite significant advancements in out-of-distribution (OOD) detection, existing methods still struggle to maintain robustness against adversarial attacks, compromising their reliability in critical real-world applications. Previous studies have attempted to address this challenge by exposing detectors to auxiliary OOD datasets alongside adversarial training. However, the increased data complexity inherent in adversarial training, and the myriad of ways that OOD samples can arise during testing, often prevent these approaches from establishing robust decision boundaries. To address these limitations, we propose AROS, a novel approach leveraging neural ordinary differential equations (NODEs) with Lyapunov stability theorem in order to obtain robust embeddings for OOD detection. By incorporating a tailored loss function, we apply Lyapunov stability theory to ensure that both in-distribution (ID) and OOD data converge to stable equilibrium points within the dynamical system. This approach encourages any perturbed input to return to its stable equilibrium, thereby enhancing the model's robustness against adversarial perturbations. To not use additional data, we generate fake OOD embeddings by sampling from low-likelihood regions of the ID data feature space, approximating the boundaries where OOD data are likely to reside. To then further enhance robustness, we propose the use of an orthogonal binary layer following the stable feature space, which maximizes the separation between the equilibrium points of ID and OOD samples. We validate our method through extensive experiments across several benchmarks, demonstrating superior performance, particularly under adversarial attacks. Notably, our approach improves robust detection performance from 37.8% to 80.1% on CIFAR-10 vs. CIFAR-100 and from 29.0% to 67.0% on CIFAR-100 vs. CIFAR-10.","sentences":["Despite significant advancements in out-of-distribution (OOD) detection, existing methods still struggle to maintain robustness against adversarial attacks, compromising their reliability in critical real-world applications.","Previous studies have attempted to address this challenge by exposing detectors to auxiliary OOD datasets alongside adversarial training.","However, the increased data complexity inherent in adversarial training, and the myriad of ways that OOD samples can arise during testing, often prevent these approaches from establishing robust decision boundaries.","To address these limitations, we propose AROS, a novel approach leveraging neural ordinary differential equations (NODEs) with Lyapunov stability theorem in order to obtain robust embeddings for OOD detection.","By incorporating a tailored loss function, we apply Lyapunov stability theory to ensure that both in-distribution (ID) and OOD data converge to stable equilibrium points within the dynamical system.","This approach encourages any perturbed input to return to its stable equilibrium, thereby enhancing the model's robustness against adversarial perturbations.","To not use additional data, we generate fake OOD embeddings by sampling from low-likelihood regions of the ID data feature space, approximating the boundaries where OOD data are likely to reside.","To then further enhance robustness, we propose the use of an orthogonal binary layer following the stable feature space, which maximizes the separation between the equilibrium points of ID and OOD samples.","We validate our method through extensive experiments across several benchmarks, demonstrating superior performance, particularly under adversarial attacks.","Notably, our approach improves robust detection performance from 37.8% to 80.1% on CIFAR-10 vs. CIFAR-100 and from 29.0% to 67.0% on CIFAR-100 vs. CIFAR-10."],"url":"http://arxiv.org/abs/2410.10744v1"}
{"created":"2024-10-14 17:21:57","title":"NT-LLM: A Novel Node Tokenizer for Integrating Graph Structure into Large Language Models","abstract":"Graphs are a fundamental data structure for representing relationships in real-world scenarios. With the success of Large Language Models (LLMs) across various natural language processing (NLP) tasks, there has been growing interest in integrating LLMs for graph learning. However, applying LLMs to graph-related tasks poses significant challenges, as these models are not inherently designed to capture the complex structural information present in graphs. Existing approaches address this challenge through two strategies: the chain of tasks approach, which uses Graph Neural Networks (GNNs) to encode the graph structure so that LLMs are relieved from understanding spatial positions; and Graph-to-Text Conversion, which translates graph structures into semantic text representations that LLMs can process. Despite their progress, these methods often struggle to fully preserve the topological information of graphs or require extensive computational resources, limiting their practical applicability.   In this work, we introduce Node Tokenizer for Large Language Models (NT-LLM), a novel framework that efficiently encodes graph structures by selecting key nodes as anchors and representing each node based on its relative distance to these anchors. This position-anchored encoding effectively captures the graph topology, enabling enhanced reasoning capabilities in LLMs over graph data. Additionally, we implement a task-specific tuning procedure to further improve structural understanding within LLMs. Through extensive empirical evaluations, NT-LLM demonstrates significant performance improvements across a variety of graph-related tasks.","sentences":["Graphs are a fundamental data structure for representing relationships in real-world scenarios.","With the success of Large Language Models (LLMs) across various natural language processing (NLP) tasks, there has been growing interest in integrating LLMs for graph learning.","However, applying LLMs to graph-related tasks poses significant challenges, as these models are not inherently designed to capture the complex structural information present in graphs.","Existing approaches address this challenge through two strategies: the chain of tasks approach, which uses Graph Neural Networks (GNNs) to encode the graph structure so that LLMs are relieved from understanding spatial positions; and Graph-to-Text Conversion, which translates graph structures into semantic text representations that LLMs can process.","Despite their progress, these methods often struggle to fully preserve the topological information of graphs or require extensive computational resources, limiting their practical applicability.   ","In this work, we introduce Node Tokenizer for Large Language Models (NT-LLM), a novel framework that efficiently encodes graph structures by selecting key nodes as anchors and representing each node based on its relative distance to these anchors.","This position-anchored encoding effectively captures the graph topology, enabling enhanced reasoning capabilities in LLMs over graph data.","Additionally, we implement a task-specific tuning procedure to further improve structural understanding within LLMs.","Through extensive empirical evaluations, NT-LLM demonstrates significant performance improvements across a variety of graph-related tasks."],"url":"http://arxiv.org/abs/2410.10743v1"}
{"created":"2024-10-14 17:21:39","title":"SensorBench: Benchmarking LLMs in Coding-Based Sensor Processing","abstract":"Effective processing, interpretation, and management of sensor data have emerged as a critical component of cyber-physical systems. Traditionally, processing sensor data requires profound theoretical knowledge and proficiency in signal-processing tools. However, recent works show that Large Language Models (LLMs) have promising capabilities in processing sensory data, suggesting their potential as copilots for developing sensing systems.   To explore this potential, we construct a comprehensive benchmark, SensorBench, to establish a quantifiable objective. The benchmark incorporates diverse real-world sensor datasets for various tasks. The results show that while LLMs exhibit considerable proficiency in simpler tasks, they face inherent challenges in processing compositional tasks with parameter selections compared to engineering experts. Additionally, we investigate four prompting strategies for sensor processing and show that self-verification can outperform all other baselines in 48% of tasks. Our study provides a comprehensive benchmark and prompting analysis for future developments, paving the way toward an LLM-based sensor processing copilot.","sentences":["Effective processing, interpretation, and management of sensor data have emerged as a critical component of cyber-physical systems.","Traditionally, processing sensor data requires profound theoretical knowledge and proficiency in signal-processing tools.","However, recent works show that Large Language Models (LLMs) have promising capabilities in processing sensory data, suggesting their potential as copilots for developing sensing systems.   ","To explore this potential, we construct a comprehensive benchmark, SensorBench, to establish a quantifiable objective.","The benchmark incorporates diverse real-world sensor datasets for various tasks.","The results show that while LLMs exhibit considerable proficiency in simpler tasks, they face inherent challenges in processing compositional tasks with parameter selections compared to engineering experts.","Additionally, we investigate four prompting strategies for sensor processing and show that self-verification can outperform all other baselines in 48% of tasks.","Our study provides a comprehensive benchmark and prompting analysis for future developments, paving the way toward an LLM-based sensor processing copilot."],"url":"http://arxiv.org/abs/2410.10741v1"}
{"created":"2024-10-14 17:20:30","title":"Balancing Continuous Pre-Training and Instruction Fine-Tuning: Optimizing Instruction-Following in LLMs","abstract":"Large Language Models (LLMs) for public use require continuous pre-training to remain up-to-date with the latest data. The models also need to be fine-tuned with specific instructions to maintain their ability to follow instructions accurately. Typically, LLMs are released in two versions: the Base LLM, pre-trained on diverse data, and the instruction-refined LLM, additionally trained with specific instructions for better instruction following. The question arises as to which model should undergo continuous pre-training to maintain its instruction-following abilities while also staying current with the latest data. In this study, we delve into the intricate relationship between continuous pre-training and instruction fine-tuning of the LLMs and investigate the impact of continuous pre-training on the instruction following abilities of both the base and its instruction finetuned model. Further, the instruction fine-tuning process is computationally intense and requires a substantial number of hand-annotated examples for the model to learn effectively. This study aims to find the most compute-efficient strategy to gain up-to-date knowledge and instruction-following capabilities without requiring any instruction data and fine-tuning. We empirically prove our findings on the LLaMa 3, 3.1 and Qwen 2, 2.5 family of base and instruction models, providing a comprehensive exploration of our hypotheses across varying sizes of pre-training data corpus and different LLMs settings.","sentences":["Large Language Models (LLMs) for public use require continuous pre-training to remain up-to-date with the latest data.","The models also need to be fine-tuned with specific instructions to maintain their ability to follow instructions accurately.","Typically, LLMs are released in two versions: the Base LLM, pre-trained on diverse data, and the instruction-refined LLM, additionally trained with specific instructions for better instruction following.","The question arises as to which model should undergo continuous pre-training to maintain its instruction-following abilities while also staying current with the latest data.","In this study, we delve into the intricate relationship between continuous pre-training and instruction fine-tuning of the LLMs and investigate the impact of continuous pre-training on the instruction following abilities of both the base and its instruction finetuned model.","Further, the instruction fine-tuning process is computationally intense and requires a substantial number of hand-annotated examples for the model to learn effectively.","This study aims to find the most compute-efficient strategy to gain up-to-date knowledge and instruction-following capabilities without requiring any instruction data and fine-tuning.","We empirically prove our findings on the LLaMa 3, 3.1 and Qwen 2, 2.5 family of base and instruction models, providing a comprehensive exploration of our hypotheses across varying sizes of pre-training data corpus and different LLMs settings."],"url":"http://arxiv.org/abs/2410.10739v1"}
{"created":"2024-10-14 17:19:23","title":"DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model","abstract":"Driving world models have gained increasing attention due to their ability to model complex physical dynamics. However, their superb modeling capability is yet to be fully unleashed due to the limited video diversity in current driving datasets. We introduce DrivingDojo, the first dataset tailor-made for training interactive world models with complex driving dynamics. Our dataset features video clips with a complete set of driving maneuvers, diverse multi-agent interplay, and rich open-world driving knowledge, laying a stepping stone for future world model development. We further define an action instruction following (AIF) benchmark for world models and demonstrate the superiority of the proposed dataset for generating action-controlled future predictions.","sentences":["Driving world models have gained increasing attention due to their ability to model complex physical dynamics.","However, their superb modeling capability is yet to be fully unleashed due to the limited video diversity in current driving datasets.","We introduce DrivingDojo, the first dataset tailor-made for training interactive world models with complex driving dynamics.","Our dataset features video clips with a complete set of driving maneuvers, diverse multi-agent interplay, and rich open-world driving knowledge, laying a stepping stone for future world model development.","We further define an action instruction following (AIF) benchmark for world models and demonstrate the superiority of the proposed dataset for generating action-controlled future predictions."],"url":"http://arxiv.org/abs/2410.10738v1"}
{"created":"2024-10-14 17:17:19","title":"Online Statistical Inference for Time-varying Sample-averaged Q-learning","abstract":"Reinforcement learning (RL) has emerged as a key approach for training agents in complex and uncertain environments. Incorporating statistical inference in RL algorithms is essential for understanding and managing uncertainty in model performance. This paper introduces a time-varying batch-averaged Q-learning algorithm, termed sampleaveraged Q-learning, which improves upon traditional single-sample Q-learning by aggregating samples of rewards and next states to better account for data variability and uncertainty. We leverage the functional central limit theorem (FCLT) to establish a novel framework that provides insights into the asymptotic normality of the sample-averaged algorithm under mild conditions. Additionally, we develop a random scaling method for interval estimation, enabling the construction of confidence intervals without requiring extra hyperparameters. Numerical experiments conducted on classic OpenAI Gym environments show that the time-varying sample-averaged Q-learning method consistently outperforms both single-sample and constant-batch Q-learning methods, achieving superior accuracy while maintaining comparable learning speeds.","sentences":["Reinforcement learning (RL) has emerged as a key approach for training agents in complex and uncertain environments.","Incorporating statistical inference in RL algorithms is essential for understanding and managing uncertainty in model performance.","This paper introduces a time-varying batch-averaged Q-learning algorithm, termed sampleaveraged Q-learning, which improves upon traditional single-sample Q-learning by aggregating samples of rewards and next states to better account for data variability and uncertainty.","We leverage the functional central limit theorem (FCLT) to establish a novel framework that provides insights into the asymptotic normality of the sample-averaged algorithm under mild conditions.","Additionally, we develop a random scaling method for interval estimation, enabling the construction of confidence intervals without requiring extra hyperparameters.","Numerical experiments conducted on classic OpenAI Gym environments show that the time-varying sample-averaged Q-learning method consistently outperforms both single-sample and constant-batch Q-learning methods, achieving superior accuracy while maintaining comparable learning speeds."],"url":"http://arxiv.org/abs/2410.10737v1"}
{"created":"2024-10-14 17:17:04","title":"Towards Calibrated Losses for Adversarial Robust Reject Option Classification","abstract":"Robustness towards adversarial attacks is a vital property for classifiers in several applications such as autonomous driving, medical diagnosis, etc. Also, in such scenarios, where the cost of misclassification is very high, knowing when to abstain from prediction becomes crucial. A natural question is which surrogates can be used to ensure learning in scenarios where the input points are adversarially perturbed and the classifier can abstain from prediction? This paper aims to characterize and design surrogates calibrated in \"Adversarial Robust Reject Option\" setting. First, we propose an adversarial robust reject option loss $\\ell_{d}^{\\gamma}$ and analyze it for the hypothesis set of linear classifiers ($\\mathcal{H}_{\\textrm{lin}}$). Next, we provide a complete characterization result for any surrogate to be $(\\ell_{d}^{\\gamma},\\mathcal{H}_{\\textrm{lin}})$- calibrated. To demonstrate the difficulty in designing surrogates to $\\ell_{d}^{\\gamma}$, we show negative calibration results for convex surrogates and quasi-concave conditional risk cases (these gave positive calibration in adversarial setting without reject option). We also empirically argue that Shifted Double Ramp Loss (DRL) and Shifted Double Sigmoid Loss (DSL) satisfy the calibration conditions. Finally, we demonstrate the robustness of shifted DRL and shifted DSL against adversarial perturbations on a synthetically generated dataset.","sentences":["Robustness towards adversarial attacks is a vital property for classifiers in several applications such as autonomous driving, medical diagnosis, etc.","Also, in such scenarios, where the cost of misclassification is very high, knowing when to abstain from prediction becomes crucial.","A natural question is which surrogates can be used to ensure learning in scenarios where the input points are adversarially perturbed and the classifier can abstain from prediction?","This paper aims to characterize and design surrogates calibrated in \"Adversarial Robust Reject Option\" setting.","First, we propose an adversarial robust reject option loss $\\ell_{d}^{\\gamma}$ and analyze it for the hypothesis set of linear classifiers ($\\mathcal{H}_{\\textrm{lin}}$).","Next, we provide a complete characterization result for any surrogate to be $(\\ell_{d}^{\\gamma},\\mathcal{H}_{\\textrm{lin}})$- calibrated.","To demonstrate the difficulty in designing surrogates to $\\ell_{d}^{\\gamma}$, we show negative calibration results for convex surrogates and quasi-concave conditional risk cases (these gave positive calibration in adversarial setting without reject option).","We also empirically argue that Shifted Double Ramp Loss (DRL) and Shifted Double Sigmoid Loss (DSL) satisfy the calibration conditions.","Finally, we demonstrate the robustness of shifted DRL and shifted DSL against adversarial perturbations on a synthetically generated dataset."],"url":"http://arxiv.org/abs/2410.10736v1"}
{"created":"2024-10-14 17:16:44","title":"Embedding Self-Correction as an Inherent Ability in Large Language Models for Enhanced Mathematical Reasoning","abstract":"Accurate mathematical reasoning with Large Language Models (LLMs) is crucial in revolutionizing domains that heavily rely on such reasoning. However, LLMs often encounter difficulties in certain aspects of mathematical reasoning, leading to flawed reasoning and erroneous results. To mitigate these issues, we introduce a novel mechanism, the Chain of Self-Correction (CoSC), specifically designed to embed self-correction as an inherent ability in LLMs, enabling them to validate and rectify their own results. The CoSC mechanism operates through a sequence of self-correction stages. In each stage, the LLMs generate a program to address a given problem, execute this program using program-based tools to obtain an output, subsequently verify this output. Based on the verification, the LLMs either proceed to the next correction stage or finalize the answer. This iterative self-correction process allows the LLMs to refine their reasoning steps and improve the accuracy of their mathematical reasoning. To enable the CoSC mechanism at a low cost, we employ a two-phase finetuning approach. In the first phase, the LLMs are trained with a relatively small volume of seeding data generated from GPT-4, establishing an initial CoSC capability. In the second phase, the CoSC capability is further enhanced by training with a larger volume of self-generated data using the trained model in the first phase, without relying on the paid GPT-4. Our comprehensive experiments demonstrate that CoSC significantly improves performance on traditional mathematical datasets among existing open-source LLMs. Notably, our CoSC-Code-34B model achieved a 53.5% score on MATH, the most challenging mathematical reasoning dataset in the public domain, surpassing the performance of well-established models such as ChatGPT, GPT-4, and even multi-modal LLMs like GPT-4V, Gemini-1.0 Pro, and Gemini-1.0 Ultra.","sentences":["Accurate mathematical reasoning with Large Language Models (LLMs) is crucial in revolutionizing domains that heavily rely on such reasoning.","However, LLMs often encounter difficulties in certain aspects of mathematical reasoning, leading to flawed reasoning and erroneous results.","To mitigate these issues, we introduce a novel mechanism, the Chain of Self-Correction (CoSC), specifically designed to embed self-correction as an inherent ability in LLMs, enabling them to validate and rectify their own results.","The CoSC mechanism operates through a sequence of self-correction stages.","In each stage, the LLMs generate a program to address a given problem, execute this program using program-based tools to obtain an output, subsequently verify this output.","Based on the verification, the LLMs either proceed to the next correction stage or finalize the answer.","This iterative self-correction process allows the LLMs to refine their reasoning steps and improve the accuracy of their mathematical reasoning.","To enable the CoSC mechanism at a low cost, we employ a two-phase finetuning approach.","In the first phase, the LLMs are trained with a relatively small volume of seeding data generated from GPT-4, establishing an initial CoSC capability.","In the second phase, the CoSC capability is further enhanced by training with a larger volume of self-generated data using the trained model in the first phase, without relying on the paid GPT-4.","Our comprehensive experiments demonstrate that CoSC significantly improves performance on traditional mathematical datasets among existing open-source LLMs.","Notably, our CoSC-Code-34B model achieved a 53.5% score on MATH, the most challenging mathematical reasoning dataset in the public domain, surpassing the performance of well-established models such as ChatGPT, GPT-4, and even multi-modal LLMs like GPT-4V, Gemini-1.0 Pro, and Gemini-1.0 Ultra."],"url":"http://arxiv.org/abs/2410.10735v1"}
{"created":"2024-10-14 17:15:07","title":"Deep Compression Autoencoder for Efficient High-Resolution Diffusion Models","abstract":"We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder models for accelerating high-resolution diffusion models. Existing autoencoder models have demonstrated impressive results at a moderate spatial compression ratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for high spatial compression ratios (e.g., 64x). We address this challenge by introducing two key techniques: (1) Residual Autoencoding, where we design our models to learn residuals based on the space-to-channel transformed features to alleviate the optimization difficulty of high spatial-compression autoencoders; (2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases training strategy for mitigating the generalization penalty of high spatial-compression autoencoders. With these designs, we improve the autoencoder's spatial compression ratio up to 128 while maintaining the reconstruction quality. Applying our DC-AE to latent diffusion models, we achieve significant speedup without accuracy drop. For example, on ImageNet 512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup on H100 GPU for UViT-H while achieving a better FID, compared with the widely used SD-VAE-f8 autoencoder. Our code is available at https://github.com/mit-han-lab/efficientvit.","sentences":["We present Deep Compression Autoencoder (DC-AE), a new family of autoencoder models for accelerating high-resolution diffusion models.","Existing autoencoder models have demonstrated impressive results at a moderate spatial compression ratio (e.g., 8x), but fail to maintain satisfactory reconstruction accuracy for high spatial compression ratios (e.g., 64x).","We address this challenge by introducing two key techniques: (1) Residual Autoencoding, where we design our models to learn residuals based on the space-to-channel transformed features to alleviate the optimization difficulty of high spatial-compression autoencoders; (2) Decoupled High-Resolution Adaptation, an efficient decoupled three-phases training strategy for mitigating the generalization penalty of high spatial-compression autoencoders.","With these designs, we improve the autoencoder's spatial compression ratio up to 128 while maintaining the reconstruction quality.","Applying our DC-AE to latent diffusion models, we achieve significant speedup without accuracy drop.","For example, on ImageNet 512x512, our DC-AE provides 19.1x inference speedup and 17.9x training speedup on H100 GPU for UViT-H while achieving a better FID, compared with the widely used SD-VAE-f8 autoencoder.","Our code is available at https://github.com/mit-han-lab/efficientvit."],"url":"http://arxiv.org/abs/2410.10733v1"}
{"created":"2024-10-14 17:09:21","title":"Harnessing with Twisting: Single-Arm Deformable Linear Object Manipulation for Industrial Harnessing Task","abstract":"Wire-harnessing tasks pose great challenges to be automated by the robot due to the complex dynamics and unpredictable behavior of the deformable wire. Traditional methods, often reliant on dual-robot arms or tactile sensing, face limitations in adaptability, cost, and scalability. This paper introduces a novel single-robot wire-harnessing pipeline that leverages a robot's twisting motion to generate necessary wire tension for precise insertion into clamps, using only one robot arm with an integrated force/torque (F/T) sensor. Benefiting from this design, the single robot arm can efficiently apply tension for wire routing and insertion into clamps in a narrow space. Our approach is structured around four principal components: a Model Predictive Control (MPC) based on the Koopman operator for tension tracking and wire following, a motion planner for sequencing harnessing waypoints, a suite of insertion primitives for clamp engagement, and a fix-point switching mechanism for wire constraint updating. Evaluated on an industrial-level wire harnessing task, our method demonstrated superior performance and reliability over conventional approaches, efficiently handling both single and multiple wire configurations with high success rates.","sentences":["Wire-harnessing tasks pose great challenges to be automated by the robot due to the complex dynamics and unpredictable behavior of the deformable wire.","Traditional methods, often reliant on dual-robot arms or tactile sensing, face limitations in adaptability, cost, and scalability.","This paper introduces a novel single-robot wire-harnessing pipeline that leverages a robot's twisting motion to generate necessary wire tension for precise insertion into clamps, using only one robot arm with an integrated force/torque (F/T) sensor.","Benefiting from this design, the single robot arm can efficiently apply tension for wire routing and insertion into clamps in a narrow space.","Our approach is structured around four principal components: a Model Predictive Control (MPC) based on the Koopman operator for tension tracking and wire following, a motion planner for sequencing harnessing waypoints, a suite of insertion primitives for clamp engagement, and a fix-point switching mechanism for wire constraint updating.","Evaluated on an industrial-level wire harnessing task, our method demonstrated superior performance and reliability over conventional approaches, efficiently handling both single and multiple wire configurations with high success rates."],"url":"http://arxiv.org/abs/2410.10729v1"}
{"created":"2024-10-14 17:09:14","title":"Towards LLM-guided Efficient and Interpretable Multi-linear Tensor Network Rank Selection","abstract":"We propose a novel framework that leverages large language models (LLMs) to guide the rank selection in tensor network models for higher-order data analysis. By utilising the intrinsic reasoning capabilities and domain knowledge of LLMs, our approach offers enhanced interpretability of the rank choices and can effectively optimise the objective function. This framework enables users without specialised domain expertise to utilise tensor network decompositions and understand the underlying rationale within the rank selection process. Experimental results validate our method on financial higher-order datasets, demonstrating interpretable reasoning, strong generalisation to unseen test data, and its potential for self-enhancement over successive iterations. This work is placed at the intersection of large language models and higher-order data analysis.","sentences":["We propose a novel framework that leverages large language models (LLMs) to guide the rank selection in tensor network models for higher-order data analysis.","By utilising the intrinsic reasoning capabilities and domain knowledge of LLMs, our approach offers enhanced interpretability of the rank choices and can effectively optimise the objective function.","This framework enables users without specialised domain expertise to utilise tensor network decompositions and understand the underlying rationale within the rank selection process.","Experimental results validate our method on financial higher-order datasets, demonstrating interpretable reasoning, strong generalisation to unseen test data, and its potential for self-enhancement over successive iterations.","This work is placed at the intersection of large language models and higher-order data analysis."],"url":"http://arxiv.org/abs/2410.10728v1"}
{"created":"2024-10-14 17:05:03","title":"A Counterexample in Image Registration","abstract":"Image registration is a widespread problem which applies models about image transformation or image similarity to align discrete images of the same scene. Nevertheless, the theoretical limits on its accuracy are not understood even in the case of one-dimensional data. Just as Nyquist's sampling theorem states conditions for the perfect reconstruction of signals from samples, there are bounds to the quality of reproductions of quantized functions from sets of ideal, noiseless samples in the absence of additional assumptions. In this work we estimate spatially-limited piecewise constant signals from two or more sets of noiseless sampling patterns. We mainly focus on the energy of the error function and find that the uncertainties of the positions of the discontinuity points of the function depend on the discontinuity point selected as the reference point of the signal. As a consequence, the accuracy of the estimate of the signal depends on the reference point of that signal.","sentences":["Image registration is a widespread problem which applies models about image transformation or image similarity to align discrete images of the same scene.","Nevertheless, the theoretical limits on its accuracy are not understood even in the case of one-dimensional data.","Just as Nyquist's sampling theorem states conditions for the perfect reconstruction of signals from samples, there are bounds to the quality of reproductions of quantized functions from sets of ideal, noiseless samples in the absence of additional assumptions.","In this work we estimate spatially-limited piecewise constant signals from two or more sets of noiseless sampling patterns.","We mainly focus on the energy of the error function and find that the uncertainties of the positions of the discontinuity points of the function depend on the discontinuity point selected as the reference point of the signal.","As a consequence, the accuracy of the estimate of the signal depends on the reference point of that signal."],"url":"http://arxiv.org/abs/2410.10725v1"}
{"created":"2024-10-14 17:04:41","title":"Large Language Models Are Active Critics in NLG Evaluation","abstract":"The conventional paradigm of using large language models (LLMs) for evaluating natural language generation (NLG) systems typically relies on two key inputs: (1) a clear definition of the NLG task to be evaluated and (2) a list of pre-defined evaluation criteria. This process treats LLMs as ''passive critics,'' strictly following human-defined criteria for evaluation. However, as new NLG tasks emerge, the criteria for assessing text quality can vary greatly. Consequently, these rigid evaluation methods struggle to adapt to diverse NLG tasks without extensive prompt engineering customized for each specific task. To address this limitation, we introduce Active-Critic, a novel LLM-based NLG evaluation protocol that enables LLMs to function as ''active critics.'' Specifically, our protocol comprises two key stages. In the first stage, the LLM is instructed to infer the target NLG task and establish relevant evaluation criteria from the data. Building on this self-inferred information, the second stage dynamically optimizes the prompt to guide the LLM toward more human-aligned scoring decisions, while also generating detailed explanations to justify its evaluations. Experiments across four NLG evaluation tasks show that our approach achieves stronger alignment with human judgments than state-of-the-art evaluation methods. Our comprehensive analysis further highlights the effectiveness and explainability of Active-Critic with only a small amount of labeled data. We will share our code and data on GitHub.","sentences":["The conventional paradigm of using large language models (LLMs) for evaluating natural language generation (NLG) systems typically relies on two key inputs: (1) a clear definition of the NLG task to be evaluated and (2) a list of pre-defined evaluation criteria.","This process treats LLMs as ''passive critics,'' strictly following human-defined criteria for evaluation.","However, as new NLG tasks emerge, the criteria for assessing text quality can vary greatly.","Consequently, these rigid evaluation methods struggle to adapt to diverse NLG tasks without extensive prompt engineering customized for each specific task.","To address this limitation, we introduce Active-Critic, a novel LLM-based NLG evaluation protocol that enables LLMs to function as ''active critics.''","Specifically, our protocol comprises two key stages.","In the first stage, the LLM is instructed to infer the target NLG task and establish relevant evaluation criteria from the data.","Building on this self-inferred information, the second stage dynamically optimizes the prompt to guide the LLM toward more human-aligned scoring decisions, while also generating detailed explanations to justify its evaluations.","Experiments across four NLG evaluation tasks show that our approach achieves stronger alignment with human judgments than state-of-the-art evaluation methods.","Our comprehensive analysis further highlights the effectiveness and explainability of Active-Critic with only a small amount of labeled data.","We will share our code and data on GitHub."],"url":"http://arxiv.org/abs/2410.10724v1"}
{"created":"2024-10-14 17:00:53","title":"4-LEGS: 4D Language Embedded Gaussian Splatting","abstract":"The emergence of neural representations has revolutionized our means for digitally viewing a wide range of 3D scenes, enabling the synthesis of photorealistic images rendered from novel views. Recently, several techniques have been proposed for connecting these low-level representations with the high-level semantics understanding embodied within the scene. These methods elevate the rich semantic understanding from 2D imagery to 3D representations, distilling high-dimensional spatial features onto 3D space. In our work, we are interested in connecting language with a dynamic modeling of the world. We show how to lift spatio-temporal features to a 4D representation based on 3D Gaussian Splatting. %, \\gal{while introducing a feature-proximity attention mechanism that allows for neighboring features in 3D space to interact}. This enables an interactive interface where the user can spatiotemporally localize events in the video from text prompts. We demonstrate our system on public 3D video datasets of people and animals performing various actions.","sentences":["The emergence of neural representations has revolutionized our means for digitally viewing a wide range of 3D scenes, enabling the synthesis of photorealistic images rendered from novel views.","Recently, several techniques have been proposed for connecting these low-level representations with the high-level semantics understanding embodied within the scene.","These methods elevate the rich semantic understanding from 2D imagery to 3D representations, distilling high-dimensional spatial features onto 3D space.","In our work, we are interested in connecting language with a dynamic modeling of the world.","We show how to lift spatio-temporal features to a 4D representation based on 3D Gaussian Splatting.","%, \\gal{while introducing a feature-proximity attention mechanism that allows for neighboring features in 3D space to interact}.","This enables an interactive interface where the user can spatiotemporally localize events in the video from text prompts.","We demonstrate our system on public 3D video datasets of people and animals performing various actions."],"url":"http://arxiv.org/abs/2410.10719v1"}
{"created":"2024-10-14 16:57:23","title":"SeedLM: Compressing LLM Weights into Seeds of Pseudo-Random Generators","abstract":"Large Language Models (LLMs) have transformed natural language processing, but face significant challenges in widespread deployment due to their high runtime cost. In this paper, we introduce SeedLM, a novel post-training compression method that uses seeds of pseudo-random generators to encode and compress model weights. Specifically, for each block of weights, we find a seed that is fed into a Linear Feedback Shift Register (LFSR) during inference to efficiently generate a random matrix. This matrix is then linearly combined with compressed coefficients to reconstruct the weight block. SeedLM reduces memory access and leverages idle compute cycles during inference, effectively speeding up memory-bound tasks by trading compute for fewer memory accesses. Unlike state-of-the-art compression methods that rely on calibration data, our approach is data-free and generalizes well across diverse tasks. Our experiments with Llama 3 70B, which is particularly challenging to compress, show that SeedLM achieves significantly better zero-shot accuracy retention at 4- and 3-bit than state-of-the-art techniques, while maintaining performance comparable to FP16 baselines. Additionally, FPGA-based tests demonstrate that 4-bit SeedLM, as model size increases to 70B, approaches a 4x speed-up over an FP16 Llama 2/3 baseline.","sentences":["Large Language Models (LLMs) have transformed natural language processing, but face significant challenges in widespread deployment due to their high runtime cost.","In this paper, we introduce SeedLM, a novel post-training compression method that uses seeds of pseudo-random generators to encode and compress model weights.","Specifically, for each block of weights, we find a seed that is fed into a Linear Feedback Shift Register (LFSR) during inference to efficiently generate a random matrix.","This matrix is then linearly combined with compressed coefficients to reconstruct the weight block.","SeedLM reduces memory access and leverages idle compute cycles during inference, effectively speeding up memory-bound tasks by trading compute for fewer memory accesses.","Unlike state-of-the-art compression methods that rely on calibration data, our approach is data-free and generalizes well across diverse tasks.","Our experiments with Llama 3 70B, which is particularly challenging to compress, show that SeedLM achieves significantly better zero-shot accuracy retention at 4- and 3-bit than state-of-the-art techniques, while maintaining performance comparable to FP16 baselines.","Additionally, FPGA-based tests demonstrate that 4-bit SeedLM, as model size increases to 70B, approaches a 4x speed-up over an FP16 Llama 2/3 baseline."],"url":"http://arxiv.org/abs/2410.10714v1"}
{"created":"2024-10-14 16:51:59","title":"Benefiting from Quantum? A Comparative Study of Q-Seg, Quantum-Inspired Techniques, and U-Net for Crack Segmentation","abstract":"Exploring the potential of quantum hardware for enhancing classical and real-world applications is an ongoing challenge. This study evaluates the performance of quantum and quantum-inspired methods compared to classical models for crack segmentation. Using annotated gray-scale image patches of concrete samples, we benchmark a classical mean Gaussian mixture technique, a quantum-inspired fermion-based method, Q-Seg a quantum annealing-based method, and a U-Net deep learning architecture. Our results indicate that quantum-inspired and quantum methods offer a promising alternative for image segmentation, particularly for complex crack patterns, and could be applied in near-future applications.","sentences":["Exploring the potential of quantum hardware for enhancing classical and real-world applications is an ongoing challenge.","This study evaluates the performance of quantum and quantum-inspired methods compared to classical models for crack segmentation.","Using annotated gray-scale image patches of concrete samples, we benchmark a classical mean Gaussian mixture technique, a quantum-inspired fermion-based method, Q-Seg a quantum annealing-based method, and a U-Net deep learning architecture.","Our results indicate that quantum-inspired and quantum methods offer a promising alternative for image segmentation, particularly for complex crack patterns, and could be applied in near-future applications."],"url":"http://arxiv.org/abs/2410.10713v1"}
{"created":"2024-10-14 16:50:14","title":"Composability in Watermarking Schemes","abstract":"Software watermarking allows for embedding a mark into a piece of code, such that any attempt to remove the mark will render the code useless. Provably secure watermarking schemes currently seems limited to programs computing various cryptographic operations, such as evaluating pseudorandom functions (PRFs), signing messages, or decrypting ciphertexts (the latter often going by the name ``traitor tracing''). Moreover, each of these watermarking schemes has an ad-hoc construction of its own.   We observe, however, that many cryptographic objects are used as building blocks in larger protocols. We ask: just as we can compose building blocks to obtain larger protocols, can we compose watermarking schemes for the building blocks to obtain watermarking schemes for the larger protocols? We give an affirmative answer to this question, by precisely formulating a set of requirements that allow for composing watermarking schemes. We use our formulation to derive a number of applications.","sentences":["Software watermarking allows for embedding a mark into a piece of code, such that any attempt to remove the mark will render the code useless.","Provably secure watermarking schemes currently seems limited to programs computing various cryptographic operations, such as evaluating pseudorandom functions (PRFs), signing messages, or decrypting ciphertexts (the latter often going by the name ``traitor tracing'').","Moreover, each of these watermarking schemes has an ad-hoc construction of its own.   ","We observe, however, that many cryptographic objects are used as building blocks in larger protocols.","We ask: just as we can compose building blocks to obtain larger protocols, can we compose watermarking schemes for the building blocks to obtain watermarking schemes for the larger protocols?","We give an affirmative answer to this question, by precisely formulating a set of requirements that allow for composing watermarking schemes.","We use our formulation to derive a number of applications."],"url":"http://arxiv.org/abs/2410.10712v1"}
{"created":"2024-10-14 16:49:14","title":"Ensemble of ConvNeXt V2 and MaxViT for Long-Tailed CXR Classification with View-Based Aggregation","abstract":"In this work, we present our solution for the MICCAI 2024 CXR-LT challenge, achieving 4th place in Subtask 2 and 5th in Subtask 1. We leveraged an ensemble of ConvNeXt V2 and MaxViT models, pretrained on an external chest X-ray dataset, to address the long-tailed distribution of chest findings. The proposed method combines state-of-the-art image classification techniques, asymmetric loss for handling class imbalance, and view-based prediction aggregation to enhance classification performance. Through experiments, we demonstrate the advantages of our approach in improving both detection accuracy and the handling of the long-tailed distribution in CXR findings. The code is available at \\url{https://github.com/yamagishi0824/cxrlt24-multiview-pp}.","sentences":["In this work, we present our solution for the MICCAI 2024 CXR-LT challenge, achieving 4th place in Subtask 2 and 5th in Subtask 1.","We leveraged an ensemble of ConvNeXt V2 and MaxViT models, pretrained on an external chest X-ray dataset, to address the long-tailed distribution of chest findings.","The proposed method combines state-of-the-art image classification techniques, asymmetric loss for handling class imbalance, and view-based prediction aggregation to enhance classification performance.","Through experiments, we demonstrate the advantages of our approach in improving both detection accuracy and the handling of the long-tailed distribution in CXR findings.","The code is available at \\url{https://github.com/yamagishi0824/cxrlt24-multiview-pp}."],"url":"http://arxiv.org/abs/2410.10710v1"}
{"created":"2024-10-14 16:45:19","title":"Exact Exploration","abstract":"Recent analysis of classical algorithms resulted in their axiomatization as transition systems satisfying some simple postulates, and in the formulation of the Abstract State Machine Theorem, which assures us that any classical algorithm can be emulated step-by-step by a most general model of computation, called an ``abstract state machine''. We refine that analysis to take details of intra-step behavior into account, and show that there is in fact an abstract state machine that not only has the same state transitions as does a given algorithm but also performs the exact same tests on states when determining how to proceed to the next state. This enhancement allows the inclusion -- within the abstract-state-machine framework -- of algorithms whose states only have partially-defined equality, or employ other native partial functions, as is the case, for instance, with inversion of a matrix of computable reals.","sentences":["Recent analysis of classical algorithms resulted in their axiomatization as transition systems satisfying some simple postulates, and in the formulation of the Abstract State Machine Theorem, which assures us that any classical algorithm can be emulated step-by-step by a most general model of computation, called an ``abstract state machine''.","We refine that analysis to take details of intra-step behavior into account, and show that there is in fact an abstract state machine that not only has the same state transitions as does a given algorithm but also performs the exact same tests on states when determining how to proceed to the next state.","This enhancement allows the inclusion -- within the abstract-state-machine framework -- of algorithms whose states only have partially-defined equality, or employ other native partial functions, as is the case, for instance, with inversion of a matrix of computable reals."],"url":"http://arxiv.org/abs/2410.10706v1"}
{"created":"2024-10-14 16:42:07","title":"Early Diagnoses of Acute Lymphoblastic Leukemia Using YOLOv8 and YOLOv11 Deep Learning Models","abstract":"Thousands of individuals succumb annually to leukemia alone. This study explores the application of image processing and deep learning techniques for detecting Acute Lymphoblastic Leukemia (ALL), a severe form of blood cancer responsible for numerous annual fatalities. As artificial intelligence technologies advance, the research investigates the reliability of these methods in real-world scenarios. The study focuses on recent developments in ALL detection, particularly using the latest YOLO series models, to distinguish between malignant and benign white blood cells and to identify different stages of ALL, including early stages. Additionally, the models are capable of detecting hematogones, which are often misclassified as ALL. By utilizing advanced deep learning models like YOLOv8 and YOLOv11, the study achieves high accuracy rates reaching 98.8%, demonstrating the effectiveness of these algorithms across multiple datasets and various real-world situations.","sentences":["Thousands of individuals succumb annually to leukemia alone.","This study explores the application of image processing and deep learning techniques for detecting Acute Lymphoblastic Leukemia (ALL), a severe form of blood cancer responsible for numerous annual fatalities.","As artificial intelligence technologies advance, the research investigates the reliability of these methods in real-world scenarios.","The study focuses on recent developments in ALL detection, particularly using the latest YOLO series models, to distinguish between malignant and benign white blood cells and to identify different stages of ALL, including early stages.","Additionally, the models are capable of detecting hematogones, which are often misclassified as ALL.","By utilizing advanced deep learning models like YOLOv8 and YOLOv11, the study achieves high accuracy rates reaching 98.8%, demonstrating the effectiveness of these algorithms across multiple datasets and various real-world situations."],"url":"http://arxiv.org/abs/2410.10701v1"}
{"created":"2024-10-14 16:41:49","title":"Derail Yourself: Multi-turn LLM Jailbreak Attack through Self-discovered Clues","abstract":"This study exposes the safety vulnerabilities of Large Language Models (LLMs) in multi-turn interactions, where malicious users can obscure harmful intents across several queries. We introduce ActorAttack, a novel multi-turn attack method inspired by actor-network theory, which models a network of semantically linked actors as attack clues to generate diverse and effective attack paths toward harmful targets. ActorAttack addresses two main challenges in multi-turn attacks: (1) concealing harmful intents by creating an innocuous conversation topic about the actor, and (2) uncovering diverse attack paths towards the same harmful target by leveraging LLMs' knowledge to specify the correlated actors as various attack clues. In this way, ActorAttack outperforms existing single-turn and multi-turn attack methods across advanced aligned LLMs, even for GPT-o1. We will publish a dataset called SafeMTData, which includes multi-turn adversarial prompts and safety alignment data, generated by ActorAttack. We demonstrate that models safety-tuned using our safety dataset are more robust to multi-turn attacks. Code is available at https://github.com/renqibing/ActorAttack.","sentences":["This study exposes the safety vulnerabilities of Large Language Models (LLMs) in multi-turn interactions, where malicious users can obscure harmful intents across several queries.","We introduce ActorAttack, a novel multi-turn attack method inspired by actor-network theory, which models a network of semantically linked actors as attack clues to generate diverse and effective attack paths toward harmful targets.","ActorAttack addresses two main challenges in multi-turn attacks: (1) concealing harmful intents by creating an innocuous conversation topic about the actor, and (2) uncovering diverse attack paths towards the same harmful target by leveraging LLMs' knowledge to specify the correlated actors as various attack clues.","In this way, ActorAttack outperforms existing single-turn and multi-turn attack methods across advanced aligned LLMs, even for GPT-o1.","We will publish a dataset called SafeMTData, which includes multi-turn adversarial prompts and safety alignment data, generated by ActorAttack.","We demonstrate that models safety-tuned using our safety dataset are more robust to multi-turn attacks.","Code is available at https://github.com/renqibing/ActorAttack."],"url":"http://arxiv.org/abs/2410.10700v1"}
{"created":"2024-10-14 16:38:10","title":"TALK-Act: Enhance Textural-Awareness for 2D Speaking Avatar Reenactment with Diffusion Model","abstract":"Recently, 2D speaking avatars have increasingly participated in everyday scenarios due to the fast development of facial animation techniques. However, most existing works neglect the explicit control of human bodies. In this paper, we propose to drive not only the faces but also the torso and gesture movements of a speaking figure. Inspired by recent advances in diffusion models, we propose the Motion-Enhanced Textural-Aware ModeLing for SpeaKing Avatar Reenactment (TALK-Act) framework, which enables high-fidelity avatar reenactment from only short footage of monocular video. Our key idea is to enhance the textural awareness with explicit motion guidance in diffusion modeling. Specifically, we carefully construct 2D and 3D structural information as intermediate guidance. While recent diffusion models adopt a side network for control information injection, they fail to synthesize temporally stable results even with person-specific fine-tuning. We propose a Motion-Enhanced Textural Alignment module to enhance the bond between driving and target signals. Moreover, we build a Memory-based Hand-Recovering module to help with the difficulties in hand-shape preserving. After pre-training, our model can achieve high-fidelity 2D avatar reenactment with only 30 seconds of person-specific data. Extensive experiments demonstrate the effectiveness and superiority of our proposed framework. Resources can be found at https://guanjz20.github.io/projects/TALK-Act.","sentences":["Recently, 2D speaking avatars have increasingly participated in everyday scenarios due to the fast development of facial animation techniques.","However, most existing works neglect the explicit control of human bodies.","In this paper, we propose to drive not only the faces but also the torso and gesture movements of a speaking figure.","Inspired by recent advances in diffusion models, we propose the Motion-Enhanced Textural-Aware ModeLing for SpeaKing Avatar Reenactment (TALK-Act) framework, which enables high-fidelity avatar reenactment from only short footage of monocular video.","Our key idea is to enhance the textural awareness with explicit motion guidance in diffusion modeling.","Specifically, we carefully construct 2D and 3D structural information as intermediate guidance.","While recent diffusion models adopt a side network for control information injection, they fail to synthesize temporally stable results even with person-specific fine-tuning.","We propose a Motion-Enhanced Textural Alignment module to enhance the bond between driving and target signals.","Moreover, we build a Memory-based Hand-Recovering module to help with the difficulties in hand-shape preserving.","After pre-training, our model can achieve high-fidelity 2D avatar reenactment with only 30 seconds of person-specific data.","Extensive experiments demonstrate the effectiveness and superiority of our proposed framework.","Resources can be found at https://guanjz20.github.io/projects/TALK-Act."],"url":"http://arxiv.org/abs/2410.10696v1"}
{"created":"2024-10-14 16:27:03","title":"Dynamical loss functions shape landscape topography and improve learning in artificial neural networks","abstract":"Dynamical loss functions are derived from standard loss functions used in supervised classification tasks, but they are modified such that the contribution from each class periodically increases and decreases. These oscillations globally alter the loss landscape without affecting the global minima. In this paper, we demonstrate how to transform cross-entropy and mean squared error into dynamical loss functions. We begin by discussing the impact of increasing the size of the neural network or the learning rate on the learning process. Building on this intuition, we propose several versions of dynamical loss functions and show how they significantly improve validation accuracy for networks of varying sizes. Finally, we explore how the landscape of these dynamical loss functions evolves during training, highlighting the emergence of instabilities that may be linked to edge-of-instability minimization.","sentences":["Dynamical loss functions are derived from standard loss functions used in supervised classification tasks, but they are modified such that the contribution from each class periodically increases and decreases.","These oscillations globally alter the loss landscape without affecting the global minima.","In this paper, we demonstrate how to transform cross-entropy and mean squared error into dynamical loss functions.","We begin by discussing the impact of increasing the size of the neural network or the learning rate on the learning process.","Building on this intuition, we propose several versions of dynamical loss functions and show how they significantly improve validation accuracy for networks of varying sizes.","Finally, we explore how the landscape of these dynamical loss functions evolves during training, highlighting the emergence of instabilities that may be linked to edge-of-instability minimization."],"url":"http://arxiv.org/abs/2410.10690v1"}
{"created":"2024-10-14 16:25:54","title":"Building a Multivariate Time Series Benchmarking Datasets Inspired by Natural Language Processing (NLP)","abstract":"Time series analysis has become increasingly important in various domains, and developing effective models relies heavily on high-quality benchmark datasets. Inspired by the success of Natural Language Processing (NLP) benchmark datasets in advancing pre-trained models, we propose a new approach to create a comprehensive benchmark dataset for time series analysis. This paper explores the methodologies used in NLP benchmark dataset creation and adapts them to the unique challenges of time series data. We discuss the process of curating diverse, representative, and challenging time series datasets, highlighting the importance of domain relevance and data complexity. Additionally, we investigate multi-task learning strategies that leverage the benchmark dataset to enhance the performance of time series models. This research contributes to the broader goal of advancing the state-of-the-art in time series modeling by adopting successful strategies from the NLP domain.","sentences":["Time series analysis has become increasingly important in various domains, and developing effective models relies heavily on high-quality benchmark datasets.","Inspired by the success of Natural Language Processing (NLP) benchmark datasets in advancing pre-trained models, we propose a new approach to create a comprehensive benchmark dataset for time series analysis.","This paper explores the methodologies used in NLP benchmark dataset creation and adapts them to the unique challenges of time series data.","We discuss the process of curating diverse, representative, and challenging time series datasets, highlighting the importance of domain relevance and data complexity.","Additionally, we investigate multi-task learning strategies that leverage the benchmark dataset to enhance the performance of time series models.","This research contributes to the broader goal of advancing the state-of-the-art in time series modeling by adopting successful strategies from the NLP domain."],"url":"http://arxiv.org/abs/2410.10687v1"}
{"created":"2024-10-14 16:21:33","title":"Active Learning of Robot Vision Using Adaptive Path Planning","abstract":"Robots need robust and flexible vision systems to perceive and reason about their environments beyond geometry. Most of such systems build upon deep learning approaches. As autonomous robots are commonly deployed in initially unknown environments, pre-training on static datasets cannot always capture the variety of domains and limits the robot's vision performance during missions. Recently, self-supervised as well as fully supervised active learning methods emerged to improve robotic vision. These approaches rely on large in-domain pre-training datasets or require substantial human labelling effort. To address these issues, we present a recent adaptive planning framework for efficient training data collection to substantially reduce human labelling requirements in semantic terrain monitoring missions. To this end, we combine high-quality human labels with automatically generated pseudo labels. Experimental results show that the framework reaches segmentation performance close to fully supervised approaches with drastically reduced human labelling effort while outperforming purely self-supervised approaches. We discuss the advantages and limitations of current methods and outline valuable future research avenues towards more robust and flexible robotic vision systems in unknown environments.","sentences":["Robots need robust and flexible vision systems to perceive and reason about their environments beyond geometry.","Most of such systems build upon deep learning approaches.","As autonomous robots are commonly deployed in initially unknown environments, pre-training on static datasets cannot always capture the variety of domains and limits the robot's vision performance during missions.","Recently, self-supervised as well as fully supervised active learning methods emerged to improve robotic vision.","These approaches rely on large in-domain pre-training datasets or require substantial human labelling effort.","To address these issues, we present a recent adaptive planning framework for efficient training data collection to substantially reduce human labelling requirements in semantic terrain monitoring missions.","To this end, we combine high-quality human labels with automatically generated pseudo labels.","Experimental results show that the framework reaches segmentation performance close to fully supervised approaches with drastically reduced human labelling effort while outperforming purely self-supervised approaches.","We discuss the advantages and limitations of current methods and outline valuable future research avenues towards more robust and flexible robotic vision systems in unknown environments."],"url":"http://arxiv.org/abs/2410.10684v1"}
{"created":"2024-10-14 16:21:23","title":"SAMPa: Sharpness-aware Minimization Parallelized","abstract":"Sharpness-aware minimization (SAM) has been shown to improve the generalization of neural networks. However, each SAM update requires \\emph{sequentially} computing two gradients, effectively doubling the per-iteration cost compared to base optimizers like SGD. We propose a simple modification of SAM, termed SAMPa, which allows us to fully parallelize the two gradient computations. SAMPa achieves a twofold speedup of SAM under the assumption that communication costs between devices are negligible. Empirical results show that SAMPa ranks among the most efficient variants of SAM in terms of computational time. Additionally, our method consistently outperforms SAM across both vision and language tasks. Notably, SAMPa theoretically maintains convergence guarantees even for \\emph{fixed} perturbation sizes, which is established through a novel Lyapunov function. We in fact arrive at SAMPa by treating this convergence guarantee as a hard requirement -- an approach we believe is promising for developing SAM-based methods in general. Our code is available at \\url{https://github.com/LIONS-EPFL/SAMPa}.","sentences":["Sharpness-aware minimization (SAM) has been shown to improve the generalization of neural networks.","However, each SAM update requires \\emph{sequentially} computing two gradients, effectively doubling the per-iteration cost compared to base optimizers like SGD.","We propose a simple modification of SAM, termed SAMPa, which allows us to fully parallelize the two gradient computations.","SAMPa achieves a twofold speedup of SAM under the assumption that communication costs between devices are negligible.","Empirical results show that SAMPa ranks among the most efficient variants of SAM in terms of computational time.","Additionally, our method consistently outperforms SAM across both vision and language tasks.","Notably, SAMPa theoretically maintains convergence guarantees even for \\emph{fixed} perturbation sizes, which is established through a novel Lyapunov function.","We in fact arrive at SAMPa by treating this convergence guarantee as a hard requirement -- an approach we believe is promising for developing SAM-based methods in general.","Our code is available at \\url{https://github.com/LIONS-EPFL/SAMPa}."],"url":"http://arxiv.org/abs/2410.10683v1"}
{"created":"2024-10-14 16:20:36","title":"Evaluating SQL Understanding in Large Language Models","abstract":"The rise of large language models (LLMs) has significantly impacted various domains, including natural language processing (NLP) and image generation, by making complex computational tasks more accessible. While LLMs demonstrate impressive generative capabilities, there is an ongoing debate about their level of \"understanding,\" particularly in structured domains like SQL. In this paper, we evaluate the extent to which LLMs \"understand\" SQL by testing them on a series of key SQL tasks. These tasks, such as syntax error detection, missing token identification, query performance prediction, query equivalence checking, and query explanation, assess the models' proficiency in recognition, context awareness, semantics, and coherence, which are essential skills for SQL understanding. We generate labeled datasets from well-known workloads, and evaluate the latest LLMs, focusing on how query complexity and syntactic features influence performance. Our results indicate that while GPT4 excels at tasks requiring recognition and context, all models struggle with deeper semantic understanding and coherence, especially in query equivalence and performance estimation, revealing the limitations of current LLMs in achieving full SQL comprehension.","sentences":["The rise of large language models (LLMs) has significantly impacted various domains, including natural language processing (NLP) and image generation, by making complex computational tasks more accessible.","While LLMs demonstrate impressive generative capabilities, there is an ongoing debate about their level of \"understanding,\" particularly in structured domains like SQL.","In this paper, we evaluate the extent to which LLMs \"understand\" SQL by testing them on a series of key SQL tasks.","These tasks, such as syntax error detection, missing token identification, query performance prediction, query equivalence checking, and query explanation, assess the models' proficiency in recognition, context awareness, semantics, and coherence, which are essential skills for SQL understanding.","We generate labeled datasets from well-known workloads, and evaluate the latest LLMs, focusing on how query complexity and syntactic features influence performance.","Our results indicate that while GPT4 excels at tasks requiring recognition and context, all models struggle with deeper semantic understanding and coherence, especially in query equivalence and performance estimation, revealing the limitations of current LLMs in achieving full SQL comprehension."],"url":"http://arxiv.org/abs/2410.10680v1"}
{"created":"2024-10-14 16:19:57","title":"Combinatorial Multi-armed Bandits: Arm Selection via Group Testing","abstract":"This paper considers the problem of combinatorial multi-armed bandits with semi-bandit feedback and a cardinality constraint on the super-arm size. Existing algorithms for solving this problem typically involve two key sub-routines: (1) a parameter estimation routine that sequentially estimates a set of base-arm parameters, and (2) a super-arm selection policy for selecting a subset of base arms deemed optimal based on these parameters. State-of-the-art algorithms assume access to an exact oracle for super-arm selection with unbounded computational power. At each instance, this oracle evaluates a list of score functions, the number of which grows as low as linearly and as high as exponentially with the number of arms. This can be prohibitive in the regime of a large number of arms. This paper introduces a novel realistic alternative to the perfect oracle. This algorithm uses a combination of group-testing for selecting the super arms and quantized Thompson sampling for parameter estimation. Under a general separability assumption on the reward function, the proposed algorithm reduces the complexity of the super-arm-selection oracle to be logarithmic in the number of base arms while achieving the same regret order as the state-of-the-art algorithms that use exact oracles. This translates to at least an exponential reduction in complexity compared to the oracle-based approaches.","sentences":["This paper considers the problem of combinatorial multi-armed bandits with semi-bandit feedback and a cardinality constraint on the super-arm size.","Existing algorithms for solving this problem typically involve two key sub-routines: (1) a parameter estimation routine that sequentially estimates a set of base-arm parameters, and (2) a super-arm selection policy for selecting a subset of base arms deemed optimal based on these parameters.","State-of-the-art algorithms assume access to an exact oracle for super-arm selection with unbounded computational power.","At each instance, this oracle evaluates a list of score functions, the number of which grows as low as linearly and as high as exponentially with the number of arms.","This can be prohibitive in the regime of a large number of arms.","This paper introduces a novel realistic alternative to the perfect oracle.","This algorithm uses a combination of group-testing for selecting the super arms and quantized Thompson sampling for parameter estimation.","Under a general separability assumption on the reward function, the proposed algorithm reduces the complexity of the super-arm-selection oracle to be logarithmic in the number of base arms while achieving the same regret order as the state-of-the-art algorithms that use exact oracles.","This translates to at least an exponential reduction in complexity compared to the oracle-based approaches."],"url":"http://arxiv.org/abs/2410.10679v1"}
{"created":"2024-10-14 16:18:29","title":"Both Ears Wide Open: Towards Language-Driven Spatial Audio Generation","abstract":"Recently, diffusion models have achieved great success in mono-channel audio generation. However, when it comes to stereo audio generation, the soundscapes often have a complex scene of multiple objects and directions. Controlling stereo audio with spatial contexts remains challenging due to high data costs and unstable generative models. To the best of our knowledge, this work represents the first attempt to address these issues. We first construct a large-scale, simulation-based, and GPT-assisted dataset, BEWO-1M, with abundant soundscapes and descriptions even including moving and multiple sources. Beyond text modality, we have also acquired a set of images and rationally paired stereo audios through retrieval to advance multimodal generation. Existing audio generation models tend to generate rather random and indistinct spatial audio. To provide accurate guidance for latent diffusion models, we introduce the SpatialSonic model utilizing spatial-aware encoders and azimuth state matrices to reveal reasonable spatial guidance. By leveraging spatial guidance, our unified model not only achieves the objective of generating immersive and controllable spatial audio from text and image but also enables interactive audio generation during inference. Finally, under fair settings, we conduct subjective and objective evaluations on simulated and real-world data to compare our approach with prevailing methods. The results demonstrate the effectiveness of our method, highlighting its capability to generate spatial audio that adheres to physical rules.","sentences":["Recently, diffusion models have achieved great success in mono-channel audio generation.","However, when it comes to stereo audio generation, the soundscapes often have a complex scene of multiple objects and directions.","Controlling stereo audio with spatial contexts remains challenging due to high data costs and unstable generative models.","To the best of our knowledge, this work represents the first attempt to address these issues.","We first construct a large-scale, simulation-based, and GPT-assisted dataset, BEWO-1M, with abundant soundscapes and descriptions even including moving and multiple sources.","Beyond text modality, we have also acquired a set of images and rationally paired stereo audios through retrieval to advance multimodal generation.","Existing audio generation models tend to generate rather random and indistinct spatial audio.","To provide accurate guidance for latent diffusion models, we introduce the SpatialSonic model utilizing spatial-aware encoders and azimuth state matrices to reveal reasonable spatial guidance.","By leveraging spatial guidance, our unified model not only achieves the objective of generating immersive and controllable spatial audio from text and image but also enables interactive audio generation during inference.","Finally, under fair settings, we conduct subjective and objective evaluations on simulated and real-world data to compare our approach with prevailing methods.","The results demonstrate the effectiveness of our method, highlighting its capability to generate spatial audio that adheres to physical rules."],"url":"http://arxiv.org/abs/2410.10676v1"}
{"created":"2024-10-14 16:16:43","title":"Enhancing Robustness in Deep Reinforcement Learning: A Lyapunov Exponent Approach","abstract":"Deep reinforcement learning agents achieve state-of-the-art performance in a wide range of simulated control tasks. However, successful applications to real-world problems remain limited. One reason for this dichotomy is because the learned policies are not robust to observation noise or adversarial attacks. In this paper, we investigate the robustness of deep RL policies to a single small state perturbation in deterministic continuous control tasks. We demonstrate that RL policies can be deterministically chaotic as small perturbations to the system state have a large impact on subsequent state and reward trajectories. This unstable non-linear behaviour has two consequences: First, inaccuracies in sensor readings, or adversarial attacks, can cause significant performance degradation; Second, even policies that show robust performance in terms of rewards may have unpredictable behaviour in practice. These two facets of chaos in RL policies drastically restrict the application of deep RL to real-world problems. To address this issue, we propose an improvement on the successful Dreamer V3 architecture, implementing a Maximal Lyapunov Exponent regularisation. This new approach reduces the chaotic state dynamics, rendering the learnt policies more resilient to sensor noise or adversarial attacks and thereby improving the suitability of Deep Reinforcement Learning for real-world applications.","sentences":["Deep reinforcement learning agents achieve state-of-the-art performance in a wide range of simulated control tasks.","However, successful applications to real-world problems remain limited.","One reason for this dichotomy is because the learned policies are not robust to observation noise or adversarial attacks.","In this paper, we investigate the robustness of deep RL policies to a single small state perturbation in deterministic continuous control tasks.","We demonstrate that RL policies can be deterministically chaotic as small perturbations to the system state have a large impact on subsequent state and reward trajectories.","This unstable non-linear behaviour has two consequences:","First, inaccuracies in sensor readings, or adversarial attacks, can cause significant performance degradation; Second, even policies that show robust performance in terms of rewards may have unpredictable behaviour in practice.","These two facets of chaos in RL policies drastically restrict the application of deep RL to real-world problems.","To address this issue, we propose an improvement on the successful Dreamer V3 architecture, implementing a Maximal Lyapunov Exponent regularisation.","This new approach reduces the chaotic state dynamics, rendering the learnt policies more resilient to sensor noise or adversarial attacks and thereby improving the suitability of Deep Reinforcement Learning for real-world applications."],"url":"http://arxiv.org/abs/2410.10674v1"}
{"created":"2024-10-14 16:15:57","title":"Large Language Model Evaluation via Matrix Nuclear-Norm","abstract":"As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy. While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD). To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity. By employing the \\( L_{1,2}\\text{-norm} \\) to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities. This approach reduces the time complexity to \\( O(n^2) \\) and eliminates the need for SVD computation. Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B. This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia. Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency. The code is available at https://github.com/MLGroupJLU/MatrixNuclearNorm.","sentences":["As large language models (LLMs) continue to evolve, efficient evaluation metrics are vital for assessing their ability to compress information and reduce redundancy.","While traditional metrics like Matrix Entropy offer valuable insights, they are computationally intensive for large-scale models due to their \\( O(n^3) \\) time complexity with Singular Value Decomposition (SVD).","To mitigate this issue, we introduce the Matrix Nuclear-Norm, which not only serves as a metric to quantify the data compression proficiency of LLM but also provides a convex approximation of matrix rank to capture both predictive discriminability and diversity.","By employing the \\( L_{1,2}\\text{-norm} \\) to further approximate the nuclear norm, we can effectively assess the model's information compression capabilities.","This approach reduces the time complexity to \\( O(n^2) \\) and eliminates the need for SVD computation.","Consequently, the Matrix Nuclear-Norm achieves speeds 8 to 24 times faster than Matrix Entropy for the CEREBRAS-GPT model as sizes increase from 111M to 6.7B.","This performance gap becomes more pronounced with larger models, as validated in tests with other models like Pythia.","Additionally, evaluations on benchmarks and model responses confirm that our proposed Matrix Nuclear-Norm is a reliable, scalable, and efficient tool for assessing LLMs' performance, striking a balance between accuracy and computational efficiency.","The code is available at https://github.com/MLGroupJLU/MatrixNuclearNorm."],"url":"http://arxiv.org/abs/2410.10672v1"}
{"created":"2024-10-14 16:13:52","title":"MLP-SLAM: Multilayer Perceptron-Based Simultaneous Localization and Mapping With a Dynamic and Static Object Discriminator","abstract":"The Visual Simultaneous Localization and Mapping (V-SLAM) system has seen significant development in recent years, demonstrating high precision in environments with limited dynamic objects. However, their performance significantly deteriorates when deployed in settings with a higher presence of movable objects, such as environments with pedestrians, cars, and buses, which are common in outdoor scenes. To address this issue, we propose a Multilayer Perceptron (MLP)-based real-time stereo SLAM system that leverages complete geometry information to avoid information loss. Moreover, there is currently no publicly available dataset for directly evaluating the effectiveness of dynamic and static feature classification methods, and to bridge this gap, we have created a publicly available dataset containing over 50,000 feature points. Experimental results demonstrate that our MLP-based dynamic and static feature point discriminator has achieved superior performance compared to other methods on this dataset. Furthermore, the MLP-based real-time stereo SLAM system has shown the highest average precision and fastest speed on the outdoor KITTI tracking datasets compared to other dynamic SLAM systems.The open-source code and datasets are available at https://github.com/TaozheLi/MLP-SLAM.","sentences":["The Visual Simultaneous Localization and Mapping (V-SLAM) system has seen significant development in recent years, demonstrating high precision in environments with limited dynamic objects.","However, their performance significantly deteriorates when deployed in settings with a higher presence of movable objects, such as environments with pedestrians, cars, and buses, which are common in outdoor scenes.","To address this issue, we propose a Multilayer Perceptron (MLP)-based real-time stereo SLAM system that leverages complete geometry information to avoid information loss.","Moreover, there is currently no publicly available dataset for directly evaluating the effectiveness of dynamic and static feature classification methods, and to bridge this gap, we have created a publicly available dataset containing over 50,000 feature points.","Experimental results demonstrate that our MLP-based dynamic and static feature point discriminator has achieved superior performance compared to other methods on this dataset.","Furthermore, the MLP-based real-time stereo SLAM system has shown the highest average precision and fastest speed on the outdoor KITTI tracking datasets compared to other dynamic SLAM systems.","The open-source code and datasets are available at https://github.com/TaozheLi/MLP-SLAM."],"url":"http://arxiv.org/abs/2410.10669v1"}
{"created":"2024-10-14 16:11:04","title":"Double Jeopardy and Climate Impact in the Use of Large Language Models: Socio-economic Disparities and Reduced Utility for Non-English Speakers","abstract":"Artificial Intelligence (AI), particularly large language models (LLMs), holds the potential to bridge language and information gaps, which can benefit the economies of developing nations. However, our analysis of FLORES-200, FLORES+, Ethnologue, and World Development Indicators data reveals that these benefits largely favor English speakers. Speakers of languages in low-income and lower-middle-income countries face higher costs when using OpenAI's GPT models via APIs because of how the system processes the input -- tokenization. Around 1.5 billion people, speaking languages primarily from lower-middle-income countries, could incur costs that are 4 to 6 times higher than those faced by English speakers. Disparities in LLM performance are significant, and tokenization in models priced per token amplifies inequalities in access, cost, and utility. Moreover, using the quality of translation tasks as a proxy measure, we show that LLMs perform poorly in low-resource languages, presenting a ``double jeopardy\" of higher costs and poor performance for these users. We also discuss the direct impact of fragmentation in tokenizing low-resource languages on climate. This underscores the need for fairer algorithm development to benefit all linguistic groups.","sentences":["Artificial Intelligence (AI), particularly large language models (LLMs), holds the potential to bridge language and information gaps, which can benefit the economies of developing nations.","However, our analysis of FLORES-200, FLORES+, Ethnologue, and World Development Indicators data reveals that these benefits largely favor English speakers.","Speakers of languages in low-income and lower-middle-income countries face higher costs when using OpenAI's GPT models via APIs because of how the system processes the input -- tokenization.","Around 1.5 billion people, speaking languages primarily from lower-middle-income countries, could incur costs that are 4 to 6 times higher than those faced by English speakers.","Disparities in LLM performance are significant, and tokenization in models priced per token amplifies inequalities in access, cost, and utility.","Moreover, using the quality of translation tasks as a proxy measure, we show that LLMs perform poorly in low-resource languages, presenting a ``double jeopardy\" of higher costs and poor performance for these users.","We also discuss the direct impact of fragmentation in tokenizing low-resource languages on climate.","This underscores the need for fairer algorithm development to benefit all linguistic groups."],"url":"http://arxiv.org/abs/2410.10665v1"}
{"created":"2024-10-14 16:09:38","title":"Cross-Modal Few-Shot Learning: a Generative Transfer Learning Framework","abstract":"Most existing studies on few-shot learning focus on unimodal settings, where models are trained to generalize on unseen data using only a small number of labeled examples from the same modality. However, real-world data are inherently multi-modal, and unimodal approaches limit the practical applications of few-shot learning. To address this gap, this paper introduces the Cross-modal Few-Shot Learning (CFSL) task, which aims to recognize instances from multiple modalities when only a few labeled examples are available. This task presents additional challenges compared to classical few-shot learning due to the distinct visual characteristics and structural properties unique to each modality. To tackle these challenges, we propose a Generative Transfer Learning (GTL) framework consisting of two stages: the first stage involves training on abundant unimodal data, and the second stage focuses on transfer learning to adapt to novel data. Our GTL framework jointly estimates the latent shared concept across modalities and in-modality disturbance in both stages, while freezing the generative module during the transfer phase to maintain the stability of the learned representations and prevent overfitting to the limited multi-modal samples. Our finds demonstrate that GTL has superior performance compared to state-of-the-art methods across four distinct multi-modal datasets: Sketchy, TU-Berlin, Mask1K, and SKSF-A. Additionally, the results suggest that the model can estimate latent concepts from vast unimodal data and generalize these concepts to unseen modalities using only a limited number of available samples, much like human cognitive processes.","sentences":["Most existing studies on few-shot learning focus on unimodal settings, where models are trained to generalize on unseen data using only a small number of labeled examples from the same modality.","However, real-world data are inherently multi-modal, and unimodal approaches limit the practical applications of few-shot learning.","To address this gap, this paper introduces the Cross-modal Few-Shot Learning (CFSL) task, which aims to recognize instances from multiple modalities when only a few labeled examples are available.","This task presents additional challenges compared to classical few-shot learning due to the distinct visual characteristics and structural properties unique to each modality.","To tackle these challenges, we propose a Generative Transfer Learning (GTL) framework consisting of two stages: the first stage involves training on abundant unimodal data, and the second stage focuses on transfer learning to adapt to novel data.","Our GTL framework jointly estimates the latent shared concept across modalities and in-modality disturbance in both stages, while freezing the generative module during the transfer phase to maintain the stability of the learned representations and prevent overfitting to the limited multi-modal samples.","Our finds demonstrate that GTL has superior performance compared to state-of-the-art methods across four distinct multi-modal datasets: Sketchy, TU-Berlin, Mask1K, and SKSF-A. Additionally, the results suggest that the model can estimate latent concepts from vast unimodal data and generalize these concepts to unseen modalities using only a limited number of available samples, much like human cognitive processes."],"url":"http://arxiv.org/abs/2410.10663v1"}
{"created":"2024-10-14 16:08:15","title":"Transforming Game Play: A Comparative Study of DCQN and DTQN Architectures in Reinforcement Learning","abstract":"In this study, we investigate the performance of Deep Q-Networks utilizing Convolutional Neural Networks (CNNs) and Transformer architectures across three different Atari games. The advent of DQNs has significantly advanced Reinforcement Learning, enabling agents to directly learn optimal policies from high-dimensional sensory inputs from pixel or RAM data. While CNN-based DQNs have been extensively studied and deployed in various domains, Transformer-based DQNs are relatively unexplored. Our research aims to fill this gap by benchmarking the performance of both DCQNs and DTQNs across the Atari games Asteroids, Space Invaders, and Centipede. We find that in the 35-40 million parameter range, the DCQN outperforms the DTQN in speed across both ViT and Projection Architectures. We also find the DCQN outperforms the DTQN in all games except for Centipede.","sentences":["In this study, we investigate the performance of Deep Q-Networks utilizing Convolutional Neural Networks (CNNs) and Transformer architectures across three different Atari games.","The advent of DQNs has significantly advanced Reinforcement Learning, enabling agents to directly learn optimal policies from high-dimensional sensory inputs from pixel or RAM data.","While CNN-based DQNs have been extensively studied and deployed in various domains, Transformer-based DQNs are relatively unexplored.","Our research aims to fill this gap by benchmarking the performance of both DCQNs and DTQNs across the Atari games Asteroids, Space Invaders, and Centipede.","We find that in the 35-40 million parameter range, the DCQN outperforms the DTQN in speed across both ViT and Projection Architectures.","We also find the DCQN outperforms the DTQN in all games except for Centipede."],"url":"http://arxiv.org/abs/2410.10660v1"}
{"created":"2024-10-14 16:06:59","title":"PCF-Lift: Panoptic Lifting by Probabilistic Contrastive Fusion","abstract":"Panoptic lifting is an effective technique to address the 3D panoptic segmentation task by unprojecting 2D panoptic segmentations from multi-views to 3D scene. However, the quality of its results largely depends on the 2D segmentations, which could be noisy and error-prone, so its performance often drops significantly for complex scenes. In this work, we design a new pipeline coined PCF-Lift based on our Probabilis-tic Contrastive Fusion (PCF) to learn and embed probabilistic features throughout our pipeline to actively consider inaccurate segmentations and inconsistent instance IDs. Technical-wise, we first model the probabilistic feature embeddings through multivariate Gaussian distributions. To fuse the probabilistic features, we incorporate the probability product kernel into the contrastive loss formulation and design a cross-view constraint to enhance the feature consistency across different views. For the inference, we introduce a new probabilistic clustering method to effectively associate prototype features with the underlying 3D object instances for the generation of consistent panoptic segmentation results. Further, we provide a theoretical analysis to justify the superiority of the proposed probabilistic solution. By conducting extensive experiments, our PCF-lift not only significantly outperforms the state-of-the-art methods on widely used benchmarks including the ScanNet dataset and the challenging Messy Room dataset (4.4% improvement of scene-level PQ), but also demonstrates strong robustness when incorporating various 2D segmentation models or different levels of hand-crafted noise.","sentences":["Panoptic lifting is an effective technique to address the 3D panoptic segmentation task by unprojecting 2D panoptic segmentations from multi-views to 3D scene.","However, the quality of its results largely depends on the 2D segmentations, which could be noisy and error-prone, so its performance often drops significantly for complex scenes.","In this work, we design a new pipeline coined PCF-Lift based on our Probabilis-tic Contrastive Fusion (PCF) to learn and embed probabilistic features throughout our pipeline to actively consider inaccurate segmentations and inconsistent instance IDs.","Technical-wise, we first model the probabilistic feature embeddings through multivariate Gaussian distributions.","To fuse the probabilistic features, we incorporate the probability product kernel into the contrastive loss formulation and design a cross-view constraint to enhance the feature consistency across different views.","For the inference, we introduce a new probabilistic clustering method to effectively associate prototype features with the underlying 3D object instances for the generation of consistent panoptic segmentation results.","Further, we provide a theoretical analysis to justify the superiority of the proposed probabilistic solution.","By conducting extensive experiments, our PCF-lift not only significantly outperforms the state-of-the-art methods on widely used benchmarks including the ScanNet dataset and the challenging Messy Room dataset (4.4% improvement of scene-level PQ), but also demonstrates strong robustness when incorporating various 2D segmentation models or different levels of hand-crafted noise."],"url":"http://arxiv.org/abs/2410.10659v1"}
{"created":"2024-10-14 16:06:56","title":"A Personalized MOOC Learning Group and Course Recommendation Method Based on Graph Neural Network and Social Network Analysis","abstract":"In order to enhance students' initiative and participation in MOOC learning, this study constructed a multi-level network model based on Social Network Analysis (SNA). The model makes use of data pertaining to nearly 40,000 users and tens of thousands of courses from various higher education MOOC platforms. Furthermore, an AI-based assistant has been developed which utilises the collected data to provide personalised recommendations regarding courses and study groups for students. The objective is to examine the relationship between students' course selection preferences and their academic interest levels. Based on the results of the relationship analysis, the AI assistant employs technologies such as GNN to recommend suitable courses and study groups to students. This study offers new insights into the potential of personalised teaching on MOOC platforms, demonstrating the value of data-driven and AI-assisted methods in improving the quality of online learning experiences, increasing student engagement, and enhancing learning outcomes.","sentences":["In order to enhance students' initiative and participation in MOOC learning, this study constructed a multi-level network model based on Social Network Analysis (SNA).","The model makes use of data pertaining to nearly 40,000 users and tens of thousands of courses from various higher education MOOC platforms.","Furthermore, an AI-based assistant has been developed which utilises the collected data to provide personalised recommendations regarding courses and study groups for students.","The objective is to examine the relationship between students' course selection preferences and their academic interest levels.","Based on the results of the relationship analysis, the AI assistant employs technologies such as GNN to recommend suitable courses and study groups to students.","This study offers new insights into the potential of personalised teaching on MOOC platforms, demonstrating the value of data-driven and AI-assisted methods in improving the quality of online learning experiences, increasing student engagement, and enhancing learning outcomes."],"url":"http://arxiv.org/abs/2410.10658v1"}
{"created":"2024-10-14 16:04:04","title":"Kub: Enabling Elastic HPC Workloads on Containerized Environments","abstract":"The conventional model of resource allocation in HPC systems is static. Thus, a job cannot leverage newly available resources in the system or release underutilized resources during the execution. In this paper, we present Kub, a methodology that enables elastic execution of HPC workloads on Kubernetes so that the resources allocated to a job can be dynamically scaled during the execution. One main optimization of our method is to maximize the reuse of the originally allocated resources so that the disruption to the running job can be minimized. The scaling procedure is coordinated among nodes through remote procedure calls on Kubernetes for deploying workloads in the cloud. We evaluate our approach using one synthetic benchmark and two production-level MPI-based HPC applications -- GROMACS and CM1. Our results demonstrate that the benefits of adapting the allocated resources depend on the workload characteristics. In the tested cases, a properly chosen scaling point for increasing resources during execution achieved up to 2x speedup. Also, the overhead of checkpointing and data reshuffling significantly influences the selection of optimal scaling points and requires application-specific knowledge.","sentences":["The conventional model of resource allocation in HPC systems is static.","Thus, a job cannot leverage newly available resources in the system or release underutilized resources during the execution.","In this paper, we present Kub, a methodology that enables elastic execution of HPC workloads on Kubernetes so that the resources allocated to a job can be dynamically scaled during the execution.","One main optimization of our method is to maximize the reuse of the originally allocated resources so that the disruption to the running job can be minimized.","The scaling procedure is coordinated among nodes through remote procedure calls on Kubernetes for deploying workloads in the cloud.","We evaluate our approach using one synthetic benchmark and two production-level MPI-based HPC applications -- GROMACS and CM1.","Our results demonstrate that the benefits of adapting the allocated resources depend on the workload characteristics.","In the tested cases, a properly chosen scaling point for increasing resources during execution achieved up to 2x speedup.","Also, the overhead of checkpointing and data reshuffling significantly influences the selection of optimal scaling points and requires application-specific knowledge."],"url":"http://arxiv.org/abs/2410.10655v1"}
{"created":"2024-10-14 16:03:41","title":"Navigation under uncertainty: Trajectory prediction and occlusion reasoning with switching dynamical systems","abstract":"Predicting future trajectories of nearby objects, especially under occlusion, is a crucial task in autonomous driving and safe robot navigation. Prior works typically neglect to maintain uncertainty about occluded objects and only predict trajectories of observed objects using high-capacity models such as Transformers trained on large datasets. While these approaches are effective in standard scenarios, they can struggle to generalize to the long-tail, safety-critical scenarios. In this work, we explore a conceptual framework unifying trajectory prediction and occlusion reasoning under the same class of structured probabilistic generative model, namely, switching dynamical systems. We then present some initial experiments illustrating its capabilities using the Waymo open dataset.","sentences":["Predicting future trajectories of nearby objects, especially under occlusion, is a crucial task in autonomous driving and safe robot navigation.","Prior works typically neglect to maintain uncertainty about occluded objects and only predict trajectories of observed objects using high-capacity models such as Transformers trained on large datasets.","While these approaches are effective in standard scenarios, they can struggle to generalize to the long-tail, safety-critical scenarios.","In this work, we explore a conceptual framework unifying trajectory prediction and occlusion reasoning under the same class of structured probabilistic generative model, namely, switching dynamical systems.","We then present some initial experiments illustrating its capabilities using the Waymo open dataset."],"url":"http://arxiv.org/abs/2410.10653v1"}
{"created":"2024-10-14 16:01:01","title":"Generative AI and Its Impact on Personalized Intelligent Tutoring Systems","abstract":"Generative Artificial Intelligence (AI) is revolutionizing educational technology by enabling highly personalized and adaptive learning environments within Intelligent Tutoring Systems (ITS). This report delves into the integration of Generative AI, particularly large language models (LLMs) like GPT-4, into ITS to enhance personalized education through dynamic content generation, real-time feedback, and adaptive learning pathways. We explore key applications such as automated question generation, customized feedback mechanisms, and interactive dialogue systems that respond to individual learner needs. The report also addresses significant challenges, including ensuring pedagogical accuracy, mitigating inherent biases in AI models, and maintaining learner engagement. Future directions highlight the potential advancements in multimodal AI integration, emotional intelligence in tutoring systems, and the ethical implications of AI-driven education. By synthesizing current research and practical implementations, this report underscores the transformative potential of Generative AI in creating more effective, equitable, and engaging educational experiences.","sentences":["Generative Artificial Intelligence (AI) is revolutionizing educational technology by enabling highly personalized and adaptive learning environments within Intelligent Tutoring Systems (ITS).","This report delves into the integration of Generative AI, particularly large language models (LLMs) like GPT-4, into ITS to enhance personalized education through dynamic content generation, real-time feedback, and adaptive learning pathways.","We explore key applications such as automated question generation, customized feedback mechanisms, and interactive dialogue systems that respond to individual learner needs.","The report also addresses significant challenges, including ensuring pedagogical accuracy, mitigating inherent biases in AI models, and maintaining learner engagement.","Future directions highlight the potential advancements in multimodal AI integration, emotional intelligence in tutoring systems, and the ethical implications of AI-driven education.","By synthesizing current research and practical implementations, this report underscores the transformative potential of Generative AI in creating more effective, equitable, and engaging educational experiences."],"url":"http://arxiv.org/abs/2410.10650v1"}
{"created":"2024-10-14 15:59:16","title":"A Simple Baseline for Predicting Events with Auto-Regressive Tabular Transformers","abstract":"Many real-world applications of tabular data involve using historic events to predict properties of new ones, for example whether a credit card transaction is fraudulent or what rating a customer will assign a product on a retail platform. Existing approaches to event prediction include costly, brittle, and application-dependent techniques such as time-aware positional embeddings, learned row and field encodings, and oversampling methods for addressing class imbalance. Moreover, these approaches often assume specific use-cases, for example that we know the labels of all historic events or that we only predict a pre-specified label and not the data's features themselves. In this work, we propose a simple but flexible baseline using standard autoregressive LLM-style transformers with elementary positional embeddings and a causal language modeling objective. Our baseline outperforms existing approaches across popular datasets and can be employed for various use-cases. We demonstrate that the same model can predict labels, impute missing values, or model event sequences.","sentences":["Many real-world applications of tabular data involve using historic events to predict properties of new ones, for example whether a credit card transaction is fraudulent or what rating a customer will assign a product on a retail platform.","Existing approaches to event prediction include costly, brittle, and application-dependent techniques such as time-aware positional embeddings, learned row and field encodings, and oversampling methods for addressing class imbalance.","Moreover, these approaches often assume specific use-cases, for example that we know the labels of all historic events or that we only predict a pre-specified label and not the data's features themselves.","In this work, we propose a simple but flexible baseline using standard autoregressive LLM-style transformers with elementary positional embeddings and a causal language modeling objective.","Our baseline outperforms existing approaches across popular datasets and can be employed for various use-cases.","We demonstrate that the same model can predict labels, impute missing values, or model event sequences."],"url":"http://arxiv.org/abs/2410.10648v1"}
{"created":"2024-10-14 15:56:43","title":"DR-MPC: Deep Residual Model Predictive Control for Real-world Social Navigation","abstract":"How can a robot safely navigate around people exhibiting complex motion patterns? Reinforcement Learning (RL) or Deep RL (DRL) in simulation holds some promise, although much prior work relies on simulators that fail to precisely capture the nuances of real human motion. To address this gap, we propose Deep Residual Model Predictive Control (DR-MPC), a method to enable robots to quickly and safely perform DRL from real-world crowd navigation data. By blending MPC with model-free DRL, DR-MPC overcomes the traditional DRL challenges of large data requirements and unsafe initial behavior. DR-MPC is initialized with MPC-based path tracking, and gradually learns to interact more effectively with humans. To further accelerate learning, a safety component estimates when the robot encounters out-of-distribution states and guides it away from likely collisions. In simulation, we show that DR-MPC substantially outperforms prior work, including traditional DRL and residual DRL models. Real-world experiments show our approach successfully enables a robot to navigate a variety of crowded situations with few errors using less than 4 hours of training data.","sentences":["How can a robot safely navigate around people exhibiting complex motion patterns?","Reinforcement Learning (RL) or Deep RL (DRL) in simulation holds some promise, although much prior work relies on simulators that fail to precisely capture the nuances of real human motion.","To address this gap, we propose Deep Residual Model Predictive Control (DR-MPC), a method to enable robots to quickly and safely perform DRL from real-world crowd navigation data.","By blending MPC with model-free DRL, DR-MPC overcomes the traditional DRL challenges of large data requirements and unsafe initial behavior.","DR-MPC is initialized with MPC-based path tracking, and gradually learns to interact more effectively with humans.","To further accelerate learning, a safety component estimates when the robot encounters out-of-distribution states and guides it away from likely collisions.","In simulation, we show that DR-MPC substantially outperforms prior work, including traditional DRL and residual DRL models.","Real-world experiments show our approach successfully enables a robot to navigate a variety of crowded situations with few errors using less than 4 hours of training data."],"url":"http://arxiv.org/abs/2410.10646v1"}
{"created":"2024-10-14 15:53:30","title":"Functional Flexibility in Generative AI Interfaces: Text Editing with LLMs through Conversations, Toolbars, and Prompts","abstract":"Prompting-based user interfaces (UIs) shift the task of defining and accessing relevant functions from developers to users. However, how UIs shape this flexibility has not yet been investigated explicitly. We explored interaction with Large Language Models (LLMs) over four years, before and after the rise of general-purpose LLMs: (1) Our survey (N=121) elicited how users envision to delegate writing tasks to AI. This informed a conversational UI design. (2) A user study (N=10) revealed that people regressed to using short command-like prompts. (3) When providing these directly as shortcuts in a toolbar UI, in addition to prompting, users in our second study (N=12) dynamically switched between specified and flexible AI functions. We discuss functional flexibility as a new theoretical construct and thinking tool. Our work highlights the value of moving beyond conversational UIs, by considering how different UIs shape users' access to the functional space of generative AI models.","sentences":["Prompting-based user interfaces (UIs) shift the task of defining and accessing relevant functions from developers to users.","However, how UIs shape this flexibility has not yet been investigated explicitly.","We explored interaction with Large Language Models (LLMs) over four years, before and after the rise of general-purpose LLMs: (1) Our survey (N=121) elicited how users envision to delegate writing tasks to AI.","This informed a conversational UI design.","(2) A user study (N=10) revealed that people regressed to using short command-like prompts.","(3) When providing these directly as shortcuts in a toolbar UI, in addition to prompting, users in our second study (N=12) dynamically switched between specified and flexible AI functions.","We discuss functional flexibility as a new theoretical construct and thinking tool.","Our work highlights the value of moving beyond conversational UIs, by considering how different UIs shape users' access to the functional space of generative AI models."],"url":"http://arxiv.org/abs/2410.10644v1"}
{"created":"2024-10-14 15:52:26","title":"A Simple Formal Language for Probabilistic Decision Problems","abstract":"Probabilistic puzzles can be confusing, partly because they are formulated in natural languages - full of unclarities and ambiguities - and partly because there is no widely accepted and intuitive formal language to express them. We propose a simple formal language with arrow notation ($\\gets$) for sampling from a distribution and with observe statements for conditioning (updating, belief revision). We demonstrate the usefulness of this simple language by solving several famous puzzles from probabilistic decision theory. The operational semantics of our language is expressed via the (finite, discrete) subdistribution monad. Our broader message is that proper formalisation dispels confusion.","sentences":["Probabilistic puzzles can be confusing, partly because they are formulated in natural languages - full of unclarities and ambiguities - and partly because there is no widely accepted and intuitive formal language to express them.","We propose a simple formal language with arrow notation ($\\gets$) for sampling from a distribution and with observe statements for conditioning (updating, belief revision).","We demonstrate the usefulness of this simple language by solving several famous puzzles from probabilistic decision theory.","The operational semantics of our language is expressed via the (finite, discrete) subdistribution monad.","Our broader message is that proper formalisation dispels confusion."],"url":"http://arxiv.org/abs/2410.10643v1"}
{"created":"2024-10-14 15:51:06","title":"Echo State Networks for Spatio-Temporal Area-Level Data","abstract":"Spatio-temporal area-level datasets play a critical role in official statistics, providing valuable insights for policy-making and regional planning. Accurate modeling and forecasting of these datasets can be extremely useful for policymakers to develop informed strategies for future planning. Echo State Networks (ESNs) are efficient methods for capturing nonlinear temporal dynamics and generating forecasts. However, ESNs lack a direct mechanism to account for the neighborhood structure inherent in area-level data. Ignoring these spatial relationships can significantly compromise the accuracy and utility of forecasts. In this paper, we incorporate approximate graph spectral filters at the input stage of the ESN, thereby improving forecast accuracy while preserving the model's computational efficiency during training. We demonstrate the effectiveness of our approach using Eurostat's tourism occupancy dataset and show how it can support more informed decision-making in policy and planning contexts.","sentences":["Spatio-temporal area-level datasets play a critical role in official statistics, providing valuable insights for policy-making and regional planning.","Accurate modeling and forecasting of these datasets can be extremely useful for policymakers to develop informed strategies for future planning.","Echo State Networks (ESNs) are efficient methods for capturing nonlinear temporal dynamics and generating forecasts.","However, ESNs lack a direct mechanism to account for the neighborhood structure inherent in area-level data.","Ignoring these spatial relationships can significantly compromise the accuracy and utility of forecasts.","In this paper, we incorporate approximate graph spectral filters at the input stage of the ESN, thereby improving forecast accuracy while preserving the model's computational efficiency during training.","We demonstrate the effectiveness of our approach using Eurostat's tourism occupancy dataset and show how it can support more informed decision-making in policy and planning contexts."],"url":"http://arxiv.org/abs/2410.10641v1"}
{"created":"2024-10-14 15:50:35","title":"Generating Model Parameters for Controlling: Parameter Diffusion for Controllable Multi-Task Recommendation","abstract":"Commercial recommender systems face the challenge that task requirements from platforms or users often change dynamically (e.g., varying preferences for accuracy or diversity). Ideally, the model should be re-trained after resetting a new objective function, adapting to these changes in task requirements. However, in practice, the high computational costs associated with retraining make this process impractical for models already deployed to online environments. This raises a new challenging problem: how to efficiently adapt the learning model to different task requirements by controlling model parameters after deployment, without the need for retraining. To address this issue, we propose a novel controllable learning approach via Parameter Diffusion for controllable multi-task Recommendation (PaDiRec), which allows the customization and adaptation of recommendation model parameters to new task requirements without retraining. Specifically, we first obtain the optimized model parameters through adapter tunning based on the feasible task requirements. Then, we utilize the diffusion model as a parameter generator, employing classifier-free guidance in conditional training to learn the distribution of optimized model parameters under various task requirements. Finally, the diffusion model is applied to effectively generate model parameters in a test-time adaptation manner given task requirements. As a model-agnostic approach, PaDiRec can leverage existing recommendation models as backbones to enhance their controllability. Extensive experiments on public datasets and a dataset from a commercial app, indicate that PaDiRec can effectively enhance controllability through efficient model parameter generation. The code is released at https://anonymous.4open.science/r/PaDiRec-DD13.","sentences":["Commercial recommender systems face the challenge that task requirements from platforms or users often change dynamically (e.g., varying preferences for accuracy or diversity).","Ideally, the model should be re-trained after resetting a new objective function, adapting to these changes in task requirements.","However, in practice, the high computational costs associated with retraining make this process impractical for models already deployed to online environments.","This raises a new challenging problem: how to efficiently adapt the learning model to different task requirements by controlling model parameters after deployment, without the need for retraining.","To address this issue, we propose a novel controllable learning approach via Parameter Diffusion for controllable multi-task Recommendation (PaDiRec), which allows the customization and adaptation of recommendation model parameters to new task requirements without retraining.","Specifically, we first obtain the optimized model parameters through adapter tunning based on the feasible task requirements.","Then, we utilize the diffusion model as a parameter generator, employing classifier-free guidance in conditional training to learn the distribution of optimized model parameters under various task requirements.","Finally, the diffusion model is applied to effectively generate model parameters in a test-time adaptation manner given task requirements.","As a model-agnostic approach, PaDiRec can leverage existing recommendation models as backbones to enhance their controllability.","Extensive experiments on public datasets and a dataset from a commercial app, indicate that PaDiRec can effectively enhance controllability through efficient model parameter generation.","The code is released at https://anonymous.4open.science/r/PaDiRec-DD13."],"url":"http://arxiv.org/abs/2410.10639v1"}
{"created":"2024-10-14 15:48:09","title":"Adapt-$\\infty$: Scalable Lifelong Multimodal Instruction Tuning via Dynamic Data Selection","abstract":"Visual instruction datasets from various distributors are released at different times and often contain a significant number of semantically redundant text-image pairs, depending on their task compositions (i.e., skills) or reference sources. This redundancy greatly limits the efficient deployment of lifelong adaptable multimodal large language models, hindering their ability to refine existing skills and acquire new competencies over time. To address this, we reframe the problem of Lifelong Instruction Tuning (LiIT) via data selection, where the model automatically selects beneficial samples to learn from earlier and new datasets based on the current state of acquired knowledge in the model. Based on empirical analyses that show that selecting the best data subset using a static importance measure is often ineffective for multi-task datasets with evolving distributions, we propose Adapt-$\\infty$, a new multi-way and adaptive data selection approach that dynamically balances sample efficiency and effectiveness during LiIT. We construct pseudo-skill clusters by grouping gradient-based sample vectors. Next, we select the best-performing data selector for each skill cluster from a pool of selector experts, including our newly proposed scoring function, Image Grounding score. This data selector samples a subset of the most important samples from each skill cluster for training. To prevent the continuous increase in the size of the dataset pool during LiIT, which would result in excessive computation, we further introduce a cluster-wise permanent data pruning strategy to remove the most semantically redundant samples from each cluster, keeping computational requirements manageable. Training with samples selected by Adapt-$\\infty$ alleviates catastrophic forgetting, especially for rare tasks, and promotes forward transfer across the continuum using only a fraction of the original datasets.","sentences":["Visual instruction datasets from various distributors are released at different times and often contain a significant number of semantically redundant text-image pairs, depending on their task compositions (i.e., skills) or reference sources.","This redundancy greatly limits the efficient deployment of lifelong adaptable multimodal large language models, hindering their ability to refine existing skills and acquire new competencies over time.","To address this, we reframe the problem of Lifelong Instruction Tuning (LiIT) via data selection, where the model automatically selects beneficial samples to learn from earlier and new datasets based on the current state of acquired knowledge in the model.","Based on empirical analyses that show that selecting the best data subset using a static importance measure is often ineffective for multi-task datasets with evolving distributions, we propose Adapt-$\\infty$, a new multi-way and adaptive data selection approach that dynamically balances sample efficiency and effectiveness during LiIT.","We construct pseudo-skill clusters by grouping gradient-based sample vectors.","Next, we select the best-performing data selector for each skill cluster from a pool of selector experts, including our newly proposed scoring function, Image Grounding score.","This data selector samples a subset of the most important samples from each skill cluster for training.","To prevent the continuous increase in the size of the dataset pool during LiIT, which would result in excessive computation, we further introduce a cluster-wise permanent data pruning strategy to remove the most semantically redundant samples from each cluster, keeping computational requirements manageable.","Training with samples selected by Adapt-$\\infty$ alleviates catastrophic forgetting, especially for rare tasks, and promotes forward transfer across the continuum using only a fraction of the original datasets."],"url":"http://arxiv.org/abs/2410.10636v1"}
{"created":"2024-10-14 15:45:54","title":"A GPU-accelerated Molecular Docking Workflow with Kubernetes and Apache Airflow","abstract":"Complex workflows play a critical role in accelerating scientific discovery. In many scientific domains, efficient workflow management can lead to faster scientific output and broader user groups. Workflows that can leverage resources across the boundary between cloud and HPC are a strong driver for the convergence of HPC and cloud. This study investigates the transition and deployment of a GPU-accelerated molecular docking workflow that was designed for HPC systems onto a cloud-native environment with Kubernetes and Apache Airflow. The case study focuses on state-of-of-the-art molecular docking software for drug discovery. We provide a DAG-based implementation in Apache Airflow and technical details for GPU-accelerated deployment. We evaluated the workflow using the SWEETLEAD bioinformatics dataset and executed it in a Cloud environment with heterogeneous computing resources. Our workflow can effectively overlap different stages when mapped onto different computing resources.","sentences":["Complex workflows play a critical role in accelerating scientific discovery.","In many scientific domains, efficient workflow management can lead to faster scientific output and broader user groups.","Workflows that can leverage resources across the boundary between cloud and HPC are a strong driver for the convergence of HPC and cloud.","This study investigates the transition and deployment of a GPU-accelerated molecular docking workflow that was designed for HPC systems onto a cloud-native environment with Kubernetes and Apache Airflow.","The case study focuses on state-of-of-the-art molecular docking software for drug discovery.","We provide a DAG-based implementation in Apache Airflow and technical details for GPU-accelerated deployment.","We evaluated the workflow using the SWEETLEAD bioinformatics dataset and executed it in a Cloud environment with heterogeneous computing resources.","Our workflow can effectively overlap different stages when mapped onto different computing resources."],"url":"http://arxiv.org/abs/2410.10634v1"}
{"created":"2024-10-14 15:38:56","title":"Thinking LLMs: General Instruction Following with Thought Generation","abstract":"LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond. However, in the standard alignment framework they lack the basic ability of explicit thinking before answering. Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task. We propose a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data. We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision. For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization. We show that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning & problem-solving tasks.","sentences":["LLMs are typically trained to answer user questions or follow instructions similarly to how human experts respond.","However, in the standard alignment framework they lack the basic ability of explicit thinking before answering.","Thinking is important for complex questions that require reasoning and planning -- but can be applied to any task.","We propose a training method for equipping existing LLMs with such thinking abilities for general instruction following without use of additional human data.","We achieve this by an iterative search and optimization procedure that explores the space of possible thought generations, allowing the model to learn how to think without direct supervision.","For each instruction, the thought candidates are scored using a judge model to evaluate their responses only, and then optimized via preference optimization.","We show that this procedure leads to superior performance on AlpacaEval and Arena-Hard, and shows gains from thinking on non-reasoning categories such as marketing, health and general knowledge, in addition to more traditional reasoning & problem-solving tasks."],"url":"http://arxiv.org/abs/2410.10630v1"}
{"created":"2024-10-14 15:36:42","title":"SANA: Efficient High-Resolution Image Synthesis with Linear Diffusion Transformers","abstract":"We introduce \\model, a text-to-image framework that can efficiently generate images up to 4096$\\times$4096 resolution. \\model can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU. Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8$\\times$, we trained an AE that can compress images 32$\\times$, effectively reducing the number of latent tokens. (2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality. (3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment. (4) Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence. As a result, \\model-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput. Moreover, \\model-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024$\\times$1024 resolution image. Sana enables content creation at low cost. Code and model will be publicly released.","sentences":["We introduce \\model, a text-to-image framework that can efficiently generate images up to 4096$\\times$4096 resolution.","\\model can synthesize high-resolution, high-quality images with strong text-image alignment at a remarkably fast speed, deployable on laptop GPU.","Core designs include: (1) Deep compression autoencoder: unlike traditional AEs, which compress images only 8$\\times$, we trained an AE that can compress images 32$\\times$, effectively reducing the number of latent tokens.","(2) Linear DiT: we replace all vanilla attention in DiT with linear attention, which is more efficient at high resolutions without sacrificing quality.","(3) Decoder-only text encoder: we replaced T5 with modern decoder-only small LLM as the text encoder and designed complex human instruction with in-context learning to enhance the image-text alignment.","(4) Efficient training and sampling: we propose Flow-DPM-Solver to reduce sampling steps, with efficient caption labeling and selection to accelerate convergence.","As a result, \\model-0.6B is very competitive with modern giant diffusion model (e.g. Flux-12B), being 20 times smaller and 100+ times faster in measured throughput.","Moreover, \\model-0.6B can be deployed on a 16GB laptop GPU, taking less than 1 second to generate a 1024$\\times$1024 resolution image.","Sana enables content creation at low cost.","Code and model will be publicly released."],"url":"http://arxiv.org/abs/2410.10629v1"}
{"created":"2024-10-14 15:35:44","title":"Test smells in LLM-Generated Unit Tests","abstract":"The use of Large Language Models (LLMs) in automated test generation is gaining popularity, with much of the research focusing on metrics like compilability rate, code coverage and bug detection. However, an equally important quality metric is the presence of test smells design flaws or anti patterns in test code that hinder maintainability and readability. In this study, we explore the diffusion of test smells in LLM generated unit test suites and compare them to those found in human written ones. We analyze a benchmark of 20,500 LLM-generated test suites produced by four models (GPT-3.5, GPT-4, Mistral 7B, and Mixtral 8x7B) across five prompt engineering techniques, alongside a dataset of 780,144 human written test suites from 34,637 projects. Leveraging TsDetect, a state of the art tool capable of detecting 21 different types of test smells, we identify and analyze the prevalence and co-occurrence of various test smells in both human written and LLM-generated test suites. Our findings reveal new insights into the strengths and limitations of LLMs in test generation. First, regarding prevalence, we observe that LLMs frequently generate tests with common test smells, such as Magic Number Test and Assertion Roulette. Second, in terms of co occurrence, certain smells, like Long Test and Useless Test, tend to co occur in LLM-generated suites, influenced by specific prompt techniques. Third, we find that project complexity and LLM specific factors, including model size and context length, significantly affect the prevalence of test smells. Finally, the patterns of test smells in LLM-generated tests often mirror those in human-written tests, suggesting potential data leakage from training datasets. These insights underscore the need to refine LLM-based test generation for cleaner code and suggest improvements in both LLM capabilities and software testing practices.","sentences":["The use of Large Language Models (LLMs) in automated test generation is gaining popularity, with much of the research focusing on metrics like compilability rate, code coverage and bug detection.","However, an equally important quality metric is the presence of test smells design flaws or anti patterns in test code that hinder maintainability and readability.","In this study, we explore the diffusion of test smells in LLM generated unit test suites and compare them to those found in human written ones.","We analyze a benchmark of 20,500 LLM-generated test suites produced by four models (GPT-3.5, GPT-4, Mistral 7B, and Mixtral 8x7B) across five prompt engineering techniques, alongside a dataset of 780,144 human written test suites from 34,637 projects.","Leveraging TsDetect, a state of the art tool capable of detecting 21 different types of test smells, we identify and analyze the prevalence and co-occurrence of various test smells in both human written and LLM-generated test suites.","Our findings reveal new insights into the strengths and limitations of LLMs in test generation.","First, regarding prevalence, we observe that LLMs frequently generate tests with common test smells, such as Magic Number Test and Assertion Roulette.","Second, in terms of co occurrence, certain smells, like Long Test and Useless Test, tend to co occur in LLM-generated suites, influenced by specific prompt techniques.","Third, we find that project complexity and LLM specific factors, including model size and context length, significantly affect the prevalence of test smells.","Finally, the patterns of test smells in LLM-generated tests often mirror those in human-written tests, suggesting potential data leakage from training datasets.","These insights underscore the need to refine LLM-based test generation for cleaner code and suggest improvements in both LLM capabilities and software testing practices."],"url":"http://arxiv.org/abs/2410.10628v1"}
{"created":"2024-10-14 15:32:18","title":"Effectful Mealy Machines: Bisimulation and Trace","abstract":"We introduce effectful Mealy machines - a general notion of Mealy machine with global effects - and give them semantics in terms of both effectful bisimilarity and traces. Bisimilarity of effectful Mealy machines is characterised syntactically in terms of uniform feedback. Traces of effectful Mealy machines are given a novel semantic coinductive universe, in terms of effectful streams. We prove that effectful streams generalise a standard notion of causal process, capturing existing flavours of Mealy machine, bisimilarity, and trace.","sentences":["We introduce effectful Mealy machines - a general notion of Mealy machine with global effects - and give them semantics in terms of both effectful bisimilarity and traces.","Bisimilarity of effectful Mealy machines is characterised syntactically in terms of uniform feedback.","Traces of effectful Mealy machines are given a novel semantic coinductive universe, in terms of effectful streams.","We prove that effectful streams generalise a standard notion of causal process, capturing existing flavours of Mealy machine, bisimilarity, and trace."],"url":"http://arxiv.org/abs/2410.10627v1"}
{"created":"2024-10-14 15:31:54","title":"Efficiently Democratizing Medical LLMs for 50 Languages via a Mixture of Language Family Experts","abstract":"Adapting medical Large Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages. To address this, we first construct a high-quality medical dataset and conduct analysis to ensure its quality. In order to leverage the generalization capability of multilingual LLMs to efficiently scale to more resource-constrained languages, we explore the internal information flow of LLMs from a multilingual perspective using Mixture of Experts (MoE) modularity. Technically, we propose a novel MoE routing method that employs language-specific experts and cross-lingual routing. Inspired by circuit theory, our routing analysis revealed a Spread Out in the End information flow mechanism: while earlier layers concentrate cross-lingual information flow, the later layers exhibit language-specific divergence. This insight directly led to the development of the Post-MoE architecture, which applies sparse routing only in the later layers while maintaining dense others. Experimental results demonstrate that this approach enhances the generalization of multilingual models to other languages while preserving interpretability. Finally, to efficiently scale the model to 50 languages, we introduce the concept of language family experts, drawing on linguistic priors, which enables scaling the number of languages without adding additional parameters.","sentences":["Adapting medical Large Language Models to local languages can reduce barriers to accessing healthcare services, but data scarcity remains a significant challenge, particularly for low-resource languages.","To address this, we first construct a high-quality medical dataset and conduct analysis to ensure its quality.","In order to leverage the generalization capability of multilingual LLMs to efficiently scale to more resource-constrained languages, we explore the internal information flow of LLMs from a multilingual perspective using Mixture of Experts (MoE) modularity.","Technically, we propose a novel MoE routing method that employs language-specific experts and cross-lingual routing.","Inspired by circuit theory, our routing analysis revealed a Spread Out in the End information flow mechanism: while earlier layers concentrate cross-lingual information flow, the later layers exhibit language-specific divergence.","This insight directly led to the development of the Post-MoE architecture, which applies sparse routing only in the later layers while maintaining dense others.","Experimental results demonstrate that this approach enhances the generalization of multilingual models to other languages while preserving interpretability.","Finally, to efficiently scale the model to 50 languages, we introduce the concept of language family experts, drawing on linguistic priors, which enables scaling the number of languages without adding additional parameters."],"url":"http://arxiv.org/abs/2410.10626v1"}
{"created":"2024-10-14 15:30:41","title":"SensorLLM: Aligning Large Language Models with Motion Sensors for Human Activity Recognition","abstract":"In this work, we bridge the gap between wearable sensor technology and personalized AI assistants by enabling Large Language Models (LLMs) to understand time-series tasks like human activity recognition (HAR). Despite the strong reasoning and generalization capabilities of LLMs, leveraging them for sensor data tasks remains largely unexplored. This gap stems from challenges like the lack of semantic context in time-series data, computational limitations, and LLMs' difficulty processing numerical inputs. To address these issues, we introduce SensorLLM, a two-stage framework to unlock LLMs' potential for sensor data tasks. In the Sensor-Language Alignment Stage, we introduce special tokens for each sensor channel and automatically generate trend-descriptive text to align sensor data with textual inputs, enabling SensorLLM to capture numerical changes, channel-specific information, and sensor data of varying lengths-capabilities that existing LLMs typically struggle with, all without the need for human annotations. Next, in Task-Aware Tuning Stage, we refine the model for HAR classification using the frozen LLM and alignment module, achieving performance on par with or surpassing state-of-the-art models. We further demonstrate that SensorLLM evolves into an effective sensor learner, reasoner, and classifier through Sensor-Language Alignment, enabling it to generalize across diverse datasets for HAR tasks. We strongly believe our work lays the stepstone for future time-series and text alignment research, offering a path toward foundation models for sensor data.","sentences":["In this work, we bridge the gap between wearable sensor technology and personalized AI assistants by enabling Large Language Models (LLMs) to understand time-series tasks like human activity recognition (HAR).","Despite the strong reasoning and generalization capabilities of LLMs, leveraging them for sensor data tasks remains largely unexplored.","This gap stems from challenges like the lack of semantic context in time-series data, computational limitations, and LLMs' difficulty processing numerical inputs.","To address these issues, we introduce SensorLLM, a two-stage framework to unlock LLMs' potential for sensor data tasks.","In the Sensor-Language Alignment Stage, we introduce special tokens for each sensor channel and automatically generate trend-descriptive text to align sensor data with textual inputs, enabling SensorLLM to capture numerical changes, channel-specific information, and sensor data of varying lengths-capabilities that existing LLMs typically struggle with, all without the need for human annotations.","Next, in Task-Aware Tuning Stage, we refine the model for HAR classification using the frozen LLM and alignment module, achieving performance on par with or surpassing state-of-the-art models.","We further demonstrate that SensorLLM evolves into an effective sensor learner, reasoner, and classifier through Sensor-Language Alignment, enabling it to generalize across diverse datasets for HAR tasks.","We strongly believe our work lays the stepstone for future time-series and text alignment research, offering a path toward foundation models for sensor data."],"url":"http://arxiv.org/abs/2410.10624v1"}
{"created":"2024-10-14 15:25:55","title":"Traversability-Aware Legged Navigation by Learning from Real-World Visual Data","abstract":"The enhanced mobility brought by legged locomotion empowers quadrupedal robots to navigate through complex and unstructured environments. However, optimizing agile locomotion while accounting for the varying energy costs of traversing different terrains remains an open challenge. Most previous work focuses on planning trajectories with traversability cost estimation based on human-labeled environmental features. However, this human-centric approach is insufficient because it does not account for the varying capabilities of the robot locomotion controllers over challenging terrains. To address this, we develop a novel traversability estimator in a robot-centric manner, based on the value function of the robot's locomotion controller. This estimator is integrated into a new learning-based RGBD navigation framework. The framework develops a planner that guides the robot in avoiding obstacles and hard-to-traverse terrains while reaching its goals. The training of the navigation planner is directly performed in the real world using a sample efficient reinforcement learning method. Through extensive benchmarking, we demonstrate that the proposed framework achieves the best performance in accurate traversability cost estimation and efficient learning from multi-modal data (the robot's color and depth vision, and proprioceptive feedback) for real-world training. Using the proposed method, a quadrupedal robot learns to perform traversability-aware navigation through trial and error in various real-world environments with challenging terrains that are difficult to classify using depth vision alone.","sentences":["The enhanced mobility brought by legged locomotion empowers quadrupedal robots to navigate through complex and unstructured environments.","However, optimizing agile locomotion while accounting for the varying energy costs of traversing different terrains remains an open challenge.","Most previous work focuses on planning trajectories with traversability cost estimation based on human-labeled environmental features.","However, this human-centric approach is insufficient because it does not account for the varying capabilities of the robot locomotion controllers over challenging terrains.","To address this, we develop a novel traversability estimator in a robot-centric manner, based on the value function of the robot's locomotion controller.","This estimator is integrated into a new learning-based RGBD navigation framework.","The framework develops a planner that guides the robot in avoiding obstacles and hard-to-traverse terrains while reaching its goals.","The training of the navigation planner is directly performed in the real world using a sample efficient reinforcement learning method.","Through extensive benchmarking, we demonstrate that the proposed framework achieves the best performance in accurate traversability cost estimation and efficient learning from multi-modal data (the robot's color and depth vision, and proprioceptive feedback) for real-world training.","Using the proposed method, a quadrupedal robot learns to perform traversability-aware navigation through trial and error in various real-world environments with challenging terrains that are difficult to classify using depth vision alone."],"url":"http://arxiv.org/abs/2410.10621v1"}
{"created":"2024-10-14 15:24:39","title":"On the sparsity of binary numbers","abstract":"We introduce the concept of negative coefficients in various number-based systems, with a focus on decimal and binary systems. We demonstrate that every binary number can be transformed into a sparse form, significantly enhancing computational speed by converting binary numbers into this form.","sentences":["We introduce the concept of negative coefficients in various number-based systems, with a focus on decimal and binary systems.","We demonstrate that every binary number can be transformed into a sparse form, significantly enhancing computational speed by converting binary numbers into this form."],"url":"http://arxiv.org/abs/2410.10620v1"}
