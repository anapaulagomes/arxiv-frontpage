{"created":"2024-12-16 18:59:53","title":"MaxInfoRL: Boosting exploration in reinforcement learning through information gain maximization","abstract":"Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards. Most common RL algorithms use undirected exploration, i.e., select random sequences of actions. Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty. However, effectively balancing task and intrinsic rewards is challenging and often task-dependent. In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration. MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task. When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions. We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits. We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks.","sentences":["Reinforcement learning (RL) algorithms aim to balance exploiting the current best strategy with exploring new options that could lead to higher rewards.","Most common RL algorithms use undirected exploration, i.e., select random sequences of actions.","Exploration can also be directed using intrinsic rewards, such as curiosity or model epistemic uncertainty.","However, effectively balancing task and intrinsic rewards is challenging and often task-dependent.","In this work, we introduce a framework, MaxInfoRL, for balancing intrinsic and extrinsic exploration.","MaxInfoRL steers exploration towards informative transitions, by maximizing intrinsic rewards such as the information gain about the underlying task.","When combined with Boltzmann exploration, this approach naturally trades off maximization of the value function with that of the entropy over states, rewards, and actions.","We show that our approach achieves sublinear regret in the simplified setting of multi-armed bandits.","We then apply this general formulation to a variety of off-policy model-free RL methods for continuous state-action spaces, yielding novel algorithms that achieve superior performance across hard exploration problems and complex scenarios such as visual control tasks."],"url":"http://arxiv.org/abs/2412.12098v1"}
{"created":"2024-12-16 18:59:45","title":"PanSplat: 4K Panorama Synthesis with Feed-Forward Gaussian Splatting","abstract":"With the advent of portable 360{\\deg} cameras, panorama has gained significant attention in applications like virtual reality (VR), virtual tours, robotics, and autonomous driving. As a result, wide-baseline panorama view synthesis has emerged as a vital task, where high resolution, fast inference, and memory efficiency are essential. Nevertheless, existing methods are typically constrained to lower resolutions (512 $\\times$ 1024) due to demanding memory and computational requirements. In this paper, we present PanSplat, a generalizable, feed-forward approach that efficiently supports resolution up to 4K (2048 $\\times$ 4096). Our approach features a tailored spherical 3D Gaussian pyramid with a Fibonacci lattice arrangement, enhancing image quality while reducing information redundancy. To accommodate the demands of high resolution, we propose a pipeline that integrates a hierarchical spherical cost volume and Gaussian heads with local operations, enabling two-step deferred backpropagation for memory-efficient training on a single A100 GPU. Experiments demonstrate that PanSplat achieves state-of-the-art results with superior efficiency and image quality across both synthetic and real-world datasets. Code will be available at \\url{https://github.com/chengzhag/PanSplat}.","sentences":["With the advent of portable 360{\\deg} cameras, panorama has gained significant attention in applications like virtual reality (VR), virtual tours, robotics, and autonomous driving.","As a result, wide-baseline panorama view synthesis has emerged as a vital task, where high resolution, fast inference, and memory efficiency are essential.","Nevertheless, existing methods are typically constrained to lower resolutions (512 $\\times$ 1024) due to demanding memory and computational requirements.","In this paper, we present PanSplat, a generalizable, feed-forward approach that efficiently supports resolution up to 4K (2048 $\\times$ 4096).","Our approach features a tailored spherical 3D Gaussian pyramid with a Fibonacci lattice arrangement, enhancing image quality while reducing information redundancy.","To accommodate the demands of high resolution, we propose a pipeline that integrates a hierarchical spherical cost volume and Gaussian heads with local operations, enabling two-step deferred backpropagation for memory-efficient training on a single A100 GPU.","Experiments demonstrate that PanSplat achieves state-of-the-art results with superior efficiency and image quality across both synthetic and real-world datasets.","Code will be available at \\url{https://github.com/chengzhag/PanSplat}."],"url":"http://arxiv.org/abs/2412.12096v1"}
{"created":"2024-12-16 18:59:29","title":"Causal Diffusion Transformers for Generative Modeling","abstract":"We introduce Causal Diffusion as the autoregressive (AR) counterpart of Diffusion models. It is a next-token(s) forecasting framework that is friendly to both discrete and continuous modalities and compatible with existing next-token prediction models like LLaMA and GPT. While recent works attempt to combine diffusion with AR models, we show that introducing sequential factorization to a diffusion model can substantially improve its performance and enables a smooth transition between AR and diffusion generation modes. Hence, we propose CausalFusion - a decoder-only transformer that dual-factorizes data across sequential tokens and diffusion noise levels, leading to state-of-the-art results on the ImageNet generation benchmark while also enjoying the AR advantage of generating an arbitrary number of tokens for in-context reasoning. We further demonstrate CausalFusion's multimodal capabilities through a joint image generation and captioning model, and showcase CausalFusion's ability for zero-shot in-context image manipulations. We hope that this work could provide the community with a fresh perspective on training multimodal models over discrete and continuous data.","sentences":["We introduce Causal Diffusion as the autoregressive (AR) counterpart of Diffusion models.","It is a next-token(s) forecasting framework that is friendly to both discrete and continuous modalities and compatible with existing next-token prediction models like LLaMA and GPT.","While recent works attempt to combine diffusion with AR models, we show that introducing sequential factorization to a diffusion model can substantially improve its performance and enables a smooth transition between AR and diffusion generation modes.","Hence, we propose CausalFusion - a decoder-only transformer that dual-factorizes data across sequential tokens and diffusion noise levels, leading to state-of-the-art results on the ImageNet generation benchmark while also enjoying the AR advantage of generating an arbitrary number of tokens for in-context reasoning.","We further demonstrate CausalFusion's multimodal capabilities through a joint image generation and captioning model, and showcase CausalFusion's ability for zero-shot in-context image manipulations.","We hope that this work could provide the community with a fresh perspective on training multimodal models over discrete and continuous data."],"url":"http://arxiv.org/abs/2412.12095v1"}
{"created":"2024-12-16 18:58:57","title":"SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator","abstract":"Large Language Models (LLMs) have exhibited exceptional performance across a spectrum of natural language processing tasks. However, their substantial sizes pose considerable challenges, particularly in computational demands and inference speed, due to their quadratic complexity. In this work, we have identified a key pattern: certain seemingly meaningless special tokens (i.e., separators) contribute disproportionately to attention scores compared to semantically meaningful tokens. This observation suggests that information of the segments between these separator tokens can be effectively condensed into the separator tokens themselves without significant information loss. Guided by this insight, we introduce SepLLM, a plug-and-play framework that accelerates inference by compressing these segments and eliminating redundant tokens. Additionally, we implement efficient kernels for training acceleration. Experimental results across training-free, training-from-scratch, and post-training settings demonstrate SepLLM's effectiveness. Notably, using the Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in streaming settings, SepLLM effectively processes sequences of up to 4 million tokens or more while maintaining consistent language modeling capabilities.","sentences":["Large Language Models (LLMs) have exhibited exceptional performance across a spectrum of natural language processing tasks.","However, their substantial sizes pose considerable challenges, particularly in computational demands and inference speed, due to their quadratic complexity.","In this work, we have identified a key pattern: certain seemingly meaningless special tokens (i.e., separators) contribute disproportionately to attention scores compared to semantically meaningful tokens.","This observation suggests that information of the segments between these separator tokens can be effectively condensed into the separator tokens themselves without significant information loss.","Guided by this insight, we introduce SepLLM, a plug-and-play framework that accelerates inference by compressing these segments and eliminating redundant tokens.","Additionally, we implement efficient kernels for training acceleration.","Experimental results across training-free, training-from-scratch, and post-training settings demonstrate SepLLM's effectiveness.","Notably, using the Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the GSM8K-CoT benchmark while maintaining comparable performance.","Furthermore, in streaming settings, SepLLM effectively processes sequences of up to 4 million tokens or more while maintaining consistent language modeling capabilities."],"url":"http://arxiv.org/abs/2412.12094v1"}
{"created":"2024-12-16 18:58:51","title":"CAP4D: Creating Animatable 4D Portrait Avatars with Morphable Multi-View Diffusion Models","abstract":"Reconstructing photorealistic and dynamic portrait avatars from images is essential to many applications including advertising, visual effects, and virtual reality. Depending on the application, avatar reconstruction involves different capture setups and constraints $-$ for example, visual effects studios use camera arrays to capture hundreds of reference images, while content creators may seek to animate a single portrait image downloaded from the internet. As such, there is a large and heterogeneous ecosystem of methods for avatar reconstruction. Techniques based on multi-view stereo or neural rendering achieve the highest quality results, but require hundreds of reference images. Recent generative models produce convincing avatars from a single reference image, but visual fidelity yet lags behind multi-view techniques. Here, we present CAP4D: an approach that uses a morphable multi-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait avatars from any number of reference images (i.e., one to 100) and animate and render them in real time. Our approach demonstrates state-of-the-art performance for single-, few-, and multi-image 4D portrait avatar reconstruction, and takes steps to bridge the gap in visual fidelity between single-image and multi-view reconstruction techniques.","sentences":["Reconstructing photorealistic and dynamic portrait avatars from images is essential to many applications including advertising, visual effects, and virtual reality.","Depending on the application, avatar reconstruction involves different capture setups and constraints $-$ for example, visual effects studios use camera arrays to capture hundreds of reference images, while content creators may seek to animate a single portrait image downloaded from the internet.","As such, there is a large and heterogeneous ecosystem of methods for avatar reconstruction.","Techniques based on multi-view stereo or neural rendering achieve the highest quality results, but require hundreds of reference images.","Recent generative models produce convincing avatars from a single reference image, but visual fidelity yet lags behind multi-view techniques.","Here, we present CAP4D: an approach that uses a morphable multi-view diffusion model to reconstruct photoreal 4D (dynamic 3D) portrait avatars from any number of reference images (i.e., one to 100) and animate and render them in real time.","Our approach demonstrates state-of-the-art performance for single-, few-, and multi-image 4D portrait avatar reconstruction, and takes steps to bridge the gap in visual fidelity between single-image and multi-view reconstruction techniques."],"url":"http://arxiv.org/abs/2412.12093v1"}
{"created":"2024-12-16 18:58:28","title":"No More Tuning: Prioritized Multi-Task Learning with Lagrangian Differential Multiplier Methods","abstract":"Given the ubiquity of multi-task in practical systems, Multi-Task Learning (MTL) has found widespread application across diverse domains. In real-world scenarios, these tasks often have different priorities. For instance, In web search, relevance is often prioritized over other metrics, such as click-through rates or user engagement. Existing frameworks pay insufficient attention to the prioritization among different tasks, which typically adjust task-specific loss function weights to differentiate task priorities. However, this approach encounters challenges as the number of tasks grows, leading to exponential increases in hyper-parameter tuning complexity. Furthermore, the simultaneous optimization of multiple objectives can negatively impact the performance of high-priority tasks due to interference from lower-priority tasks.   In this paper, we introduce a novel multi-task learning framework employing Lagrangian Differential Multiplier Methods for step-wise multi-task optimization. It is designed to boost the performance of high-priority tasks without interference from other tasks. Its primary advantage lies in its ability to automatically optimize multiple objectives without requiring balancing hyper-parameters for different tasks, thereby eliminating the need for manual tuning. Additionally, we provide theoretical analysis demonstrating that our method ensures optimization guarantees, enhancing the reliability of the process. We demonstrate its effectiveness through experiments on multiple public datasets and its application in Taobao search, a large-scale industrial search ranking system, resulting in significant improvements across various business metrics.","sentences":["Given the ubiquity of multi-task in practical systems, Multi-Task Learning (MTL) has found widespread application across diverse domains.","In real-world scenarios, these tasks often have different priorities.","For instance, In web search, relevance is often prioritized over other metrics, such as click-through rates or user engagement.","Existing frameworks pay insufficient attention to the prioritization among different tasks, which typically adjust task-specific loss function weights to differentiate task priorities.","However, this approach encounters challenges as the number of tasks grows, leading to exponential increases in hyper-parameter tuning complexity.","Furthermore, the simultaneous optimization of multiple objectives can negatively impact the performance of high-priority tasks due to interference from lower-priority tasks.   ","In this paper, we introduce a novel multi-task learning framework employing Lagrangian Differential Multiplier Methods for step-wise multi-task optimization.","It is designed to boost the performance of high-priority tasks without interference from other tasks.","Its primary advantage lies in its ability to automatically optimize multiple objectives without requiring balancing hyper-parameters for different tasks, thereby eliminating the need for manual tuning.","Additionally, we provide theoretical analysis demonstrating that our method ensures optimization guarantees, enhancing the reliability of the process.","We demonstrate its effectiveness through experiments on multiple public datasets and its application in Taobao search, a large-scale industrial search ranking system, resulting in significant improvements across various business metrics."],"url":"http://arxiv.org/abs/2412.12092v1"}
{"created":"2024-12-16 18:58:17","title":"Wonderland: Navigating 3D Scenes from a Single Image","abstract":"This paper addresses a challenging question: How can we efficiently create high-quality, wide-scope 3D scenes from a single arbitrary image? Existing methods face several constraints, such as requiring multi-view data, time-consuming per-scene optimization, low visual quality in backgrounds, and distorted reconstructions in unseen areas. We propose a novel pipeline to overcome these limitations. Specifically, we introduce a large-scale reconstruction model that uses latents from a video diffusion model to predict 3D Gaussian Splattings for the scenes in a feed-forward manner. The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that contain multi-view information while maintaining 3D consistency. We train the 3D reconstruction model to operate on the video latent space with a progressive training strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes. Extensive evaluations across various datasets demonstrate that our model significantly outperforms existing methods for single-view 3D scene generation, particularly with out-of-domain images. For the first time, we demonstrate that a 3D reconstruction model can be effectively built upon the latent space of a diffusion model to realize efficient 3D scene generation.","sentences":["This paper addresses a challenging question: How can we efficiently create high-quality, wide-scope 3D scenes from a single arbitrary image?","Existing methods face several constraints, such as requiring multi-view data, time-consuming per-scene optimization, low visual quality in backgrounds, and distorted reconstructions in unseen areas.","We propose a novel pipeline to overcome these limitations.","Specifically, we introduce a large-scale reconstruction model that uses latents from a video diffusion model to predict 3D Gaussian Splattings for the scenes in a feed-forward manner.","The video diffusion model is designed to create videos precisely following specified camera trajectories, allowing it to generate compressed video latents that contain multi-view information while maintaining 3D consistency.","We train the 3D reconstruction model to operate on the video latent space with a progressive training strategy, enabling the efficient generation of high-quality, wide-scope, and generic 3D scenes.","Extensive evaluations across various datasets demonstrate that our model significantly outperforms existing methods for single-view 3D scene generation, particularly with out-of-domain images.","For the first time, we demonstrate that a 3D reconstruction model can be effectively built upon the latent space of a diffusion model to realize efficient 3D scene generation."],"url":"http://arxiv.org/abs/2412.12091v1"}
{"created":"2024-12-16 18:56:24","title":"Stabilizing Reinforcement Learning in Differentiable Multiphysics Simulation","abstract":"Recent advances in GPU-based parallel simulation have enabled practitioners to collect large amounts of data and train complex control policies using deep reinforcement learning (RL), on commodity GPUs. However, such successes for RL in robotics have been limited to tasks sufficiently simulated by fast rigid-body dynamics. Simulation techniques for soft bodies are comparatively several orders of magnitude slower, thereby limiting the use of RL due to sample complexity requirements. To address this challenge, this paper presents both a novel RL algorithm and a simulation platform to enable scaling RL on tasks involving rigid bodies and deformables. We introduce Soft Analytic Policy Optimization (SAPO), a maximum entropy first-order model-based actor-critic RL algorithm, which uses first-order analytic gradients from differentiable simulation to train a stochastic actor to maximize expected return and entropy. Alongside our approach, we develop Rewarped, a parallel differentiable multiphysics simulation platform that supports simulating various materials beyond rigid bodies. We re-implement challenging manipulation and locomotion tasks in Rewarped, and show that SAPO outperforms baselines over a range of tasks that involve interaction between rigid bodies, articulations, and deformables.","sentences":["Recent advances in GPU-based parallel simulation have enabled practitioners to collect large amounts of data and train complex control policies using deep reinforcement learning (RL), on commodity GPUs.","However, such successes for RL in robotics have been limited to tasks sufficiently simulated by fast rigid-body dynamics.","Simulation techniques for soft bodies are comparatively several orders of magnitude slower, thereby limiting the use of RL due to sample complexity requirements.","To address this challenge, this paper presents both a novel RL algorithm and a simulation platform to enable scaling RL on tasks involving rigid bodies and deformables.","We introduce Soft Analytic Policy Optimization (SAPO), a maximum entropy first-order model-based actor-critic RL algorithm, which uses first-order analytic gradients from differentiable simulation to train a stochastic actor to maximize expected return and entropy.","Alongside our approach, we develop Rewarped, a parallel differentiable multiphysics simulation platform that supports simulating various materials beyond rigid bodies.","We re-implement challenging manipulation and locomotion tasks in Rewarped, and show that SAPO outperforms baselines over a range of tasks that involve interaction between rigid bodies, articulations, and deformables."],"url":"http://arxiv.org/abs/2412.12089v1"}
{"created":"2024-12-16 18:56:17","title":"Instruction-based Image Manipulation by Watching How Things Move","abstract":"This paper introduces a novel dataset construction pipeline that samples pairs of frames from videos and uses multimodal large language models (MLLMs) to generate editing instructions for training instruction-based image manipulation models. Video frames inherently preserve the identity of subjects and scenes, ensuring consistent content preservation during editing. Additionally, video data captures diverse, natural dynamics-such as non-rigid subject motion and complex camera movements-that are difficult to model otherwise, making it an ideal source for scalable dataset construction. Using this approach, we create a new dataset to train InstructMove, a model capable of instruction-based complex manipulations that are difficult to achieve with synthetically generated datasets. Our model demonstrates state-of-the-art performance in tasks such as adjusting subject poses, rearranging elements, and altering camera perspectives.","sentences":["This paper introduces a novel dataset construction pipeline that samples pairs of frames from videos and uses multimodal large language models (MLLMs) to generate editing instructions for training instruction-based image manipulation models.","Video frames inherently preserve the identity of subjects and scenes, ensuring consistent content preservation during editing.","Additionally, video data captures diverse, natural dynamics-such as non-rigid subject motion and complex camera movements-that are difficult to model otherwise, making it an ideal source for scalable dataset construction.","Using this approach, we create a new dataset to train InstructMove, a model capable of instruction-based complex manipulations that are difficult to achieve with synthetically generated datasets.","Our model demonstrates state-of-the-art performance in tasks such as adjusting subject poses, rearranging elements, and altering camera perspectives."],"url":"http://arxiv.org/abs/2412.12087v1"}
{"created":"2024-12-16 18:52:56","title":"IDArb: Intrinsic Decomposition for Arbitrary Number of Input Views and Illuminations","abstract":"Capturing geometric and material information from images remains a fundamental challenge in computer vision and graphics. Traditional optimization-based methods often require hours of computational time to reconstruct geometry, material properties, and environmental lighting from dense multi-view inputs, while still struggling with inherent ambiguities between lighting and material. On the other hand, learning-based approaches leverage rich material priors from existing 3D object datasets but face challenges with maintaining multi-view consistency. In this paper, we introduce IDArb, a diffusion-based model designed to perform intrinsic decomposition on an arbitrary number of images under varying illuminations. Our method achieves accurate and multi-view consistent estimation on surface normals and material properties. This is made possible through a novel cross-view, cross-domain attention module and an illumination-augmented, view-adaptive training strategy. Additionally, we introduce ARB-Objaverse, a new dataset that provides large-scale multi-view intrinsic data and renderings under diverse lighting conditions, supporting robust training. Extensive experiments demonstrate that IDArb outperforms state-of-the-art methods both qualitatively and quantitatively. Moreover, our approach facilitates a range of downstream tasks, including single-image relighting, photometric stereo, and 3D reconstruction, highlighting its broad applications in realistic 3D content creation.","sentences":["Capturing geometric and material information from images remains a fundamental challenge in computer vision and graphics.","Traditional optimization-based methods often require hours of computational time to reconstruct geometry, material properties, and environmental lighting from dense multi-view inputs, while still struggling with inherent ambiguities between lighting and material.","On the other hand, learning-based approaches leverage rich material priors from existing 3D object datasets but face challenges with maintaining multi-view consistency.","In this paper, we introduce IDArb, a diffusion-based model designed to perform intrinsic decomposition on an arbitrary number of images under varying illuminations.","Our method achieves accurate and multi-view consistent estimation on surface normals and material properties.","This is made possible through a novel cross-view, cross-domain attention module and an illumination-augmented, view-adaptive training strategy.","Additionally, we introduce ARB-Objaverse, a new dataset that provides large-scale multi-view intrinsic data and renderings under diverse lighting conditions, supporting robust training.","Extensive experiments demonstrate that IDArb outperforms state-of-the-art methods both qualitatively and quantitatively.","Moreover, our approach facilitates a range of downstream tasks, including single-image relighting, photometric stereo, and 3D reconstruction, highlighting its broad applications in realistic 3D content creation."],"url":"http://arxiv.org/abs/2412.12083v1"}
{"created":"2024-12-16 18:48:58","title":"UniLoc: Towards Universal Place Recognition Using Any Single Modality","abstract":"To date, most place recognition methods focus on single-modality retrieval. While they perform well in specific environments, cross-modal methods offer greater flexibility by allowing seamless switching between map and query sources. It also promises to reduce computation requirements by having a unified model, and achieving greater sample efficiency by sharing parameters. In this work, we develop a universal solution to place recognition, UniLoc, that works with any single query modality (natural language, image, or point cloud). UniLoc leverages recent advances in large-scale contrastive learning, and learns by matching hierarchically at two levels: instance-level matching and scene-level matching. Specifically, we propose a novel Self-Attention based Pooling (SAP) module to evaluate the importance of instance descriptors when aggregated into a place-level descriptor. Experiments on the KITTI-360 dataset demonstrate the benefits of cross-modality for place recognition, achieving superior performance in cross-modal settings and competitive results also for uni-modal scenarios. Our project page is publicly available at https://yan-xia.github.io/projects/UniLoc/.","sentences":["To date, most place recognition methods focus on single-modality retrieval.","While they perform well in specific environments, cross-modal methods offer greater flexibility by allowing seamless switching between map and query sources.","It also promises to reduce computation requirements by having a unified model, and achieving greater sample efficiency by sharing parameters.","In this work, we develop a universal solution to place recognition, UniLoc, that works with any single query modality (natural language, image, or point cloud).","UniLoc leverages recent advances in large-scale contrastive learning, and learns by matching hierarchically at two levels: instance-level matching and scene-level matching.","Specifically, we propose a novel Self-Attention based Pooling (SAP) module to evaluate the importance of instance descriptors when aggregated into a place-level descriptor.","Experiments on the KITTI-360 dataset demonstrate the benefits of cross-modality for place recognition, achieving superior performance in cross-modal settings and competitive results also for uni-modal scenarios.","Our project page is publicly available at https://yan-xia.github.io/projects/UniLoc/."],"url":"http://arxiv.org/abs/2412.12079v1"}
{"created":"2024-12-16 18:46:58","title":"CPath-Omni: A Unified Multimodal Foundation Model for Patch and Whole Slide Image Analysis in Computational Pathology","abstract":"The emergence of large multimodal models (LMMs) has brought significant advancements to pathology. Previous research has primarily focused on separately training patch-level and whole-slide image (WSI)-level models, limiting the integration of learned knowledge across patches and WSIs, and resulting in redundant models. In this work, we introduce CPath-Omni, the first 15-billion-parameter LMM designed to unify both patch and WSI level image analysis, consolidating a variety of tasks at both levels, including classification, visual question answering, captioning, and visual referring prompting. Extensive experiments demonstrate that CPath-Omni achieves state-of-the-art (SOTA) performance across seven diverse tasks on 39 out of 42 datasets, outperforming or matching task-specific models trained for individual tasks. Additionally, we develop a specialized pathology CLIP-based visual processor for CPath-Omni, CPath-CLIP, which, for the first time, integrates different vision models and incorporates a large language model as a text encoder to build a more powerful CLIP model, which achieves SOTA performance on nine zero-shot and four few-shot datasets. Our findings highlight CPath-Omni's ability to unify diverse pathology tasks, demonstrating its potential to streamline and advance the field of foundation model in pathology.","sentences":["The emergence of large multimodal models (LMMs) has brought significant advancements to pathology.","Previous research has primarily focused on separately training patch-level and whole-slide image (WSI)-level models, limiting the integration of learned knowledge across patches and WSIs, and resulting in redundant models.","In this work, we introduce CPath-Omni, the first 15-billion-parameter LMM designed to unify both patch and WSI level image analysis, consolidating a variety of tasks at both levels, including classification, visual question answering, captioning, and visual referring prompting.","Extensive experiments demonstrate that CPath-Omni achieves state-of-the-art (SOTA) performance across seven diverse tasks on 39 out of 42 datasets, outperforming or matching task-specific models trained for individual tasks.","Additionally, we develop a specialized pathology CLIP-based visual processor for CPath-Omni, CPath-CLIP, which, for the first time, integrates different vision models and incorporates a large language model as a text encoder to build a more powerful CLIP model, which achieves SOTA performance on nine zero-shot and four few-shot datasets.","Our findings highlight CPath-Omni's ability to unify diverse pathology tasks, demonstrating its potential to streamline and advance the field of foundation model in pathology."],"url":"http://arxiv.org/abs/2412.12077v1"}
{"created":"2024-12-16 18:46:45","title":"CG-Bench: Clue-grounded Question Answering Benchmark for Long Video Understanding","abstract":"Most existing video understanding benchmarks for multimodal large language models (MLLMs) focus only on short videos. The limited number of benchmarks for long video understanding often rely solely on multiple-choice questions (MCQs). However, because of the inherent limitation of MCQ-based evaluation and the increasing reasoning ability of MLLMs, models can give the current answer purely by combining short video understanding with elimination, without genuinely understanding the video content. To address this gap, we introduce CG-Bench, a novel benchmark designed for clue-grounded question answering in long videos. CG-Bench emphasizes the model's ability to retrieve relevant clues for questions, enhancing evaluation credibility. It features 1,219 manually curated videos categorized by a granular system with 14 primary categories, 171 secondary categories, and 638 tertiary categories, making it the largest benchmark for long video analysis. The benchmark includes 12,129 QA pairs in three major question types: perception, reasoning, and hallucination. Compensating the drawbacks of pure MCQ-based evaluation, we design two novel clue-based evaluation methods: clue-grounded white box and black box evaluations, to assess whether the model generates answers based on the correct understanding of the video. We evaluate multiple closed-source and open-source MLLMs on CG-Bench. Results indicate that current models significantly underperform in understanding long videos compared to short ones, and a significant gap exists between open-source and commercial models. We hope CG-Bench can advance the development of more trustworthy and capable MLLMs for long video understanding. All annotations and video data are released at https://cg-bench.github.io/leaderboard/.","sentences":["Most existing video understanding benchmarks for multimodal large language models (MLLMs) focus only on short videos.","The limited number of benchmarks for long video understanding often rely solely on multiple-choice questions (MCQs).","However, because of the inherent limitation of MCQ-based evaluation and the increasing reasoning ability of MLLMs, models can give the current answer purely by combining short video understanding with elimination, without genuinely understanding the video content.","To address this gap, we introduce CG-Bench, a novel benchmark designed for clue-grounded question answering in long videos.","CG-Bench emphasizes the model's ability to retrieve relevant clues for questions, enhancing evaluation credibility.","It features 1,219 manually curated videos categorized by a granular system with 14 primary categories, 171 secondary categories, and 638 tertiary categories, making it the largest benchmark for long video analysis.","The benchmark includes 12,129 QA pairs in three major question types: perception, reasoning, and hallucination.","Compensating the drawbacks of pure MCQ-based evaluation, we design two novel clue-based evaluation methods: clue-grounded white box and black box evaluations, to assess whether the model generates answers based on the correct understanding of the video.","We evaluate multiple closed-source and open-source MLLMs on CG-Bench.","Results indicate that current models significantly underperform in understanding long videos compared to short ones, and a significant gap exists between open-source and commercial models.","We hope CG-Bench can advance the development of more trustworthy and capable MLLMs for long video understanding.","All annotations and video data are released at https://cg-bench.github.io/leaderboard/."],"url":"http://arxiv.org/abs/2412.12075v1"}
{"created":"2024-12-16 18:46:12","title":"Making FETCH! Happen: Finding Emergent Dog Whistles Through Common Habitats","abstract":"WARNING: This paper contains content that maybe upsetting or offensive to some readers. Dog whistles are coded expressions with dual meanings: one intended for the general public (outgroup) and another that conveys a specific message to an intended audience (ingroup). Often, these expressions are used to convey controversial political opinions while maintaining plausible deniability and slip by content moderation filters. Identification of dog whistles relies on curated lexicons, which have trouble keeping up to date. We introduce \\textbf{FETCH!}, a task for finding novel dog whistles in massive social media corpora. We find that state-of-the-art systems fail to achieve meaningful results across three distinct social media case studies. We present \\textbf{EarShot}, a novel system that combines the strengths of vector databases and Large Language Models (LLMs) to efficiently and effectively identify new dog whistles.","sentences":["WARNING:","This paper contains content that maybe upsetting or offensive to some readers.","Dog whistles are coded expressions with dual meanings: one intended for the general public (outgroup) and another that conveys a specific message to an intended audience (ingroup).","Often, these expressions are used to convey controversial political opinions while maintaining plausible deniability and slip by content moderation filters.","Identification of dog whistles relies on curated lexicons, which have trouble keeping up to date.","We introduce \\textbf{FETCH!}, a task for finding novel dog whistles in massive social media corpora.","We find that state-of-the-art systems fail to achieve meaningful results across three distinct social media case studies.","We present \\textbf{EarShot}, a novel system that combines the strengths of vector databases and Large Language Models (LLMs) to efficiently and effectively identify new dog whistles."],"url":"http://arxiv.org/abs/2412.12072v1"}
{"created":"2024-12-16 18:42:05","title":"SPADE: Spectroscopic Photoacoustic Denoising using an Analytical and Data-free Enhancement Framework","abstract":"Spectroscopic photoacoustic (sPA) imaging uses multiple wavelengths to differentiate chromophores based on their unique optical absorption spectra. This technique has been widely applied in areas such as vascular mapping, tumor detection, and therapeutic monitoring. However, sPA imaging is highly susceptible to noise, leading to poor signal-to-noise ratio (SNR) and compromised image quality. Traditional denoising techniques like frame averaging, though effective in improving SNR, can be impractical for dynamic imaging scenarios due to reduced frame rates. Advanced methods, including learning-based approaches and analytical algorithms, have demonstrated promise but often require extensive training data and parameter tuning, limiting their adaptability for real-time clinical use. In this work, we propose a sPA denoising using a tuning-free analytical and data-free enhancement (SPADE) framework for denoising sPA images. This framework integrates a data-free learning-based method with an efficient BM3D-based analytical approach while preserves spectral linearity, providing noise reduction and ensuring that functional information is maintained. The SPADE framework was validated through simulation, phantom, ex vivo, and in vivo experiments. Results demonstrated that SPADE improved SNR and preserved spectral information, outperforming conventional methods, especially in challenging imaging conditions. SPADE presents a promising solution for enhancing sPA imaging quality in clinical applications where noise reduction and spectral preservation are critical.","sentences":["Spectroscopic photoacoustic (sPA) imaging uses multiple wavelengths to differentiate chromophores based on their unique optical absorption spectra.","This technique has been widely applied in areas such as vascular mapping, tumor detection, and therapeutic monitoring.","However, sPA imaging is highly susceptible to noise, leading to poor signal-to-noise ratio (SNR) and compromised image quality.","Traditional denoising techniques like frame averaging, though effective in improving SNR, can be impractical for dynamic imaging scenarios due to reduced frame rates.","Advanced methods, including learning-based approaches and analytical algorithms, have demonstrated promise but often require extensive training data and parameter tuning, limiting their adaptability for real-time clinical use.","In this work, we propose a sPA denoising using a tuning-free analytical and data-free enhancement (SPADE) framework for denoising sPA images.","This framework integrates a data-free learning-based method with an efficient BM3D-based analytical approach while preserves spectral linearity, providing noise reduction and ensuring that functional information is maintained.","The SPADE framework was validated through simulation, phantom, ex vivo, and in vivo experiments.","Results demonstrated that SPADE improved SNR and preserved spectral information, outperforming conventional methods, especially in challenging imaging conditions.","SPADE presents a promising solution for enhancing sPA imaging quality in clinical applications where noise reduction and spectral preservation are critical."],"url":"http://arxiv.org/abs/2412.12068v1"}
{"created":"2024-12-16 18:37:06","title":"Revelations: A Decidable Class of POMDPs with Omega-Regular Objectives","abstract":"Partially observable Markov decision processes (POMDPs) form a prominent model for uncertainty in sequential decision making. We are interested in constructing algorithms with theoretical guarantees to determine whether the agent has a strategy ensuring a given specification with probability 1. This well-studied problem is known to be undecidable already for very simple omega-regular objectives, because of the difficulty of reasoning on uncertain events. We introduce a revelation mechanism which restricts information loss by requiring that almost surely the agent has eventually full information of the current state. Our main technical results are to construct exact algorithms for two classes of POMDPs called weakly and strongly revealing. Importantly, the decidable cases reduce to the analysis of a finite belief-support Markov decision process. This yields a conceptually simple and exact algorithm for a large class of POMDPs.","sentences":["Partially observable Markov decision processes (POMDPs) form a prominent model for uncertainty in sequential decision making.","We are interested in constructing algorithms with theoretical guarantees to determine whether the agent has a strategy ensuring a given specification with probability 1.","This well-studied problem is known to be undecidable already for very simple omega-regular objectives, because of the difficulty of reasoning on uncertain events.","We introduce a revelation mechanism which restricts information loss by requiring that almost surely the agent has eventually full information of the current state.","Our main technical results are to construct exact algorithms for two classes of POMDPs called weakly and strongly revealing.","Importantly, the decidable cases reduce to the analysis of a finite belief-support Markov decision process.","This yields a conceptually simple and exact algorithm for a large class of POMDPs."],"url":"http://arxiv.org/abs/2412.12063v1"}
{"created":"2024-12-16 18:35:58","title":"Semi-automated analysis of audio-recorded lessons: The case of teachers' engaging messages","abstract":"Engaging messages delivered by teachers are a key aspect of the classroom discourse that influences student outcomes. However, improving this communication is challenging due to difficulties in obtaining observations. This study presents a methodology for efficiently extracting actual observations of engaging messages from audio-recorded lessons. We collected 2,477 audio-recorded lessons from 75 teachers over two academic years. Using automatic transcription and keyword-based filtering analysis, we identified and classified engaging messages. This method reduced the information to be analysed by 90%, optimising the time and resources required compared to traditional manual coding. Subsequent descriptive analysis revealed that the most used messages emphasised the future benefits of participating in school activities. In addition, the use of engaging messages decreased as the academic year progressed. This study offers insights for researchers seeking to extract information from teachers' discourse in naturalistic settings and provides useful information for designing interventions to improve teachers' communication strategies.","sentences":["Engaging messages delivered by teachers are a key aspect of the classroom discourse that influences student outcomes.","However, improving this communication is challenging due to difficulties in obtaining observations.","This study presents a methodology for efficiently extracting actual observations of engaging messages from audio-recorded lessons.","We collected 2,477 audio-recorded lessons from 75 teachers over two academic years.","Using automatic transcription and keyword-based filtering analysis, we identified and classified engaging messages.","This method reduced the information to be analysed by 90%, optimising the time and resources required compared to traditional manual coding.","Subsequent descriptive analysis revealed that the most used messages emphasised the future benefits of participating in school activities.","In addition, the use of engaging messages decreased as the academic year progressed.","This study offers insights for researchers seeking to extract information from teachers' discourse in naturalistic settings and provides useful information for designing interventions to improve teachers' communication strategies."],"url":"http://arxiv.org/abs/2412.12062v1"}
{"created":"2024-12-16 18:34:32","title":"Virtual Agent-Based Communication Skills Training to Facilitate Health Persuasion Among Peers","abstract":"Many laypeople are motivated to improve the health behavior of their family or friends but do not know where to start, especially if the health behavior is potentially stigmatizing or controversial. We present an approach that uses virtual agents to coach community-based volunteers in health counseling techniques, such as motivational interviewing, and allows them to practice these skills in role-playing scenarios. We use this approach in a virtual agent-based system to increase COVID-19 vaccination by empowering users to influence their social network. In a between-subjects comparative design study, we test the effects of agent system interactivity and role-playing functionality on counseling outcomes, with participants evaluated by standardized patients and objective judges. We find that all versions are effective at producing peer counselors who score adequately on a standardized measure of counseling competence, and that participants were significantly more satisfied with interactive virtual agents compared to passive viewing of the training material. We discuss design implications for interpersonal skills training systems based on our findings.","sentences":["Many laypeople are motivated to improve the health behavior of their family or friends but do not know where to start, especially if the health behavior is potentially stigmatizing or controversial.","We present an approach that uses virtual agents to coach community-based volunteers in health counseling techniques, such as motivational interviewing, and allows them to practice these skills in role-playing scenarios.","We use this approach in a virtual agent-based system to increase COVID-19 vaccination by empowering users to influence their social network.","In a between-subjects comparative design study, we test the effects of agent system interactivity and role-playing functionality on counseling outcomes, with participants evaluated by standardized patients and objective judges.","We find that all versions are effective at producing peer counselors who score adequately on a standardized measure of counseling competence, and that participants were significantly more satisfied with interactive virtual agents compared to passive viewing of the training material.","We discuss design implications for interpersonal skills training systems based on our findings."],"url":"http://arxiv.org/abs/2412.12061v1"}
{"created":"2024-12-16 18:20:06","title":"Exploring Semantic Consistency and Style Diversity for Domain Generalized Semantic Segmentation","abstract":"Domain Generalized Semantic Segmentation (DGSS) seeks to utilize source domain data exclusively to enhance the generalization of semantic segmentation across unknown target domains. Prevailing studies predominantly concentrate on feature normalization and domain randomization, these approaches exhibit significant limitations. Feature normalization-based methods tend to confuse semantic features in the process of constraining the feature space distribution, resulting in classification misjudgment. Domain randomization-based methods frequently incorporate domain-irrelevant noise due to the uncontrollability of style transformations, resulting in segmentation ambiguity. To address these challenges, we introduce a novel framework, named SCSD for Semantic Consistency prediction and Style Diversity generalization. It comprises three pivotal components: Firstly, a Semantic Query Booster is designed to enhance the semantic awareness and discrimination capabilities of object queries in the mask decoder, enabling cross-domain semantic consistency prediction. Secondly, we develop a Text-Driven Style Transform module that utilizes domain difference text embeddings to controllably guide the style transformation of image features, thereby increasing inter-domain style diversity. Lastly, to prevent the collapse of similar domain feature spaces, we introduce a Style Synergy Optimization mechanism that fortifies the separation of inter-domain features and the aggregation of intra-domain features by synergistically weighting style contrastive loss and style aggregation loss. Extensive experiments demonstrate that the proposed SCSD significantly outperforms existing state-of-theart methods. Notably, SCSD trained on GTAV achieved an average of 49.11 mIoU on the four unseen domain datasets, surpassing the previous state-of-the-art method by +4.08 mIoU. Code is available at https://github.com/nhw649/SCSD.","sentences":["Domain Generalized Semantic Segmentation (DGSS) seeks to utilize source domain data exclusively to enhance the generalization of semantic segmentation across unknown target domains.","Prevailing studies predominantly concentrate on feature normalization and domain randomization, these approaches exhibit significant limitations.","Feature normalization-based methods tend to confuse semantic features in the process of constraining the feature space distribution, resulting in classification misjudgment.","Domain randomization-based methods frequently incorporate domain-irrelevant noise due to the uncontrollability of style transformations, resulting in segmentation ambiguity.","To address these challenges, we introduce a novel framework, named SCSD for Semantic Consistency prediction and Style Diversity generalization.","It comprises three pivotal components: Firstly, a Semantic Query Booster is designed to enhance the semantic awareness and discrimination capabilities of object queries in the mask decoder, enabling cross-domain semantic consistency prediction.","Secondly, we develop a Text-Driven Style Transform module that utilizes domain difference text embeddings to controllably guide the style transformation of image features, thereby increasing inter-domain style diversity.","Lastly, to prevent the collapse of similar domain feature spaces, we introduce a Style Synergy Optimization mechanism that fortifies the separation of inter-domain features and the aggregation of intra-domain features by synergistically weighting style contrastive loss and style aggregation loss.","Extensive experiments demonstrate that the proposed SCSD significantly outperforms existing state-of-theart methods.","Notably, SCSD trained on GTAV achieved an average of 49.11 mIoU on the four unseen domain datasets, surpassing the previous state-of-the-art method by +4.08 mIoU. Code is available at https://github.com/nhw649/SCSD."],"url":"http://arxiv.org/abs/2412.12050v1"}
{"created":"2024-12-16 18:18:17","title":"A LoRA is Worth a Thousand Pictures","abstract":"Recent advances in diffusion models and parameter-efficient fine-tuning (PEFT) have made text-to-image generation and customization widely accessible, with Low Rank Adaptation (LoRA) able to replicate an artist's style or subject using minimal data and computation. In this paper, we examine the relationship between LoRA weights and artistic styles, demonstrating that LoRA weights alone can serve as an effective descriptor of style, without the need for additional image generation or knowledge of the original training set. Our findings show that LoRA weights yield better performance in clustering of artistic styles compared to traditional pre-trained features, such as CLIP and DINO, with strong structural similarities between LoRA-based and conventional image-based embeddings observed both qualitatively and quantitatively. We identify various retrieval scenarios for the growing collection of customized models and show that our approach enables more accurate retrieval in real-world settings where knowledge of the training images is unavailable and additional generation is required. We conclude with a discussion on potential future applications, such as zero-shot LoRA fine-tuning and model attribution.","sentences":["Recent advances in diffusion models and parameter-efficient fine-tuning (PEFT) have made text-to-image generation and customization widely accessible, with Low Rank Adaptation (LoRA) able to replicate an artist's style or subject using minimal data and computation.","In this paper, we examine the relationship between LoRA weights and artistic styles, demonstrating that LoRA weights alone can serve as an effective descriptor of style, without the need for additional image generation or knowledge of the original training set.","Our findings show that LoRA weights yield better performance in clustering of artistic styles compared to traditional pre-trained features, such as CLIP and DINO, with strong structural similarities between LoRA-based and conventional image-based embeddings observed both qualitatively and quantitatively.","We identify various retrieval scenarios for the growing collection of customized models and show that our approach enables more accurate retrieval in real-world settings where knowledge of the training images is unavailable and additional generation is required.","We conclude with a discussion on potential future applications, such as zero-shot LoRA fine-tuning and model attribution."],"url":"http://arxiv.org/abs/2412.12048v1"}
{"created":"2024-12-16 18:15:49","title":"Artificial Intelligence in Traffic Systems","abstract":"Existing research on AI-based traffic management systems, utilizing techniques such as fuzzy logic, reinforcement learning, deep neural networks, and evolutionary algorithms, demonstrates the potential of AI to transform the traffic landscape. This article endeavors to review the topics where AI and traffic management intersect. It comprises areas like AI-powered traffic signal control systems, automatic distance and velocity recognition (for instance, in autonomous vehicles, hereafter AVs), smart parking systems, and Intelligent Traffic Management Systems (ITMS), which use data captured in real-time to keep track of traffic conditions, and traffic-related law enforcement and surveillance using AI. AI applications in traffic management cover a wide range of spheres. The spheres comprise, inter alia, streamlining traffic signal timings, predicting traffic bottlenecks in specific areas, detecting potential accidents and road hazards, managing incidents accurately, advancing public transportation systems, development of innovative driver assistance systems, and minimizing environmental impact through simplified routes and reduced emissions. The benefits of AI in traffic management are also diverse. They comprise improved management of traffic data, sounder route decision automation, easier and speedier identification and resolution of vehicular issues through monitoring the condition of individual vehicles, decreased traffic snarls and mishaps, superior resource utilization, alleviated stress of traffic management manpower, greater on-road safety, and better emergency response time.","sentences":["Existing research on AI-based traffic management systems, utilizing techniques such as fuzzy logic, reinforcement learning, deep neural networks, and evolutionary algorithms, demonstrates the potential of AI to transform the traffic landscape.","This article endeavors to review the topics where AI and traffic management intersect.","It comprises areas like AI-powered traffic signal control systems, automatic distance and velocity recognition (for instance, in autonomous vehicles, hereafter AVs), smart parking systems, and Intelligent Traffic Management Systems (ITMS), which use data captured in real-time to keep track of traffic conditions, and traffic-related law enforcement and surveillance using AI.","AI applications in traffic management cover a wide range of spheres.","The spheres comprise, inter alia, streamlining traffic signal timings, predicting traffic bottlenecks in specific areas, detecting potential accidents and road hazards, managing incidents accurately, advancing public transportation systems, development of innovative driver assistance systems, and minimizing environmental impact through simplified routes and reduced emissions.","The benefits of AI in traffic management are also diverse.","They comprise improved management of traffic data, sounder route decision automation, easier and speedier identification and resolution of vehicular issues through monitoring the condition of individual vehicles, decreased traffic snarls and mishaps, superior resource utilization, alleviated stress of traffic management manpower, greater on-road safety, and better emergency response time."],"url":"http://arxiv.org/abs/2412.12046v1"}
{"created":"2024-12-16 18:10:49","title":"The Impact of AI Assistance on Radiology Reporting: A Pilot Study Using Simulated AI Draft Reports","abstract":"Radiologists face increasing workload pressures amid growing imaging volumes, creating risks of burnout and delayed reporting times. While artificial intelligence (AI) based automated radiology report generation shows promise for reporting workflow optimization, evidence of its real-world impact on clinical accuracy and efficiency remains limited. This study evaluated the effect of draft reports on radiology reporting workflows by conducting a three reader multi-case study comparing standard versus AI-assisted reporting workflows. In both workflows, radiologists reviewed the cases and modified either a standard template (standard workflow) or an AI-generated draft report (AI-assisted workflow) to create the final report. For controlled evaluation, we used GPT-4 to generate simulated AI drafts and deliberately introduced 1-3 errors in half the cases to mimic real AI system performance. The AI-assisted workflow significantly reduced average reporting time from 573 to 435 seconds (p=0.003), without a statistically significant difference in clinically significant errors between workflows. These findings suggest that AI-generated drafts can meaningfully accelerate radiology reporting while maintaining diagnostic accuracy, offering a practical solution to address mounting workload challenges in clinical practice.","sentences":["Radiologists face increasing workload pressures amid growing imaging volumes, creating risks of burnout and delayed reporting times.","While artificial intelligence (AI) based automated radiology report generation shows promise for reporting workflow optimization, evidence of its real-world impact on clinical accuracy and efficiency remains limited.","This study evaluated the effect of draft reports on radiology reporting workflows by conducting a three reader multi-case study comparing standard versus AI-assisted reporting workflows.","In both workflows, radiologists reviewed the cases and modified either a standard template (standard workflow) or an AI-generated draft report (AI-assisted workflow) to create the final report.","For controlled evaluation, we used GPT-4 to generate simulated AI drafts and deliberately introduced 1-3 errors in half the cases to mimic real AI system performance.","The AI-assisted workflow significantly reduced average reporting time from 573 to 435 seconds (p=0.003), without a statistically significant difference in clinically significant errors between workflows.","These findings suggest that AI-generated drafts can meaningfully accelerate radiology reporting while maintaining diagnostic accuracy, offering a practical solution to address mounting workload challenges in clinical practice."],"url":"http://arxiv.org/abs/2412.12042v1"}
{"created":"2024-12-16 18:08:22","title":"How Private are Language Models in Abstractive Summarization?","abstract":"Language models (LMs) have shown outstanding performance in text summarization including sensitive domains such as medicine and law. In these settings, it is important that personally identifying information (PII) included in the source document should not leak in the summary. Prior efforts have mostly focused on studying how LMs may inadvertently elicit PII from training data. However, to what extent LMs can provide privacy-preserving summaries given a non-private source document remains under-explored. In this paper, we perform a comprehensive study across two closed- and three open-weight LMs of different sizes and families. We experiment with prompting and fine-tuning strategies for privacy-preservation across a range of summarization datasets across three domains. Our extensive quantitative and qualitative analysis including human evaluation shows that LMs often cannot prevent PII leakage on their summaries and that current widely-used metrics cannot capture context dependent privacy risks.","sentences":["Language models (LMs) have shown outstanding performance in text summarization including sensitive domains such as medicine and law.","In these settings, it is important that personally identifying information (PII) included in the source document should not leak in the summary.","Prior efforts have mostly focused on studying how LMs may inadvertently elicit PII from training data.","However, to what extent LMs can provide privacy-preserving summaries given a non-private source document remains under-explored.","In this paper, we perform a comprehensive study across two closed- and three open-weight LMs of different sizes and families.","We experiment with prompting and fine-tuning strategies for privacy-preservation across a range of summarization datasets across three domains.","Our extensive quantitative and qualitative analysis including human evaluation shows that LMs often cannot prevent PII leakage on their summaries and that current widely-used metrics cannot capture context dependent privacy risks."],"url":"http://arxiv.org/abs/2412.12040v1"}
{"created":"2024-12-16 18:08:14","title":"Can LLM Prompting Serve as a Proxy for Static Analysis in Vulnerability Detection","abstract":"Despite their remarkable success, large language models (LLMs) have shown limited ability on applied tasks such as vulnerability detection. We investigate various prompting strategies for vulnerability detection and, as part of this exploration, propose a prompting strategy that integrates natural language descriptions of vulnerabilities with a contrastive chain-of-thought reasoning approach, augmented using contrastive samples from a synthetic dataset. Our study highlights the potential of LLMs to detect vulnerabilities by integrating natural language descriptions, contrastive reasoning, and synthetic examples into a comprehensive prompting framework. Our results show that this approach can enhance LLM understanding of vulnerabilities. On a high-quality vulnerability detection dataset such as SVEN, our prompting strategies can improve accuracies, F1-scores, and pairwise accuracies by 23%, 11%, and 14%, respectively.","sentences":["Despite their remarkable success, large language models (LLMs) have shown limited ability on applied tasks such as vulnerability detection.","We investigate various prompting strategies for vulnerability detection and, as part of this exploration, propose a prompting strategy that integrates natural language descriptions of vulnerabilities with a contrastive chain-of-thought reasoning approach, augmented using contrastive samples from a synthetic dataset.","Our study highlights the potential of LLMs to detect vulnerabilities by integrating natural language descriptions, contrastive reasoning, and synthetic examples into a comprehensive prompting framework.","Our results show that this approach can enhance LLM understanding of vulnerabilities.","On a high-quality vulnerability detection dataset such as SVEN, our prompting strategies can improve accuracies, F1-scores, and pairwise accuracies by 23%, 11%, and 14%, respectively."],"url":"http://arxiv.org/abs/2412.12039v1"}
{"created":"2024-12-16 18:03:57","title":"LLMs for Cold-Start Cutting Plane Separator Configuration","abstract":"Mixed integer linear programming (MILP) solvers ship with a staggering number of parameters that are challenging to select a priori for all but expert optimization users, but can have an outsized impact on the performance of the MILP solver. Existing machine learning (ML) approaches to configure solvers require training ML models by solving thousands of related MILP instances, generalize poorly to new problem sizes, and often require implementing complex ML pipelines and custom solver interfaces that can be difficult to integrate into existing optimization workflows. In this paper, we introduce a new LLM-based framework to configure which cutting plane separators to use for a given MILP problem with little to no training data based on characteristics of the instance, such as a natural language description of the problem and the associated LaTeX formulation. We augment these LLMs with descriptions of cutting plane separators available in a given solver, grounded by summarizing the existing research literature on separators. While individual solver configurations have a large variance in performance, we present a novel ensembling strategy that clusters and aggregates configurations to create a small portfolio of high-performing configurations. Our LLM-based methodology requires no custom solver interface, can find a high-performing configuration by solving only a small number of MILPs, and can generate the configuration with simple API calls that run in under a second. Numerical results show our approach is competitive with existing configuration approaches on a suite of classic combinatorial optimization problems and real-world datasets with only a fraction of the training data and computation time.","sentences":["Mixed integer linear programming (MILP) solvers ship with a staggering number of parameters that are challenging to select a priori for all but expert optimization users, but can have an outsized impact on the performance of the MILP solver.","Existing machine learning (ML) approaches to configure solvers require training ML models by solving thousands of related MILP instances, generalize poorly to new problem sizes, and often require implementing complex ML pipelines and custom solver interfaces that can be difficult to integrate into existing optimization workflows.","In this paper, we introduce a new LLM-based framework to configure which cutting plane separators to use for a given MILP problem with little to no training data based on characteristics of the instance, such as a natural language description of the problem and the associated LaTeX formulation.","We augment these LLMs with descriptions of cutting plane separators available in a given solver, grounded by summarizing the existing research literature on separators.","While individual solver configurations have a large variance in performance, we present a novel ensembling strategy that clusters and aggregates configurations to create a small portfolio of high-performing configurations.","Our LLM-based methodology requires no custom solver interface, can find a high-performing configuration by solving only a small number of MILPs, and can generate the configuration with simple API calls that run in under a second.","Numerical results show our approach is competitive with existing configuration approaches on a suite of classic combinatorial optimization problems and real-world datasets with only a fraction of the training data and computation time."],"url":"http://arxiv.org/abs/2412.12038v1"}
{"created":"2024-12-16 18:03:23","title":"LeARN: Learnable and Adaptive Representations for Nonlinear Dynamics in System Identification","abstract":"System identification, the process of deriving mathematical models of dynamical systems from observed input-output data, has undergone a paradigm shift with the advent of learning-based methods. Addressing the intricate challenges of data-driven discovery in nonlinear dynamical systems, these methods have garnered significant attention. Among them, Sparse Identification of Nonlinear Dynamics (SINDy) has emerged as a transformative approach, distilling complex dynamical behaviors into interpretable linear combinations of basis functions. However, SINDy relies on domain-specific expertise to construct its foundational \"library\" of basis functions, which limits its adaptability and universality. In this work, we introduce a nonlinear system identification framework called LeARN that transcends the need for prior domain knowledge by learning the library of basis functions directly from data. To enhance adaptability to evolving system dynamics under varying noise conditions, we employ a novel meta-learning-based system identification approach that uses a lightweight deep neural network (DNN) to dynamically refine these basis functions. This not only captures intricate system behaviors but also adapts seamlessly to new dynamical regimes. We validate our framework on the Neural Fly dataset, showcasing its robust adaptation and generalization capabilities. Despite its simplicity, our LeARN achieves competitive dynamical error performance compared to SINDy. This work presents a step toward the autonomous discovery of dynamical systems, paving the way for a future where machine learning uncovers the governing principles of complex systems without requiring extensive domain-specific interventions.","sentences":["System identification, the process of deriving mathematical models of dynamical systems from observed input-output data, has undergone a paradigm shift with the advent of learning-based methods.","Addressing the intricate challenges of data-driven discovery in nonlinear dynamical systems, these methods have garnered significant attention.","Among them, Sparse Identification of Nonlinear Dynamics (SINDy) has emerged as a transformative approach, distilling complex dynamical behaviors into interpretable linear combinations of basis functions.","However, SINDy relies on domain-specific expertise to construct its foundational \"library\" of basis functions, which limits its adaptability and universality.","In this work, we introduce a nonlinear system identification framework called LeARN that transcends the need for prior domain knowledge by learning the library of basis functions directly from data.","To enhance adaptability to evolving system dynamics under varying noise conditions, we employ a novel meta-learning-based system identification approach that uses a lightweight deep neural network (DNN) to dynamically refine these basis functions.","This not only captures intricate system behaviors but also adapts seamlessly to new dynamical regimes.","We validate our framework on the Neural Fly dataset, showcasing its robust adaptation and generalization capabilities.","Despite its simplicity, our LeARN achieves competitive dynamical error performance compared to SINDy.","This work presents a step toward the autonomous discovery of dynamical systems, paving the way for a future where machine learning uncovers the governing principles of complex systems without requiring extensive domain-specific interventions."],"url":"http://arxiv.org/abs/2412.12036v1"}
{"created":"2024-12-16 18:03:22","title":"Backstepping Control of Tendon-Driven Continuum Robots in Large Deflections Using the Cosserat Rod Model","abstract":"This paper presents a study on the backstepping control of tendon-driven continuum robots for large deflections using the Cosserat rod model. Continuum robots are known for their flexibility and adaptability, making them suitable for various applications. However, modeling and controlling them pose challenges due to their nonlinear dynamics. To model their dynamics, the Cosserat rod method is employed to account for significant deflections, and a numerical solution method is developed to solve the resulting partial differential equations. Previous studies on controlling tendon-driven continuum robots using Cosserat rod theory focused on sliding mode control and were not tested for large deflections, lacking experimental validation. In this paper, backstepping control is proposed as an alternative to sliding mode control for achieving a significant bending. The numerical results are validated through experiments in this study, demonstrating that the proposed backstepping control approach is a promising solution for achieving large deflections with smoother trajectories, reduced settling time, and lower overshoot. Furthermore, two scenarios involving external forces and disturbances were introduced to further highlight the robustness of the backstepping control approach.","sentences":["This paper presents a study on the backstepping control of tendon-driven continuum robots for large deflections using the Cosserat rod model.","Continuum robots are known for their flexibility and adaptability, making them suitable for various applications.","However, modeling and controlling them pose challenges due to their nonlinear dynamics.","To model their dynamics, the Cosserat rod method is employed to account for significant deflections, and a numerical solution method is developed to solve the resulting partial differential equations.","Previous studies on controlling tendon-driven continuum robots using Cosserat rod theory focused on sliding mode control and were not tested for large deflections, lacking experimental validation.","In this paper, backstepping control is proposed as an alternative to sliding mode control for achieving a significant bending.","The numerical results are validated through experiments in this study, demonstrating that the proposed backstepping control approach is a promising solution for achieving large deflections with smoother trajectories, reduced settling time, and lower overshoot.","Furthermore, two scenarios involving external forces and disturbances were introduced to further highlight the robustness of the backstepping control approach."],"url":"http://arxiv.org/abs/2412.12035v1"}
{"created":"2024-12-16 18:01:40","title":"Thermodynamics-informed graph neural networks for real-time simulation of digital human twins","abstract":"The growing importance of real-time simulation in the medical field has exposed the limitations and bottlenecks inherent in the digital representation of complex biological systems. This paper presents a novel methodology aimed at advancing current lines of research in soft tissue simulation. The proposed approach introduces a hybrid model that integrates the geometric bias of graph neural networks with the physical bias derived from the imposition of a metriplectic structure as soft and hard constrains in the architecture, being able to simulate hepatic tissue with dissipative properties. This approach provides an efficient solution capable of generating predictions at high feedback rate while maintaining a remarkable generalization ability for previously unseen anatomies. This makes these features particularly relevant in the context of precision medicine and haptic rendering.   Based on the adopted methodologies, we propose a model that predicts human liver responses to traction and compression loads in as little as 7.3 milliseconds for optimized configurations and as fast as 1.65 milliseconds in the most efficient cases, all in the forward pass. The model achieves relative position errors below 0.15\\%, with stress tensor and velocity estimations maintaining relative errors under 7\\%. This demonstrates the robustness of the approach developed, which is capable of handling diverse load states and anatomies effectively. This work highlights the feasibility of integrating real-time simulation with patient-specific geometries through deep learning, paving the way for more robust digital human twins in medical applications.","sentences":["The growing importance of real-time simulation in the medical field has exposed the limitations and bottlenecks inherent in the digital representation of complex biological systems.","This paper presents a novel methodology aimed at advancing current lines of research in soft tissue simulation.","The proposed approach introduces a hybrid model that integrates the geometric bias of graph neural networks with the physical bias derived from the imposition of a metriplectic structure as soft and hard constrains in the architecture, being able to simulate hepatic tissue with dissipative properties.","This approach provides an efficient solution capable of generating predictions at high feedback rate while maintaining a remarkable generalization ability for previously unseen anatomies.","This makes these features particularly relevant in the context of precision medicine and haptic rendering.   ","Based on the adopted methodologies, we propose a model that predicts human liver responses to traction and compression loads in as little as 7.3 milliseconds for optimized configurations and as fast as 1.65 milliseconds in the most efficient cases, all in the forward pass.","The model achieves relative position errors below 0.15\\%, with stress tensor and velocity estimations maintaining relative errors under 7\\%.","This demonstrates the robustness of the approach developed, which is capable of handling diverse load states and anatomies effectively.","This work highlights the feasibility of integrating real-time simulation with patient-specific geometries through deep learning, paving the way for more robust digital human twins in medical applications."],"url":"http://arxiv.org/abs/2412.12034v1"}
{"created":"2024-12-16 17:58:45","title":"FSFM: A Generalizable Face Security Foundation Model via Self-Supervised Facial Representation Learning","abstract":"This work asks: with abundant, unlabeled real faces, how to learn a robust and transferable facial representation that boosts various face security tasks with respect to generalization performance? We make the first attempt and propose a self-supervised pretraining framework to learn fundamental representations of real face images, FSFM, that leverages the synergy between masked image modeling (MIM) and instance discrimination (ID). We explore various facial masking strategies for MIM and present a simple yet powerful CRFR-P masking, which explicitly forces the model to capture meaningful intra-region consistency and challenging inter-region coherency. Furthermore, we devise the ID network that naturally couples with MIM to establish underlying local-to-global correspondence via tailored self-distillation. These three learning objectives, namely 3C, empower encoding both local features and global semantics of real faces. After pretraining, a vanilla ViT serves as a universal vision foundation model for downstream face security tasks: cross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen diffusion facial forgery detection. Extensive experiments on 10 public datasets demonstrate that our model transfers better than supervised pretraining, visual and facial self-supervised learning arts, and even outperforms task-specialized SOTA methods.","sentences":["This work asks: with abundant, unlabeled real faces, how to learn a robust and transferable facial representation that boosts various face security tasks with respect to generalization performance?","We make the first attempt and propose a self-supervised pretraining framework to learn fundamental representations of real face images, FSFM, that leverages the synergy between masked image modeling (MIM) and instance discrimination (ID).","We explore various facial masking strategies for MIM and present a simple yet powerful CRFR-P masking, which explicitly forces the model to capture meaningful intra-region consistency and challenging inter-region coherency.","Furthermore, we devise the ID network that naturally couples with MIM to establish underlying local-to-global correspondence via tailored self-distillation.","These three learning objectives, namely 3C, empower encoding both local features and global semantics of real faces.","After pretraining, a vanilla ViT serves as a universal vision foundation model for downstream face security tasks: cross-dataset deepfake detection, cross-domain face anti-spoofing, and unseen diffusion facial forgery detection.","Extensive experiments on 10 public datasets demonstrate that our model transfers better than supervised pretraining, visual and facial self-supervised learning arts, and even outperforms task-specialized SOTA methods."],"url":"http://arxiv.org/abs/2412.12032v1"}
{"created":"2024-12-16 17:57:33","title":"RepFace: Refining Closed-Set Noise with Progressive Label Correction for Face Recognition","abstract":"Face recognition has made remarkable strides, driven by the expanding scale of datasets, advancements in various backbone and discriminative losses. However, face recognition performance is heavily affected by the label noise, especially closed-set noise. While numerous studies have focused on handling label noise, addressing closed-set noise still poses challenges. This paper identifies this challenge as training isn't robust to noise at the early-stage training, and necessitating an appropriate learning strategy for samples with low confidence, which are often misclassified as closed-set noise in later training phases. To address these issues, we propose a new framework to stabilize the training at early stages and split the samples into clean, ambiguous and noisy groups which are devised with separate training strategies. Initially, we employ generated auxiliary closed-set noisy samples to enable the model to identify noisy data at the early stages of training. Subsequently, we introduce how samples are split into clean, ambiguous and noisy groups by their similarity to the positive and nearest negative centers. Then we perform label fusion for ambiguous samples by incorporating accumulated model predictions. Finally, we apply label smoothing within the closed set, adjusting the label to a point between the nearest negative class and the initially assigned label. Extensive experiments validate the effectiveness of our method on mainstream face datasets, achieving state-of-the-art results. The code will be released upon acceptance.","sentences":["Face recognition has made remarkable strides, driven by the expanding scale of datasets, advancements in various backbone and discriminative losses.","However, face recognition performance is heavily affected by the label noise, especially closed-set noise.","While numerous studies have focused on handling label noise, addressing closed-set noise still poses challenges.","This paper identifies this challenge as training isn't robust to noise at the early-stage training, and necessitating an appropriate learning strategy for samples with low confidence, which are often misclassified as closed-set noise in later training phases.","To address these issues, we propose a new framework to stabilize the training at early stages and split the samples into clean, ambiguous and noisy groups which are devised with separate training strategies.","Initially, we employ generated auxiliary closed-set noisy samples to enable the model to identify noisy data at the early stages of training.","Subsequently, we introduce how samples are split into clean, ambiguous and noisy groups by their similarity to the positive and nearest negative centers.","Then we perform label fusion for ambiguous samples by incorporating accumulated model predictions.","Finally, we apply label smoothing within the closed set, adjusting the label to a point between the nearest negative class and the initially assigned label.","Extensive experiments validate the effectiveness of our method on mainstream face datasets, achieving state-of-the-art results.","The code will be released upon acceptance."],"url":"http://arxiv.org/abs/2412.12031v1"}
{"created":"2024-12-16 17:55:55","title":"Memory-Reduced Meta-Learning with Guaranteed Convergence","abstract":"The optimization-based meta-learning approach is gaining increased traction because of its unique ability to quickly adapt to a new task using only small amounts of data. However, existing optimization-based meta-learning approaches, such as MAML, ANIL and their variants, generally employ backpropagation for upper-level gradient estimation, which requires using historical lower-level parameters/gradients and thus increases computational and memory overhead in each iteration. In this paper, we propose a meta-learning algorithm that can avoid using historical parameters/gradients and significantly reduce memory costs in each iteration compared to existing optimization-based meta-learning approaches. In addition to memory reduction, we prove that our proposed algorithm converges sublinearly with the iteration number of upper-level optimization, and the convergence error decays sublinearly with the batch size of sampled tasks. In the specific case in terms of deterministic meta-learning, we also prove that our proposed algorithm converges to an exact solution. Moreover, we quantify that the computational complexity of the algorithm is on the order of $\\mathcal{O}(\\epsilon^{-1})$, which matches existing convergence results on meta-learning even without using any historical parameters/gradients. Experimental results on meta-learning benchmarks confirm the efficacy of our proposed algorithm.","sentences":["The optimization-based meta-learning approach is gaining increased traction because of its unique ability to quickly adapt to a new task using only small amounts of data.","However, existing optimization-based meta-learning approaches, such as MAML, ANIL and their variants, generally employ backpropagation for upper-level gradient estimation, which requires using historical lower-level parameters/gradients and thus increases computational and memory overhead in each iteration.","In this paper, we propose a meta-learning algorithm that can avoid using historical parameters/gradients and significantly reduce memory costs in each iteration compared to existing optimization-based meta-learning approaches.","In addition to memory reduction, we prove that our proposed algorithm converges sublinearly with the iteration number of upper-level optimization, and the convergence error decays sublinearly with the batch size of sampled tasks.","In the specific case in terms of deterministic meta-learning, we also prove that our proposed algorithm converges to an exact solution.","Moreover, we quantify that the computational complexity of the algorithm is on the order of $\\mathcal{O}(\\epsilon^{-1})$, which matches existing convergence results on meta-learning even without using any historical parameters/gradients.","Experimental results on meta-learning benchmarks confirm the efficacy of our proposed algorithm."],"url":"http://arxiv.org/abs/2412.12030v1"}
{"created":"2024-12-16 17:51:09","title":"Learning to Navigate in Mazes with Novel Layouts using Abstract Top-down Maps","abstract":"Learning navigation capabilities in different environments has long been one of the major challenges in decision-making. In this work, we focus on zero-shot navigation ability using given abstract $2$-D top-down maps. Like human navigation by reading a paper map, the agent reads the map as an image when navigating in a novel layout, after learning to navigate on a set of training maps. We propose a model-based reinforcement learning approach for this multi-task learning problem, where it jointly learns a hypermodel that takes top-down maps as input and predicts the weights of the transition network. We use the DeepMind Lab environment and customize layouts using generated maps. Our method can adapt better to novel environments in zero-shot and is more robust to noise.","sentences":["Learning navigation capabilities in different environments has long been one of the major challenges in decision-making.","In this work, we focus on zero-shot navigation ability using given abstract $2$-D top-down maps.","Like human navigation by reading a paper map, the agent reads the map as an image when navigating in a novel layout, after learning to navigate on a set of training maps.","We propose a model-based reinforcement learning approach for this multi-task learning problem, where it jointly learns a hypermodel that takes top-down maps as input and predicts the weights of the transition network.","We use the DeepMind Lab environment and customize layouts using generated maps.","Our method can adapt better to novel environments in zero-shot and is more robust to noise."],"url":"http://arxiv.org/abs/2412.12024v1"}
{"created":"2024-12-16 17:41:33","title":"Deep-learning-based identification of individual motion characteristics from upper-limb trajectories towards disorder stage evaluation","abstract":"The identification of individual movement characteristics sets the foundation for the assessment of personal rehabilitation progress and can provide diagnostic information on levels and stages of movement disorders. This work presents a preliminary study for differentiating individual motion patterns using a dataset of 3D upper-limb transport trajectories measured in task-space. Identifying individuals by deep time series learning can be a key step to abstracting individual motion properties. In this study, a classification accuracy of about 95% is reached for a subset of nine, and about 78% for the full set of 31 individuals. This provides insights into the separability of patient attributes by exerting a simple standardized task to be transferred to portable systems.","sentences":["The identification of individual movement characteristics sets the foundation for the assessment of personal rehabilitation progress and can provide diagnostic information on levels and stages of movement disorders.","This work presents a preliminary study for differentiating individual motion patterns using a dataset of 3D upper-limb transport trajectories measured in task-space.","Identifying individuals by deep time series learning can be a key step to abstracting individual motion properties.","In this study, a classification accuracy of about 95% is reached for a subset of nine, and about 78% for the full set of 31 individuals.","This provides insights into the separability of patient attributes by exerting a simple standardized task to be transferred to portable systems."],"url":"http://arxiv.org/abs/2412.12016v1"}
{"created":"2024-12-16 17:32:38","title":"Agentic AI-Driven Technical Troubleshooting for Enterprise Systems: A Novel Weighted Retrieval-Augmented Generation Paradigm","abstract":"Technical troubleshooting in enterprise environments often involves navigating diverse, heterogeneous data sources to resolve complex issues effectively. This paper presents a novel agentic AI solution built on a Weighted Retrieval-Augmented Generation (RAG) Framework tailored for enterprise technical troubleshooting. By dynamically weighting retrieval sources such as product manuals, internal knowledge bases, FAQs, and troubleshooting guides based on query context, the framework prioritizes the most relevant data. For instance, it gives precedence to product manuals for SKU-specific queries while incorporating general FAQs for broader issues. The system employs FAISS for efficient dense vector search, coupled with a dynamic aggregation mechanism to seamlessly integrate results from multiple sources. A Llama-based self-evaluator ensures the contextual accuracy and confidence of the generated responses before delivering them. This iterative cycle of retrieval and validation enhances precision, diversity, and reliability in response generation. Preliminary evaluations on large enterprise datasets demonstrate the framework's efficacy in improving troubleshooting accuracy, reducing resolution times, and adapting to varied technical challenges. Future research aims to enhance the framework by integrating advanced conversational AI capabilities, enabling more interactive and intuitive troubleshooting experiences. Efforts will also focus on refining the dynamic weighting mechanism through reinforcement learning to further optimize the relevance and precision of retrieved information. By incorporating these advancements, the proposed framework is poised to evolve into a comprehensive, autonomous AI solution, redefining technical service workflows across enterprise settings.","sentences":["Technical troubleshooting in enterprise environments often involves navigating diverse, heterogeneous data sources to resolve complex issues effectively.","This paper presents a novel agentic AI solution built on a Weighted Retrieval-Augmented Generation (RAG) Framework tailored for enterprise technical troubleshooting.","By dynamically weighting retrieval sources such as product manuals, internal knowledge bases, FAQs, and troubleshooting guides based on query context, the framework prioritizes the most relevant data.","For instance, it gives precedence to product manuals for SKU-specific queries while incorporating general FAQs for broader issues.","The system employs FAISS for efficient dense vector search, coupled with a dynamic aggregation mechanism to seamlessly integrate results from multiple sources.","A Llama-based self-evaluator ensures the contextual accuracy and confidence of the generated responses before delivering them.","This iterative cycle of retrieval and validation enhances precision, diversity, and reliability in response generation.","Preliminary evaluations on large enterprise datasets demonstrate the framework's efficacy in improving troubleshooting accuracy, reducing resolution times, and adapting to varied technical challenges.","Future research aims to enhance the framework by integrating advanced conversational AI capabilities, enabling more interactive and intuitive troubleshooting experiences.","Efforts will also focus on refining the dynamic weighting mechanism through reinforcement learning to further optimize the relevance and precision of retrieved information.","By incorporating these advancements, the proposed framework is poised to evolve into a comprehensive, autonomous AI solution, redefining technical service workflows across enterprise settings."],"url":"http://arxiv.org/abs/2412.12006v1"}
{"created":"2024-12-16 17:32:25","title":"Codes from $A_m$-invariant polynomials","abstract":"Let $q$ be a prime power. This paper provides a new class of linear codes that arises from the action of the alternating group on $\\mathbb F_q[x_1,\\dots,x_m]$ combined with the ideas in (M. Datta and T. Johnsen, 2022). Compared with Generalized Reed-Muller codes with similar parameters, our codes have the same asymptotic relative distance but a better rate. Our results follow from combinations of Galois theoretical methods with Weil-type bounds for the number of points of hypersurfaces over finite fields.","sentences":["Let $q$ be a prime power.","This paper provides a new class of linear codes that arises from the action of the alternating group on $\\mathbb F_q[x_1,\\dots,x_m]$ combined with the ideas in (M. Datta and T. Johnsen, 2022).","Compared with Generalized Reed-Muller codes with similar parameters, our codes have the same asymptotic relative distance but a better rate.","Our results follow from combinations of Galois theoretical methods with Weil-type bounds for the number of points of hypersurfaces over finite fields."],"url":"http://arxiv.org/abs/2412.12005v1"}
{"created":"2024-12-16 17:32:11","title":"The Open Source Advantage in Large Language Models (LLMs)","abstract":"Large language models (LLMs) mark a key shift in natural language processing (NLP), having advanced text generation, translation, and domain-specific reasoning. Closed-source models like GPT-4, powered by proprietary datasets and extensive computational resources, lead with state-of-the-art performance today. However, they face criticism for their \"black box\" nature and for limiting accessibility in a manner that hinders reproducibility and equitable AI development. By contrast, open-source initiatives like LLaMA and BLOOM prioritize democratization through community-driven development and computational efficiency. These models have significantly reduced performance gaps, particularly in linguistic diversity and domain-specific applications, while providing accessible tools for global researchers and developers. Notably, both paradigms rely on foundational architectural innovations, such as the Transformer framework by Vaswani et al. (2017). Closed-source models excel by scaling effectively, while open-source models adapt to real-world applications in underrepresented languages and domains. Techniques like Low-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source models to achieve competitive results despite limited resources. To be sure, the tension between closed-source and open-source approaches underscores a broader debate on transparency versus proprietary control in AI. Ethical considerations further highlight this divide. Closed-source systems restrict external scrutiny, while open-source models promote reproducibility and collaboration but lack standardized auditing documentation frameworks to mitigate biases. Hybrid approaches that leverage the strengths of both paradigms are likely to shape the future of LLM innovation, ensuring accessibility, competitive technical performance, and ethical deployment.","sentences":["Large language models (LLMs) mark a key shift in natural language processing (NLP), having advanced text generation, translation, and domain-specific reasoning.","Closed-source models like GPT-4, powered by proprietary datasets and extensive computational resources, lead with state-of-the-art performance today.","However, they face criticism for their \"black box\" nature and for limiting accessibility in a manner that hinders reproducibility and equitable AI development.","By contrast, open-source initiatives like LLaMA and BLOOM prioritize democratization through community-driven development and computational efficiency.","These models have significantly reduced performance gaps, particularly in linguistic diversity and domain-specific applications, while providing accessible tools for global researchers and developers.","Notably, both paradigms rely on foundational architectural innovations, such as the Transformer framework by Vaswani et al. (2017).","Closed-source models excel by scaling effectively, while open-source models adapt to real-world applications in underrepresented languages and domains.","Techniques like Low-Rank Adaptation (LoRA) and instruction-tuning datasets enable open-source models to achieve competitive results despite limited resources.","To be sure, the tension between closed-source and open-source approaches underscores a broader debate on transparency versus proprietary control in AI.","Ethical considerations further highlight this divide.","Closed-source systems restrict external scrutiny, while open-source models promote reproducibility and collaboration but lack standardized auditing documentation frameworks to mitigate biases.","Hybrid approaches that leverage the strengths of both paradigms are likely to shape the future of LLM innovation, ensuring accessibility, competitive technical performance, and ethical deployment."],"url":"http://arxiv.org/abs/2412.12004v1"}
{"created":"2024-12-16 17:29:51","title":"LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse Input Contexts","abstract":"Drafting radiology reports is a complex task requiring flexibility, where radiologists tail content to available information and particular clinical demands. However, most current radiology report generation (RRG) models are constrained to a fixed task paradigm, such as predicting the full ``finding'' section from a single image, inherently involving a mismatch between inputs and outputs. The trained models lack the flexibility for diverse inputs and could generate harmful, input-agnostic hallucinations. To bridge the gap between current RRG models and the clinical demands in practice, we first develop a data generation pipeline to create a new MIMIC-RG4 dataset, which considers four common radiology report drafting scenarios and has perfectly corresponded input and output. Secondly, we propose a novel large language model (LLM) based RRG framework, namely LLM-RG4, which utilizes LLM's flexible instruction-following capabilities and extensive general knowledge. We further develop an adaptive token fusion module that offers flexibility to handle diverse scenarios with different input combinations, while minimizing the additional computational burden associated with increased input volumes. Besides, we propose a token-level loss weighting strategy to direct the model's attention towards positive and uncertain descriptions. Experimental results demonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical efficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR datasets. We quantitatively demonstrate that our model has minimal input-agnostic hallucinations, whereas current open-source models commonly suffer from this problem.","sentences":["Drafting radiology reports is a complex task requiring flexibility, where radiologists tail content to available information and particular clinical demands.","However, most current radiology report generation (RRG) models are constrained to a fixed task paradigm, such as predicting the full ``finding'' section from a single image, inherently involving a mismatch between inputs and outputs.","The trained models lack the flexibility for diverse inputs and could generate harmful, input-agnostic hallucinations.","To bridge the gap between current RRG models and the clinical demands in practice, we first develop a data generation pipeline to create a new MIMIC-RG4 dataset, which considers four common radiology report drafting scenarios and has perfectly corresponded input and output.","Secondly, we propose a novel large language model (LLM) based RRG framework, namely LLM-RG4, which utilizes LLM's flexible instruction-following capabilities and extensive general knowledge.","We further develop an adaptive token fusion module that offers flexibility to handle diverse scenarios with different input combinations, while minimizing the additional computational burden associated with increased input volumes.","Besides, we propose a token-level loss weighting strategy to direct the model's attention towards positive and uncertain descriptions.","Experimental results demonstrate that LLM-RG4 achieves state-of-the-art performance in both clinical efficiency and natural language generation on the MIMIC-RG4 and MIMIC-CXR datasets.","We quantitatively demonstrate that our model has minimal input-agnostic hallucinations, whereas current open-source models commonly suffer from this problem."],"url":"http://arxiv.org/abs/2412.12001v1"}
{"created":"2024-12-16 17:28:25","title":"CP-Guard: Malicious Agent Detection and Defense in Collaborative Bird's Eye View Perception","abstract":"Collaborative Perception (CP) has shown a promising technique for autonomous driving, where multiple connected and autonomous vehicles (CAVs) share their perception information to enhance the overall perception performance and expand the perception range. However, in CP, ego CAV needs to receive messages from its collaborators, which makes it easy to be attacked by malicious agents. For example, a malicious agent can send harmful information to the ego CAV to mislead it. To address this critical issue, we propose a novel method, \\textbf{CP-Guard}, a tailored defense mechanism for CP that can be deployed by each agent to accurately detect and eliminate malicious agents in its collaboration network. Our key idea is to enable CP to reach a consensus rather than a conflict against the ego CAV's perception results. Based on this idea, we first develop a probability-agnostic sample consensus (PASAC) method to effectively sample a subset of the collaborators and verify the consensus without prior probabilities of malicious agents. Furthermore, we define a collaborative consistency loss (CCLoss) to capture the discrepancy between the ego CAV and its collaborators, which is used as a verification criterion for consensus. Finally, we conduct extensive experiments in collaborative bird's eye view (BEV) tasks and our results demonstrate the effectiveness of our CP-Guard.","sentences":["Collaborative Perception (CP) has shown a promising technique for autonomous driving, where multiple connected and autonomous vehicles (CAVs) share their perception information to enhance the overall perception performance and expand the perception range.","However, in CP, ego CAV needs to receive messages from its collaborators, which makes it easy to be attacked by malicious agents.","For example, a malicious agent can send harmful information to the ego CAV to mislead it.","To address this critical issue, we propose a novel method, \\textbf{CP-Guard}, a tailored defense mechanism for CP that can be deployed by each agent to accurately detect and eliminate malicious agents in its collaboration network.","Our key idea is to enable CP to reach a consensus rather than a conflict against the ego CAV's perception results.","Based on this idea, we first develop a probability-agnostic sample consensus (PASAC) method to effectively sample a subset of the collaborators and verify the consensus without prior probabilities of malicious agents.","Furthermore, we define a collaborative consistency loss (CCLoss) to capture the discrepancy between the ego CAV and its collaborators, which is used as a verification criterion for consensus.","Finally, we conduct extensive experiments in collaborative bird's eye view (BEV) tasks and our results demonstrate the effectiveness of our CP-Guard."],"url":"http://arxiv.org/abs/2412.12000v1"}
{"created":"2024-12-16 17:26:06","title":"SAMIC: Segment Anything with In-Context Spatial Prompt Engineering","abstract":"Few-shot segmentation is the problem of learning to identify specific types of objects (e.g., airplanes) in images from a small set of labeled reference images. The current state of the art is driven by resource-intensive construction of models for every new domain-specific application. Such models must be trained on enormous labeled datasets of unrelated objects (e.g., cars, trains, animals) so that their ``knowledge'' can be transferred to new types of objects. In this paper, we show how to leverage existing vision foundation models (VFMs) to reduce the incremental cost of creating few-shot segmentation models for new domains. Specifically, we introduce SAMIC, a small network that learns how to prompt VFMs in order to segment new types of objects in domain-specific applications. SAMIC enables any task to be approached as a few-shot learning problem. At 2.6 million parameters, it is 94% smaller than the leading models (e.g., having ResNet 101 backbone with 45+ million parameters). Even using 1/5th of the training data provided by one-shot benchmarks, SAMIC is competitive with, or sets the state of the art, on a variety of few-shot and semantic segmentation datasets including COCO-$20^i$, Pascal-$5^i$, PerSeg, FSS-1000, and NWPU VHR-10.","sentences":["Few-shot segmentation is the problem of learning to identify specific types of objects (e.g., airplanes) in images from a small set of labeled reference images.","The current state of the art is driven by resource-intensive construction of models for every new domain-specific application.","Such models must be trained on enormous labeled datasets of unrelated objects (e.g., cars, trains, animals) so that their ``knowledge'' can be transferred to new types of objects.","In this paper, we show how to leverage existing vision foundation models (VFMs) to reduce the incremental cost of creating few-shot segmentation models for new domains.","Specifically, we introduce SAMIC, a small network that learns how to prompt VFMs in order to segment new types of objects in domain-specific applications.","SAMIC enables any task to be approached as a few-shot learning problem.","At 2.6 million parameters, it is 94% smaller than the leading models (e.g., having ResNet 101 backbone with 45+ million parameters).","Even using 1/5th of the training data provided by one-shot benchmarks, SAMIC is competitive with, or sets the state of the art, on a variety of few-shot and semantic segmentation datasets including COCO-$20^i$, Pascal-$5^i$, PerSeg, FSS-1000, and NWPU VHR-10."],"url":"http://arxiv.org/abs/2412.11998v1"}
{"created":"2024-12-16 17:23:41","title":"Efficient Layered New Bit-Flipping QC-MDPC Decoder for BIKE Post-Quantum Cryptography","abstract":"The medium-density parity-check (MDPC) code-based Bit Flipping Key Encapsulation (BIKE) mechanism remains a candidate of post-quantum cryptography standardization. The latest version utilizes a new bit-flipping (BF) decoding algorithm, which decides the BF threshold by an affine function with high-precision coefficients. Previous BF decoder implementations can be extended to the new algorithm. However, they suffer from large memories that dominate the overall complexity. This paper proposes a column-layered decoder for the new BIKE BF decoding algorithm to substantially reduce the memory requirement, and optimizes the affine BF threshold function coefficients to reduce the code length needed for the same security level. For the first time, our work also investigates the impact of finite precision representation of the threshold coefficients on the decoding performance. For an example MDPC code considered for the standard, the proposed layered BF decoder achieves 20% complexity reduction compared to the best prior effort with a very small latency overhead.","sentences":["The medium-density parity-check (MDPC) code-based Bit Flipping Key Encapsulation (BIKE) mechanism remains a candidate of post-quantum cryptography standardization.","The latest version utilizes a new bit-flipping (BF) decoding algorithm, which decides the BF threshold by an affine function with high-precision coefficients.","Previous BF decoder implementations can be extended to the new algorithm.","However, they suffer from large memories that dominate the overall complexity.","This paper proposes a column-layered decoder for the new BIKE BF decoding algorithm to substantially reduce the memory requirement, and optimizes the affine BF threshold function coefficients to reduce the code length needed for the same security level.","For the first time, our work also investigates the impact of finite precision representation of the threshold coefficients on the decoding performance.","For an example MDPC code considered for the standard, the proposed layered BF decoder achieves 20% complexity reduction compared to the best prior effort with a very small latency overhead."],"url":"http://arxiv.org/abs/2412.11997v1"}
{"created":"2024-12-16 17:22:40","title":"Combining Large Language Models with Tutoring System Intelligence: A Case Study in Caregiver Homework Support","abstract":"Caregivers (i.e., parents and members of a child's caring community) are underappreciated stakeholders in learning analytics. Although caregiver involvement can enhance student academic outcomes, many obstacles hinder involvement, most notably knowledge gaps with respect to modern school curricula. An emerging topic of interest in learning analytics is hybrid tutoring, which includes instructional and motivational support. Caregivers assert similar roles in homework, yet it is unknown how learning analytics can support them. Our past work with caregivers suggested that conversational support is a promising method of providing caregivers with the guidance needed to effectively support student learning. We developed a system that provides instructional support to caregivers through conversational recommendations generated by a Large Language Model (LLM). Addressing known instructional limitations of LLMs, we use instructional intelligence from tutoring systems while conducting prompt engineering experiments with the open-source Llama 3 LLM. This LLM generated message recommendations for caregivers supporting their child's math practice via chat. Few-shot prompting and combining real-time problem-solving context from tutoring systems with examples of tutoring practices yielded desirable message recommendations. These recommendations were evaluated with ten middle school caregivers, who valued recommendations facilitating content-level support and student metacognition through self-explanation. We contribute insights into how tutoring systems can best be merged with LLMs to support hybrid tutoring settings through conversational assistance, facilitating effective caregiver involvement in tutoring systems.","sentences":["Caregivers (i.e., parents and members of a child's caring community) are underappreciated stakeholders in learning analytics.","Although caregiver involvement can enhance student academic outcomes, many obstacles hinder involvement, most notably knowledge gaps with respect to modern school curricula.","An emerging topic of interest in learning analytics is hybrid tutoring, which includes instructional and motivational support.","Caregivers assert similar roles in homework, yet it is unknown how learning analytics can support them.","Our past work with caregivers suggested that conversational support is a promising method of providing caregivers with the guidance needed to effectively support student learning.","We developed a system that provides instructional support to caregivers through conversational recommendations generated by a Large Language Model (LLM).","Addressing known instructional limitations of LLMs, we use instructional intelligence from tutoring systems while conducting prompt engineering experiments with the open-source Llama 3 LLM.","This LLM generated message recommendations for caregivers supporting their child's math practice via chat.","Few-shot prompting and combining real-time problem-solving context from tutoring systems with examples of tutoring practices yielded desirable message recommendations.","These recommendations were evaluated with ten middle school caregivers, who valued recommendations facilitating content-level support and student metacognition through self-explanation.","We contribute insights into how tutoring systems can best be merged with LLMs to support hybrid tutoring settings through conversational assistance, facilitating effective caregiver involvement in tutoring systems."],"url":"http://arxiv.org/abs/2412.11995v1"}
{"created":"2024-12-16 17:21:12","title":"Fairness Shields: Safeguarding against Biased Decision Makers","abstract":"As AI-based decision-makers increasingly influence human lives, it is a growing concern that their decisions are often unfair or biased with respect to people's sensitive attributes, such as gender and race. Most existing bias prevention measures provide probabilistic fairness guarantees in the long run, and it is possible that the decisions are biased on specific instances of short decision sequences. We introduce fairness shielding, where a symbolic decision-maker -- the fairness shield -- continuously monitors the sequence of decisions of another deployed black-box decision-maker, and makes interventions so that a given fairness criterion is met while the total intervention costs are minimized. We present four different algorithms for computing fairness shields, among which one guarantees fairness over fixed horizons, and three guarantee fairness periodically after fixed intervals. Given a distribution over future decisions and their intervention costs, our algorithms solve different instances of bounded-horizon optimal control problems with different levels of computational costs and optimality guarantees. Our empirical evaluation demonstrates the effectiveness of these shields in ensuring fairness while maintaining cost efficiency across various scenarios.","sentences":["As AI-based decision-makers increasingly influence human lives, it is a growing concern that their decisions are often unfair or biased with respect to people's sensitive attributes, such as gender and race.","Most existing bias prevention measures provide probabilistic fairness guarantees in the long run, and it is possible that the decisions are biased on specific instances of short decision sequences.","We introduce fairness shielding, where a symbolic decision-maker -- the fairness shield -- continuously monitors the sequence of decisions of another deployed black-box decision-maker, and makes interventions so that a given fairness criterion is met while the total intervention costs are minimized.","We present four different algorithms for computing fairness shields, among which one guarantees fairness over fixed horizons, and three guarantee fairness periodically after fixed intervals.","Given a distribution over future decisions and their intervention costs, our algorithms solve different instances of bounded-horizon optimal control problems with different levels of computational costs and optimality guarantees.","Our empirical evaluation demonstrates the effectiveness of these shields in ensuring fairness while maintaining cost efficiency across various scenarios."],"url":"http://arxiv.org/abs/2412.11994v1"}
{"created":"2024-12-16 17:14:35","title":"ExecRepoBench: Multi-level Executable Code Completion Evaluation","abstract":"Code completion has become an essential tool for daily software development. Existing evaluation benchmarks often employ static methods that do not fully capture the dynamic nature of real-world coding environments and face significant challenges, including limited context length, reliance on superficial evaluation metrics, and potential overfitting to training datasets. In this work, we introduce a novel framework for enhancing code completion in software development through the creation of a repository-level benchmark ExecRepoBench and the instruction corpora Repo-Instruct, aim at improving the functionality of open-source large language models (LLMs) in real-world coding scenarios that involve complex interdependencies across multiple files. ExecRepoBench includes 1.2K samples from active Python repositories. Plus, we present a multi-level grammar-based completion methodology conditioned on the abstract syntax tree to mask code fragments at various logical units (e.g. statements, expressions, and functions). Then, we fine-tune the open-source LLM with 7B parameters on Repo-Instruct to produce a strong code completion baseline model Qwen2.5-Coder-Instruct-C based on the open-source model. Qwen2.5-Coder-Instruct-C is rigorously evaluated against existing benchmarks, including MultiPL-E and ExecRepoBench, which consistently outperforms prior baselines across all programming languages. The deployment of \\ourmethod{} can be used as a high-performance, local service for programming development\\footnote{\\url{https://execrepobench.github.io/}}.","sentences":["Code completion has become an essential tool for daily software development.","Existing evaluation benchmarks often employ static methods that do not fully capture the dynamic nature of real-world coding environments and face significant challenges, including limited context length, reliance on superficial evaluation metrics, and potential overfitting to training datasets.","In this work, we introduce a novel framework for enhancing code completion in software development through the creation of a repository-level benchmark ExecRepoBench and the instruction corpora Repo-Instruct, aim at improving the functionality of open-source large language models (LLMs) in real-world coding scenarios that involve complex interdependencies across multiple files.","ExecRepoBench includes 1.2K samples from active Python repositories.","Plus, we present a multi-level grammar-based completion methodology conditioned on the abstract syntax tree to mask code fragments at various logical units (e.g. statements, expressions, and functions).","Then, we fine-tune the open-source LLM with 7B parameters on Repo-Instruct to produce a strong code completion baseline model Qwen2.5-Coder-Instruct-C based on the open-source model.","Qwen2.5-Coder-Instruct-C is rigorously evaluated against existing benchmarks, including MultiPL-E and ExecRepoBench, which consistently outperforms prior baselines across all programming languages.","The deployment of \\ourmethod{} can be used as a high-performance, local service for programming development\\footnote{\\url{https://execrepobench.github.io/}}."],"url":"http://arxiv.org/abs/2412.11990v1"}
{"created":"2024-12-16 17:11:48","title":"SciFaultyQA: Benchmarking LLMs on Faulty Science Question Detection with a GAN-Inspired Approach to Synthetic Dataset Generation","abstract":"Consider the problem: ``If one man and one woman can produce one child in one year, how many children will be produced by one woman and three men in 0.5 years?\" Current large language models (LLMs) such as GPT-4o, GPT-o1-preview, and Gemini Flash frequently answer \"0.5,\" which does not make sense. While these models sometimes acknowledge the unrealistic nature of the question, in many cases (8 out of 10 trials), they provide the nonsensical answer of \"0.5 child.\" Additionally, temporal variation has been observed: if an LLM answers correctly once (by recognizing the faulty nature of the question), subsequent responses are more likely to also reflect this understanding. However, this is inconsistent.   These types of questions have motivated us to develop a dataset of science questions, SciFaultyQA, where the questions themselves are intentionally faulty. We observed that LLMs often proceed to answer these flawed questions without recognizing their inherent issues, producing results that are logically or scientifically invalid. By analyzing such patterns, we developed a novel method for generating synthetic datasets to evaluate and benchmark the performance of various LLMs in identifying these flawed questions. We have also developed novel approaches to reduce the errors.","sentences":["Consider the problem: ``If one man and one woman can produce one child in one year, how many children will be produced by one woman and three men in 0.5 years?\"","Current large language models (LLMs) such as GPT-4o, GPT-o1-preview, and Gemini Flash frequently answer \"0.5,\" which does not make sense.","While these models sometimes acknowledge the unrealistic nature of the question, in many cases (8 out of 10 trials), they provide the nonsensical answer of \"0.5 child.\"","Additionally, temporal variation has been observed: if an LLM answers correctly once (by recognizing the faulty nature of the question), subsequent responses are more likely to also reflect this understanding.","However, this is inconsistent.   ","These types of questions have motivated us to develop a dataset of science questions, SciFaultyQA, where the questions themselves are intentionally faulty.","We observed that LLMs often proceed to answer these flawed questions without recognizing their inherent issues, producing results that are logically or scientifically invalid.","By analyzing such patterns, we developed a novel method for generating synthetic datasets to evaluate and benchmark the performance of various LLMs in identifying these flawed questions.","We have also developed novel approaches to reduce the errors."],"url":"http://arxiv.org/abs/2412.11988v1"}
{"created":"2024-12-16 17:07:26","title":"Speak & Improve Corpus 2025: an L2 English Speech Corpus for Language Assessment and Feedback","abstract":"We introduce the Speak \\& Improve Corpus 2025, a dataset of L2 learner English data with holistic scores and language error annotation, collected from open (spontaneous) speaking tests on the Speak \\& Improve learning platform https://speakandimprove.com . The aim of the corpus release is to address a major challenge to developing L2 spoken language processing systems, the lack of publicly available data with high-quality annotations. It is being made available for non-commercial use on the ELiT website. In designing this corpus we have sought to make it cover a wide-range of speaker attributes, from their L1 to their speaking ability, as well as providing manual annotations. This enables a range of language-learning tasks to be examined, such as assessing speaking proficiency or providing feedback on grammatical errors in a learner's speech. Additionally, the data supports research into the underlying technology required for these tasks including automatic speech recognition (ASR) of low resource L2 learner English, disfluency detection or spoken grammatical error correction (GEC). The corpus consists of around 340 hours of L2 English learners audio with holistic scores, and a subset of audio annotated with transcriptions and error labels.","sentences":["We introduce the Speak \\& Improve Corpus 2025, a dataset of L2 learner English data with holistic scores and language error annotation, collected from open (spontaneous) speaking tests on the Speak \\& Improve learning platform https://speakandimprove.com .","The aim of the corpus release is to address a major challenge to developing L2 spoken language processing systems, the lack of publicly available data with high-quality annotations.","It is being made available for non-commercial use on the ELiT website.","In designing this corpus we have sought to make it cover a wide-range of speaker attributes, from their L1 to their speaking ability, as well as providing manual annotations.","This enables a range of language-learning tasks to be examined, such as assessing speaking proficiency or providing feedback on grammatical errors in a learner's speech.","Additionally, the data supports research into the underlying technology required for these tasks including automatic speech recognition (ASR) of low resource L2 learner English, disfluency detection or spoken grammatical error correction (GEC).","The corpus consists of around 340 hours of L2 English learners audio with holistic scores, and a subset of audio annotated with transcriptions and error labels."],"url":"http://arxiv.org/abs/2412.11986v1"}
{"created":"2024-12-16 17:05:18","title":"Speak & Improve Challenge 2025: Tasks and Baseline Systems","abstract":"This paper presents the \"Speak & Improve Challenge 2025: Spoken Language Assessment and Feedback\" -- a challenge associated with the ISCA SLaTE 2025 Workshop. The goal of the challenge is to advance research on spoken language assessment and feedback, with tasks associated with both the underlying technology and language learning feedback. Linked with the challenge, the Speak & Improve (S&I) Corpus 2025 is being pre-released, a dataset of L2 learner English data with holistic scores and language error annotation, collected from open (spontaneous) speaking tests on the Speak & Improve learning platform. The corpus consists of 340 hours of audio data from second language English learners with holistic scores, and a 60-hour subset with manual transcriptions and error labels. The Challenge has four shared tasks: Automatic Speech Recognition (ASR), Spoken Language Assessment (SLA), Spoken Grammatical Error Correction (SGEC), and Spoken Grammatical Error Correction Feedback (SGECF). Each of these tasks has a closed track where a predetermined set of models and data sources are allowed to be used, and an open track where any public resource may be used. Challenge participants may do one or more of the tasks. This paper describes the challenge, the S&I Corpus 2025, and the baseline systems released for the Challenge.","sentences":["This paper presents the \"Speak & Improve Challenge 2025: Spoken Language Assessment and Feedback\" -- a challenge associated with the ISCA SLaTE 2025 Workshop.","The goal of the challenge is to advance research on spoken language assessment and feedback, with tasks associated with both the underlying technology and language learning feedback.","Linked with the challenge, the Speak & Improve (S&I) Corpus 2025 is being pre-released, a dataset of L2 learner English data with holistic scores and language error annotation, collected from open (spontaneous) speaking tests on the Speak & Improve learning platform.","The corpus consists of 340 hours of audio data from second language English learners with holistic scores, and a 60-hour subset with manual transcriptions and error labels.","The Challenge has four shared tasks: Automatic Speech Recognition (ASR), Spoken Language Assessment (SLA), Spoken Grammatical Error Correction (SGEC), and Spoken Grammatical Error Correction Feedback (SGECF).","Each of these tasks has a closed track where a predetermined set of models and data sources are allowed to be used, and an open track where any public resource may be used.","Challenge participants may do one or more of the tasks.","This paper describes the challenge, the S&I Corpus 2025, and the baseline systems released for the Challenge."],"url":"http://arxiv.org/abs/2412.11985v1"}
{"created":"2024-12-16 17:04:40","title":"Cost-Effective Label-free Node Classification with LLMs","abstract":"Graph neural networks (GNNs) have emerged as go-to models for node classification in graph data due to their powerful abilities in fusing graph structures and attributes. However, such models strongly rely on adequate high-quality labeled data for training, which are expensive to acquire in practice. With the advent of large language models (LLMs), a promising way is to leverage their superb zero-shot capabilities and massive knowledge for node labeling. Despite promising results reported, this methodology either demands considerable queries to LLMs, or suffers from compromised performance caused by noisy labels produced by LLMs.   To remedy these issues, this work presents Cella, an active self-training framework that integrates LLMs into GNNs in a cost-effective manner. The design recipe of Cella is to iteratively identify small sets of \"critical\" samples using GNNs and extract informative pseudo-labels for them with both LLMs and GNNs as additional supervision signals to enhance model training. Particularly, Cella includes three major components: (i) an effective active node selection strategy for initial annotations; (ii) a judicious sample selection scheme to sift out the \"critical\" nodes based on label disharmonicity and entropy; and (iii) a label refinement module combining LLMs and GNNs with rewired topology. Our extensive experiments over five benchmark text-attributed graph datasets demonstrate that Cella significantly outperforms the state of the arts under the same query budget to LLMs in terms of label-free node classification. In particular, on the DBLP dataset with 14.3k nodes, Cella is able to achieve an 8.08% conspicuous improvement in accuracy over the state-of-the-art at a cost of less than one cent.","sentences":["Graph neural networks (GNNs) have emerged as go-to models for node classification in graph data due to their powerful abilities in fusing graph structures and attributes.","However, such models strongly rely on adequate high-quality labeled data for training, which are expensive to acquire in practice.","With the advent of large language models (LLMs), a promising way is to leverage their superb zero-shot capabilities and massive knowledge for node labeling.","Despite promising results reported, this methodology either demands considerable queries to LLMs, or suffers from compromised performance caused by noisy labels produced by LLMs.   ","To remedy these issues, this work presents Cella, an active self-training framework that integrates LLMs into GNNs in a cost-effective manner.","The design recipe of Cella is to iteratively identify small sets of \"critical\" samples using GNNs and extract informative pseudo-labels for them with both LLMs and GNNs as additional supervision signals to enhance model training.","Particularly, Cella includes three major components: (i) an effective active node selection strategy for initial annotations; (ii) a judicious sample selection scheme to sift out the \"critical\" nodes based on label disharmonicity and entropy; and (iii) a label refinement module combining LLMs and GNNs with rewired topology.","Our extensive experiments over five benchmark text-attributed graph datasets demonstrate that Cella significantly outperforms the state of the arts under the same query budget to LLMs in terms of label-free node classification.","In particular, on the DBLP dataset with 14.3k nodes, Cella is able to achieve an 8.08% conspicuous improvement in accuracy over the state-of-the-art at a cost of less than one cent."],"url":"http://arxiv.org/abs/2412.11983v1"}
{"created":"2024-12-16 17:03:04","title":"Industrial-scale Prediction of Cement Clinker Phases using Machine Learning","abstract":"Cement production, exceeding 4.1 billion tonnes and contributing 2.4 tonnes of CO2 annually, faces critical challenges in quality control and process optimization. While traditional process models for cement manufacturing are confined to steady-state conditions with limited predictive capability for mineralogical phases, modern plants operate under dynamic conditions that demand real-time quality assessment. Here, exploiting a comprehensive two-year operational dataset from an industrial cement plant, we present a machine learning framework that accurately predicts clinker mineralogy from process data. Our model achieves unprecedented prediction accuracy for major clinker phases while requiring minimal input parameters, demonstrating robust performance under varying operating conditions. Through post-hoc explainable algorithms, we interpret the hierarchical relationships between clinker oxides and phase formation, providing insights into the functioning of an otherwise black-box model. This digital twin framework can potentially enable real-time optimization of cement production, thereby providing a route toward reducing material waste and ensuring quality while reducing the associated emissions under real plant conditions. Our approach represents a significant advancement in industrial process control, offering a scalable solution for sustainable cement manufacturing.","sentences":["Cement production, exceeding 4.1 billion tonnes and contributing 2.4 tonnes of CO2 annually, faces critical challenges in quality control and process optimization.","While traditional process models for cement manufacturing are confined to steady-state conditions with limited predictive capability for mineralogical phases, modern plants operate under dynamic conditions that demand real-time quality assessment.","Here, exploiting a comprehensive two-year operational dataset from an industrial cement plant, we present a machine learning framework that accurately predicts clinker mineralogy from process data.","Our model achieves unprecedented prediction accuracy for major clinker phases while requiring minimal input parameters, demonstrating robust performance under varying operating conditions.","Through post-hoc explainable algorithms, we interpret the hierarchical relationships between clinker oxides and phase formation, providing insights into the functioning of an otherwise black-box model.","This digital twin framework can potentially enable real-time optimization of cement production, thereby providing a route toward reducing material waste and ensuring quality while reducing the associated emissions under real plant conditions.","Our approach represents a significant advancement in industrial process control, offering a scalable solution for sustainable cement manufacturing."],"url":"http://arxiv.org/abs/2412.11981v1"}
{"created":"2024-12-16 16:59:55","title":"AlphaZero Neural Scaling and Zipf's Law: a Tale of Board Games and Power Laws","abstract":"Neural scaling laws are observed in a range of domains, to date with no clear understanding of why they occur. Recent theories suggest that loss power laws arise from Zipf's law, a power law observed in domains like natural language. One theory suggests that language scaling laws emerge when Zipf-distributed task quanta are learned in descending order of frequency. In this paper we examine power-law scaling in AlphaZero, a reinforcement learning algorithm, using a theory of language-model scaling. We find that game states in training and inference data scale with Zipf's law, which is known to arise from the tree structure of the environment, and examine the correlation between scaling-law and Zipf's-law exponents. In agreement with quanta scaling theory, we find that agents optimize state loss in descending order of frequency, even though this order scales inversely with modelling complexity. We also find that inverse scaling, the failure of models to improve with size, is correlated with unusual Zipf curves where end-game states are among the most frequent states. We show evidence that larger models shift their focus to these less-important states, sacrificing their understanding of important early-game states.","sentences":["Neural scaling laws are observed in a range of domains, to date with no clear understanding of why they occur.","Recent theories suggest that loss power laws arise from Zipf's law, a power law observed in domains like natural language.","One theory suggests that language scaling laws emerge when Zipf-distributed task quanta are learned in descending order of frequency.","In this paper we examine power-law scaling in AlphaZero, a reinforcement learning algorithm, using a theory of language-model scaling.","We find that game states in training and inference data scale with Zipf's law, which is known to arise from the tree structure of the environment, and examine the correlation between scaling-law and Zipf's-law exponents.","In agreement with quanta scaling theory, we find that agents optimize state loss in descending order of frequency, even though this order scales inversely with modelling complexity.","We also find that inverse scaling, the failure of models to improve with size, is correlated with unusual Zipf curves where end-game states are among the most frequent states.","We show evidence that larger models shift their focus to these less-important states, sacrificing their understanding of important early-game states."],"url":"http://arxiv.org/abs/2412.11979v1"}
{"created":"2024-12-16 16:59:22","title":"Speech Foundation Models and Crowdsourcing for Efficient, High-Quality Data Collection","abstract":"While crowdsourcing is an established solution for facilitating and scaling the collection of speech data, the involvement of non-experts necessitates protocols to ensure final data quality. To reduce the costs of these essential controls, this paper investigates the use of Speech Foundation Models (SFMs) to automate the validation process, examining for the first time the cost/quality trade-off in data acquisition. Experiments conducted on French, German, and Korean data demonstrate that SFM-based validation has the potential to reduce reliance on human validation, resulting in an estimated cost saving of over 40.0% without degrading final data quality. These findings open new opportunities for more efficient, cost-effective, and scalable speech data acquisition.","sentences":["While crowdsourcing is an established solution for facilitating and scaling the collection of speech data, the involvement of non-experts necessitates protocols to ensure final data quality.","To reduce the costs of these essential controls, this paper investigates the use of Speech Foundation Models (SFMs) to automate the validation process, examining for the first time the cost/quality trade-off in data acquisition.","Experiments conducted on French, German, and Korean data demonstrate that SFM-based validation has the potential to reduce reliance on human validation, resulting in an estimated cost saving of over 40.0% without degrading final data quality.","These findings open new opportunities for more efficient, cost-effective, and scalable speech data acquisition."],"url":"http://arxiv.org/abs/2412.11978v1"}
{"created":"2024-12-16 16:59:14","title":"Weak Strategyproofness in Randomized Social Choice","abstract":"An important -- but very demanding -- property in collective decision-making is strategyproofness, which requires that voters cannot benefit from submitting insincere preferences. Gibbard (1977) has shown that only rather unattractive rules are strategyproof, even when allowing for randomization. However, Gibbard's theorem is based on a rather strong interpretation of strategyproofness, which deems a manipulation successful if it increases the voter's expected utility for at least one utility function consistent with his ordinal preferences. In this paper, we study weak strategyproofness, which deems a manipulation successful if it increases the voter's expected utility for all utility functions consistent with his ordinal preferences. We show how to systematically design attractive, weakly strategyproof social decision schemes (SDSs) and explore their limitations for both strict and weak preferences. In particular, for strict preferences, we show that there are weakly strategyproof SDSs that are either ex post efficient or Condorcet-consistent, while neither even-chance SDSs nor pairwise SDSs satisfy both properties and weak strategyproofness at the same time. By contrast, for the case of weak preferences, we discuss two sweeping impossibility results that preclude the existence of appealing weakly strategyproof SDSs.","sentences":["An important -- but very demanding -- property in collective decision-making is strategyproofness, which requires that voters cannot benefit from submitting insincere preferences.","Gibbard (1977) has shown that only rather unattractive rules are strategyproof, even when allowing for randomization.","However, Gibbard's theorem is based on a rather strong interpretation of strategyproofness, which deems a manipulation successful if it increases the voter's expected utility for at least one utility function consistent with his ordinal preferences.","In this paper, we study weak strategyproofness, which deems a manipulation successful if it increases the voter's expected utility for all utility functions consistent with his ordinal preferences.","We show how to systematically design attractive, weakly strategyproof social decision schemes (SDSs) and explore their limitations for both strict and weak preferences.","In particular, for strict preferences, we show that there are weakly strategyproof SDSs that are either ex post efficient or Condorcet-consistent, while neither even-chance SDSs nor pairwise SDSs satisfy both properties and weak strategyproofness at the same time.","By contrast, for the case of weak preferences, we discuss two sweeping impossibility results that preclude the existence of appealing weakly strategyproof SDSs."],"url":"http://arxiv.org/abs/2412.11977v1"}
{"created":"2024-12-16 16:58:28","title":"Emma-X: An Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning","abstract":"Traditional reinforcement learning-based robotic control methods are often task-specific and fail to generalize across diverse environments or unseen objects and instructions. Visual Language Models (VLMs) demonstrate strong scene understanding and planning capabilities but lack the ability to generate actionable policies tailored to specific robotic embodiments. To address this, Visual-Language-Action (VLA) models have emerged, yet they face challenges in long-horizon spatial reasoning and grounded task planning. In this work, we propose the Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed hierarchical embodiment dataset based on BridgeV2, containing 60,000 robot manipulation trajectories auto-annotated with grounded task reasoning and spatial guidance. Additionally, we introduce a trajectory segmentation strategy based on gripper states and motion trajectories, which can help mitigate hallucination in grounding subtask reasoning generation. Experimental results demonstrate that Emma-X achieves superior performance over competitive baselines, particularly in real-world robotic tasks requiring spatial reasoning.","sentences":["Traditional reinforcement learning-based robotic control methods are often task-specific and fail to generalize across diverse environments or unseen objects and instructions.","Visual Language Models (VLMs) demonstrate strong scene understanding and planning capabilities but lack the ability to generate actionable policies tailored to specific robotic embodiments.","To address this, Visual-Language-Action (VLA) models have emerged, yet they face challenges in long-horizon spatial reasoning and grounded task planning.","In this work, we propose the Embodied Multimodal Action Model with Grounded Chain of Thought and Look-ahead Spatial Reasoning, Emma-X. Emma-X leverages our constructed hierarchical embodiment dataset based on BridgeV2, containing 60,000 robot manipulation trajectories auto-annotated with grounded task reasoning and spatial guidance.","Additionally, we introduce a trajectory segmentation strategy based on gripper states and motion trajectories, which can help mitigate hallucination in grounding subtask reasoning generation.","Experimental results demonstrate that Emma-X achieves superior performance over competitive baselines, particularly in real-world robotic tasks requiring spatial reasoning."],"url":"http://arxiv.org/abs/2412.11974v1"}
{"created":"2024-12-16 16:55:22","title":"Controllable Shadow Generation with Single-Step Diffusion Models from Synthetic Data","abstract":"Realistic shadow generation is a critical component for high-quality image compositing and visual effects, yet existing methods suffer from certain limitations: Physics-based approaches require a 3D scene geometry, which is often unavailable, while learning-based techniques struggle with control and visual artifacts. We introduce a novel method for fast, controllable, and background-free shadow generation for 2D object images. We create a large synthetic dataset using a 3D rendering engine to train a diffusion model for controllable shadow generation, generating shadow maps for diverse light source parameters. Through extensive ablation studies, we find that rectified flow objective achieves high-quality results with just a single sampling step enabling real-time applications. Furthermore, our experiments demonstrate that the model generalizes well to real-world images. To facilitate further research in evaluating quality and controllability in shadow generation, we release a new public benchmark containing a diverse set of object images and shadow maps in various settings. The project page is available at https://gojasper.github.io/controllable-shadow-generation-project/","sentences":["Realistic shadow generation is a critical component for high-quality image compositing and visual effects, yet existing methods suffer from certain limitations: Physics-based approaches require a 3D scene geometry, which is often unavailable, while learning-based techniques struggle with control and visual artifacts.","We introduce a novel method for fast, controllable, and background-free shadow generation for 2D object images.","We create a large synthetic dataset using a 3D rendering engine to train a diffusion model for controllable shadow generation, generating shadow maps for diverse light source parameters.","Through extensive ablation studies, we find that rectified flow objective achieves high-quality results with just a single sampling step enabling real-time applications.","Furthermore, our experiments demonstrate that the model generalizes well to real-world images.","To facilitate further research in evaluating quality and controllability in shadow generation, we release a new public benchmark containing a diverse set of object images and shadow maps in various settings.","The project page is available at https://gojasper.github.io/controllable-shadow-generation-project/"],"url":"http://arxiv.org/abs/2412.11972v1"}
{"created":"2024-12-16 16:51:27","title":"DARWIN 1.5: Large Language Models as Materials Science Adapted Learners","abstract":"Materials discovery and design aim to find components and structures with desirable properties over highly complex and diverse search spaces. Traditional solutions, such as high-throughput simulations and machine learning (ML), often rely on complex descriptors, which hinder generalizability and transferability across tasks. Moreover, these descriptors may deviate from experimental data due to inevitable defects and purity issues in the real world, which may reduce their effectiveness in practical applications. To address these challenges, we propose Darwin 1.5, an open-source large language model (LLM) tailored for materials science. By leveraging natural language as input, Darwin eliminates the need for task-specific descriptors and enables a flexible, unified approach to material property prediction and discovery. We employ a two-stage training strategy combining question-answering (QA) fine-tuning with multi-task learning (MTL) to inject domain-specific knowledge in various modalities and facilitate cross-task knowledge transfer. Through our strategic approach, we achieved a significant enhancement in the prediction accuracy of LLMs, with a maximum improvement of 60\\% compared to LLaMA-7B base models. It further outperforms traditional machine learning models on various tasks in material science, showcasing the potential of LLMs to provide a more versatile and scalable foundation model for materials discovery and design.","sentences":["Materials discovery and design aim to find components and structures with desirable properties over highly complex and diverse search spaces.","Traditional solutions, such as high-throughput simulations and machine learning (ML), often rely on complex descriptors, which hinder generalizability and transferability across tasks.","Moreover, these descriptors may deviate from experimental data due to inevitable defects and purity issues in the real world, which may reduce their effectiveness in practical applications.","To address these challenges, we propose Darwin 1.5, an open-source large language model (LLM) tailored for materials science.","By leveraging natural language as input, Darwin eliminates the need for task-specific descriptors and enables a flexible, unified approach to material property prediction and discovery.","We employ a two-stage training strategy combining question-answering (QA) fine-tuning with multi-task learning (MTL) to inject domain-specific knowledge in various modalities and facilitate cross-task knowledge transfer.","Through our strategic approach, we achieved a significant enhancement in the prediction accuracy of LLMs, with a maximum improvement of 60\\% compared to LLaMA-7B base models.","It further outperforms traditional machine learning models on various tasks in material science, showcasing the potential of LLMs to provide a more versatile and scalable foundation model for materials discovery and design."],"url":"http://arxiv.org/abs/2412.11970v1"}
{"created":"2024-12-16 16:47:15","title":"A Digital twin for Diesel Engines: Operator-infused PINNs with Transfer Learning for Engine Health Monitoring","abstract":"Improving diesel engine efficiency and emission reduction have been critical research topics. Recent government regulations have shifted this focus to another important area related to engine health and performance monitoring. Although the advancements in the use of deep learning methods for system monitoring have shown promising results in this direction, designing efficient methods suitable for field systems remains an open research challenge. The objective of this study is to develop a computationally efficient neural network-based approach for identifying unknown parameters of a mean value diesel engine model to facilitate physics-based health monitoring and maintenance forecasting. We propose a hybrid method combining physics informed neural networks, PINNs, and a deep neural operator, DeepONet to predict unknown parameters and gas flow dynamics in a diesel engine. The operator network predicts independent actuator dynamics learnt through offline training, thereby reducing the PINNs online computational cost. To address PINNs need for retraining with changing input scenarios, we propose two transfer learning (TL) strategies. The first strategy involves multi-stage transfer learning for parameter identification. While this method is computationally efficient as compared to online PINN training, improvements are required to meet field requirements. The second TL strategy focuses solely on training the output weights and biases of a subset of multi-head networks pretrained on a larger dataset, substantially reducing computation time during online prediction. We also evaluate our model for epistemic and aleatoric uncertainty by incorporating dropout in pretrained networks and Gaussian noise in the training dataset. This strategy offers a tailored, computationally inexpensive, and physics-based approach for parameter identification in diesel engine sub systems.","sentences":["Improving diesel engine efficiency and emission reduction have been critical research topics.","Recent government regulations have shifted this focus to another important area related to engine health and performance monitoring.","Although the advancements in the use of deep learning methods for system monitoring have shown promising results in this direction, designing efficient methods suitable for field systems remains an open research challenge.","The objective of this study is to develop a computationally efficient neural network-based approach for identifying unknown parameters of a mean value diesel engine model to facilitate physics-based health monitoring and maintenance forecasting.","We propose a hybrid method combining physics informed neural networks, PINNs, and a deep neural operator, DeepONet to predict unknown parameters and gas flow dynamics in a diesel engine.","The operator network predicts independent actuator dynamics learnt through offline training, thereby reducing the PINNs online computational cost.","To address PINNs need for retraining with changing input scenarios, we propose two transfer learning (TL) strategies.","The first strategy involves multi-stage transfer learning for parameter identification.","While this method is computationally efficient as compared to online PINN training, improvements are required to meet field requirements.","The second TL strategy focuses solely on training the output weights and biases of a subset of multi-head networks pretrained on a larger dataset, substantially reducing computation time during online prediction.","We also evaluate our model for epistemic and aleatoric uncertainty by incorporating dropout in pretrained networks and Gaussian noise in the training dataset.","This strategy offers a tailored, computationally inexpensive, and physics-based approach for parameter identification in diesel engine sub systems."],"url":"http://arxiv.org/abs/2412.11967v1"}
{"created":"2024-12-16 16:45:33","title":"Inferring Functionality of Attention Heads from their Parameters","abstract":"Attention heads are one of the building blocks of large language models (LLMs). Prior work on investigating their operation mostly focused on analyzing their behavior during inference for specific circuits or tasks. In this work, we seek a comprehensive mapping of the operations they implement in a model. We propose MAPS (Mapping Attention head ParameterS), an efficient framework that infers the functionality of attention heads from their parameters, without any model training or inference. We showcase the utility of MAPS for answering two types of questions: (a) given a predefined operation, mapping how strongly heads across the model implement it, and (b) given an attention head, inferring its salient functionality. Evaluating MAPS on 20 operations across 6 popular LLMs shows its estimations correlate with the head's outputs during inference and are causally linked to the model's predictions. Moreover, its mappings reveal attention heads of certain operations that were overlooked in previous studies, and valuable insights on function universality and architecture biases in LLMs. Next, we present an automatic pipeline and analysis that leverage MAPS to characterize the salient operations of a given head. Our pipeline produces plausible operation descriptions for most heads, as assessed by human judgment, while revealing diverse operations.","sentences":["Attention heads are one of the building blocks of large language models (LLMs).","Prior work on investigating their operation mostly focused on analyzing their behavior during inference for specific circuits or tasks.","In this work, we seek a comprehensive mapping of the operations they implement in a model.","We propose MAPS (Mapping Attention head ParameterS), an efficient framework that infers the functionality of attention heads from their parameters, without any model training or inference.","We showcase the utility of MAPS for answering two types of questions: (a) given a predefined operation, mapping how strongly heads across the model implement it, and (b) given an attention head, inferring its salient functionality.","Evaluating MAPS on 20 operations across 6 popular LLMs shows its estimations correlate with the head's outputs during inference and are causally linked to the model's predictions.","Moreover, its mappings reveal attention heads of certain operations that were overlooked in previous studies, and valuable insights on function universality and architecture biases in LLMs.","Next, we present an automatic pipeline and analysis that leverage MAPS to characterize the salient operations of a given head.","Our pipeline produces plausible operation descriptions for most heads, as assessed by human judgment, while revealing diverse operations."],"url":"http://arxiv.org/abs/2412.11965v1"}
{"created":"2024-12-16 16:45:16","title":"Approximating the Top Eigenvector in Random Order Streams","abstract":"When rows of an $n \\times d$ matrix $A$ are given in a stream, we study algorithms for approximating the top eigenvector of the matrix ${A}^TA$ (equivalently, the top right singular vector of $A$). We consider worst case inputs $A$ but assume that the rows are presented to the streaming algorithm in a uniformly random order. We show that when the gap parameter $R = \\sigma_1(A)^2/\\sigma_2(A)^2 = \\Omega(1)$, then there is a randomized algorithm that uses $O(h \\cdot d \\cdot \\operatorname{polylog}(d))$ bits of space and outputs a unit vector $v$ that has a correlation $1 - O(1/\\sqrt{R})$ with the top eigenvector $v_1$. Here $h$ denotes the number of \\emph{heavy rows} in the matrix, defined as the rows with Euclidean norm at least $\\|{A}\\|_F/\\sqrt{d \\cdot \\operatorname{polylog}(d)}$. We also provide a lower bound showing that any algorithm using $O(hd/R)$ bits of space can obtain at most $1 - \\Omega(1/R^2)$ correlation with the top eigenvector. Thus, parameterizing the space complexity in terms of the number of heavy rows is necessary for high accuracy solutions.   Our results improve upon the $R = \\Omega(\\log n \\cdot \\log d)$ requirement in a recent work of Price and Xun (FOCS 2024). We note that the algorithm of Price and Xun works for arbitrary order streams whereas our algorithm requires a stronger assumption that the rows are presented in a uniformly random order. We additionally show that the gap requirements in their analysis can be brought down to $R = \\Omega(\\log^2 d)$ for arbitrary order streams and $R = \\Omega(\\log d)$ for random order streams. The requirement of $R = \\Omega(\\log d)$ for random order streams is nearly tight for their analysis as we obtain a simple instance with $R = \\Omega(\\log d/\\log\\log d)$ for which their algorithm, with any fixed learning rate, cannot output a vector approximating the top eigenvector $v_1$.","sentences":["When rows of an $n \\times d$ matrix $A$ are given in a stream, we study algorithms for approximating the top eigenvector of the matrix ${A}^TA$ (equivalently, the top right singular vector of $A$).","We consider worst case inputs $A$ but assume that the rows are presented to the streaming algorithm in a uniformly random order.","We show that when the gap parameter $R = \\sigma_1(A)^2/\\sigma_2(A)^2 = \\Omega(1)$, then there is a randomized algorithm that uses $O(h \\cdot d \\cdot \\operatorname{polylog}(d))$ bits of space and outputs a unit vector $v$ that has a correlation $1 - O(1/\\sqrt{R})$ with the top eigenvector $v_1$. Here $h$ denotes the number of \\emph{heavy rows} in the matrix, defined as the rows with Euclidean norm at least $\\|{A}\\|_F/\\sqrt{d \\cdot \\operatorname{polylog}(d)}$. We also provide a lower bound showing that any algorithm using $O(hd/R)$ bits of space can obtain at most $1 - \\Omega(1/R^2)$ correlation with the top eigenvector.","Thus, parameterizing the space complexity in terms of the number of heavy rows is necessary for high accuracy solutions.   ","Our results improve upon the $R = \\Omega(\\log n \\cdot \\log d)$ requirement in a recent work of Price and Xun (FOCS 2024).","We note that the algorithm of Price and Xun works for arbitrary order streams whereas our algorithm requires a stronger assumption that the rows are presented in a uniformly random order.","We additionally show that the gap requirements in their analysis can be brought down to $R = \\Omega(\\log^2 d)$ for arbitrary order streams and $R = \\Omega(\\log d)$ for random order streams.","The requirement of $R = \\Omega(\\log d)$ for random order streams is nearly tight for their analysis as we obtain a simple instance with $R = \\Omega(\\log d/\\log\\log d)$ for which their algorithm, with any fixed learning rate, cannot output a vector approximating the top eigenvector $v_1$."],"url":"http://arxiv.org/abs/2412.11963v1"}
{"created":"2024-12-16 16:41:51","title":"Gramian Multimodal Representation Learning and Alignment","abstract":"Human perception integrates multiple modalities, such as vision, hearing, and language, into a unified understanding of the surrounding reality. While recent multimodal models have achieved significant progress by aligning pairs of modalities via contrastive learning, their solutions are unsuitable when scaling to multiple modalities. These models typically align each modality to a designated anchor without ensuring the alignment of all modalities with each other, leading to suboptimal performance in tasks requiring a joint understanding of multiple modalities. In this paper, we structurally rethink the pairwise conventional approach to multimodal learning and we present the novel Gramian Representation Alignment Measure (GRAM), which overcomes the above-mentioned limitations. GRAM learns and then aligns $n$ modalities directly in the higher-dimensional space in which modality embeddings lie by minimizing the Gramian volume of the $k$-dimensional parallelotope spanned by the modality vectors, ensuring the geometric alignment of all modalities simultaneously. GRAM can replace cosine similarity in any downstream method, holding for 2 to $n$ modality and providing more meaningful alignment with respect to previous similarity measures. The novel GRAM-based contrastive loss function enhances the alignment of multimodal models in the higher-dimensional embedding space, leading to new state-of-the-art performance in downstream tasks such as video-audio-text retrieval and audio-video classification. The project page, the code, and the pretrained models are available at https://ispamm.github.io/GRAM/.","sentences":["Human perception integrates multiple modalities, such as vision, hearing, and language, into a unified understanding of the surrounding reality.","While recent multimodal models have achieved significant progress by aligning pairs of modalities via contrastive learning, their solutions are unsuitable when scaling to multiple modalities.","These models typically align each modality to a designated anchor without ensuring the alignment of all modalities with each other, leading to suboptimal performance in tasks requiring a joint understanding of multiple modalities.","In this paper, we structurally rethink the pairwise conventional approach to multimodal learning and we present the novel Gramian Representation Alignment Measure (GRAM), which overcomes the above-mentioned limitations.","GRAM learns and then aligns $n$ modalities directly in the higher-dimensional space in which modality embeddings lie by minimizing the Gramian volume of the $k$-dimensional parallelotope spanned by the modality vectors, ensuring the geometric alignment of all modalities simultaneously.","GRAM can replace cosine similarity in any downstream method, holding for 2 to $n$ modality and providing more meaningful alignment with respect to previous similarity measures.","The novel GRAM-based contrastive loss function enhances the alignment of multimodal models in the higher-dimensional embedding space, leading to new state-of-the-art performance in downstream tasks such as video-audio-text retrieval and audio-video classification.","The project page, the code, and the pretrained models are available at https://ispamm.github.io/GRAM/."],"url":"http://arxiv.org/abs/2412.11959v1"}
{"created":"2024-12-16 16:38:50","title":"Witty: An Efficient Solver for Computing Minimum-Size Decision Trees","abstract":"Decision trees are a classic model for summarizing and classifying data. To enhance interpretability and generalization properties, it has been proposed to favor small decision trees. Accordingly, in the minimum-size decision tree training problem (MSDT), the input is a set of training examples in $\\mathbb{R}^d$ with class labels and we aim to find a decision tree that classifies all training examples correctly and has a minimum number of nodes. MSDT is NP-hard and therefore presumably not solvable in polynomial time. Nevertheless, Komusiewicz et al. [ICML '23] developed a promising algorithmic paradigm called witness trees which solves MSDT efficiently if the solution tree is small. In this work, we test this paradigm empirically. We provide an implementation, augment it with extensive heuristic improvements, and scrutinize it on standard benchmark instances. The augmentations achieve a mean 324-fold (median 84-fold) speedup over the naive implementation. Compared to the state of the art they achieve a mean 32-fold (median 7-fold) speedup over the dynamic programming based MurTree solver [Demirovi\\'c et al., J. Mach. Learn. Res. '22] and a mean 61-fold (median 25-fold) speedup over SAT-based implementations [Janota and Morgado, SAT '20]. As a theoretical result we obtain an improved worst-case running-time bound for MSDT.","sentences":["Decision trees are a classic model for summarizing and classifying data.","To enhance interpretability and generalization properties, it has been proposed to favor small decision trees.","Accordingly, in the minimum-size decision tree training problem (MSDT), the input is a set of training examples in $\\mathbb{R}^d$ with class labels and we aim to find a decision tree that classifies all training examples correctly and has a minimum number of nodes.","MSDT is NP-hard and therefore presumably not solvable in polynomial time.","Nevertheless, Komusiewicz et al.","[ICML '23] developed a promising algorithmic paradigm called witness trees which solves MSDT efficiently if the solution tree is small.","In this work, we test this paradigm empirically.","We provide an implementation, augment it with extensive heuristic improvements, and scrutinize it on standard benchmark instances.","The augmentations achieve a mean 324-fold (median 84-fold) speedup over the naive implementation.","Compared to the state of the art they achieve a mean 32-fold (median 7-fold) speedup over the dynamic programming based MurTree solver","[Demirovi\\'c et al., J. Mach.","Learn.","Res. '22] and a mean 61-fold (median 25-fold) speedup over SAT-based implementations [Janota and Morgado, SAT '20].","As a theoretical result we obtain an improved worst-case running-time bound for MSDT."],"url":"http://arxiv.org/abs/2412.11954v1"}
{"created":"2024-12-16 16:37:03","title":"Reliable Breast Cancer Molecular Subtype Prediction based on uncertainty-aware Bayesian Deep Learning by Mammography","abstract":"Breast cancer is a heterogeneous disease with different molecular subtypes, clinical behavior, treatment responses as well as survival outcomes. The development of a reliable, accurate, available and inexpensive method to predict the molecular subtypes using medical images plays an important role in the diagnosis and prognosis of breast cancer. Recently, deep learning methods have shown good performance in the breast cancer classification tasks using various medical images. Despite all that success, classical deep learning cannot deliver the predictive uncertainty. The uncertainty represents the validity of the predictions.Therefore, the high predicted uncertainty might cause a negative effect in the accurate diagnosis of breast cancer molecular subtypes. To overcome this, uncertainty quantification methods are used to determine the predictive uncertainty. Accordingly, in this study, we proposed an uncertainty-aware Bayesian deep learning model using the full mammogram images. In addition, to increase the performance of the multi-class molecular subtype classification task, we proposed a novel hierarchical classification strategy, named the two-stage classification strategy. The separate AUC of the proposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched, luminal and triple-negative classes, respectively. The proposed model not only has a comparable performance to other studies in the field of breast cancer molecular subtypes prediction, even using full mammography images, but it is also more reliable, due to quantify the predictive uncertainty.","sentences":["Breast cancer is a heterogeneous disease with different molecular subtypes, clinical behavior, treatment responses as well as survival outcomes.","The development of a reliable, accurate, available and inexpensive method to predict the molecular subtypes using medical images plays an important role in the diagnosis and prognosis of breast cancer.","Recently, deep learning methods have shown good performance in the breast cancer classification tasks using various medical images.","Despite all that success, classical deep learning cannot deliver the predictive uncertainty.","The uncertainty represents the validity of the predictions.","Therefore, the high predicted uncertainty might cause a negative effect in the accurate diagnosis of breast cancer molecular subtypes.","To overcome this, uncertainty quantification methods are used to determine the predictive uncertainty.","Accordingly, in this study, we proposed an uncertainty-aware Bayesian deep learning model using the full mammogram images.","In addition, to increase the performance of the multi-class molecular subtype classification task, we proposed a novel hierarchical classification strategy, named the two-stage classification strategy.","The separate AUC of the proposed model for each subtype was 0.71, 0.75 and 0.86 for HER2-enriched, luminal and triple-negative classes, respectively.","The proposed model not only has a comparable performance to other studies in the field of breast cancer molecular subtypes prediction, even using full mammography images, but it is also more reliable, due to quantify the predictive uncertainty."],"url":"http://arxiv.org/abs/2412.11953v1"}
{"created":"2024-12-16 16:35:35","title":"Advancing Comprehensive Aesthetic Insight with Multi-Scale Text-Guided Self-Supervised Learning","abstract":"Image Aesthetic Assessment (IAA) is a vital and intricate task that entails analyzing and assessing an image's aesthetic values, and identifying its highlights and areas for improvement. Traditional methods of IAA often concentrate on a single aesthetic task and suffer from inadequate labeled datasets, thus impairing in-depth aesthetic comprehension. Despite efforts to overcome this challenge through the application of Multi-modal Large Language Models (MLLMs), such models remain underdeveloped for IAA purposes. To address this, we propose a comprehensive aesthetic MLLM capable of nuanced aesthetic insight. Central to our approach is an innovative multi-scale text-guided self-supervised learning technique. This technique features a multi-scale feature alignment module and capitalizes on a wealth of unlabeled data in a self-supervised manner to structurally and functionally enhance aesthetic ability. The empirical evidence indicates that accompanied with extensive instruct-tuning, our model sets new state-of-the-art benchmarks across multiple tasks, including aesthetic scoring, aesthetic commenting, and personalized image aesthetic assessment. Remarkably, it also demonstrates zero-shot learning capabilities in the emerging task of aesthetic suggesting. Furthermore, for personalized image aesthetic assessment, we harness the potential of in-context learning and showcase its inherent advantages.","sentences":["Image Aesthetic Assessment (IAA) is a vital and intricate task that entails analyzing and assessing an image's aesthetic values, and identifying its highlights and areas for improvement.","Traditional methods of IAA often concentrate on a single aesthetic task and suffer from inadequate labeled datasets, thus impairing in-depth aesthetic comprehension.","Despite efforts to overcome this challenge through the application of Multi-modal Large Language Models (MLLMs), such models remain underdeveloped for IAA purposes.","To address this, we propose a comprehensive aesthetic MLLM capable of nuanced aesthetic insight.","Central to our approach is an innovative multi-scale text-guided self-supervised learning technique.","This technique features a multi-scale feature alignment module and capitalizes on a wealth of unlabeled data in a self-supervised manner to structurally and functionally enhance aesthetic ability.","The empirical evidence indicates that accompanied with extensive instruct-tuning, our model sets new state-of-the-art benchmarks across multiple tasks, including aesthetic scoring, aesthetic commenting, and personalized image aesthetic assessment.","Remarkably, it also demonstrates zero-shot learning capabilities in the emerging task of aesthetic suggesting.","Furthermore, for personalized image aesthetic assessment, we harness the potential of in-context learning and showcase its inherent advantages."],"url":"http://arxiv.org/abs/2412.11952v1"}
{"created":"2024-12-16 16:35:31","title":"The Impact of Generalization Techniques on the Interplay Among Privacy, Utility, and Fairness in Image Classification","abstract":"This study investigates the trade-offs between fairness, privacy, and utility in image classification using machine learning (ML). Recent research suggests that generalization techniques can improve the balance between privacy and utility. One focus of this work is sharpness-aware training (SAT) and its integration with differential privacy (DP-SAT) to further improve this balance. Additionally, we examine fairness in both private and non-private learning models trained on datasets with synthetic and real-world biases. We also measure the privacy risks involved in these scenarios by performing membership inference attacks (MIAs) and explore the consequences of eliminating high-privacy risk samples, termed outliers. Moreover, we introduce a new metric, named \\emph{harmonic score}, which combines accuracy, privacy, and fairness into a single measure.   Through empirical analysis using generalization techniques, we achieve an accuracy of 81.11\\% under $(8, 10^{-5})$-DP on CIFAR-10, surpassing the 79.5\\% reported by De et al. (2022). Moreover, our experiments show that memorization of training samples can begin before the overfitting point, and generalization techniques do not guarantee the prevention of this memorization. Our analysis of synthetic biases shows that generalization techniques can amplify model bias in both private and non-private models. Additionally, our results indicate that increased bias in training data leads to reduced accuracy, greater vulnerability to privacy attacks, and higher model bias. We validate these findings with the CelebA dataset, demonstrating that similar trends persist with real-world attribute imbalances. Finally, our experiments show that removing outlier data decreases accuracy and further amplifies model bias.","sentences":["This study investigates the trade-offs between fairness, privacy, and utility in image classification using machine learning (ML).","Recent research suggests that generalization techniques can improve the balance between privacy and utility.","One focus of this work is sharpness-aware training (SAT) and its integration with differential privacy (DP-SAT) to further improve this balance.","Additionally, we examine fairness in both private and non-private learning models trained on datasets with synthetic and real-world biases.","We also measure the privacy risks involved in these scenarios by performing membership inference attacks (MIAs) and explore the consequences of eliminating high-privacy risk samples, termed outliers.","Moreover, we introduce a new metric, named \\emph{harmonic score}, which combines accuracy, privacy, and fairness into a single measure.   ","Through empirical analysis using generalization techniques, we achieve an accuracy of 81.11\\% under $(8, 10^{-5})$-DP on CIFAR-10, surpassing the 79.5\\% reported by De et al. (2022).","Moreover, our experiments show that memorization of training samples can begin before the overfitting point, and generalization techniques do not guarantee the prevention of this memorization.","Our analysis of synthetic biases shows that generalization techniques can amplify model bias in both private and non-private models.","Additionally, our results indicate that increased bias in training data leads to reduced accuracy, greater vulnerability to privacy attacks, and higher model bias.","We validate these findings with the CelebA dataset, demonstrating that similar trends persist with real-world attribute imbalances.","Finally, our experiments show that removing outlier data decreases accuracy and further amplifies model bias."],"url":"http://arxiv.org/abs/2412.11951v1"}
{"created":"2024-12-16 16:34:48","title":"Asynchronous Distributed Gaussian Process Regression for Online Learning and Dynamical Systems: Complementary Document","abstract":"This is a complementary document for the paper titled \"Asynchronous Distributed Gaussian Process Regression for Online Learning and Dynamical Systems\".","sentences":["This is a complementary document for the paper titled \"Asynchronous Distributed Gaussian Process Regression for Online Learning and Dynamical Systems\"."],"url":"http://arxiv.org/abs/2412.11950v1"}
{"created":"2024-12-16 16:33:28","title":"Coconut Palm Tree Counting on Drone Images with Deep Object Detection and Synthetic Training Data","abstract":"Drones have revolutionized various domains, including agriculture. Recent advances in deep learning have propelled among other things object detection in computer vision. This study utilized YOLO, a real-time object detector, to identify and count coconut palm trees in Ghanaian farm drone footage. The farm presented has lost track of its trees due to different planting phases. While manual counting would be very tedious and error-prone, accurately determining the number of trees is crucial for efficient planning and management of agricultural processes, especially for optimizing yields and predicting production. We assessed YOLO for palm detection within a semi-automated framework, evaluated accuracy augmentations, and pondered its potential for farmers. Data was captured in September 2022 via drones. To optimize YOLO with scarce data, synthetic images were created for model training and validation. The YOLOv7 model, pretrained on the COCO dataset (excluding coconut palms), was adapted using tailored data. Trees from footage were repositioned on synthetic images, with testing on distinct authentic images. In our experiments, we adjusted hyperparameters, improving YOLO's mean average precision (mAP). We also tested various altitudes to determine the best drone height. From an initial mAP@.5 of $0.65$, we achieved 0.88, highlighting the value of synthetic images in agricultural scenarios.","sentences":["Drones have revolutionized various domains, including agriculture.","Recent advances in deep learning have propelled among other things object detection in computer vision.","This study utilized YOLO, a real-time object detector, to identify and count coconut palm trees in Ghanaian farm drone footage.","The farm presented has lost track of its trees due to different planting phases.","While manual counting would be very tedious and error-prone, accurately determining the number of trees is crucial for efficient planning and management of agricultural processes, especially for optimizing yields and predicting production.","We assessed YOLO for palm detection within a semi-automated framework, evaluated accuracy augmentations, and pondered its potential for farmers.","Data was captured in September 2022 via drones.","To optimize YOLO with scarce data, synthetic images were created for model training and validation.","The YOLOv7 model, pretrained on the COCO dataset (excluding coconut palms), was adapted using tailored data.","Trees from footage were repositioned on synthetic images, with testing on distinct authentic images.","In our experiments, we adjusted hyperparameters, improving YOLO's mean average precision (mAP).","We also tested various altitudes to determine the best drone height.","From an initial mAP@.5 of $0.65$, we achieved 0.88, highlighting the value of synthetic images in agricultural scenarios."],"url":"http://arxiv.org/abs/2412.11949v1"}
{"created":"2024-12-16 16:31:00","title":"OpenReviewer: A Specialized Large Language Model for Generating Critical Scientific Paper Reviews","abstract":"We present OpenReviewer, an open-source system for generating high-quality peer reviews of machine learning and AI conference papers. At its core is Llama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned on 79,000 expert reviews from top ML conferences. Given a PDF paper submission and review template as input, OpenReviewer extracts the full text, including technical content like equations and tables, and generates a structured review following conference-specific guidelines. Our evaluation on 400 test papers shows that OpenReviewer produces significantly more critical and realistic reviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other LLMs tend toward overly positive assessments, OpenReviewer's recommendations closely match the distribution of human reviewer ratings. The system provides authors with rapid, constructive feedback to improve their manuscripts before submission, though it is not intended to replace human peer review. OpenReviewer is available as an online demo and open-source tool.","sentences":["We present OpenReviewer, an open-source system for generating high-quality peer reviews of machine learning and AI conference papers.","At its core is Llama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned on 79,000 expert reviews from top ML conferences.","Given a PDF paper submission and review template as input, OpenReviewer extracts the full text, including technical content like equations and tables, and generates a structured review following conference-specific guidelines.","Our evaluation on 400 test papers shows that OpenReviewer produces significantly more critical and realistic reviews compared to general-purpose LLMs like GPT-4 and Claude-3.5.","While other LLMs tend toward overly positive assessments, OpenReviewer's recommendations closely match the distribution of human reviewer ratings.","The system provides authors with rapid, constructive feedback to improve their manuscripts before submission, though it is not intended to replace human peer review.","OpenReviewer is available as an online demo and open-source tool."],"url":"http://arxiv.org/abs/2412.11948v1"}
{"created":"2024-12-16 16:26:20","title":"User-Centered Course Reengineering: An Analytical Approach to Enhancing Reading Comprehension in Educational Content","abstract":"Delivering high-quality content is crucial for effective reading comprehension and successful learning. Ensuring educational materials are interpreted as intended by their authors is a persistent challenge, especially with the added complexity of multimedia and interactivity in the digital age. Authors must continuously revise their materials to meet learners' evolving needs. Detecting comprehension barriers and identifying actionable improvements within documents is complex, particularly in education where reading is fundamental. This study presents an analytical framework to help course designers enhance educational content to better support learning outcomes. Grounded in a robust theoretical foundation integrating learning analytics, reading comprehension, and content revision, our approach introduces usage-based document reengineering. This methodology adapts document content and structure based on insights from analyzing digital reading traces-interactions between readers and content. We define reading sessions to capture these interactions and develop indicators to detect comprehension challenges. Our framework enables authors to receive tailored content revision recommendations through an interactive dashboard, presenting actionable insights from reading activity. The proposed approach was implemented and evaluated using data from a European e-learning platform. Evaluations validate the framework's effectiveness, demonstrating its capacity to empower authors with data-driven insights for targeted revisions. The findings highlight the framework's ability to enhance educational content quality, making it more responsive to learners' needs. This research significantly contributes to learning analytics and content optimization, offering practical tools to improve educational outcomes and inform future developments in e-learning.","sentences":["Delivering high-quality content is crucial for effective reading comprehension and successful learning.","Ensuring educational materials are interpreted as intended by their authors is a persistent challenge, especially with the added complexity of multimedia and interactivity in the digital age.","Authors must continuously revise their materials to meet learners' evolving needs.","Detecting comprehension barriers and identifying actionable improvements within documents is complex, particularly in education where reading is fundamental.","This study presents an analytical framework to help course designers enhance educational content to better support learning outcomes.","Grounded in a robust theoretical foundation integrating learning analytics, reading comprehension, and content revision, our approach introduces usage-based document reengineering.","This methodology adapts document content and structure based on insights from analyzing digital reading traces-interactions between readers and content.","We define reading sessions to capture these interactions and develop indicators to detect comprehension challenges.","Our framework enables authors to receive tailored content revision recommendations through an interactive dashboard, presenting actionable insights from reading activity.","The proposed approach was implemented and evaluated using data from a European e-learning platform.","Evaluations validate the framework's effectiveness, demonstrating its capacity to empower authors with data-driven insights for targeted revisions.","The findings highlight the framework's ability to enhance educational content quality, making it more responsive to learners' needs.","This research significantly contributes to learning analytics and content optimization, offering practical tools to improve educational outcomes and inform future developments in e-learning."],"url":"http://arxiv.org/abs/2412.11944v1"}
{"created":"2024-12-16 16:25:58","title":"autrainer: A Modular and Extensible Deep Learning Toolkit for Computer Audition Tasks","abstract":"This work introduces the key operating principles for autrainer, our new deep learning training framework for computer audition tasks. autrainer is a PyTorch-based toolkit that allows for rapid, reproducible, and easily extensible training on a variety of different computer audition tasks. Concretely, autrainer offers low-code training and supports a wide range of neural networks as well as preprocessing routines. In this work, we present an overview of its inner workings and key capabilities.","sentences":["This work introduces the key operating principles for autrainer, our new deep learning training framework for computer audition tasks.","autrainer is a PyTorch-based toolkit that allows for rapid, reproducible, and easily extensible training on a variety of different computer audition tasks.","Concretely, autrainer offers low-code training and supports a wide range of neural networks as well as preprocessing routines.","In this work, we present an overview of its inner workings and key capabilities."],"url":"http://arxiv.org/abs/2412.11943v1"}
{"created":"2024-12-16 16:25:08","title":"An Integer Linear Program for Periodic Scheduling in Universities","abstract":"Efficient scheduling of periodic meetings is a critical challenge in various service-oriented domains, including academic settings, healthcare, and legal consultancy. This study presents a robust Integer Linear Programming (ILP) model to optimize the scheduling of faculty-student meetings. The proposed model incorporates practical constraints such as minimum intervals between consecutive meetings, differing time requirements for undergraduate, masters, and PhD students, and dedicated emergency time slots for unplanned visits. The objective function aims to achieve an equitable distribution of meetings throughout the planning period while prioritizing earlier time slots and seamlessly integrating emergency appointments. To validate the effectiveness of the model, both numerical examples and a case study are examined. The results highlight the ability of the model ability to generate optimal schedules within a computationally efficient framework, leveraging the power of Gurobi optimization software. The model demonstrates significant versatility, extending its applicability beyond academic settings to any scenario requiring periodic scheduling of services, such as patient visits in healthcare or client consultations in legal practices. Future extensions of the model may include dynamic scheduling to adapt to real-time changes in availability and the coordination of schedules across multiple service providers.","sentences":["Efficient scheduling of periodic meetings is a critical challenge in various service-oriented domains, including academic settings, healthcare, and legal consultancy.","This study presents a robust Integer Linear Programming (ILP) model to optimize the scheduling of faculty-student meetings.","The proposed model incorporates practical constraints such as minimum intervals between consecutive meetings, differing time requirements for undergraduate, masters, and PhD students, and dedicated emergency time slots for unplanned visits.","The objective function aims to achieve an equitable distribution of meetings throughout the planning period while prioritizing earlier time slots and seamlessly integrating emergency appointments.","To validate the effectiveness of the model, both numerical examples and a case study are examined.","The results highlight the ability of the model ability to generate optimal schedules within a computationally efficient framework, leveraging the power of Gurobi optimization software.","The model demonstrates significant versatility, extending its applicability beyond academic settings to any scenario requiring periodic scheduling of services, such as patient visits in healthcare or client consultations in legal practices.","Future extensions of the model may include dynamic scheduling to adapt to real-time changes in availability and the coordination of schedules across multiple service providers."],"url":"http://arxiv.org/abs/2412.11941v1"}
{"created":"2024-12-16 16:24:58","title":"The Impact of Token Granularity on the Predictive Power of Language Model Surprisal","abstract":"Word-by-word language model surprisal is often used to model the incremental processing of human readers, which raises questions about how various choices in language modeling influence its predictive power. One factor that has been overlooked in cognitive modeling is the granularity of subword tokens, which explicitly encodes information about word length and frequency, and ultimately influences the quality of vector representations that are learned. This paper presents experiments that manipulate the token granularity and evaluate its impact on the ability of surprisal to account for processing difficulty of naturalistic text and garden-path constructions. Experiments with naturalistic reading times reveal a substantial influence of token granularity on surprisal, with tokens defined by a vocabulary size of 8,000 resulting in surprisal that is most predictive. In contrast, on garden-path constructions, language models trained on coarser-grained tokens generally assigned higher surprisal to critical regions, suggesting their increased sensitivity to syntax. Taken together, these results suggest a large role of token granularity on the quality of language model surprisal for cognitive modeling.","sentences":["Word-by-word language model surprisal is often used to model the incremental processing of human readers, which raises questions about how various choices in language modeling influence its predictive power.","One factor that has been overlooked in cognitive modeling is the granularity of subword tokens, which explicitly encodes information about word length and frequency, and ultimately influences the quality of vector representations that are learned.","This paper presents experiments that manipulate the token granularity and evaluate its impact on the ability of surprisal to account for processing difficulty of naturalistic text and garden-path constructions.","Experiments with naturalistic reading times reveal a substantial influence of token granularity on surprisal, with tokens defined by a vocabulary size of 8,000 resulting in surprisal that is most predictive.","In contrast, on garden-path constructions, language models trained on coarser-grained tokens generally assigned higher surprisal to critical regions, suggesting their increased sensitivity to syntax.","Taken together, these results suggest a large role of token granularity on the quality of language model surprisal for cognitive modeling."],"url":"http://arxiv.org/abs/2412.11940v1"}
{"created":"2024-12-16 16:24:36","title":"SEAGraph: Unveiling the Whole Story of Paper Review Comments","abstract":"Peer review, as a cornerstone of scientific research, ensures the integrity and quality of scholarly work by providing authors with objective feedback for refinement. However, in the traditional peer review process, authors often receive vague or insufficiently detailed feedback, which provides limited assistance and leads to a more time-consuming review cycle. If authors can identify some specific weaknesses in their paper, they can not only address the reviewer's concerns but also improve their work. This raises the critical question of how to enhance authors' comprehension of review comments. In this paper, we present SEAGraph, a novel framework developed to clarify review comments by uncovering the underlying intentions behind them. We construct two types of graphs for each paper: the semantic mind graph, which captures the author's thought process, and the hierarchical background graph, which delineates the research domains related to the paper. A retrieval method is then designed to extract relevant content from both graphs, facilitating coherent explanations for the review comments. Extensive experiments show that SEAGraph excels in review comment understanding tasks, offering significant benefits to authors.","sentences":["Peer review, as a cornerstone of scientific research, ensures the integrity and quality of scholarly work by providing authors with objective feedback for refinement.","However, in the traditional peer review process, authors often receive vague or insufficiently detailed feedback, which provides limited assistance and leads to a more time-consuming review cycle.","If authors can identify some specific weaknesses in their paper, they can not only address the reviewer's concerns but also improve their work.","This raises the critical question of how to enhance authors' comprehension of review comments.","In this paper, we present SEAGraph, a novel framework developed to clarify review comments by uncovering the underlying intentions behind them.","We construct two types of graphs for each paper: the semantic mind graph, which captures the author's thought process, and the hierarchical background graph, which delineates the research domains related to the paper.","A retrieval method is then designed to extract relevant content from both graphs, facilitating coherent explanations for the review comments.","Extensive experiments show that SEAGraph excels in review comment understanding tasks, offering significant benefits to authors."],"url":"http://arxiv.org/abs/2412.11939v1"}
{"created":"2024-12-16 16:22:27","title":"Precise Length Control in Large Language Models","abstract":"Large Language Models (LLMs) are increasingly used in production systems, powering applications such as chatbots, summarization, and question answering. Despite their success, controlling the length of their response remains a significant challenge, particularly for tasks requiring structured outputs or specific levels of detail. In this work, we propose a method to adapt pre-trained decoder-only LLMs for precise control of response length. Our approach incorporates a secondary length-difference positional encoding (LDPE) into the input embeddings, which counts down to a user-set response termination length. Fine-tuning with LDPE allows the model to learn to terminate responses coherently at the desired length, achieving mean token errors of less than 3 tokens. We also introduce Max New Tokens++, an extension that enables flexible upper-bound length control, rather than an exact target. Experimental results on tasks such as question answering and document summarization demonstrate that our method enables precise length control without compromising response quality.","sentences":["Large Language Models (LLMs) are increasingly used in production systems, powering applications such as chatbots, summarization, and question answering.","Despite their success, controlling the length of their response remains a significant challenge, particularly for tasks requiring structured outputs or specific levels of detail.","In this work, we propose a method to adapt pre-trained decoder-only LLMs for precise control of response length.","Our approach incorporates a secondary length-difference positional encoding (LDPE) into the input embeddings, which counts down to a user-set response termination length.","Fine-tuning with LDPE allows the model to learn to terminate responses coherently at the desired length, achieving mean token errors of less than 3 tokens.","We also introduce Max New Tokens++, an extension that enables flexible upper-bound length control, rather than an exact target.","Experimental results on tasks such as question answering and document summarization demonstrate that our method enables precise length control without compromising response quality."],"url":"http://arxiv.org/abs/2412.11937v1"}
{"created":"2024-12-16 16:21:41","title":"A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges","abstract":"Mathematical reasoning, a core aspect of human cognition, is vital across many domains, from educational problem-solving to scientific advancements. As artificial general intelligence (AGI) progresses, integrating large language models (LLMs) with mathematical reasoning tasks is becoming increasingly significant. This survey provides the first comprehensive analysis of mathematical reasoning in the era of multimodal large language models (MLLMs). We review over 200 studies published since 2021, and examine the state-of-the-art developments in Math-LLMs, with a focus on multimodal settings. We categorize the field into three dimensions: benchmarks, methodologies, and challenges. In particular, we explore multimodal mathematical reasoning pipeline, as well as the role of (M)LLMs and the associated methodologies. Finally, we identify five major challenges hindering the realization of AGI in this domain, offering insights into the future direction for enhancing multimodal reasoning capabilities. This survey serves as a critical resource for the research community in advancing the capabilities of LLMs to tackle complex multimodal reasoning tasks.","sentences":["Mathematical reasoning, a core aspect of human cognition, is vital across many domains, from educational problem-solving to scientific advancements.","As artificial general intelligence (AGI) progresses, integrating large language models (LLMs) with mathematical reasoning tasks is becoming increasingly significant.","This survey provides the first comprehensive analysis of mathematical reasoning in the era of multimodal large language models (MLLMs).","We review over 200 studies published since 2021, and examine the state-of-the-art developments in Math-LLMs, with a focus on multimodal settings.","We categorize the field into three dimensions: benchmarks, methodologies, and challenges.","In particular, we explore multimodal mathematical reasoning pipeline, as well as the role of (M)LLMs and the associated methodologies.","Finally, we identify five major challenges hindering the realization of AGI in this domain, offering insights into the future direction for enhancing multimodal reasoning capabilities.","This survey serves as a critical resource for the research community in advancing the capabilities of LLMs to tackle complex multimodal reasoning tasks."],"url":"http://arxiv.org/abs/2412.11936v1"}
{"created":"2024-12-16 16:20:41","title":"Stepwise Reasoning Error Disruption Attack of LLMs","abstract":"Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored. Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability. To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers. Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction. Extensive experiments on four datasets across four different models demonstrate SEED's effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes. These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications.","sentences":["Large language models (LLMs) have made remarkable strides in complex reasoning tasks, but their safety and robustness in reasoning processes remain underexplored.","Existing attacks on LLM reasoning are constrained by specific settings or lack of imperceptibility, limiting their feasibility and generalizability.","To address these challenges, we propose the Stepwise rEasoning Error Disruption (SEED) attack, which subtly injects errors into prior reasoning steps to mislead the model into producing incorrect subsequent reasoning and final answers.","Unlike previous methods, SEED is compatible with zero-shot and few-shot settings, maintains the natural reasoning flow, and ensures covert execution without modifying the instruction.","Extensive experiments on four datasets across four different models demonstrate SEED's effectiveness, revealing the vulnerabilities of LLMs to disruptions in reasoning processes.","These findings underscore the need for greater attention to the robustness of LLM reasoning to ensure safety in practical applications."],"url":"http://arxiv.org/abs/2412.11934v1"}
{"created":"2024-12-16 16:15:37","title":"Speeding Up the NSGA-II With a Simple Tie-Breaking Rule","abstract":"The non-dominated sorting genetic algorithm~II (NSGA-II) is the most popular multi-objective optimization heuristic. Recent mathematical runtime analyses have detected two shortcomings in discrete search spaces, namely, that the NSGA-II has difficulties with more than two objectives and that it is very sensitive to the choice of the population size. To overcome these difficulties, we analyze a simple tie-breaking rule in the selection of the next population. Similar rules have been proposed before, but have found only little acceptance. We prove the effectiveness of our tie-breaking rule via mathematical runtime analyses on the classic OneMinMax, LeadingOnesTrailingZeros, and OneJumpZeroJump benchmarks. We prove that this modified NSGA-II can optimize the three benchmarks efficiently also for many objectives, in contrast to the exponential lower runtime bound previously shown for OneMinMax with three or more objectives. For the bi-objective problems, we show runtime guarantees that do not increase when moderately increasing the population size over the minimum admissible size. For example, for the OneJumpZeroJump problem with representation length $n$ and gap parameter $k$, we show a runtime guarantee of $O(\\max\\{n^{k+1},Nn\\})$ function evaluations when the population size is at least four times the size of the Pareto front. For population sizes larger than the minimal choice $N = \\Theta(n)$, this result improves considerably over the $\\Theta(Nn^k)$ runtime of the classic NSGA-II.","sentences":["The non-dominated sorting genetic algorithm~II (NSGA-II) is the most popular multi-objective optimization heuristic.","Recent mathematical runtime analyses have detected two shortcomings in discrete search spaces, namely, that the NSGA-II has difficulties with more than two objectives and that it is very sensitive to the choice of the population size.","To overcome these difficulties, we analyze a simple tie-breaking rule in the selection of the next population.","Similar rules have been proposed before, but have found only little acceptance.","We prove the effectiveness of our tie-breaking rule via mathematical runtime analyses on the classic OneMinMax, LeadingOnesTrailingZeros, and OneJumpZeroJump benchmarks.","We prove that this modified NSGA-II can optimize the three benchmarks efficiently also for many objectives, in contrast to the exponential lower runtime bound previously shown for OneMinMax with three or more objectives.","For the bi-objective problems, we show runtime guarantees that do not increase when moderately increasing the population size over the minimum admissible size.","For example, for the OneJumpZeroJump problem with representation length $n$ and gap parameter $k$, we show a runtime guarantee of $O(\\max\\{n^{k+1},Nn\\})$ function evaluations when the population size is at least four times the size of the Pareto front.","For population sizes larger than the minimal choice $N = \\Theta(n)$, this result improves considerably over the $\\Theta(Nn^k)$ runtime of the classic NSGA-II."],"url":"http://arxiv.org/abs/2412.11931v1"}
{"created":"2024-12-16 16:15:36","title":"Hierarchical Meta-Reinforcement Learning via Automated Macro-Action Discovery","abstract":"Meta-Reinforcement Learning (Meta-RL) enables fast adaptation to new testing tasks. Despite recent advancements, it is still challenging to learn performant policies across multiple complex and high-dimensional tasks. To address this, we propose a novel architecture with three hierarchical levels for 1) learning task representations, 2) discovering task-agnostic macro-actions in an automated manner, and 3) learning primitive actions. The macro-action can guide the low-level primitive policy learning to more efficiently transition to goal states. This can address the issue that the policy may forget previously learned behavior while learning new, conflicting tasks. Moreover, the task-agnostic nature of the macro-actions is enabled by removing task-specific components from the state space. Hence, this makes them amenable to re-composition across different tasks and leads to promising fast adaptation to new tasks. Also, the prospective instability from the tri-level hierarchies is effectively mitigated by our innovative, independently tailored training schemes. Experiments in the MetaWorld framework demonstrate the improved sample efficiency and success rate of our approach compared to previous state-of-the-art methods.","sentences":["Meta-Reinforcement Learning (Meta-RL) enables fast adaptation to new testing tasks.","Despite recent advancements, it is still challenging to learn performant policies across multiple complex and high-dimensional tasks.","To address this, we propose a novel architecture with three hierarchical levels for 1) learning task representations, 2) discovering task-agnostic macro-actions in an automated manner, and 3) learning primitive actions.","The macro-action can guide the low-level primitive policy learning to more efficiently transition to goal states.","This can address the issue that the policy may forget previously learned behavior while learning new, conflicting tasks.","Moreover, the task-agnostic nature of the macro-actions is enabled by removing task-specific components from the state space.","Hence, this makes them amenable to re-composition across different tasks and leads to promising fast adaptation to new tasks.","Also, the prospective instability from the tri-level hierarchies is effectively mitigated by our innovative, independently tailored training schemes.","Experiments in the MetaWorld framework demonstrate the improved sample efficiency and success rate of our approach compared to previous state-of-the-art methods."],"url":"http://arxiv.org/abs/2412.11930v1"}
{"created":"2024-12-16 16:13:55","title":"Explainable Procedural Mistake Detection","abstract":"Automated task guidance has recently attracted attention from the AI research community. Procedural mistake detection (PMD) is a challenging sub-problem of classifying whether a human user (observed through egocentric video) has successfully executed the task at hand (specified by a procedural text). Despite significant efforts in building resources and models for PMD, machine performance remains nonviable, and the reasoning processes underlying this performance are opaque. As such, we recast PMD to an explanatory self-dialog of questions and answers, which serve as evidence for a decision. As this reformulation enables an unprecedented transparency, we leverage a fine-tuned natural language inference (NLI) model to formulate two automated coherence metrics for generated explanations. Our results show that while open-source VLMs struggle with this task off-the-shelf, their accuracy, coherence, and dialog efficiency can be vastly improved by incorporating these coherence metrics into common inference and fine-tuning methods. Furthermore, our multi-faceted metrics can visualize common outcomes at a glance, highlighting areas for improvement.","sentences":["Automated task guidance has recently attracted attention from the AI research community.","Procedural mistake detection (PMD) is a challenging sub-problem of classifying whether a human user (observed through egocentric video) has successfully executed the task at hand (specified by a procedural text).","Despite significant efforts in building resources and models for PMD, machine performance remains nonviable, and the reasoning processes underlying this performance are opaque.","As such, we recast PMD to an explanatory self-dialog of questions and answers, which serve as evidence for a decision.","As this reformulation enables an unprecedented transparency, we leverage a fine-tuned natural language inference (NLI) model to formulate two automated coherence metrics for generated explanations.","Our results show that while open-source VLMs struggle with this task off-the-shelf, their accuracy, coherence, and dialog efficiency can be vastly improved by incorporating these coherence metrics into common inference and fine-tuning methods.","Furthermore, our multi-faceted metrics can visualize common outcomes at a glance, highlighting areas for improvement."],"url":"http://arxiv.org/abs/2412.11927v1"}
{"created":"2024-12-16 16:09:35","title":"PICLe: Pseudo-Annotations for In-Context Learning in Low-Resource Named Entity Detection","abstract":"In-context learning (ICL) enables Large Language Models (LLMs) to perform tasks using few demonstrations, facilitating task adaptation when labeled examples are hard to obtain. However, ICL is sensitive to the choice of demonstrations, and it remains unclear which demonstration attributes enable in-context generalization. In this work, we conduct a perturbation study of in-context demonstrations for low-resource Named Entity Detection (NED). Our surprising finding is that in-context demonstrations with partially correct annotated entity mentions can be as effective for task transfer as fully correct demonstrations. Based off our findings, we propose Pseudo-annotated In-Context Learning (PICLe), a framework for in-context learning with noisy, pseudo-annotated demonstrations. PICLe leverages LLMs to annotate many demonstrations in a zero-shot first pass. We then cluster these synthetic demonstrations, sample specific sets of in-context demonstrations from each cluster, and predict entity mentions using each set independently. Finally, we use self-verification to select the final set of entity mentions. We evaluate PICLe on five biomedical NED datasets and show that, with zero human annotation, PICLe outperforms ICL in low-resource settings where limited gold examples can be used as in-context demonstrations.","sentences":["In-context learning (ICL) enables Large Language Models (LLMs) to perform tasks using few demonstrations, facilitating task adaptation when labeled examples are hard to obtain.","However, ICL is sensitive to the choice of demonstrations, and it remains unclear which demonstration attributes enable in-context generalization.","In this work, we conduct a perturbation study of in-context demonstrations for low-resource Named Entity Detection (NED).","Our surprising finding is that in-context demonstrations with partially correct annotated entity mentions can be as effective for task transfer as fully correct demonstrations.","Based off our findings, we propose Pseudo-annotated In-Context Learning (PICLe), a framework for in-context learning with noisy, pseudo-annotated demonstrations.","PICLe leverages LLMs to annotate many demonstrations in a zero-shot first pass.","We then cluster these synthetic demonstrations, sample specific sets of in-context demonstrations from each cluster, and predict entity mentions using each set independently.","Finally, we use self-verification to select the final set of entity mentions.","We evaluate PICLe on five biomedical NED datasets and show that, with zero human annotation, PICLe outperforms ICL in low-resource settings where limited gold examples can be used as in-context demonstrations."],"url":"http://arxiv.org/abs/2412.11923v1"}
{"created":"2024-12-16 16:03:25","title":"RetroLLM: Empowering Large Language Models to Retrieve Fine-grained Evidence within Generation","abstract":"Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations. Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several limitations: additional deployment costs of separate retrievers, redundant input tokens from retrieved text chunks, and the lack of joint optimization of retrieval and generation. To address these issues, we propose \\textbf{RetroLLM}, a unified framework that integrates retrieval and generation into a single, cohesive process, enabling LLMs to directly generate fine-grained evidence from the corpus with constrained decoding. Moreover, to mitigate false pruning in the process of constrained evidence generation, we introduce (1) hierarchical FM-Index constraints, which generate corpus-constrained clues to identify a subset of relevant documents before evidence generation, reducing irrelevant decoding space; and (2) a forward-looking constrained decoding strategy, which considers the relevance of future sequences to improve evidence accuracy. Extensive experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance across both in-domain and out-of-domain tasks. The code is available at \\url{https://github.com/sunnynexus/RetroLLM}.","sentences":["Large language models (LLMs) exhibit remarkable generative capabilities but often suffer from hallucinations.","Retrieval-augmented generation (RAG) offers an effective solution by incorporating external knowledge, but existing methods still face several limitations: additional deployment costs of separate retrievers, redundant input tokens from retrieved text chunks, and the lack of joint optimization of retrieval and generation.","To address these issues, we propose \\textbf{RetroLLM}, a unified framework that integrates retrieval and generation into a single, cohesive process, enabling LLMs to directly generate fine-grained evidence from the corpus with constrained decoding.","Moreover, to mitigate false pruning in the process of constrained evidence generation, we introduce (1) hierarchical FM-Index constraints, which generate corpus-constrained clues to identify a subset of relevant documents before evidence generation, reducing irrelevant decoding space; and (2) a forward-looking constrained decoding strategy, which considers the relevance of future sequences to improve evidence accuracy.","Extensive experiments on five open-domain QA datasets demonstrate RetroLLM's superior performance across both in-domain and out-of-domain tasks.","The code is available at \\url{https://github.com/sunnynexus/RetroLLM}."],"url":"http://arxiv.org/abs/2412.11919v1"}
{"created":"2024-12-16 16:01:18","title":"Does VLM Classification Benefit from LLM Description Semantics?","abstract":"Accurately describing images via text is a foundation of explainable AI. Vision-Language Models (VLMs) like CLIP have recently addressed this by aligning images and texts in a shared embedding space, expressing semantic similarities between vision and language embeddings. VLM classification can be improved with descriptions generated by Large Language Models (LLMs). However, it is difficult to determine the contribution of actual description semantics, as the performance gain may also stem from a semantic-agnostic ensembling effect. Considering this, we ask how to distinguish the actual discriminative power of descriptions from performance boosts that potentially rely on an ensembling effect. To study this, we propose an alternative evaluation scenario that shows a characteristic behavior if the used descriptions have discriminative power. Furthermore, we propose a training-free method to select discriminative descriptions that work independently of classname ensembling effects. The training-free method works in the following way: A test image has a local CLIP label neighborhood, i.e., its top-$k$ label predictions. Then, w.r.t. to a small selection set, we extract descriptions that distinguish each class well in the local neighborhood. Using the selected descriptions, we demonstrate improved classification accuracy across seven datasets and provide in-depth analysis and insights into the explainability of description-based image classification by VLMs.","sentences":["Accurately describing images via text is a foundation of explainable AI.","Vision-Language Models (VLMs) like CLIP have recently addressed this by aligning images and texts in a shared embedding space, expressing semantic similarities between vision and language embeddings.","VLM classification can be improved with descriptions generated by Large Language Models (LLMs).","However, it is difficult to determine the contribution of actual description semantics, as the performance gain may also stem from a semantic-agnostic ensembling effect.","Considering this, we ask how to distinguish the actual discriminative power of descriptions from performance boosts that potentially rely on an ensembling effect.","To study this, we propose an alternative evaluation scenario that shows a characteristic behavior if the used descriptions have discriminative power.","Furthermore, we propose a training-free method to select discriminative descriptions that work independently of classname ensembling effects.","The training-free method works in the following way: A test image has a local CLIP label neighborhood, i.e., its top-$k$ label predictions.","Then, w.r.t. to a small selection set, we extract descriptions that distinguish each class well in the local neighborhood.","Using the selected descriptions, we demonstrate improved classification accuracy across seven datasets and provide in-depth analysis and insights into the explainability of description-based image classification by VLMs."],"url":"http://arxiv.org/abs/2412.11917v1"}
{"created":"2024-12-16 16:00:55","title":"Lightweight Decentralized Neural Network-Based Strategies for Multi-Robot Patrolling","abstract":"The problem of decentralized multi-robot patrol has previously been approached primarily with hand-designed strategies for minimization of 'idlenes' over the vertices of a graph-structured environment. Here we present two lightweight neural network-based strategies to tackle this problem, and show that they significantly outperform existing strategies in both idleness minimization and against an intelligent intruder model, as well as presenting an examination of robustness to communication failure. Our results also indicate important considerations for future strategy design.","sentences":["The problem of decentralized multi-robot patrol has previously been approached primarily with hand-designed strategies for minimization of 'idlenes' over the vertices of a graph-structured environment.","Here we present two lightweight neural network-based strategies to tackle this problem, and show that they significantly outperform existing strategies in both idleness minimization and against an intelligent intruder model, as well as presenting an examination of robustness to communication failure.","Our results also indicate important considerations for future strategy design."],"url":"http://arxiv.org/abs/2412.11916v1"}
{"created":"2024-12-16 15:56:04","title":"Learning Human-Aware Robot Policies for Adaptive Assistance","abstract":"Developing robots that can assist humans efficiently, safely, and adaptively is crucial for real-world applications such as healthcare. While previous work often assumes a centralized system for co-optimizing human-robot interactions, we argue that real-world scenarios are much more complicated, as humans have individual preferences regarding how tasks are performed. Robots typically lack direct access to these implicit preferences. However, to provide effective assistance, robots must still be able to recognize and adapt to the individual needs and preferences of different users. To address these challenges, we propose a novel framework in which robots infer human intentions and reason about human utilities through interaction. Our approach features two critical modules: the anticipation module is a motion predictor that captures the spatial-temporal relationship between the robot agent and user agent, which contributes to predicting human behavior; the utility module infers the underlying human utility functions through progressive task demonstration sampling. Extensive experiments across various robot types and assistive tasks demonstrate that the proposed framework not only enhances task success and efficiency but also significantly improves user satisfaction, paving the way for more personalized and adaptive assistive robotic systems. Code and demos are available at https://asonin.github.io/Human-Aware-Assistance/.","sentences":["Developing robots that can assist humans efficiently, safely, and adaptively is crucial for real-world applications such as healthcare.","While previous work often assumes a centralized system for co-optimizing human-robot interactions, we argue that real-world scenarios are much more complicated, as humans have individual preferences regarding how tasks are performed.","Robots typically lack direct access to these implicit preferences.","However, to provide effective assistance, robots must still be able to recognize and adapt to the individual needs and preferences of different users.","To address these challenges, we propose a novel framework in which robots infer human intentions and reason about human utilities through interaction.","Our approach features two critical modules: the anticipation module is a motion predictor that captures the spatial-temporal relationship between the robot agent and user agent, which contributes to predicting human behavior; the utility module infers the underlying human utility functions through progressive task demonstration sampling.","Extensive experiments across various robot types and assistive tasks demonstrate that the proposed framework not only enhances task success and efficiency but also significantly improves user satisfaction, paving the way for more personalized and adaptive assistive robotic systems.","Code and demos are available at https://asonin.github.io/Human-Aware-Assistance/."],"url":"http://arxiv.org/abs/2412.11913v1"}
{"created":"2024-12-16 15:55:34","title":"CharacterBench: Benchmarking Character Customization of Large Language Models","abstract":"Character-based dialogue (aka role-playing) enables users to freely customize characters for interaction, which often relies on LLMs, raising the need to evaluate LLMs' character customization capability. However, existing benchmarks fail to ensure a robust evaluation as they often only involve a single character category or evaluate limited dimensions. Moreover, the sparsity of character features in responses makes feature-focused generative evaluation both ineffective and inefficient. To address these issues, we propose CharacterBench, the largest bilingual generative benchmark, with 22,859 human-annotated samples covering 3,956 characters from 25 detailed character categories. We define 11 dimensions of 6 aspects, classified as sparse and dense dimensions based on whether character features evaluated by specific dimensions manifest in each response. We enable effective and efficient evaluation by crafting tailored queries for each dimension to induce characters' responses related to specific dimensions. Further, we develop CharacterJudge model for cost-effective and stable evaluations. Experiments show its superiority over SOTA automatic judges (e.g., GPT-4) and our benchmark's potential to optimize LLMs' character customization. Our repository is at https://github.com/thu-coai/CharacterBench.","sentences":["Character-based dialogue (aka role-playing) enables users to freely customize characters for interaction, which often relies on LLMs, raising the need to evaluate LLMs' character customization capability.","However, existing benchmarks fail to ensure a robust evaluation as they often only involve a single character category or evaluate limited dimensions.","Moreover, the sparsity of character features in responses makes feature-focused generative evaluation both ineffective and inefficient.","To address these issues, we propose CharacterBench, the largest bilingual generative benchmark, with 22,859 human-annotated samples covering 3,956 characters from 25 detailed character categories.","We define 11 dimensions of 6 aspects, classified as sparse and dense dimensions based on whether character features evaluated by specific dimensions manifest in each response.","We enable effective and efficient evaluation by crafting tailored queries for each dimension to induce characters' responses related to specific dimensions.","Further, we develop CharacterJudge model for cost-effective and stable evaluations.","Experiments show its superiority over SOTA automatic judges (e.g., GPT-4) and our benchmark's potential to optimize LLMs' character customization.","Our repository is at https://github.com/thu-coai/CharacterBench."],"url":"http://arxiv.org/abs/2412.11912v1"}
{"created":"2024-12-16 15:55:31","title":"What Can Youth Learn About in One Hour? Examining How Hour of Code Activities Address the Five Big Ideas of Artificial Intelligence","abstract":"The prominence of artificial intelligence and machine learning in everyday life has led to efforts to foster AI literacy for all K-12 students. In this paper, we review how Hour of Code activities engage with the five big ideas of AI, in particular with machine learning and societal impact. We found that a large majority of activities focus on perception and machine learning, with little attention paid to representation and other topics. A surprising finding was the increased attention paid to critical aspects of computing. However, we also observed a limited engagement with hands-on activities. In the discussion, we address how future introductory activities could be designed to offer a broader array of topics, including the development of tools to introduce novices to artificial intelligence and machine learning and the design of more unplugged and collaborative activities.","sentences":["The prominence of artificial intelligence and machine learning in everyday life has led to efforts to foster AI literacy for all K-12 students.","In this paper, we review how Hour of Code activities engage with the five big ideas of AI, in particular with machine learning and societal impact.","We found that a large majority of activities focus on perception and machine learning, with little attention paid to representation and other topics.","A surprising finding was the increased attention paid to critical aspects of computing.","However, we also observed a limited engagement with hands-on activities.","In the discussion, we address how future introductory activities could be designed to offer a broader array of topics, including the development of tools to introduce novices to artificial intelligence and machine learning and the design of more unplugged and collaborative activities."],"url":"http://arxiv.org/abs/2412.11911v1"}
{"created":"2024-12-16 15:54:06","title":"Can Language Models Rival Mathematics Students? Evaluating Mathematical Reasoning through Textual Manipulation and Human Experiments","abstract":"In this paper we look at the ability of recent large language models (LLMs) at solving mathematical problems in combinatorics. We compare models LLaMA-2, LLaMA-3.1, GPT-4, and Mixtral against each other and against human pupils and undergraduates with prior experience in mathematical olympiads. To facilitate these comparisons we introduce the Combi-Puzzles dataset, which contains 125 problem variants based on 25 combinatorial reasoning problems. Each problem is presented in one of five distinct forms, created by systematically manipulating the problem statements through adversarial additions, numeric parameter changes, and linguistic obfuscation. Our variations preserve the mathematical core and are designed to measure the generalisability of LLM problem-solving abilities, while also increasing confidence that problems are submitted to LLMs in forms that have not been seen as training instances. We found that a model based on GPT-4 outperformed all other models in producing correct responses, and performed significantly better in the mathematical variation of the problems than humans. We also found that modifications to problem statements significantly impact the LLM's performance, while human performance remains unaffected.","sentences":["In this paper we look at the ability of recent large language models (LLMs) at solving mathematical problems in combinatorics.","We compare models LLaMA-2, LLaMA-3.1, GPT-4, and Mixtral against each other and against human pupils and undergraduates with prior experience in mathematical olympiads.","To facilitate these comparisons we introduce the Combi-Puzzles dataset, which contains 125 problem variants based on 25 combinatorial reasoning problems.","Each problem is presented in one of five distinct forms, created by systematically manipulating the problem statements through adversarial additions, numeric parameter changes, and linguistic obfuscation.","Our variations preserve the mathematical core and are designed to measure the generalisability of LLM problem-solving abilities, while also increasing confidence that problems are submitted to LLMs in forms that have not been seen as training instances.","We found that a model based on GPT-4 outperformed all other models in producing correct responses, and performed significantly better in the mathematical variation of the problems than humans.","We also found that modifications to problem statements significantly impact the LLM's performance, while human performance remains unaffected."],"url":"http://arxiv.org/abs/2412.11908v1"}
{"created":"2024-12-16 15:53:02","title":"AudioCIL: A Python Toolbox for Audio Class-Incremental Learning with Multiple Scenes","abstract":"Deep learning, with its robust aotomatic feature extraction capabilities, has demonstrated significant success in audio signal processing. Typically, these methods rely on static, pre-collected large-scale datasets for training, performing well on a fixed number of classes. However, the real world is characterized by constant change, with new audio classes emerging from streaming or temporary availability due to privacy. This dynamic nature of audio environments necessitates models that can incrementally learn new knowledge for new classes without discarding existing information. Introducing incremental learning to the field of audio signal processing, i.e., Audio Class-Incremental Learning (AuCIL), is a meaningful endeavor. We propose such a toolbox named AudioCIL to align audio signal processing algorithms with real-world scenarios and strengthen research in audio class-incremental learning.","sentences":["Deep learning, with its robust aotomatic feature extraction capabilities, has demonstrated significant success in audio signal processing.","Typically, these methods rely on static, pre-collected large-scale datasets for training, performing well on a fixed number of classes.","However, the real world is characterized by constant change, with new audio classes emerging from streaming or temporary availability due to privacy.","This dynamic nature of audio environments necessitates models that can incrementally learn new knowledge for new classes without discarding existing information.","Introducing incremental learning to the field of audio signal processing, i.e., Audio Class-Incremental Learning (AuCIL), is a meaningful endeavor.","We propose such a toolbox named AudioCIL to align audio signal processing algorithms with real-world scenarios and strengthen research in audio class-incremental learning."],"url":"http://arxiv.org/abs/2412.11907v1"}
{"created":"2024-12-16 15:52:59","title":"PunchBench: Benchmarking MLLMs in Multimodal Punchline Comprehension","abstract":"Multimodal punchlines, which involve humor or sarcasm conveyed in image-caption pairs, are a popular way of communication on online multimedia platforms. With the rapid development of multimodal large language models (MLLMs), it is essential to assess their ability to effectively comprehend these punchlines. However, existing benchmarks on punchline comprehension suffer from three major limitations: 1) language shortcuts that allow models to solely rely on text, 2) lack of question diversity, and 3) narrow focus on a specific domain of multimodal content (e.g., cartoon). To address these limitations, we introduce a multimodal \\textbf{Punch}line comprehension \\textbf{Bench}mark, named \\textbf{PunchBench}, which is tailored for accurate and comprehensive evaluation of punchline comprehension. To enhance the evaluation accuracy, we generate synonymous and antonymous captions by modifying original captions, which mitigates the impact of shortcuts in the captions. To provide a comprehensive evaluation, PunchBench incorporates diverse question formats and image-captions from various domains. On this basis, we conduct extensive evaluations and reveal a significant gap between state-of-the-art MLLMs and humans in punchline comprehension. To improve punchline comprehension, we propose Simple-to-Complex Chain-of-Question (SC-CoQ) strategy, enabling the models to incrementally address complicated questions by first mastering simple ones. SC-CoQ effectively enhances the performance of various MLLMs on PunchBench, surpassing in-context learning and chain-of-thought.","sentences":["Multimodal punchlines, which involve humor or sarcasm conveyed in image-caption pairs, are a popular way of communication on online multimedia platforms.","With the rapid development of multimodal large language models (MLLMs), it is essential to assess their ability to effectively comprehend these punchlines.","However, existing benchmarks on punchline comprehension suffer from three major limitations: 1) language shortcuts that allow models to solely rely on text, 2) lack of question diversity, and 3) narrow focus on a specific domain of multimodal content (e.g., cartoon).","To address these limitations, we introduce a multimodal \\textbf{Punch}line comprehension \\textbf{Bench}mark, named \\textbf{PunchBench}, which is tailored for accurate and comprehensive evaluation of punchline comprehension.","To enhance the evaluation accuracy, we generate synonymous and antonymous captions by modifying original captions, which mitigates the impact of shortcuts in the captions.","To provide a comprehensive evaluation, PunchBench incorporates diverse question formats and image-captions from various domains.","On this basis, we conduct extensive evaluations and reveal a significant gap between state-of-the-art MLLMs and humans in punchline comprehension.","To improve punchline comprehension, we propose Simple-to-Complex Chain-of-Question (SC-CoQ) strategy, enabling the models to incrementally address complicated questions by first mastering simple ones.","SC-CoQ effectively enhances the performance of various MLLMs on PunchBench, surpassing in-context learning and chain-of-thought."],"url":"http://arxiv.org/abs/2412.11906v1"}
{"created":"2024-12-16 15:52:17","title":"One for Dozens: Adaptive REcommendation for All Domains with Counterfactual Augmentation","abstract":"Multi-domain recommendation (MDR) aims to enhance recommendation performance across various domains. However, real-world recommender systems in online platforms often need to handle dozens or even hundreds of domains, far exceeding the capabilities of traditional MDR algorithms, which typically focus on fewer than five domains. Key challenges include a substantial increase in parameter count, high maintenance costs, and intricate knowledge transfer patterns across domains. Furthermore, minor domains often suffer from data sparsity, leading to inadequate training in classical methods. To address these issues, we propose Adaptive REcommendation for All Domains with counterfactual augmentation (AREAD). AREAD employs a hierarchical structure with a limited number of expert networks at several layers, to effectively capture domain knowledge at different granularities. To adaptively capture the knowledge transfer pattern across domains, we generate and iteratively prune a hierarchical expert network selection mask for each domain during training. Additionally, counterfactual assumptions are used to augment data in minor domains, supporting their iterative mask pruning. Our experiments on two public datasets, each encompassing over twenty domains, demonstrate AREAD's effectiveness, especially in data-sparse domains. Source code is available at https://github.com/Chrissie-Law/AREAD-Multi-Domain-Recommendation.","sentences":["Multi-domain recommendation (MDR) aims to enhance recommendation performance across various domains.","However, real-world recommender systems in online platforms often need to handle dozens or even hundreds of domains, far exceeding the capabilities of traditional MDR algorithms, which typically focus on fewer than five domains.","Key challenges include a substantial increase in parameter count, high maintenance costs, and intricate knowledge transfer patterns across domains.","Furthermore, minor domains often suffer from data sparsity, leading to inadequate training in classical methods.","To address these issues, we propose Adaptive REcommendation for All Domains with counterfactual augmentation (AREAD).","AREAD employs a hierarchical structure with a limited number of expert networks at several layers, to effectively capture domain knowledge at different granularities.","To adaptively capture the knowledge transfer pattern across domains, we generate and iteratively prune a hierarchical expert network selection mask for each domain during training.","Additionally, counterfactual assumptions are used to augment data in minor domains, supporting their iterative mask pruning.","Our experiments on two public datasets, each encompassing over twenty domains, demonstrate AREAD's effectiveness, especially in data-sparse domains.","Source code is available at https://github.com/Chrissie-Law/AREAD-Multi-Domain-Recommendation."],"url":"http://arxiv.org/abs/2412.11905v1"}
{"created":"2024-12-16 15:45:39","title":"SareQuant: Towards a quantum-based communication network","abstract":"This paper presents the SareQuant project, which aims to evolve the Basque NREN (National Research and Education Networks) into a quantum-based communication infrastructure. SareQuant focuses on the network design and on the integration of quantum technologies into real-world scenarios and applications. Therefore, this paper provides insights into the opportunities and challenges regarding the integration of quantum technologies, thus paving the way for a secure and advanced Quantum Internet.","sentences":["This paper presents the SareQuant project, which aims to evolve the Basque NREN (National Research and Education Networks) into a quantum-based communication infrastructure.","SareQuant focuses on the network design and on the integration of quantum technologies into real-world scenarios and applications.","Therefore, this paper provides insights into the opportunities and challenges regarding the integration of quantum technologies, thus paving the way for a secure and advanced Quantum Internet."],"url":"http://arxiv.org/abs/2412.11898v1"}
{"created":"2024-12-16 15:45:10","title":"Classification of Spontaneous and Scripted Speech for Multilingual Audio","abstract":"Distinguishing scripted from spontaneous speech is an essential tool for better understanding how speech styles influence speech processing research. It can also improve recommendation systems and discovery experiences for media users through better segmentation of large recorded speech catalogues. This paper addresses the challenge of building a classifier that generalises well across different formats and languages. We systematically evaluate models ranging from traditional, handcrafted acoustic and prosodic features to advanced audio transformers, utilising a large, multilingual proprietary podcast dataset for training and validation. We break down the performance of each model across 11 language groups to evaluate cross-lingual biases. Our experimental analysis extends to publicly available datasets to assess the models' generalisability to non-podcast domains. Our results indicate that transformer-based models consistently outperform traditional feature-based techniques, achieving state-of-the-art performance in distinguishing between scripted and spontaneous speech across various languages.","sentences":["Distinguishing scripted from spontaneous speech is an essential tool for better understanding how speech styles influence speech processing research.","It can also improve recommendation systems and discovery experiences for media users through better segmentation of large recorded speech catalogues.","This paper addresses the challenge of building a classifier that generalises well across different formats and languages.","We systematically evaluate models ranging from traditional, handcrafted acoustic and prosodic features to advanced audio transformers, utilising a large, multilingual proprietary podcast dataset for training and validation.","We break down the performance of each model across 11 language groups to evaluate cross-lingual biases.","Our experimental analysis extends to publicly available datasets to assess the models' generalisability to non-podcast domains.","Our results indicate that transformer-based models consistently outperform traditional feature-based techniques, achieving state-of-the-art performance in distinguishing between scripted and spontaneous speech across various languages."],"url":"http://arxiv.org/abs/2412.11896v1"}
{"created":"2024-12-16 15:41:14","title":"From 2D CAD Drawings to 3D Parametric Models: A Vision-Language Approach","abstract":"In this paper, we present CAD2Program, a new method for reconstructing 3D parametric models from 2D CAD drawings. Our proposed method is inspired by recent successes in vision-language models (VLMs), and departs from traditional methods which rely on task-specific data representations and/or algorithms. Specifically, on the input side, we simply treat the 2D CAD drawing as a raster image, regardless of its original format, and encode the image with a standard ViT model. We show that such an encoding scheme achieves competitive performance against existing methods that operate on vector-graphics inputs, while imposing substantially fewer restrictions on the 2D drawings. On the output side, our method auto-regressively predicts a general-purpose language describing 3D parametric models in text form. Compared to other sequence modeling methods for CAD which use domain-specific sequence representations with fixed-size slots, our text-based representation is more flexible, and can be easily extended to arbitrary geometric entities and semantic or functional properties. Experimental results on a large-scale dataset of cabinet models demonstrate the effectiveness of our method.","sentences":["In this paper, we present CAD2Program, a new method for reconstructing 3D parametric models from 2D CAD drawings.","Our proposed method is inspired by recent successes in vision-language models (VLMs), and departs from traditional methods which rely on task-specific data representations and/or algorithms.","Specifically, on the input side, we simply treat the 2D CAD drawing as a raster image, regardless of its original format, and encode the image with a standard ViT model.","We show that such an encoding scheme achieves competitive performance against existing methods that operate on vector-graphics inputs, while imposing substantially fewer restrictions on the 2D drawings.","On the output side, our method auto-regressively predicts a general-purpose language describing 3D parametric models in text form.","Compared to other sequence modeling methods for CAD which use domain-specific sequence representations with fixed-size slots, our text-based representation is more flexible, and can be easily extended to arbitrary geometric entities and semantic or functional properties.","Experimental results on a large-scale dataset of cabinet models demonstrate the effectiveness of our method."],"url":"http://arxiv.org/abs/2412.11892v1"}
