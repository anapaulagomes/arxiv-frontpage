{"created":"2024-12-03 18:59:56","title":"Motion Prompting: Controlling Video Generation with Motion Trajectories","abstract":"Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions. To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories. In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts. While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion. We demonstrate the versatility of our approach through various applications, including camera and object motion control, \"interacting\" with an image, motion transfer, and image editing. Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance. Video results are available on our webpage: https://motion-prompting.github.io/","sentences":["Motion control is crucial for generating expressive and compelling video content; however, most existing video generation models rely mainly on text prompts for control, which struggle to capture the nuances of dynamic actions and temporal compositions.","To this end, we train a video generation model conditioned on spatio-temporally sparse or dense motion trajectories.","In contrast to prior motion conditioning work, this flexible representation can encode any number of trajectories, object-specific or global scene motion, and temporally sparse motion; due to its flexibility we refer to this conditioning as motion prompts.","While users may directly specify sparse trajectories, we also show how to translate high-level user requests into detailed, semi-dense motion prompts, a process we term motion prompt expansion.","We demonstrate the versatility of our approach through various applications, including camera and object motion control, \"interacting\" with an image, motion transfer, and image editing.","Our results showcase emergent behaviors, such as realistic physics, suggesting the potential of motion prompts for probing video models and interacting with future generative world models.","Finally, we evaluate quantitatively, conduct a human study, and demonstrate strong performance.","Video results are available on our webpage: https://motion-prompting.github.io/"],"url":"http://arxiv.org/abs/2412.02700v1"}
{"created":"2024-12-03 18:59:54","title":"UniGraspTransformer: Simplified Policy Distillation for Scalable Dexterous Robotic Grasping","abstract":"We introduce UniGraspTransformer, a universal Transformer-based network for dexterous robotic grasping that simplifies training while enhancing scalability and performance. Unlike prior methods such as UniDexGrasp++, which require complex, multi-step training pipelines, UniGraspTransformer follows a streamlined process: first, dedicated policy networks are trained for individual objects using reinforcement learning to generate successful grasp trajectories; then, these trajectories are distilled into a single, universal network. Our approach enables UniGraspTransformer to scale effectively, incorporating up to 12 self-attention blocks for handling thousands of objects with diverse poses. Additionally, it generalizes well to both idealized and real-world inputs, evaluated in state-based and vision-based settings. Notably, UniGraspTransformer generates a broader range of grasping poses for objects in various shapes and orientations, resulting in more diverse grasp strategies. Experimental results demonstrate significant improvements over state-of-the-art, UniDexGrasp++, across various object categories, achieving success rate gains of 3.5%, 7.7%, and 10.1% on seen objects, unseen objects within seen categories, and completely unseen objects, respectively, in the vision-based setting. Project page: https://dexhand.github.io/UniGraspTransformer.","sentences":["We introduce UniGraspTransformer, a universal Transformer-based network for dexterous robotic grasping that simplifies training while enhancing scalability and performance.","Unlike prior methods such as UniDexGrasp++, which require complex, multi-step training pipelines, UniGraspTransformer follows a streamlined process: first, dedicated policy networks are trained for individual objects using reinforcement learning to generate successful grasp trajectories; then, these trajectories are distilled into a single, universal network.","Our approach enables UniGraspTransformer to scale effectively, incorporating up to 12 self-attention blocks for handling thousands of objects with diverse poses.","Additionally, it generalizes well to both idealized and real-world inputs, evaluated in state-based and vision-based settings.","Notably, UniGraspTransformer generates a broader range of grasping poses for objects in various shapes and orientations, resulting in more diverse grasp strategies.","Experimental results demonstrate significant improvements over state-of-the-art, UniDexGrasp++, across various object categories, achieving success rate gains of 3.5%, 7.7%, and 10.1% on seen objects, unseen objects within seen categories, and completely unseen objects, respectively, in the vision-based setting.","Project page: https://dexhand.github.io/UniGraspTransformer."],"url":"http://arxiv.org/abs/2412.02699v1"}
{"created":"2024-12-03 18:59:51","title":"Scaling BERT Models for Turkish Automatic Punctuation and Capitalization Correction","abstract":"This paper investigates the effectiveness of BERT based models for automated punctuation and capitalization corrections in Turkish texts across five distinct model sizes. The models are designated as Tiny, Mini, Small, Medium, and Base. The design and capabilities of each model are tailored to address the specific challenges of the Turkish language, with a focus on optimizing performance while minimizing computational overhead. The study presents a systematic comparison of the performance metrics precision, recall, and F1 score of each model, offering insights into their applicability in diverse operational contexts. The results demonstrate a significant improvement in text readability and accuracy as model size increases, with the Base model achieving the highest correction precision. This research provides a comprehensive guide for selecting the appropriate model size based on specific user needs and computational resources, establishing a framework for deploying these models in real-world applications to enhance the quality of written Turkish.","sentences":["This paper investigates the effectiveness of BERT based models for automated punctuation and capitalization corrections in Turkish texts across five distinct model sizes.","The models are designated as Tiny, Mini, Small, Medium, and Base.","The design and capabilities of each model are tailored to address the specific challenges of the Turkish language, with a focus on optimizing performance while minimizing computational overhead.","The study presents a systematic comparison of the performance metrics precision, recall, and F1 score of each model, offering insights into their applicability in diverse operational contexts.","The results demonstrate a significant improvement in text readability and accuracy as model size increases, with the Base model achieving the highest correction precision.","This research provides a comprehensive guide for selecting the appropriate model size based on specific user needs and computational resources, establishing a framework for deploying these models in real-world applications to enhance the quality of written Turkish."],"url":"http://arxiv.org/abs/2412.02698v1"}
{"created":"2024-12-03 18:59:35","title":"An ADHD Diagnostic Interface Based on EEG Spectrograms and Deep Learning Techniques","abstract":"This paper introduces an innovative approach to Attention-deficit/hyperactivity disorder (ADHD) diagnosis by employing deep learning (DL) techniques on electroencephalography (EEG) signals. This method addresses the limitations of current behavior-based diagnostic methods, which often lead to misdiagnosis and gender bias. By utilizing a publicly available EEG dataset and converting the signals into spectrograms, a Resnet-18 convolutional neural network (CNN) architecture was used to extract features for ADHD classification. The model achieved a high precision, recall, and an overall F1 score of 0.9. Feature extraction highlighted significant brain regions (frontopolar, parietal, and occipital lobes) associated with ADHD. These insights guided the creation of a three-part digital diagnostic system, facilitating cost-effective and accessible ADHD screening, especially in school environments. This system enables earlier and more accurate identification of students at risk for ADHD, providing timely support to enhance their developmental outcomes. This study showcases the potential of integrating EEG analysis with DL to enhance ADHD diagnostics, presenting a viable alternative to traditional methods.","sentences":["This paper introduces an innovative approach to Attention-deficit/hyperactivity disorder (ADHD) diagnosis by employing deep learning (DL) techniques on electroencephalography (EEG) signals.","This method addresses the limitations of current behavior-based diagnostic methods, which often lead to misdiagnosis and gender bias.","By utilizing a publicly available EEG dataset and converting the signals into spectrograms, a Resnet-18 convolutional neural network (CNN) architecture was used to extract features for ADHD classification.","The model achieved a high precision, recall, and an overall F1 score of 0.9.","Feature extraction highlighted significant brain regions (frontopolar, parietal, and occipital lobes) associated with ADHD.","These insights guided the creation of a three-part digital diagnostic system, facilitating cost-effective and accessible ADHD screening, especially in school environments.","This system enables earlier and more accurate identification of students at risk for ADHD, providing timely support to enhance their developmental outcomes.","This study showcases the potential of integrating EEG analysis with DL to enhance ADHD diagnostics, presenting a viable alternative to traditional methods."],"url":"http://arxiv.org/abs/2412.02695v1"}
{"created":"2024-12-03 18:59:28","title":"Diffusion-based Visual Anagram as Multi-task Learning","abstract":"Visual anagrams are images that change appearance upon transformation, like flipping or rotation. With the advent of diffusion models, generating such optical illusions can be achieved by averaging noise across multiple views during the reverse denoising process. However, we observe two critical failure modes in this approach: (i) concept segregation, where concepts in different views are independently generated, which can not be considered a true anagram, and (ii) concept domination, where certain concepts overpower others. In this work, we cast the visual anagram generation problem in a multi-task learning setting, where different viewpoint prompts are analogous to different tasks,and derive denoising trajectories that align well across tasks simultaneously. At the core of our designed framework are two newly introduced techniques, where (i) an anti-segregation optimization strategy that promotes overlap in cross-attention maps between different concepts, and (ii) a noise vector balancing method that adaptively adjusts the influence of different tasks. Additionally, we observe that directly averaging noise predictions yields suboptimal performance because statistical properties may not be preserved, prompting us to derive a noise variance rectification method. Extensive qualitative and quantitative experiments demonstrate our method's superior ability to generate visual anagrams spanning diverse concepts.","sentences":["Visual anagrams are images that change appearance upon transformation, like flipping or rotation.","With the advent of diffusion models, generating such optical illusions can be achieved by averaging noise across multiple views during the reverse denoising process.","However, we observe two critical failure modes in this approach: (i) concept segregation, where concepts in different views are independently generated, which can not be considered a true anagram, and (ii) concept domination, where certain concepts overpower others.","In this work, we cast the visual anagram generation problem in a multi-task learning setting, where different viewpoint prompts are analogous to different tasks,and derive denoising trajectories that align well across tasks simultaneously.","At the core of our designed framework are two newly introduced techniques, where (i) an anti-segregation optimization strategy that promotes overlap in cross-attention maps between different concepts, and (ii) a noise vector balancing method that adaptively adjusts the influence of different tasks.","Additionally, we observe that directly averaging noise predictions yields suboptimal performance because statistical properties may not be preserved, prompting us to derive a noise variance rectification method.","Extensive qualitative and quantitative experiments demonstrate our method's superior ability to generate visual anagrams spanning diverse concepts."],"url":"http://arxiv.org/abs/2412.02693v1"}
{"created":"2024-12-03 18:59:10","title":"Taming Scalable Visual Tokenizer for Autoregressive Image Generation","abstract":"Existing vector quantization (VQ) methods struggle with scalability, largely attributed to the instability of the codebook that undergoes partial updates during training. The codebook is prone to collapse as utilization decreases, due to the progressively widening distribution gap between non-activated codes and visual features. To solve the problem, we propose Index Backpropagation Quantization (IBQ), a new VQ method for the joint optimization of all codebook embeddings and the visual encoder. Applying a straight-through estimator on the one-hot categorical distribution between the encoded feature and codebook, all codes are differentiable and maintain a consistent latent space with the visual encoder. IBQ enables scalable training of visual tokenizers and, for the first time, achieves a large-scale codebook ($2^{18}$) with high dimension ($256$) and high utilization. Experiments on the standard ImageNet benchmark demonstrate the scalability and superiority of IBQ, achieving competitive results on both reconstruction ($1.00$ rFID) and autoregressive visual generation ($2.05$ gFID). The code and models are available at https://github.com/TencentARC/SEED-Voken.","sentences":["Existing vector quantization (VQ) methods struggle with scalability, largely attributed to the instability of the codebook that undergoes partial updates during training.","The codebook is prone to collapse as utilization decreases, due to the progressively widening distribution gap between non-activated codes and visual features.","To solve the problem, we propose Index Backpropagation Quantization (IBQ), a new VQ method for the joint optimization of all codebook embeddings and the visual encoder.","Applying a straight-through estimator on the one-hot categorical distribution between the encoded feature and codebook, all codes are differentiable and maintain a consistent latent space with the visual encoder.","IBQ enables scalable training of visual tokenizers and, for the first time, achieves a large-scale codebook ($2^{18}$) with high dimension ($256$) and high utilization.","Experiments on the standard ImageNet benchmark demonstrate the scalability and superiority of IBQ, achieving competitive results on both reconstruction ($1.00$ rFID) and autoregressive visual generation ($2.05$ gFID).","The code and models are available at https://github.com/TencentARC/SEED-Voken."],"url":"http://arxiv.org/abs/2412.02692v1"}
{"created":"2024-12-03 18:58:19","title":"FoundHand: Large-Scale Domain-Specific Learning for Controllable Hand Image Generation","abstract":"Despite remarkable progress in image generation models, generating realistic hands remains a persistent challenge due to their complex articulation, varying viewpoints, and frequent occlusions. We present FoundHand, a large-scale domain-specific diffusion model for synthesizing single and dual hand images. To train our model, we introduce FoundHand-10M, a large-scale hand dataset with 2D keypoints and segmentation mask annotations. Our insight is to use 2D hand keypoints as a universal representation that encodes both hand articulation and camera viewpoint. FoundHand learns from image pairs to capture physically plausible hand articulations, natively enables precise control through 2D keypoints, and supports appearance control. Our model exhibits core capabilities that include the ability to repose hands, transfer hand appearance, and even synthesize novel views. This leads to zero-shot capabilities for fixing malformed hands in previously generated images, or synthesizing hand video sequences. We present extensive experiments and evaluations that demonstrate state-of-the-art performance of our method.","sentences":["Despite remarkable progress in image generation models, generating realistic hands remains a persistent challenge due to their complex articulation, varying viewpoints, and frequent occlusions.","We present FoundHand, a large-scale domain-specific diffusion model for synthesizing single and dual hand images.","To train our model, we introduce FoundHand-10M, a large-scale hand dataset with 2D keypoints and segmentation mask annotations.","Our insight is to use 2D hand keypoints as a universal representation that encodes both hand articulation and camera viewpoint.","FoundHand learns from image pairs to capture physically plausible hand articulations, natively enables precise control through 2D keypoints, and supports appearance control.","Our model exhibits core capabilities that include the ability to repose hands, transfer hand appearance, and even synthesize novel views.","This leads to zero-shot capabilities for fixing malformed hands in previously generated images, or synthesizing hand video sequences.","We present extensive experiments and evaluations that demonstrate state-of-the-art performance of our method."],"url":"http://arxiv.org/abs/2412.02690v1"}
{"created":"2024-12-03 18:58:11","title":"Preliminary Investigation into Data Scaling Laws for Imitation Learning-Based End-to-End Autonomous Driving","abstract":"The end-to-end autonomous driving paradigm has recently attracted lots of attention due to its scalability. However, existing methods are constrained by the limited scale of real-world data, which hinders a comprehensive exploration of the scaling laws associated with end-to-end autonomous driving. To address this issue, we collected substantial data from various driving scenarios and behaviors and conducted an extensive study on the scaling laws of existing imitation learning-based end-to-end autonomous driving paradigms. Specifically, approximately 4 million demonstrations from 23 different scenario types were gathered, amounting to over 30,000 hours of driving demonstrations. We performed open-loop evaluations and closed-loop simulation evaluations in 1,400 diverse driving demonstrations (1,300 for open-loop and 100 for closed-loop) under stringent assessment conditions. Through experimental analysis, we discovered that (1) the performance of the driving model exhibits a power-law relationship with the amount of training data; (2) a small increase in the quantity of long-tailed data can significantly improve the performance for the corresponding scenarios; (3) appropriate scaling of data enables the model to achieve combinatorial generalization in novel scenes and actions. Our results highlight the critical role of data scaling in improving the generalizability of models across diverse autonomous driving scenarios, assuring safe deployment in the real world. Project repository: https://github.com/ucaszyp/Driving-Scaling-Law","sentences":["The end-to-end autonomous driving paradigm has recently attracted lots of attention due to its scalability.","However, existing methods are constrained by the limited scale of real-world data, which hinders a comprehensive exploration of the scaling laws associated with end-to-end autonomous driving.","To address this issue, we collected substantial data from various driving scenarios and behaviors and conducted an extensive study on the scaling laws of existing imitation learning-based end-to-end autonomous driving paradigms.","Specifically, approximately 4 million demonstrations from 23 different scenario types were gathered, amounting to over 30,000 hours of driving demonstrations.","We performed open-loop evaluations and closed-loop simulation evaluations in 1,400 diverse driving demonstrations (1,300 for open-loop and 100 for closed-loop) under stringent assessment conditions.","Through experimental analysis, we discovered that (1) the performance of the driving model exhibits a power-law relationship with the amount of training data; (2) a small increase in the quantity of long-tailed data can significantly improve the performance for the corresponding scenarios; (3) appropriate scaling of data enables the model to achieve combinatorial generalization in novel scenes and actions.","Our results highlight the critical role of data scaling in improving the generalizability of models across diverse autonomous driving scenarios, assuring safe deployment in the real world.","Project repository: https://github.com/ucaszyp/Driving-Scaling-Law"],"url":"http://arxiv.org/abs/2412.02689v1"}
{"created":"2024-12-03 18:56:32","title":"SNOOPI: Supercharged One-step Diffusion Distillation with Proper Guidance","abstract":"Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones. The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources. However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss. Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation. This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference. First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach. By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance. Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images. Our experimental results show that our proposed methods significantly improve baseline models across various metrics. Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models.","sentences":["Recent approaches have yielded promising results in distilling multi-step text-to-image diffusion models into one-step ones.","The state-of-the-art efficient distillation technique, i.e., SwiftBrushv2 (SBv2), even surpasses the teacher model's performance with limited resources.","However, our study reveals its instability when handling different diffusion model backbones due to using a fixed guidance scale within the Variational Score Distillation (VSD) loss.","Another weakness of the existing one-step diffusion models is the missing support for negative prompt guidance, which is crucial in practical image generation.","This paper presents SNOOPI, a novel framework designed to address these limitations by enhancing the guidance in one-step diffusion models during both training and inference.","First, we effectively enhance training stability through Proper Guidance-SwiftBrush (PG-SB), which employs a random-scale classifier-free guidance approach.","By varying the guidance scale of both teacher models, we broaden their output distributions, resulting in a more robust VSD loss that enables SB to perform effectively across diverse backbones while maintaining competitive performance.","Second, we propose a training-free method called Negative-Away Steer Attention (NASA), which integrates negative prompts into one-step diffusion models via cross-attention to suppress undesired elements in generated images.","Our experimental results show that our proposed methods significantly improve baseline models across various metrics.","Remarkably, we achieve an HPSv2 score of 31.08, setting a new state-of-the-art benchmark for one-step diffusion models."],"url":"http://arxiv.org/abs/2412.02687v1"}
{"created":"2024-12-03 18:56:07","title":"T-REG: Preference Optimization with Token-Level Reward Regularization","abstract":"Reinforcement learning from human feedback (RLHF) has been crucial in aligning large language models (LLMs) with human values. Traditionally, RLHF involves generating responses to a query and using a reward model to assign a reward to the entire response. However, this approach faces challenges due to its reliance on a single, sparse reward, which makes it challenging for the model to identify which parts of the sequence contribute most significantly to the final reward. Recent methods have attempted to address this limitation by introducing token-level rewards. However, these methods often rely on either a trained credit assignment model or AI annotators, raising concerns about the quality and reliability of the rewards. In this paper, we propose token-level reward regularization (T-REG), a novel approach that leverages both sequence-level and token-level rewards for preference optimization. Harnessing the self-refinement capabilities of LLMs, our method uses contrastive prompting to enable LLMs to self-generate token-level rewards. These self-generated rewards then act as reward regularization, guiding the model to more effectively distribute sequence-level rewards across tokens. This facilitates better token-level credit assignment and enhances alignment performance. Experiments on the instruction following benchmarks, including Alpaca Eval 2 and Arena-Hard, show that our method consistently outperforms baseline methods by up to 3.8% and 4.4%, respectively. We will release the code and models at https://github.com/wzhouad/T-REG.","sentences":["Reinforcement learning from human feedback (RLHF) has been crucial in aligning large language models (LLMs) with human values.","Traditionally, RLHF involves generating responses to a query and using a reward model to assign a reward to the entire response.","However, this approach faces challenges due to its reliance on a single, sparse reward, which makes it challenging for the model to identify which parts of the sequence contribute most significantly to the final reward.","Recent methods have attempted to address this limitation by introducing token-level rewards.","However, these methods often rely on either a trained credit assignment model or AI annotators, raising concerns about the quality and reliability of the rewards.","In this paper, we propose token-level reward regularization (T-REG), a novel approach that leverages both sequence-level and token-level rewards for preference optimization.","Harnessing the self-refinement capabilities of LLMs, our method uses contrastive prompting to enable LLMs to self-generate token-level rewards.","These self-generated rewards then act as reward regularization, guiding the model to more effectively distribute sequence-level rewards across tokens.","This facilitates better token-level credit assignment and enhances alignment performance.","Experiments on the instruction following benchmarks, including Alpaca Eval 2 and Arena-Hard, show that our method consistently outperforms baseline methods by up to 3.8% and 4.4%, respectively.","We will release the code and models at https://github.com/wzhouad/T-REG."],"url":"http://arxiv.org/abs/2412.02685v1"}
{"created":"2024-12-03 18:55:39","title":"AniGS: Animatable Gaussian Avatar from a Single Image with Inconsistent Gaussian Reconstruction","abstract":"Generating animatable human avatars from a single image is essential for various digital human modeling applications. Existing 3D reconstruction methods often struggle to capture fine details in animatable models, while generative approaches for controllable animation, though avoiding explicit 3D modeling, suffer from viewpoint inconsistencies in extreme poses and computational inefficiencies. In this paper, we address these challenges by leveraging the power of generative models to produce detailed multi-view canonical pose images, which help resolve ambiguities in animatable human reconstruction. We then propose a robust method for 3D reconstruction of inconsistent images, enabling real-time rendering during inference. Specifically, we adapt a transformer-based video generation model to generate multi-view canonical pose images and normal maps, pretraining on a large-scale video dataset to improve generalization. To handle view inconsistencies, we recast the reconstruction problem as a 4D task and introduce an efficient 3D modeling approach using 4D Gaussian Splatting. Experiments demonstrate that our method achieves photorealistic, real-time animation of 3D human avatars from in-the-wild images, showcasing its effectiveness and generalization capability.","sentences":["Generating animatable human avatars from a single image is essential for various digital human modeling applications.","Existing 3D reconstruction methods often struggle to capture fine details in animatable models, while generative approaches for controllable animation, though avoiding explicit 3D modeling, suffer from viewpoint inconsistencies in extreme poses and computational inefficiencies.","In this paper, we address these challenges by leveraging the power of generative models to produce detailed multi-view canonical pose images, which help resolve ambiguities in animatable human reconstruction.","We then propose a robust method for 3D reconstruction of inconsistent images, enabling real-time rendering during inference.","Specifically, we adapt a transformer-based video generation model to generate multi-view canonical pose images and normal maps, pretraining on a large-scale video dataset to improve generalization.","To handle view inconsistencies, we recast the reconstruction problem as a 4D task and introduce an efficient 3D modeling approach using 4D Gaussian Splatting.","Experiments demonstrate that our method achieves photorealistic, real-time animation of 3D human avatars from in-the-wild images, showcasing its effectiveness and generalization capability."],"url":"http://arxiv.org/abs/2412.02684v1"}
{"created":"2024-12-03 18:54:49","title":"The Asymptotic Behavior of Attention in Transformers","abstract":"A key component of transformers is the attention mechanism orchestrating how each token influences the propagation of every other token through a transformer. In this paper we provide a rigorous, mathematical analysis of the asymptotic properties of attention in transformers. Although we present several results based on different assumptions, all of them point to the same conclusion, all tokens asymptotically converge to each other, a phenomenon that has been empirically reported in the literature. Our findings are carefully compared with existing theoretical results and illustrated by simulations and experimental studies using the GPT-2 model.","sentences":["A key component of transformers is the attention mechanism orchestrating how each token influences the propagation of every other token through a transformer.","In this paper we provide a rigorous, mathematical analysis of the asymptotic properties of attention in transformers.","Although we present several results based on different assumptions, all of them point to the same conclusion, all tokens asymptotically converge to each other, a phenomenon that has been empirically reported in the literature.","Our findings are carefully compared with existing theoretical results and illustrated by simulations and experimental studies using the GPT-2 model."],"url":"http://arxiv.org/abs/2412.02682v1"}
{"created":"2024-12-03 18:51:39","title":"Planning-Guided Diffusion Policy Learning for Generalizable Contact-Rich Bimanual Manipulation","abstract":"Contact-rich bimanual manipulation involves precise coordination of two arms to change object states through strategically selected contacts and motions. Due to the inherent complexity of these tasks, acquiring sufficient demonstration data and training policies that generalize to unseen scenarios remain a largely unresolved challenge. Building on recent advances in planning through contacts, we introduce Generalizable Planning-Guided Diffusion Policy Learning (GLIDE), an approach that effectively learns to solve contact-rich bimanual manipulation tasks by leveraging model-based motion planners to generate demonstration data in high-fidelity physics simulation. Through efficient planning in randomized environments, our approach generates large-scale and high-quality synthetic motion trajectories for tasks involving diverse objects and transformations. We then train a task-conditioned diffusion policy via behavior cloning using these demonstrations. To tackle the sim-to-real gap, we propose a set of essential design options in feature extraction, task representation, action prediction, and data augmentation that enable learning robust prediction of smooth action sequences and generalization to unseen scenarios. Through experiments in both simulation and the real world, we demonstrate that our approach can enable a bimanual robotic system to effectively manipulate objects of diverse geometries, dimensions, and physical properties. Website: https://glide-manip.github.io/","sentences":["Contact-rich bimanual manipulation involves precise coordination of two arms to change object states through strategically selected contacts and motions.","Due to the inherent complexity of these tasks, acquiring sufficient demonstration data and training policies that generalize to unseen scenarios remain a largely unresolved challenge.","Building on recent advances in planning through contacts, we introduce Generalizable Planning-Guided Diffusion Policy Learning (GLIDE), an approach that effectively learns to solve contact-rich bimanual manipulation tasks by leveraging model-based motion planners to generate demonstration data in high-fidelity physics simulation.","Through efficient planning in randomized environments, our approach generates large-scale and high-quality synthetic motion trajectories for tasks involving diverse objects and transformations.","We then train a task-conditioned diffusion policy via behavior cloning using these demonstrations.","To tackle the sim-to-real gap, we propose a set of essential design options in feature extraction, task representation, action prediction, and data augmentation that enable learning robust prediction of smooth action sequences and generalization to unseen scenarios.","Through experiments in both simulation and the real world, we demonstrate that our approach can enable a bimanual robotic system to effectively manipulate objects of diverse geometries, dimensions, and physical properties.","Website: https://glide-manip.github.io/"],"url":"http://arxiv.org/abs/2412.02676v1"}
{"created":"2024-12-03 18:47:26","title":"Mind the Gap: Examining the Self-Improvement Capabilities of Large Language Models","abstract":"Self-improvement is a mechanism in Large Language Model (LLM) pre-training, post-training and test-time inference. We explore a framework where the model verifies its own outputs, filters or reweights data based on this verification, and distills the filtered data. Despite several empirical successes, a fundamental understanding is still lacking. In this work, we initiate a comprehensive, modular and controlled study on LLM self-improvement. We provide a mathematical formulation for self-improvement, which is largely governed by a quantity which we formalize as the generation-verification gap. Through experiments with various model families and tasks, we discover a scaling phenomenon of self-improvement -- a variant of the generation-verification gap scales monotonically with the model pre-training flops. We also examine when self-improvement is possible, an iterative self-improvement procedure, and ways to improve its performance. Our findings not only advance understanding of LLM self-improvement with practical implications, but also open numerous avenues for future research into its capabilities and boundaries.","sentences":["Self-improvement is a mechanism in Large Language Model (LLM) pre-training, post-training and test-time inference.","We explore a framework where the model verifies its own outputs, filters or reweights data based on this verification, and distills the filtered data.","Despite several empirical successes, a fundamental understanding is still lacking.","In this work, we initiate a comprehensive, modular and controlled study on LLM self-improvement.","We provide a mathematical formulation for self-improvement, which is largely governed by a quantity which we formalize as the generation-verification gap.","Through experiments with various model families and tasks, we discover a scaling phenomenon of self-improvement -- a variant of the generation-verification gap scales monotonically with the model pre-training flops.","We also examine when self-improvement is possible, an iterative self-improvement procedure, and ways to improve its performance.","Our findings not only advance understanding of LLM self-improvement with practical implications, but also open numerous avenues for future research into its capabilities and boundaries."],"url":"http://arxiv.org/abs/2412.02674v1"}
{"created":"2024-12-03 18:38:14","title":"Probing the statistical properties of enriched co-occurrence networks","abstract":"Recent studies have explored the addition of virtual edges to word co-occurrence networks using word embeddings to enhance graph representations, particularly for short texts. While these enriched networks have demonstrated some success, the impact of incorporating semantic edges into traditional co-occurrence networks remains uncertain. This study investigates two key statistical properties of text-based network models. First, we assess whether network metrics can effectively distinguish between meaningless and meaningful texts. Second, we analyze whether these metrics are more sensitive to syntactic or semantic aspects of the text. Our results show that incorporating virtual edges can have positive and negative effects, depending on the specific network metric. For instance, the informativeness of the average shortest path and closeness centrality improves in short texts, while the clustering coefficient's informativeness decreases as more virtual edges are added. Additionally, we found that including stopwords affects the statistical properties of enriched networks. Our results can serve as a guideline for determining which network metrics are most appropriate for specific applications, depending on the typical text size and the nature of the problem.","sentences":["Recent studies have explored the addition of virtual edges to word co-occurrence networks using word embeddings to enhance graph representations, particularly for short texts.","While these enriched networks have demonstrated some success, the impact of incorporating semantic edges into traditional co-occurrence networks remains uncertain.","This study investigates two key statistical properties of text-based network models.","First, we assess whether network metrics can effectively distinguish between meaningless and meaningful texts.","Second, we analyze whether these metrics are more sensitive to syntactic or semantic aspects of the text.","Our results show that incorporating virtual edges can have positive and negative effects, depending on the specific network metric.","For instance, the informativeness of the average shortest path and closeness centrality improves in short texts, while the clustering coefficient's informativeness decreases as more virtual edges are added.","Additionally, we found that including stopwords affects the statistical properties of enriched networks.","Our results can serve as a guideline for determining which network metrics are most appropriate for specific applications, depending on the typical text size and the nature of the problem."],"url":"http://arxiv.org/abs/2412.02664v1"}
{"created":"2024-12-03 18:37:07","title":"Unconditional proofs of quantumness between small-space machines","abstract":"A proof of quantumness is a protocol through which a classical machine can test whether a purportedly quantum device, with comparable time and memory resources, is performing a computation that is impossible for classical computers. Existing approaches to provide proofs of quantumness depend on unproven assumptions about some task being impossible for machines of a particular model under certain resource restrictions. We study a setup where both devices have space bounds $\\mathit{o}(\\log \\log n)$. Under such memory budgets, it has been unconditionally proven that probabilistic Turing machines are unable to solve certain computational problems. We formulate a new class of problems, and show that these problems are polynomial-time solvable for quantum machines, impossible for classical machines, and have the property that their solutions can be \"proved\" by a small-space quantum machine to a classical machine with the same space bound. These problems form the basis of our newly defined protocol, where the polynomial-time verifier's verdict about the tested machine's quantumness is not conditional on an unproven weakness assumption.","sentences":["A proof of quantumness is a protocol through which a classical machine can test whether a purportedly quantum device, with comparable time and memory resources, is performing a computation that is impossible for classical computers.","Existing approaches to provide proofs of quantumness depend on unproven assumptions about some task being impossible for machines of a particular model under certain resource restrictions.","We study a setup where both devices have space bounds $\\mathit{o}(\\log \\log n)$.","Under such memory budgets, it has been unconditionally proven that probabilistic Turing machines are unable to solve certain computational problems.","We formulate a new class of problems, and show that these problems are polynomial-time solvable for quantum machines, impossible for classical machines, and have the property that their solutions can be \"proved\" by a small-space quantum machine to a classical machine with the same space bound.","These problems form the basis of our newly defined protocol, where the polynomial-time verifier's verdict about the tested machine's quantumness is not conditional on an unproven weakness assumption."],"url":"http://arxiv.org/abs/2412.02662v1"}
{"created":"2024-12-03 18:36:45","title":"Efficient Graph Matching for Correlated Stochastic Block Models","abstract":"We study learning problems on correlated stochastic block models with two balanced communities. Our main result gives the first efficient algorithm for graph matching in this setting. In the most interesting regime where the average degree is logarithmic in the number of vertices, this algorithm correctly matches all but a vanishing fraction of vertices with high probability, whenever the edge correlation parameter $s$ satisfies $s^2 > \\alpha \\approx 0.338$, where $\\alpha$ is Otter's tree-counting constant. Moreover, we extend this to an efficient algorithm for exact graph matching whenever this is information-theoretically possible, positively resolving an open problem of R\\'acz and Sridhar (NeurIPS 2021). Our algorithm generalizes the recent breakthrough work of Mao, Wu, Xu, and Yu (STOC 2023), which is based on centered subgraph counts of a large family of trees termed chandeliers. A major technical challenge that we overcome is dealing with the additional estimation errors that are necessarily present due to the fact that, in relevant parameter regimes, the latent community partition cannot be exactly recovered from a single graph. As an application of our results, we give an efficient algorithm for exact community recovery using multiple correlated graphs in parameter regimes where it is information-theoretically impossible to do so using just a single graph.","sentences":["We study learning problems on correlated stochastic block models with two balanced communities.","Our main result gives the first efficient algorithm for graph matching in this setting.","In the most interesting regime where the average degree is logarithmic in the number of vertices, this algorithm correctly matches all but a vanishing fraction of vertices with high probability, whenever the edge correlation parameter $s$ satisfies $s^2 > \\alpha \\approx 0.338$, where $\\alpha$ is Otter's tree-counting constant.","Moreover, we extend this to an efficient algorithm for exact graph matching whenever this is information-theoretically possible, positively resolving an open problem of R\\'acz and Sridhar (NeurIPS 2021).","Our algorithm generalizes the recent breakthrough work of Mao, Wu, Xu, and Yu (STOC 2023), which is based on centered subgraph counts of a large family of trees termed chandeliers.","A major technical challenge that we overcome is dealing with the additional estimation errors that are necessarily present due to the fact that, in relevant parameter regimes, the latent community partition cannot be exactly recovered from a single graph.","As an application of our results, we give an efficient algorithm for exact community recovery using multiple correlated graphs in parameter regimes where it is information-theoretically impossible to do so using just a single graph."],"url":"http://arxiv.org/abs/2412.02661v1"}
{"created":"2024-12-03 18:29:37","title":"LLM-Enhanced Path Planning: Safe and Efficient Autonomous Navigation with Instructional Inputs","abstract":"Autonomous navigation guided by natural language instructions is essential for improving human-robot interaction and enabling complex operations in dynamic environments. While large language models (LLMs) are not inherently designed for planning, they can significantly enhance planning efficiency by providing guidance and informing constraints to ensure safety. This paper introduces a planning framework that integrates LLMs with 2D occupancy grid maps and natural language commands to improve spatial reasoning and task execution in resource-limited settings. By decomposing high-level commands and real-time environmental data, the system generates structured navigation plans for pick-and-place tasks, including obstacle avoidance, goal prioritization, and adaptive behaviors. The framework dynamically recalculates paths to address environmental changes and aligns with implicit social norms for seamless human-robot interaction. Our results demonstrates the potential of LLMs to design context-aware system to enhance navigation efficiency and safety in industrial and dynamic environments.","sentences":["Autonomous navigation guided by natural language instructions is essential for improving human-robot interaction and enabling complex operations in dynamic environments.","While large language models (LLMs) are not inherently designed for planning, they can significantly enhance planning efficiency by providing guidance and informing constraints to ensure safety.","This paper introduces a planning framework that integrates LLMs with 2D occupancy grid maps and natural language commands to improve spatial reasoning and task execution in resource-limited settings.","By decomposing high-level commands and real-time environmental data, the system generates structured navigation plans for pick-and-place tasks, including obstacle avoidance, goal prioritization, and adaptive behaviors.","The framework dynamically recalculates paths to address environmental changes and aligns with implicit social norms for seamless human-robot interaction.","Our results demonstrates the potential of LLMs to design context-aware system to enhance navigation efficiency and safety in industrial and dynamic environments."],"url":"http://arxiv.org/abs/2412.02655v1"}
{"created":"2024-12-03 18:24:15","title":"Bridging Hard and Soft: Mechanical Metamaterials Enable Rigid Torque Transmission in Soft Robots","abstract":"Torque and continuous rotation are fundamental methods of actuation and manipulation in rigid robots. Soft robot arms use soft materials and structures to mimic the passive compliance of biological arms that bend and extend. This use of compliance prevents soft arms from continuously transmitting and exerting torques to interact with their environment. Here, we show how relying on patterning structures instead of inherent material properties allows soft robotic arms to remain compliant while continuously transmitting torque to their environment. We demonstrate a soft robotic arm made from a pair of mechanical metamaterials that act as compliant constant-velocity joints. The joints are up to 52 times stiffer in torsion than bending and can bend up to 45{\\deg}. This robot arm can continuously transmit torque while deforming in all other directions. The arm's mechanical design achieves high motion repeatability (0.4 mm and 0.1{\\deg}) when tracking trajectories. We then trained a neural network to learn the inverse kinematics, enabling us to program the arm to complete tasks that are challenging for existing soft robots such as installing light bulbs, fastening bolts, and turning valves. The arm's passive compliance makes it safe around humans and provides a source of mechanical intelligence, enabling it to adapt to misalignment when manipulating objects. This work will bridge the gap between hard and soft robotics with applications in human assistance, warehouse automation, and extreme environments.","sentences":["Torque and continuous rotation are fundamental methods of actuation and manipulation in rigid robots.","Soft robot arms use soft materials and structures to mimic the passive compliance of biological arms that bend and extend.","This use of compliance prevents soft arms from continuously transmitting and exerting torques to interact with their environment.","Here, we show how relying on patterning structures instead of inherent material properties allows soft robotic arms to remain compliant while continuously transmitting torque to their environment.","We demonstrate a soft robotic arm made from a pair of mechanical metamaterials that act as compliant constant-velocity joints.","The joints are up to 52 times stiffer in torsion than bending and can bend up to 45{\\deg}.","This robot arm can continuously transmit torque while deforming in all other directions.","The arm's mechanical design achieves high motion repeatability (0.4 mm and 0.1{\\deg}) when tracking trajectories.","We then trained a neural network to learn the inverse kinematics, enabling us to program the arm to complete tasks that are challenging for existing soft robots such as installing light bulbs, fastening bolts, and turning valves.","The arm's passive compliance makes it safe around humans and provides a source of mechanical intelligence, enabling it to adapt to misalignment when manipulating objects.","This work will bridge the gap between hard and soft robotics with applications in human assistance, warehouse automation, and extreme environments."],"url":"http://arxiv.org/abs/2412.02650v1"}
{"created":"2024-12-03 18:21:29","title":"Quaternary and Component-Binary Spreading Codes with Low Correlation for Navigation Systems","abstract":"In the first part of this two-part paper, we construct a family MFD$_2$ of low-correlation quaternary spreading codes having period $2046$. By quaternary, we mean that the spreading code symbols are drawn from $Z_4$ and are designed to be used in conjunction with QPSK modulation. Apart from low auto and crosscorrelation properties, we also require in addition, to our knowledge for the first time, that the spreading code family IZ4$_2$ obtained by taking the union of the component in-phase and quadrature-phase binary spreading codes associated to each quaternary spreading code in MFD$_2$, also have desirable low-correlation properties. We also investigate the balance of the quaternary and binary spreading codes.   The second part is motivated by an application to the design of spreading code, (in this application termed as ranging codes), having parameters suitable for use in a lunar PNT system. Two lengths that are of particular current interest for a planned lunar PNT satellite system are $2046$ and $10230$. We study the applicability of a subset IZ4$_{2S}$ of IZ4$_2$ containing balanced binary spreading codes having length $2046$ to such a lunar PNT system. We show that the spreading codes belonging to IZ4$_{2S}$ compare favorably with the spreading codes of length $2046$ appearing in a recent issue of Inside GNSS. We also show that the IZ4$_{10}$ spreading code family in which the spreading codes have length $10230$, compares well in comparison with spreading codes of length $10230$ described in this article. In addition, the IZ4$_{10}$ and IZ4$_2$ spreading codes have been paired so as to be orthogonal at zero shift despite their different lengths and chipping rates.","sentences":["In the first part of this two-part paper, we construct a family MFD$_2$ of low-correlation quaternary spreading codes having period $2046$. By quaternary, we mean that the spreading code symbols are drawn from $Z_4$ and are designed to be used in conjunction with QPSK modulation.","Apart from low auto and crosscorrelation properties, we also require in addition, to our knowledge for the first time, that the spreading code family IZ4$_2$ obtained by taking the union of the component in-phase and quadrature-phase binary spreading codes associated to each quaternary spreading code in MFD$_2$, also have desirable low-correlation properties.","We also investigate the balance of the quaternary and binary spreading codes.   ","The second part is motivated by an application to the design of spreading code, (in this application termed as ranging codes), having parameters suitable for use in a lunar PNT system.","Two lengths that are of particular current interest for a planned lunar PNT satellite system are $2046$ and $10230$. We study the applicability of a subset IZ4$_{2S}$ of IZ4$_2$ containing balanced binary spreading codes having length $2046$ to such a lunar PNT system.","We show that the spreading codes belonging to IZ4$_{2S}$ compare favorably with the spreading codes of length $2046$ appearing in a recent issue of Inside GNSS.","We also show that the IZ4$_{10}$ spreading code family in which the spreading codes have length $10230$, compares well in comparison with spreading codes of length $10230$ described in this article.","In addition, the IZ4$_{10}$ and IZ4$_2$ spreading codes have been paired so as to be orthogonal at zero shift despite their different lengths and chipping rates."],"url":"http://arxiv.org/abs/2412.02647v1"}
{"created":"2024-12-03 18:21:20","title":"Interpretable Generalized Additive Models for Datasets with Missing Values","abstract":"Many important datasets contain samples that are missing one or more feature values. Maintaining the interpretability of machine learning models in the presence of such missing data is challenging. Singly or multiply imputing missing values complicates the model's mapping from features to labels. On the other hand, reasoning on indicator variables that represent missingness introduces a potentially large number of additional terms, sacrificing sparsity. We solve these problems with M-GAM, a sparse, generalized, additive modeling approach that incorporates missingness indicators and their interaction terms while maintaining sparsity through l0 regularization. We show that M-GAM provides similar or superior accuracy to prior methods while significantly improving sparsity relative to either imputation or naive inclusion of indicator variables.","sentences":["Many important datasets contain samples that are missing one or more feature values.","Maintaining the interpretability of machine learning models in the presence of such missing data is challenging.","Singly or multiply imputing missing values complicates the model's mapping from features to labels.","On the other hand, reasoning on indicator variables that represent missingness introduces a potentially large number of additional terms, sacrificing sparsity.","We solve these problems with M-GAM, a sparse, generalized, additive modeling approach that incorporates missingness indicators and their interaction terms while maintaining sparsity through l0 regularization.","We show that M-GAM provides similar or superior accuracy to prior methods while significantly improving sparsity relative to either imputation or naive inclusion of indicator variables."],"url":"http://arxiv.org/abs/2412.02646v1"}
{"created":"2024-12-03 18:15:51","title":"Leveraging Tactile Sensing to Render both Haptic Feedback and Virtual Reality 3D Object Reconstruction in Robotic Telemanipulation","abstract":"Dexterous robotic manipulator teleoperation is widely used in many applications, either where it is convenient to keep the human inside the control loop, or to train advanced robot agents. So far, this technology has been used in combination with camera systems with remarkable success. On the other hand, only a limited number of studies have focused on leveraging haptic feedback from tactile sensors in contexts where camera-based systems fail, such as due to self-occlusions or poor light conditions like smoke. This study demonstrates the feasibility of precise pick-and-place teleoperation without cameras by leveraging tactile-based 3D object reconstruction in VR and providing haptic feedback to a blindfolded user. Our preliminary results show that integrating these technologies enables the successful completion of telemanipulation tasks previously dependent on cameras, paving the way for more complex future applications.","sentences":["Dexterous robotic manipulator teleoperation is widely used in many applications, either where it is convenient to keep the human inside the control loop, or to train advanced robot agents.","So far, this technology has been used in combination with camera systems with remarkable success.","On the other hand, only a limited number of studies have focused on leveraging haptic feedback from tactile sensors in contexts where camera-based systems fail, such as due to self-occlusions or poor light conditions like smoke.","This study demonstrates the feasibility of precise pick-and-place teleoperation without cameras by leveraging tactile-based 3D object reconstruction in VR and providing haptic feedback to a blindfolded user.","Our preliminary results show that integrating these technologies enables the successful completion of telemanipulation tasks previously dependent on cameras, paving the way for more complex future applications."],"url":"http://arxiv.org/abs/2412.02644v1"}
{"created":"2024-12-03 18:15:34","title":"A Bidirectional Long Short Term Memory Approach for Infrastructure Health Monitoring Using On-board Vibration Response","abstract":"The growing volume of available infrastructural monitoring data enables the development of powerful datadriven approaches to estimate infrastructure health conditions using direct measurements. This paper proposes a deep learning methodology to estimate infrastructure physical parameters, such as railway track stiffness, using drive-by vibration response signals. The proposed method employs a Long Short-term Memory (LSTM) feature extractor accounting for temporal dependencies in the feature extraction phase, and a bidirectional Long Short-term Memory (BiLSTM) networks to leverage bidirectional temporal dependencies in both the forward and backward paths of the drive-by vibration response in condition estimation phase. Additionally, a framing approach is employed to enhance the resolution of the monitoring task to the beam level by segmenting the vibration signal into frames equal to the distance between individual beams, centering the frames over the beam nodes. The proposed LSTM-BiLSTM model offers a versatile tool for various bridge and railway infrastructure conditions monitoring using direct drive-by vibration response measurements. The results demonstrate the potential of incorporating temporal analysis in the feature extraction phase and emphasize the pivotal role of bidirectional temporal information in infrastructure health condition estimation. The proposed methodology can accurately and automatically estimate railway track stiffness and identify local stiffness reductions in the presence of noise using drive-by measurements. An illustrative case study of vehicle-track interaction simulation is used to demonstrate the performance of the proposed model, achieving a maximum mean absolute percentage error of 1.7% and 0.7% in estimating railpad and ballast stiffness, respectively.","sentences":["The growing volume of available infrastructural monitoring data enables the development of powerful datadriven approaches to estimate infrastructure health conditions using direct measurements.","This paper proposes a deep learning methodology to estimate infrastructure physical parameters, such as railway track stiffness, using drive-by vibration response signals.","The proposed method employs a Long Short-term Memory (LSTM) feature extractor accounting for temporal dependencies in the feature extraction phase, and a bidirectional Long Short-term Memory (BiLSTM) networks to leverage bidirectional temporal dependencies in both the forward and backward paths of the drive-by vibration response in condition estimation phase.","Additionally, a framing approach is employed to enhance the resolution of the monitoring task to the beam level by segmenting the vibration signal into frames equal to the distance between individual beams, centering the frames over the beam nodes.","The proposed LSTM-BiLSTM model offers a versatile tool for various bridge and railway infrastructure conditions monitoring using direct drive-by vibration response measurements.","The results demonstrate the potential of incorporating temporal analysis in the feature extraction phase and emphasize the pivotal role of bidirectional temporal information in infrastructure health condition estimation.","The proposed methodology can accurately and automatically estimate railway track stiffness and identify local stiffness reductions in the presence of noise using drive-by measurements.","An illustrative case study of vehicle-track interaction simulation is used to demonstrate the performance of the proposed model, achieving a maximum mean absolute percentage error of 1.7% and 0.7% in estimating railpad and ballast stiffness, respectively."],"url":"http://arxiv.org/abs/2412.02643v1"}
{"created":"2024-12-03 18:13:51","title":"Robust soybean seed yield estimation using high-throughput ground robot videos","abstract":"We present a novel method for soybean (Glycine max (L.) Merr.) yield estimation leveraging high throughput seed counting via computer vision and deep learning techniques. Traditional methods for collecting yield data are labor-intensive, costly, prone to equipment failures at critical data collection times, and require transportation of equipment across field sites. Computer vision, the field of teaching computers to interpret visual data, allows us to extract detailed yield information directly from images. By treating it as a computer vision task, we report a more efficient alternative, employing a ground robot equipped with fisheye cameras to capture comprehensive videos of soybean plots from which images are extracted in a variety of development programs. These images are processed through the P2PNet-Yield model, a deep learning framework where we combined a Feature Extraction Module (the backbone of the P2PNet-Soy) and a Yield Regression Module to estimate seed yields of soybean plots. Our results are built on three years of yield testing plot data - 8500 in 2021, 2275 in 2022, and 650 in 2023. With these datasets, our approach incorporates several innovations to further improve the accuracy and generalizability of the seed counting and yield estimation architecture, such as the fisheye image correction and data augmentation with random sensor effects. The P2PNet-Yield model achieved a genotype ranking accuracy score of up to 83%. It demonstrates up to a 32% reduction in time to collect yield data as well as costs associated with traditional yield estimation, offering a scalable solution for breeding programs and agricultural productivity enhancement.","sentences":["We present a novel method for soybean (Glycine max (L.) Merr.)","yield estimation leveraging high throughput seed counting via computer vision and deep learning techniques.","Traditional methods for collecting yield data are labor-intensive, costly, prone to equipment failures at critical data collection times, and require transportation of equipment across field sites.","Computer vision, the field of teaching computers to interpret visual data, allows us to extract detailed yield information directly from images.","By treating it as a computer vision task, we report a more efficient alternative, employing a ground robot equipped with fisheye cameras to capture comprehensive videos of soybean plots from which images are extracted in a variety of development programs.","These images are processed through the P2PNet-Yield model, a deep learning framework where we combined a Feature Extraction Module (the backbone of the P2PNet-Soy) and a Yield Regression Module to estimate seed yields of soybean plots.","Our results are built on three years of yield testing plot data - 8500 in 2021, 2275 in 2022, and 650 in 2023.","With these datasets, our approach incorporates several innovations to further improve the accuracy and generalizability of the seed counting and yield estimation architecture, such as the fisheye image correction and data augmentation with random sensor effects.","The P2PNet-Yield model achieved a genotype ranking accuracy score of up to 83%.","It demonstrates up to a 32% reduction in time to collect yield data as well as costs associated with traditional yield estimation, offering a scalable solution for breeding programs and agricultural productivity enhancement."],"url":"http://arxiv.org/abs/2412.02642v1"}
{"created":"2024-12-03 18:13:37","title":"SEMANTIC SEE-THROUGH GOGGLES: Wearing Linguistic Virtual Reality in (Artificial) Intelligence","abstract":"When language is utilized as a medium to store and communicate sensory information, there arises a kind of radical virtual reality, namely \"the realities that are reduced into the same sentence are virtual/equivalent.\" In the current era, in which artificial intelligence engages in the linguistic mediation of sensory information, it is imperative to re-examine the various issues pertaining to this potential VR, particularly in relation to bias and (dis)communication. Semantic See-through Goggles represent an experimental framework for glasses through which the view is fully verbalized and re-depicted into the wearer's view. The participants wear the goggles equipped with a camera and head-mounted display (HMD). In real-time, the image captured by the camera is converted by the AI into a single line of text, which is then transformed into an image and presented to the user's eyes. This process enables users to perceive and interact with the real physical world through this redrawn view. We constructed a prototype of these goggles, examined their fundamental characteristics, and then conducted a qualitative analysis of the wearer's experience. This project investigates a methodology for subjectively capturing the situation in which AI serves as a proxy for our perception of the world. At the same time, It also attempts to appropriate some of the energy of today's debate over artificial intelligence for a classical inquiry around the fact that \"intelligence can only see the world under meaning.\"","sentences":["When language is utilized as a medium to store and communicate sensory information, there arises a kind of radical virtual reality, namely \"the realities that are reduced into the same sentence are virtual/equivalent.\"","In the current era, in which artificial intelligence engages in the linguistic mediation of sensory information, it is imperative to re-examine the various issues pertaining to this potential VR, particularly in relation to bias and (dis)communication.","Semantic See-through Goggles represent an experimental framework for glasses through which the view is fully verbalized and re-depicted into the wearer's view.","The participants wear the goggles equipped with a camera and head-mounted display (HMD).","In real-time, the image captured by the camera is converted by the AI into a single line of text, which is then transformed into an image and presented to the user's eyes.","This process enables users to perceive and interact with the real physical world through this redrawn view.","We constructed a prototype of these goggles, examined their fundamental characteristics, and then conducted a qualitative analysis of the wearer's experience.","This project investigates a methodology for subjectively capturing the situation in which AI serves as a proxy for our perception of the world.","At the same time, It also attempts to appropriate some of the energy of today's debate over artificial intelligence for a classical inquiry around the fact that \"intelligence can only see the world under meaning.\""],"url":"http://arxiv.org/abs/2412.02641v1"}
{"created":"2024-12-03 18:11:37","title":"The Space Complexity of Approximating Logistic Loss","abstract":"We provide space complexity lower bounds for data structures that approximate logistic loss up to $\\epsilon$-relative error on a logistic regression problem with data $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ and labels $\\mathbf{y} \\in \\{-1,1\\}^d$. The space complexity of existing coreset constructions depend on a natural complexity measure $\\mu_\\mathbf{y}(\\mathbf{X})$, first defined in (Munteanu, 2018). We give an $\\tilde{\\Omega}(\\frac{d}{\\epsilon^2})$ space complexity lower bound in the regime $\\mu_\\mathbf{y}(\\mathbf{X}) = O(1)$ that shows existing coresets are optimal in this regime up to lower order factors. We also prove a general $\\tilde{\\Omega}(d\\cdot \\mu_\\mathbf{y}(\\mathbf{X}))$ space lower bound when $\\epsilon$ is constant, showing that the dependency on $\\mu_\\mathbf{y}(\\mathbf{X})$ is not an artifact of mergeable coresets. Finally, we refute a prior conjecture that $\\mu_\\mathbf{y}(\\mathbf{X})$ is hard to compute by providing an efficient linear programming formulation, and we empirically compare our algorithm to prior approximate methods.","sentences":["We provide space complexity lower bounds for data structures that approximate logistic loss up to $\\epsilon$-relative error on a logistic regression problem with data $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ and labels $\\mathbf{y} \\in \\{-1,1\\}^d$.","The space complexity of existing coreset constructions depend on a natural complexity measure $\\mu_\\mathbf{y}(\\mathbf{X})$, first defined in (Munteanu, 2018).","We give an $\\tilde{\\Omega}(\\frac{d}{\\epsilon^2})$ space complexity lower bound in the regime $\\mu_\\mathbf{y}(\\mathbf{X})","= O(1)$ that shows existing coresets are optimal in this regime up to lower order factors.","We also prove a general $\\tilde{\\Omega}(d\\cdot \\mu_\\mathbf{y}(\\mathbf{X}))$ space lower bound when $\\epsilon$ is constant, showing that the dependency on $\\mu_\\mathbf{y}(\\mathbf{X})$ is not an artifact of mergeable coresets.","Finally, we refute a prior conjecture that $\\mu_\\mathbf{y}(\\mathbf{X})$ is hard to compute by providing an efficient linear programming formulation, and we empirically compare our algorithm to prior approximate methods."],"url":"http://arxiv.org/abs/2412.02639v1"}
{"created":"2024-12-03 18:10:31","title":"QA-TOOLBOX: Conversational Question-Answering for process task guidance in manufacturing","abstract":"In this work we explore utilizing LLMs for data augmentation for manufacturing task guidance system. The dataset consists of representative samples of interactions with technicians working in an advanced manufacturing setting. The purpose of this work to explore the task, data augmentation for the supported tasks and evaluating the performance of the existing LLMs. We observe that that task is complex requiring understanding from procedure specification documents, actions and objects sequenced temporally. The dataset consists of 200,000+ question/answer pairs that refer to the spec document and are grounded in narrations and/or video demonstrations. We compared the performance of several popular open-sourced LLMs by developing a baseline using each LLM and then compared the responses in a reference-free setting using LLM-as-a-judge and compared the ratings with crowd-workers whilst validating the ratings with experts.","sentences":["In this work we explore utilizing LLMs for data augmentation for manufacturing task guidance system.","The dataset consists of representative samples of interactions with technicians working in an advanced manufacturing setting.","The purpose of this work to explore the task, data augmentation for the supported tasks and evaluating the performance of the existing LLMs.","We observe that that task is complex requiring understanding from procedure specification documents, actions and objects sequenced temporally.","The dataset consists of 200,000+ question/answer pairs that refer to the spec document and are grounded in narrations and/or video demonstrations.","We compared the performance of several popular open-sourced LLMs by developing a baseline using each LLM and then compared the responses in a reference-free setting using LLM-as-a-judge and compared the ratings with crowd-workers whilst validating the ratings with experts."],"url":"http://arxiv.org/abs/2412.02638v1"}
{"created":"2024-12-03 18:10:28","title":"Words and Action: Modeling Linguistic Leadership in #BlackLivesMatter Communities","abstract":"In this project, we describe a method of modeling semantic leadership across a set of communities associated with the #BlackLivesMatter movement, which has been informed by qualitative research on the structure of social media and Black Twitter in particular. We describe our bespoke approaches to time-binning, community clustering, and connecting communities over time, as well as our adaptation of state-of-the-art approaches to semantic change detection and semantic leadership induction. We find substantial evidence of the leadership role of BLM activists and progressives, as well as Black celebrities. We also find evidence of the sustained engagement of the conservative community with this discourse, suggesting an alternative explanation for how we arrived at the present moment, in which \"anti-woke\" and \"anti-CRT\" bills are being enacted nationwide.","sentences":["In this project, we describe a method of modeling semantic leadership across a set of communities associated with the #BlackLivesMatter movement, which has been informed by qualitative research on the structure of social media and Black Twitter in particular.","We describe our bespoke approaches to time-binning, community clustering, and connecting communities over time, as well as our adaptation of state-of-the-art approaches to semantic change detection and semantic leadership induction.","We find substantial evidence of the leadership role of BLM activists and progressives, as well as Black celebrities.","We also find evidence of the sustained engagement of the conservative community with this discourse, suggesting an alternative explanation for how we arrived at the present moment, in which \"anti-woke\" and \"anti-CRT\" bills are being enacted nationwide."],"url":"http://arxiv.org/abs/2412.02637v1"}
{"created":"2024-12-03 18:04:42","title":"MetaShadow: Object-Centered Shadow Detection, Removal, and Synthesis","abstract":"Shadows are often under-considered or even ignored in image editing applications, limiting the realism of the edited results. In this paper, we introduce MetaShadow, a three-in-one versatile framework that enables detection, removal, and controllable synthesis of shadows in natural images in an object-centered fashion. MetaShadow combines the strengths of two cooperative components: Shadow Analyzer, for object-centered shadow detection and removal, and Shadow Synthesizer, for reference-based controllable shadow synthesis. Notably, we optimize the learning of the intermediate features from Shadow Analyzer to guide Shadow Synthesizer to generate more realistic shadows that blend seamlessly with the scene. Extensive evaluations on multiple shadow benchmark datasets show significant improvements of MetaShadow over the existing state-of-the-art methods on object-centered shadow detection, removal, and synthesis. MetaShadow excels in image-editing tasks such as object removal, relocation, and insertion, pushing the boundaries of object-centered image editing.","sentences":["Shadows are often under-considered or even ignored in image editing applications, limiting the realism of the edited results.","In this paper, we introduce MetaShadow, a three-in-one versatile framework that enables detection, removal, and controllable synthesis of shadows in natural images in an object-centered fashion.","MetaShadow combines the strengths of two cooperative components: Shadow Analyzer, for object-centered shadow detection and removal, and Shadow Synthesizer, for reference-based controllable shadow synthesis.","Notably, we optimize the learning of the intermediate features from Shadow Analyzer to guide Shadow Synthesizer to generate more realistic shadows that blend seamlessly with the scene.","Extensive evaluations on multiple shadow benchmark datasets show significant improvements of MetaShadow over the existing state-of-the-art methods on object-centered shadow detection, removal, and synthesis.","MetaShadow excels in image-editing tasks such as object removal, relocation, and insertion, pushing the boundaries of object-centered image editing."],"url":"http://arxiv.org/abs/2412.02635v1"}
{"created":"2024-12-03 18:03:54","title":"Liquefaction: Privately Liquefying Blockchain Assets","abstract":"Inherent in the world of cryptocurrency systems and their security models is the notion that private keys, and thus assets, are controlled by individuals or individual entities.   We present Liquefaction, a wallet platform that demonstrates the dangerous fragility of this foundational assumption by systemically breaking it. Liquefaction uses trusted execution environments (TEEs) to encumber private keys, i.e., attach rich, multi-user policies to their use. In this way, it enables the cryptocurrency credentials and assets of a single end-user address to be freely rented, shared, or pooled. It accomplishes these things privately, with no direct on-chain traces.   Liquefaction demonstrates the sweeping consequences of TEE-based key encumbrance for the cryptocurrency landscape. Liquefaction can undermine the security and economic models of many applications and resources, such as locked tokens, DAO voting, airdrops, loyalty points, soulbound tokens, and quadratic voting. It can do so with no on-chain and minimal off-chain visibility. Conversely, we also discuss beneficial applications of Liquefaction, such as privacy-preserving, cost-efficient DAOs and a countermeasure to dusting attacks. Importantly, we describe an existing TEE-based tool that applications can use as a countermeasure to Liquefaction.   Our work prompts a wholesale rethinking of existing models and enforcement of key and asset ownership in the cryptocurrency ecosystem.","sentences":["Inherent in the world of cryptocurrency systems and their security models is the notion that private keys, and thus assets, are controlled by individuals or individual entities.   ","We present Liquefaction, a wallet platform that demonstrates the dangerous fragility of this foundational assumption by systemically breaking it.","Liquefaction uses trusted execution environments (TEEs) to encumber private keys, i.e., attach rich, multi-user policies to their use.","In this way, it enables the cryptocurrency credentials and assets of a single end-user address to be freely rented, shared, or pooled.","It accomplishes these things privately, with no direct on-chain traces.   ","Liquefaction demonstrates the sweeping consequences of TEE-based key encumbrance for the cryptocurrency landscape.","Liquefaction can undermine the security and economic models of many applications and resources, such as locked tokens, DAO voting, airdrops, loyalty points, soulbound tokens, and quadratic voting.","It can do so with no on-chain and minimal off-chain visibility.","Conversely, we also discuss beneficial applications of Liquefaction, such as privacy-preserving, cost-efficient DAOs and a countermeasure to dusting attacks.","Importantly, we describe an existing TEE-based tool that applications can use as a countermeasure to Liquefaction.   ","Our work prompts a wholesale rethinking of existing models and enforcement of key and asset ownership in the cryptocurrency ecosystem."],"url":"http://arxiv.org/abs/2412.02634v1"}
{"created":"2024-12-03 18:01:45","title":"Scaling Image Tokenizers with Grouped Spherical Quantization","abstract":"Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old-school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours. To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface. Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies. Building on this, we systematically examine the scaling behaviours of GSQ, specifically in latent dimensionality, codebook size, and compression ratios, and their impact on model performance. Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces. We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality. As a result, GSQ-GAN achieves a 16x down-sampling with a reconstruction FID (rFID) of 0.50.","sentences":["Vision tokenizers have gained a lot of attraction due to their scalability and compactness; previous works depend on old-school GAN-based hyperparameters, biased comparisons, and a lack of comprehensive analysis of the scaling behaviours.","To tackle those issues, we introduce Grouped Spherical Quantization (GSQ), featuring spherical codebook initialization and lookup regularization to constrain codebook latent to a spherical surface.","Our empirical analysis of image tokenizer training strategies demonstrates that GSQ-GAN achieves superior reconstruction quality over state-of-the-art methods with fewer training iterations, providing a solid foundation for scaling studies.","Building on this, we systematically examine the scaling behaviours of GSQ, specifically in latent dimensionality, codebook size, and compression ratios, and their impact on model performance.","Our findings reveal distinct behaviours at high and low spatial compression levels, underscoring challenges in representing high-dimensional latent spaces.","We show that GSQ can restructure high-dimensional latent into compact, low-dimensional spaces, thus enabling efficient scaling with improved quality.","As a result, GSQ-GAN achieves a 16x down-sampling with a reconstruction FID (rFID) of 0.50."],"url":"http://arxiv.org/abs/2412.02632v1"}
{"created":"2024-12-03 17:58:07","title":"Sharp-It: A Multi-view to Multi-view Diffusion Model for 3D Synthesis and Manipulation","abstract":"Advancements in text-to-image diffusion models have led to significant progress in fast 3D content creation. One common approach is to generate a set of multi-view images of an object, and then reconstruct it into a 3D model. However, this approach bypasses the use of a native 3D representation of the object and is hence prone to geometric artifacts and limited in controllability and manipulation capabilities. An alternative approach involves native 3D generative models that directly produce 3D representations. These models, however, are typically limited in their resolution, resulting in lower quality 3D objects. In this work, we bridge the quality gap between methods that directly generate 3D representations and ones that reconstruct 3D objects from multi-view images. We introduce a multi-view to multi-view diffusion model called Sharp-It, which takes a 3D consistent set of multi-view images rendered from a low-quality object and enriches its geometric details and texture. The diffusion model operates on the multi-view set in parallel, in the sense that it shares features across the generated views. A high-quality 3D model can then be reconstructed from the enriched multi-view set. By leveraging the advantages of both 2D and 3D approaches, our method offers an efficient and controllable method for high-quality 3D content creation. We demonstrate that Sharp-It enables various 3D applications, such as fast synthesis, editing, and controlled generation, while attaining high-quality assets.","sentences":["Advancements in text-to-image diffusion models have led to significant progress in fast 3D content creation.","One common approach is to generate a set of multi-view images of an object, and then reconstruct it into a 3D model.","However, this approach bypasses the use of a native 3D representation of the object and is hence prone to geometric artifacts and limited in controllability and manipulation capabilities.","An alternative approach involves native 3D generative models that directly produce 3D representations.","These models, however, are typically limited in their resolution, resulting in lower quality 3D objects.","In this work, we bridge the quality gap between methods that directly generate 3D representations and ones that reconstruct 3D objects from multi-view images.","We introduce a multi-view to multi-view diffusion model called Sharp-It, which takes a 3D consistent set of multi-view images rendered from a low-quality object and enriches its geometric details and texture.","The diffusion model operates on the multi-view set in parallel, in the sense that it shares features across the generated views.","A high-quality 3D model can then be reconstructed from the enriched multi-view set.","By leveraging the advantages of both 2D and 3D approaches, our method offers an efficient and controllable method for high-quality 3D content creation.","We demonstrate that Sharp-It enables various 3D applications, such as fast synthesis, editing, and controlled generation, while attaining high-quality assets."],"url":"http://arxiv.org/abs/2412.02631v1"}
{"created":"2024-12-03 17:56:23","title":"Continual Learning of Personalized Generative Face Models with Experience Replay","abstract":"We introduce a novel continual learning problem: how to sequentially update the weights of a personalized 2D and 3D generative face model as new batches of photos in different appearances, styles, poses, and lighting are captured regularly. We observe that naive sequential fine-tuning of the model leads to catastrophic forgetting of past representations of the individual's face. We then demonstrate that a simple random sampling-based experience replay method is effective at mitigating catastrophic forgetting when a relatively large number of images can be stored and replayed. However, for long-term deployment of these models with relatively smaller storage, this simple random sampling-based replay technique also forgets past representations. Thus, we introduce a novel experience replay algorithm that combines random sampling with StyleGAN's latent space to represent the buffer as an optimal convex hull. We observe that our proposed convex hull-based experience replay is more effective in preventing forgetting than a random sampling baseline and the lower bound.","sentences":["We introduce a novel continual learning problem: how to sequentially update the weights of a personalized 2D and 3D generative face model as new batches of photos in different appearances, styles, poses, and lighting are captured regularly.","We observe that naive sequential fine-tuning of the model leads to catastrophic forgetting of past representations of the individual's face.","We then demonstrate that a simple random sampling-based experience replay method is effective at mitigating catastrophic forgetting when a relatively large number of images can be stored and replayed.","However, for long-term deployment of these models with relatively smaller storage, this simple random sampling-based replay technique also forgets past representations.","Thus, we introduce a novel experience replay algorithm that combines random sampling with StyleGAN's latent space to represent the buffer as an optimal convex hull.","We observe that our proposed convex hull-based experience replay is more effective in preventing forgetting than a random sampling baseline and the lower bound."],"url":"http://arxiv.org/abs/2412.02627v1"}
{"created":"2024-12-03 17:54:12","title":"Time-Reversal Provides Unsupervised Feedback to LLMs","abstract":"Large Language Models (LLMs) are typically trained to predict in the forward direction of time. However, recent works have shown that prompting these models to look back and critique their own generations can produce useful feedback. Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs. Towards this, we introduce Time Reversed Language Models (TRLMs), which can score and generate queries when conditioned on responses, effectively functioning in the reverse direction of time. Further, to effectively infer in the response to query direction, we pre-train and fine-tune a language model (TRLM-Ba) in the reverse token order from scratch. We show empirically (and theoretically in a stylized setting) that time-reversed models can indeed complement forward model predictions when used to score the query given response for re-ranking multiple forward generations. We obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over the competent baseline of best-of-N re-ranking using self log-perplexity scores. We further show that TRLM scoring outperforms conventional forward scoring of response given query, resulting in significant gains in applications such as citation generation and passage retrieval. We next leverage the generative ability of TRLM to augment or provide unsupervised feedback to input safety filters of LLMs, demonstrating a drastic reduction in false negative rate with negligible impact on false positive rates against several attacks published on the popular JailbreakBench leaderboard.","sentences":["Large Language Models (LLMs) are typically trained to predict in the forward direction of time.","However, recent works have shown that prompting these models to look back and critique their own generations can produce useful feedback.","Motivated by this, we explore the question of whether LLMs can be empowered to think (predict and score) backwards to provide unsupervised feedback that complements forward LLMs.","Towards this, we introduce Time Reversed Language Models (TRLMs), which can score and generate queries when conditioned on responses, effectively functioning in the reverse direction of time.","Further, to effectively infer in the response to query direction, we pre-train and fine-tune a language model (TRLM-Ba) in the reverse token order from scratch.","We show empirically (and theoretically in a stylized setting) that time-reversed models can indeed complement forward model predictions when used to score the query given response for re-ranking multiple forward generations.","We obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over the competent baseline of best-of-N re-ranking using self log-perplexity scores.","We further show that TRLM scoring outperforms conventional forward scoring of response given query, resulting in significant gains in applications such as citation generation and passage retrieval.","We next leverage the generative ability of TRLM to augment or provide unsupervised feedback to input safety filters of LLMs, demonstrating a drastic reduction in false negative rate with negligible impact on false positive rates against several attacks published on the popular JailbreakBench leaderboard."],"url":"http://arxiv.org/abs/2412.02626v1"}
{"created":"2024-12-03 17:50:19","title":"Medical Multimodal Foundation Models in Clinical Diagnosis and Treatment: Applications, Challenges, and Future Directions","abstract":"Recent advancements in deep learning have significantly revolutionized the field of clinical diagnosis and treatment, offering novel approaches to improve diagnostic precision and treatment efficacy across diverse clinical domains, thus driving the pursuit of precision medicine. The growing availability of multi-organ and multimodal datasets has accelerated the development of large-scale Medical Multimodal Foundation Models (MMFMs). These models, known for their strong generalization capabilities and rich representational power, are increasingly being adapted to address a wide range of clinical tasks, from early diagnosis to personalized treatment strategies. This review offers a comprehensive analysis of recent developments in MMFMs, focusing on three key aspects: datasets, model architectures, and clinical applications. We also explore the challenges and opportunities in optimizing multimodal representations and discuss how these advancements are shaping the future of healthcare by enabling improved patient outcomes and more efficient clinical workflows.","sentences":["Recent advancements in deep learning have significantly revolutionized the field of clinical diagnosis and treatment, offering novel approaches to improve diagnostic precision and treatment efficacy across diverse clinical domains, thus driving the pursuit of precision medicine.","The growing availability of multi-organ and multimodal datasets has accelerated the development of large-scale Medical Multimodal Foundation Models (MMFMs).","These models, known for their strong generalization capabilities and rich representational power, are increasingly being adapted to address a wide range of clinical tasks, from early diagnosis to personalized treatment strategies.","This review offers a comprehensive analysis of recent developments in MMFMs, focusing on three key aspects: datasets, model architectures, and clinical applications.","We also explore the challenges and opportunities in optimizing multimodal representations and discuss how these advancements are shaping the future of healthcare by enabling improved patient outcomes and more efficient clinical workflows."],"url":"http://arxiv.org/abs/2412.02621v1"}
{"created":"2024-12-03 17:46:43","title":"Demonstrating the Advantages of Analog Wafer-Scale Neuromorphic Hardware","abstract":"As numerical simulations grow in size and complexity, they become increasingly resource-intensive in terms of time and energy. While specialized hardware accelerators often provide order-of-magnitude gains and are state of the art in other scientific fields, their availability and applicability in computational neuroscience is still limited. In this field, neuromorphic accelerators, particularly mixed-signal architectures like the BrainScaleS systems, offer the most significant performance benefits. These systems maintain a constant, accelerated emulation speed independent of network model and size. This is especially beneficial when traditional simulators reach their limits, such as when modeling complex neuron dynamics, incorporating plasticity mechanisms, or running long or repetitive experiments. However, the analog nature of these systems introduces new challenges. In this paper we demonstrate the capabilities and advantages of the BrainScaleS-1 system and how it can be used in combination with conventional software simulations. We report the emulation time and energy consumption for two biologically inspired networks adapted to the neuromorphic hardware substrate: a balanced random network based on Brunel and the cortical microcircuit from Potjans and Diesmann.","sentences":["As numerical simulations grow in size and complexity, they become increasingly resource-intensive in terms of time and energy.","While specialized hardware accelerators often provide order-of-magnitude gains and are state of the art in other scientific fields, their availability and applicability in computational neuroscience is still limited.","In this field, neuromorphic accelerators, particularly mixed-signal architectures like the BrainScaleS systems, offer the most significant performance benefits.","These systems maintain a constant, accelerated emulation speed independent of network model and size.","This is especially beneficial when traditional simulators reach their limits, such as when modeling complex neuron dynamics, incorporating plasticity mechanisms, or running long or repetitive experiments.","However, the analog nature of these systems introduces new challenges.","In this paper we demonstrate the capabilities and advantages of the BrainScaleS-1 system and how it can be used in combination with conventional software simulations.","We report the emulation time and energy consumption for two biologically inspired networks adapted to the neuromorphic hardware substrate: a balanced random network based on Brunel and the cortical microcircuit from Potjans and Diesmann."],"url":"http://arxiv.org/abs/2412.02619v1"}
{"created":"2024-12-03 17:44:58","title":"Nondeterministic tree-walking automata are not closed under complementation","abstract":"It is proved that the family of tree languages recognized by nondeterministic tree-walking automata is not closed under complementation, solving a problem raised by Boja\\'nczyk and Colcombet (\"Tree-walking automata do not recognize all regular languages\", SIAM J. Comp. 38 (2008) 658--701). In addition, it is shown that nondeterministic tree-walking automata are stronger than unambiguous tree-walking automata.","sentences":["It is proved that the family of tree languages recognized by nondeterministic tree-walking automata is not closed under complementation, solving a problem raised by Boja\\'nczyk and Colcombet (\"Tree-walking automata do not recognize all regular languages\", SIAM J. Comp.","38 (2008)","658--701).","In addition, it is shown that nondeterministic tree-walking automata are stronger than unambiguous tree-walking automata."],"url":"http://arxiv.org/abs/2412.02618v1"}
{"created":"2024-12-03 17:44:23","title":"Improving Dynamic Object Interactions in Text-to-Video Generation with AI Feedback","abstract":"Large text-to-video models hold immense potential for a wide range of downstream applications. However, these models struggle to accurately depict dynamic object interactions, often resulting in unrealistic movements and frequent violations of real-world physics. One solution inspired by large language models is to align generated outputs with desired outcomes using external feedback. This enables the model to refine its responses autonomously, eliminating extensive manual data collection. In this work, we investigate the use of feedback to enhance the object dynamics in text-to-video models. We aim to answer a critical question: what types of feedback, paired with which specific self-improvement algorithms, can most effectively improve text-video alignment and realistic object interactions? We begin by deriving a unified probabilistic objective for offline RL finetuning of text-to-video models. This perspective highlights how design elements in existing algorithms like KL regularization and policy projection emerge as specific choices within a unified framework. We then use derived methods to optimize a set of text-video alignment metrics (e.g., CLIP scores, optical flow), but notice that they often fail to align with human perceptions of generation quality. To address this limitation, we propose leveraging vision-language models to provide more nuanced feedback specifically tailored to object dynamics in videos. Our experiments demonstrate that our method can effectively optimize a wide variety of rewards, with binary AI feedback driving the most significant improvements in video quality for dynamic interactions, as confirmed by both AI and human evaluations. Notably, we observe substantial gains when using reward signals derived from AI feedback, particularly in scenarios involving complex interactions between multiple objects and realistic depictions of objects falling.","sentences":["Large text-to-video models hold immense potential for a wide range of downstream applications.","However, these models struggle to accurately depict dynamic object interactions, often resulting in unrealistic movements and frequent violations of real-world physics.","One solution inspired by large language models is to align generated outputs with desired outcomes using external feedback.","This enables the model to refine its responses autonomously, eliminating extensive manual data collection.","In this work, we investigate the use of feedback to enhance the object dynamics in text-to-video models.","We aim to answer a critical question: what types of feedback, paired with which specific self-improvement algorithms, can most effectively improve text-video alignment and realistic object interactions?","We begin by deriving a unified probabilistic objective for offline RL finetuning of text-to-video models.","This perspective highlights how design elements in existing algorithms like KL regularization and policy projection emerge as specific choices within a unified framework.","We then use derived methods to optimize a set of text-video alignment metrics (e.g., CLIP scores, optical flow), but notice that they often fail to align with human perceptions of generation quality.","To address this limitation, we propose leveraging vision-language models to provide more nuanced feedback specifically tailored to object dynamics in videos.","Our experiments demonstrate that our method can effectively optimize a wide variety of rewards, with binary AI feedback driving the most significant improvements in video quality for dynamic interactions, as confirmed by both AI and human evaluations.","Notably, we observe substantial gains when using reward signals derived from AI feedback, particularly in scenarios involving complex interactions between multiple objects and realistic depictions of objects falling."],"url":"http://arxiv.org/abs/2412.02617v1"}
{"created":"2024-12-03 17:43:28","title":"Projection Abstractions in Planning Under the Lenses of Abstractions for MDPs","abstract":"The concept of abstraction has been independently developed both in the context of AI Planning and discounted Markov Decision Processes (MDPs). However, the way abstractions are built and used in the context of Planning and MDPs is different even though lots of commonalities can be highlighted. To this day there is no work trying to relate and unify the two fields on the matter of abstractions unraveling all the different assumptions and their effect on the way they can be used. Therefore, in this paper we aim to do so by looking at projection abstractions in Planning through the lenses of discounted MDPs. Starting from a projection abstraction built according to Classical or Probabilistic Planning techniques, we will show how the same abstraction can be obtained under the abstraction frameworks available for discounted MDPs. Along the way, we will focus on computational as well as representational advantages and disadvantages of both worlds pointing out new research directions that are of interest for both fields.","sentences":["The concept of abstraction has been independently developed both in the context of AI Planning and discounted Markov Decision Processes (MDPs).","However, the way abstractions are built and used in the context of Planning and MDPs is different even though lots of commonalities can be highlighted.","To this day there is no work trying to relate and unify the two fields on the matter of abstractions unraveling all the different assumptions and their effect on the way they can be used.","Therefore, in this paper we aim to do so by looking at projection abstractions in Planning through the lenses of discounted MDPs.","Starting from a projection abstraction built according to Classical or Probabilistic Planning techniques, we will show how the same abstraction can be obtained under the abstraction frameworks available for discounted MDPs.","Along the way, we will focus on computational as well as representational advantages and disadvantages of both worlds pointing out new research directions that are of interest for both fields."],"url":"http://arxiv.org/abs/2412.02615v1"}
{"created":"2024-12-03 17:41:53","title":"Haptic Stiffness Perception Using Hand Exoskeletons in Tactile Robotic Telemanipulation","abstract":"Robotic telemanipulation - the human-guided manipulation of remote objects - plays a pivotal role in several applications, from healthcare to operations in harsh environments. While visual feedback from cameras can provide valuable information to the human operator, haptic feedback is essential for accessing specific object properties that are difficult to be perceived by vision, such as stiffness. For the first time, we present a participant study demonstrating that operators can perceive the stiffness of remote objects during real-world telemanipulation with a dexterous robotic hand, when haptic feedback is generated from tactile sensing fingertips. Participants were tasked with squeezing soft objects by teleoperating a robotic hand, using two methods of haptic feedback: one based solely on the measured contact force, while the second also includes the squeezing displacement between the leader and follower devices. Our results demonstrate that operators are indeed capable of discriminating objects of different stiffness, relying on haptic feedback alone and without any visual feedback. Additionally, our findings suggest that the displacement feedback component may enhance discrimination with objects of similar stiffness.","sentences":["Robotic telemanipulation - the human-guided manipulation of remote objects - plays a pivotal role in several applications, from healthcare to operations in harsh environments.","While visual feedback from cameras can provide valuable information to the human operator, haptic feedback is essential for accessing specific object properties that are difficult to be perceived by vision, such as stiffness.","For the first time, we present a participant study demonstrating that operators can perceive the stiffness of remote objects during real-world telemanipulation with a dexterous robotic hand, when haptic feedback is generated from tactile sensing fingertips.","Participants were tasked with squeezing soft objects by teleoperating a robotic hand, using two methods of haptic feedback: one based solely on the measured contact force, while the second also includes the squeezing displacement between the leader and follower devices.","Our results demonstrate that operators are indeed capable of discriminating objects of different stiffness, relying on haptic feedback alone and without any visual feedback.","Additionally, our findings suggest that the displacement feedback component may enhance discrimination with objects of similar stiffness."],"url":"http://arxiv.org/abs/2412.02613v1"}
{"created":"2024-12-03 17:41:24","title":"GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot","abstract":"We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b.","sentences":["We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot.","It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions.","GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder.","To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model.","We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering.","We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality.","The open models can be accessed through https://github.com/THUDM/GLM-4-Voice and https://huggingface.co/THUDM/glm-4-voice-9b."],"url":"http://arxiv.org/abs/2412.02612v1"}
{"created":"2024-12-03 17:41:23","title":"AV-Odyssey Bench: Can Your Multimodal LLMs Really Understand Audio-Visual Information?","abstract":"Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities. While these models demonstrate impressive performance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch. Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information. This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components. To successfully infer answers, models must effectively leverage clues from both visual and audio inputs. To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment. We benchmark a series of closed-source and open-source models and summarize the observations. By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development.","sentences":["Recently, multimodal large language models (MLLMs), such as GPT-4o, Gemini 1.5 Pro, and Reka Core, have expanded their capabilities to include vision and audio modalities.","While these models demonstrate impressive performance across a wide range of audio-visual applications, our proposed DeafTest reveals that MLLMs often struggle with simple tasks humans find trivial: 1) determining which of two sounds is louder, and 2) determining which of two sounds has a higher pitch.","Motivated by these observations, we introduce AV-Odyssey Bench, a comprehensive audio-visual benchmark designed to assess whether those MLLMs can truly understand the audio-visual information.","This benchmark encompasses 4,555 carefully crafted problems, each incorporating text, visual, and audio components.","To successfully infer answers, models must effectively leverage clues from both visual and audio inputs.","To ensure precise and objective evaluation of MLLM responses, we have structured the questions as multiple-choice, eliminating the need for human evaluation or LLM-assisted assessment.","We benchmark a series of closed-source and open-source models and summarize the observations.","By revealing the limitations of current models, we aim to provide useful insight for future dataset collection and model development."],"url":"http://arxiv.org/abs/2412.02611v1"}
{"created":"2024-12-03 17:41:08","title":"AI-Driven Resource Allocation Framework for Microservices in Hybrid Cloud Platforms","abstract":"The increasing demand for scalable, efficient resource management in hybrid cloud environments has led to the exploration of AI-driven approaches for dynamic resource allocation. This paper presents an AI-driven framework for resource allocation among microservices in hybrid cloud platforms. The framework employs reinforcement learning (RL)-based resource utilization optimization to reduce costs and improve performance. The framework integrates AI models with cloud management tools to respond to challenges of dynamic scaling and cost-efficient low-latency service delivery. The reinforcement learning model continuously adjusts provisioned resources as required by the microservices and predicts the future consumption trends to minimize both under- and over-provisioning of resources. Preliminary simulation results indicate that using AI in the provision of resources related to costs can reduce expenditure by up to 30-40% compared to manual provisioning and threshold-based auto-scaling approaches. It is also estimated that the efficiency in resource utilization is expected to improve by 20%-30% with a corresponding latency cut of 15%-20% during the peak demand periods. This study compares the AI-driven approach with existing static and rule-based resource allocation methods, demonstrating the capability of this new model to outperform them in terms of flexibility and real-time interests. The results indicate that reinforcement learning can make optimization of hybrid cloud platforms even better, offering a 25-35% improvement in cost efficiency and the power of scaling for microservice-based applications. The proposed framework is a strong and scalable solution to managing cloud resources in dynamic and performance-critical environments.","sentences":["The increasing demand for scalable, efficient resource management in hybrid cloud environments has led to the exploration of AI-driven approaches for dynamic resource allocation.","This paper presents an AI-driven framework for resource allocation among microservices in hybrid cloud platforms.","The framework employs reinforcement learning (RL)-based resource utilization optimization to reduce costs and improve performance.","The framework integrates AI models with cloud management tools to respond to challenges of dynamic scaling and cost-efficient low-latency service delivery.","The reinforcement learning model continuously adjusts provisioned resources as required by the microservices and predicts the future consumption trends to minimize both under- and over-provisioning of resources.","Preliminary simulation results indicate that using AI in the provision of resources related to costs can reduce expenditure by up to 30-40% compared to manual provisioning and threshold-based auto-scaling approaches.","It is also estimated that the efficiency in resource utilization is expected to improve by 20%-30% with a corresponding latency cut of 15%-20% during the peak demand periods.","This study compares the AI-driven approach with existing static and rule-based resource allocation methods, demonstrating the capability of this new model to outperform them in terms of flexibility and real-time interests.","The results indicate that reinforcement learning can make optimization of hybrid cloud platforms even better, offering a 25-35% improvement in cost efficiency and the power of scaling for microservice-based applications.","The proposed framework is a strong and scalable solution to managing cloud resources in dynamic and performance-critical environments."],"url":"http://arxiv.org/abs/2412.02610v1"}
{"created":"2024-12-03 17:40:26","title":"Wasserstein Markets for Differentially-Private Data","abstract":"Data is an increasingly vital component of decision making processes across industries. However, data access raises privacy concerns motivating the need for privacy-preserving techniques such as differential privacy. Data markets provide a means to enable wider access as well as determine the appropriate privacy-utility trade-off. Existing data market frameworks either require a trusted third party to perform computationally expensive valuations or are unable to capture the combinatorial nature of data value and do not endogenously model the effect of differential privacy. This paper addresses these shortcomings by proposing a valuation mechanism based on the Wasserstein distance for differentially-private data, and corresponding procurement mechanisms by leveraging incentive mechanism design theory, for task-agnostic data procurement, and task-specific procurement co-optimisation. The mechanisms are reformulated into tractable mixed-integer second-order cone programs, which are validated with numerical studies.","sentences":["Data is an increasingly vital component of decision making processes across industries.","However, data access raises privacy concerns motivating the need for privacy-preserving techniques such as differential privacy.","Data markets provide a means to enable wider access as well as determine the appropriate privacy-utility trade-off.","Existing data market frameworks either require a trusted third party to perform computationally expensive valuations or are unable to capture the combinatorial nature of data value and do not endogenously model the effect of differential privacy.","This paper addresses these shortcomings by proposing a valuation mechanism based on the Wasserstein distance for differentially-private data, and corresponding procurement mechanisms by leveraging incentive mechanism design theory, for task-agnostic data procurement, and task-specific procurement co-optimisation.","The mechanisms are reformulated into tractable mixed-integer second-order cone programs, which are validated with numerical studies."],"url":"http://arxiv.org/abs/2412.02609v1"}
{"created":"2024-12-03 17:34:50","title":"Interpretable Company Similarity with Sparse Autoencoders","abstract":"Determining company similarity is a vital task in finance, underpinning hedging, risk management, portfolio diversification, and more. Practitioners often rely on sector and industry classifications to gauge similarity, such as SIC-codes and GICS-codes, the former being used by the U.S. Securities and Exchange Commission (SEC), and the latter widely used by the investment community. Clustering embeddings of company descriptions has been proposed as a potential technique for determining company similarity, but the lack of interpretability in token embeddings poses a significant barrier to adoption in high-stakes contexts. Sparse Autoencoders have shown promise in enhancing the interpretability of Large Language Models by decomposing LLM activations into interpretable features. In this paper, we explore the use of SAE features in measuring company similarity and benchmark them against (1) SIC codes and (2) Major Group codes. We conclude that SAE features can reproduce and even surpass sector classifications in quantifying fundamental characteristics of companies, evaluated by the correlation of monthly returns, a proxy for similarity, and PnL from cointegration.","sentences":["Determining company similarity is a vital task in finance, underpinning hedging, risk management, portfolio diversification, and more.","Practitioners often rely on sector and industry classifications to gauge similarity, such as SIC-codes and GICS-codes, the former being used by the U.S. Securities and Exchange Commission (SEC), and the latter widely used by the investment community.","Clustering embeddings of company descriptions has been proposed as a potential technique for determining company similarity, but the lack of interpretability in token embeddings poses a significant barrier to adoption in high-stakes contexts.","Sparse Autoencoders have shown promise in enhancing the interpretability of Large Language Models by decomposing LLM activations into interpretable features.","In this paper, we explore the use of SAE features in measuring company similarity and benchmark them against (1) SIC codes and (2) Major Group codes.","We conclude that SAE features can reproduce and even surpass sector classifications in quantifying fundamental characteristics of companies, evaluated by the correlation of monthly returns, a proxy for similarity, and PnL from cointegration."],"url":"http://arxiv.org/abs/2412.02605v1"}
{"created":"2024-12-03 17:34:29","title":"Fairness-Regulated Dense Subgraph Discovery","abstract":"Dense subgraph discovery (DSD) is a key graph mining primitive with myriad applications including finding densely connected communities which are diverse in their vertex composition. In such a context, it is desirable to extract a dense subgraph that provides fair representation of the diverse subgroups that constitute the vertex set while incurring a small loss in terms of subgraph density. Existing methods for promoting fairness in DSD have important limitations -- the associated formulations are NP-hard in the worst case and they do not provide flexible notions of fairness, making it non-trivial to analyze the inherent trade-off between density and fairness. In this paper, we introduce two tractable formulations for fair DSD, each offering a different notion of fairness. Our methods provide a structured and flexible approach to incorporate fairness, accommodating varying fairness levels. We introduce the fairness-induced relative loss in subgraph density as a price of fairness measure to quantify the associated trade-off. We are the first to study such a notion in the context of detecting fair dense subgraphs. Extensive experiments on real-world datasets demonstrate that our methods not only match but frequently outperform existing solutions, sometimes incurring even less than half the subgraph density loss compared to prior art, while achieving the target fairness levels. Importantly, they excel in scenarios that previous methods fail to adequately handle, i.e., those with extreme subgroup imbalances, highlighting their effectiveness in extracting fair and dense solutions.","sentences":["Dense subgraph discovery (DSD) is a key graph mining primitive with myriad applications including finding densely connected communities which are diverse in their vertex composition.","In such a context, it is desirable to extract a dense subgraph that provides fair representation of the diverse subgroups that constitute the vertex set while incurring a small loss in terms of subgraph density.","Existing methods for promoting fairness in DSD have important limitations -- the associated formulations are NP-hard in the worst case and they do not provide flexible notions of fairness, making it non-trivial to analyze the inherent trade-off between density and fairness.","In this paper, we introduce two tractable formulations for fair DSD, each offering a different notion of fairness.","Our methods provide a structured and flexible approach to incorporate fairness, accommodating varying fairness levels.","We introduce the fairness-induced relative loss in subgraph density as a price of fairness measure to quantify the associated trade-off.","We are the first to study such a notion in the context of detecting fair dense subgraphs.","Extensive experiments on real-world datasets demonstrate that our methods not only match but frequently outperform existing solutions, sometimes incurring even less than half the subgraph density loss compared to prior art, while achieving the target fairness levels.","Importantly, they excel in scenarios that previous methods fail to adequately handle, i.e., those with extreme subgroup imbalances, highlighting their effectiveness in extracting fair and dense solutions."],"url":"http://arxiv.org/abs/2412.02604v1"}
{"created":"2024-12-03 17:32:47","title":"CEGI: Measuring the trade-off between efficiency and carbon emissions for SLMs and VLMs","abstract":"This paper analyzes the performance of Small Language Models (SLMs) and Vision Language Models (VLMs) and evaluates the trade-off between model performance and carbon emissions across 4 essential tasks: Image Captioning, Visual Question Answering (VQA), Dialogue Summarization and Text-to-SQL conversion. Various SLMs and VLMs belonging to the Qwen and LLaMA architecture family are chosen and variants based on model size in terms of the number of parameters, quantization level and fine-tuning parameters are evaluated. The model variant's performance and carbon emissions are calculated. To quantify the trade-off between model performance and carbon emissions, we introduce a novel metric called CEGI (Carbon Efficient Gain Index). This metric represents the carbon emission per unit percentage gain per million trainable parameters . This metric provides a normalized measure to compare model's efficiency in terms of performance improvement relative to their environmental cost. The experiment's outcome demonstrates that fine-tuning SLMs and VLMs can achieve performance levels comparable to Large Language Models (LLMs) while producing significantly less carbon emissions. Our findings suggest that the marginal gains in accuracy from larger models do not justify the substantial increase in carbon emissions. Leveraging lower-bit quantization levels, the proposed metric further enhances energy efficiency without compromising performance. This study highlights balancing high performance and environmental sustainability. It offers a valuable metric for selecting models suitable for environmentally-friendly AI development.","sentences":["This paper analyzes the performance of Small Language Models (SLMs) and Vision Language Models (VLMs) and evaluates the trade-off between model performance and carbon emissions across 4 essential tasks: Image Captioning, Visual Question Answering (VQA), Dialogue Summarization and Text-to-SQL conversion.","Various SLMs and VLMs belonging to the Qwen and LLaMA architecture family are chosen and variants based on model size in terms of the number of parameters, quantization level and fine-tuning parameters are evaluated.","The model variant's performance and carbon emissions are calculated.","To quantify the trade-off between model performance and carbon emissions, we introduce a novel metric called CEGI (Carbon Efficient Gain Index).","This metric represents the carbon emission per unit percentage gain per million trainable parameters .","This metric provides a normalized measure to compare model's efficiency in terms of performance improvement relative to their environmental cost.","The experiment's outcome demonstrates that fine-tuning SLMs and VLMs can achieve performance levels comparable to Large Language Models (LLMs) while producing significantly less carbon emissions.","Our findings suggest that the marginal gains in accuracy from larger models do not justify the substantial increase in carbon emissions.","Leveraging lower-bit quantization levels, the proposed metric further enhances energy efficiency without compromising performance.","This study highlights balancing high performance and environmental sustainability.","It offers a valuable metric for selecting models suitable for environmentally-friendly AI development."],"url":"http://arxiv.org/abs/2412.02602v1"}
{"created":"2024-12-03 17:32:05","title":"MERGE: Multi-faceted Hierarchical Graph-based GNN for Gene Expression Prediction from Whole Slide Histopathology Images","abstract":"Recent advances in Spatial Transcriptomics (ST) pair histology images with spatially resolved gene expression profiles, enabling predictions of gene expression across different tissue locations based on image patches. This opens up new possibilities for enhancing whole slide image (WSI) prediction tasks with localized gene expression. However, existing methods fail to fully leverage the interactions between different tissue locations, which are crucial for accurate joint prediction. To address this, we introduce MERGE (Multi-faceted hiErarchical gRaph for Gene Expressions), which combines a multi-faceted hierarchical graph construction strategy with graph neural networks (GNN) to improve gene expression predictions from WSIs. By clustering tissue image patches based on both spatial and morphological features, and incorporating intra- and inter-cluster edges, our approach fosters interactions between distant tissue locations during GNN learning. As an additional contribution, we evaluate different data smoothing techniques that are necessary to mitigate artifacts in ST data, often caused by technical imperfections. We advocate for adopting gene-aware smoothing methods that are more biologically justified. Experimental results on gene expression prediction show that our GNN method outperforms state-of-the-art techniques across multiple metrics.","sentences":["Recent advances in Spatial Transcriptomics (ST) pair histology images with spatially resolved gene expression profiles, enabling predictions of gene expression across different tissue locations based on image patches.","This opens up new possibilities for enhancing whole slide image (WSI) prediction tasks with localized gene expression.","However, existing methods fail to fully leverage the interactions between different tissue locations, which are crucial for accurate joint prediction.","To address this, we introduce MERGE (Multi-faceted hiErarchical gRaph for Gene Expressions), which combines a multi-faceted hierarchical graph construction strategy with graph neural networks (GNN) to improve gene expression predictions from WSIs.","By clustering tissue image patches based on both spatial and morphological features, and incorporating intra- and inter-cluster edges, our approach fosters interactions between distant tissue locations during GNN learning.","As an additional contribution, we evaluate different data smoothing techniques that are necessary to mitigate artifacts in ST data, often caused by technical imperfections.","We advocate for adopting gene-aware smoothing methods that are more biologically justified.","Experimental results on gene expression prediction show that our GNN method outperforms state-of-the-art techniques across multiple metrics."],"url":"http://arxiv.org/abs/2412.02601v1"}
{"created":"2024-12-03 17:29:00","title":"Class-wise Autoencoders Measure Classification Difficulty And Detect Label Mistakes","abstract":"We introduce a new framework for analyzing classification datasets based on the ratios of reconstruction errors between autoencoders trained on individual classes. This analysis framework enables efficient characterization of datasets on the sample, class, and entire dataset levels. We define reconstruction error ratios (RERs) that probe classification difficulty and allow its decomposition into (1) finite sample size and (2) Bayes error and decision-boundary complexity. Through systematic study across 19 popular visual datasets, we find that our RER-based dataset difficulty probe strongly correlates with error rate for state-of-the-art (SOTA) classification models. By interpreting sample-level classification difficulty as a label mistakenness score, we further find that RERs achieve SOTA performance on mislabel detection tasks on hard datasets under symmetric and asymmetric label noise. Our code is publicly available at https://github.com/voxel51/reconstruction-error-ratios.","sentences":["We introduce a new framework for analyzing classification datasets based on the ratios of reconstruction errors between autoencoders trained on individual classes.","This analysis framework enables efficient characterization of datasets on the sample, class, and entire dataset levels.","We define reconstruction error ratios (RERs) that probe classification difficulty and allow its decomposition into (1) finite sample size and (2) Bayes error and decision-boundary complexity.","Through systematic study across 19 popular visual datasets, we find that our RER-based dataset difficulty probe strongly correlates with error rate for state-of-the-art (SOTA) classification models.","By interpreting sample-level classification difficulty as a label mistakenness score, we further find that RERs achieve SOTA performance on mislabel detection tasks on hard datasets under symmetric and asymmetric label noise.","Our code is publicly available at https://github.com/voxel51/reconstruction-error-ratios."],"url":"http://arxiv.org/abs/2412.02596v1"}
{"created":"2024-12-03 17:28:50","title":"Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset","abstract":"Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved significant benchmark gains via aggressive model-based filtering, but at the cost of removing 90% of data. This limits their suitability for long token horizon training, such as 15T tokens for Llama 3.1. In this paper, we show how to achieve better trade-offs between accuracy and data quantity by a combination of classifier ensembling, synthetic data rephrasing, and reduced reliance on heuristic filters. When training 8B parameter models for 1T tokens, using a high-quality subset of our data improves MMLU by 5.6 over DCLM, demonstrating the efficacy of our methods for boosting accuracies over a relatively short token horizon. Furthermore, our full 6.3T token dataset matches DCLM on MMLU, but contains four times more unique real tokens than DCLM. This unlocks state-of-the-art training over a long token horizon: an 8B parameter model trained for 15T tokens, of which 7.2T came from our dataset, is better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5 on average across ten diverse tasks. The dataset is available at https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html","sentences":["Recent English Common Crawl datasets like FineWeb-Edu and DCLM achieved significant benchmark gains via aggressive model-based filtering, but at the cost of removing 90% of data.","This limits their suitability for long token horizon training, such as 15T tokens for Llama 3.1.","In this paper, we show how to achieve better trade-offs between accuracy and data quantity by a combination of classifier ensembling, synthetic data rephrasing, and reduced reliance on heuristic filters.","When training 8B parameter models for 1T tokens, using a high-quality subset of our data improves MMLU by 5.6 over DCLM, demonstrating the efficacy of our methods for boosting accuracies over a relatively short token horizon.","Furthermore, our full 6.3T token dataset matches DCLM on MMLU, but contains four times more unique real tokens than DCLM.","This unlocks state-of-the-art training over a long token horizon: an 8B parameter model trained for 15T tokens, of which 7.2T came from our dataset, is better than the Llama 3.1 8B model: +5 on MMLU, +3.1 on ARC-Challenge, and +0.5 on average across ten diverse tasks.","The dataset is available at https://data.commoncrawl.org/contrib/Nemotron/Nemotron-CC/index.html"],"url":"http://arxiv.org/abs/2412.02595v1"}
{"created":"2024-12-03 17:26:42","title":"PrefixLLM: LLM-aided Prefix Circuit Design","abstract":"Prefix circuits are fundamental components in digital adders, widely used in digital systems due to their efficiency in calculating carry signals. Synthesizing prefix circuits with minimized area and delay is crucial for enhancing the performance of modern computing systems. Recently, large language models (LLMs) have demonstrated a surprising ability to perform text generation tasks. We propose PrefixLLM, that leverages LLMs for prefix circuit synthesis. PrefixLLM transforms the prefix circuit synthesis task into a structured text generation problem, termed the Structured Prefix Circuit Representation (SPCR), and introduces an iterative framework to automatically and accurately generate valid SPCRs. We further present a design space exploration (DSE) framework that uses LLMs to iteratively search for area and delay optimized prefix circuits. Compared to state-of-the-art, PrefixLLM can reduce the area by 3.70% under the same delay constraint. This work highlights the use of LLMs in the synthesis of arithmetic circuits, which can be transformed into the structured text generation.","sentences":["Prefix circuits are fundamental components in digital adders, widely used in digital systems due to their efficiency in calculating carry signals.","Synthesizing prefix circuits with minimized area and delay is crucial for enhancing the performance of modern computing systems.","Recently, large language models (LLMs) have demonstrated a surprising ability to perform text generation tasks.","We propose PrefixLLM, that leverages LLMs for prefix circuit synthesis.","PrefixLLM transforms the prefix circuit synthesis task into a structured text generation problem, termed the Structured Prefix Circuit Representation (SPCR), and introduces an iterative framework to automatically and accurately generate valid SPCRs.","We further present a design space exploration (DSE) framework that uses LLMs to iteratively search for area and delay optimized prefix circuits.","Compared to state-of-the-art, PrefixLLM can reduce the area by 3.70% under the same delay constraint.","This work highlights the use of LLMs in the synthesis of arithmetic circuits, which can be transformed into the structured text generation."],"url":"http://arxiv.org/abs/2412.02594v1"}
{"created":"2024-12-03 17:23:47","title":"OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation","abstract":"Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge to reduce hallucinations and incorporate up-to-date information without retraining. As an essential part of RAG, external knowledge bases are commonly built by extracting structured data from unstructured PDF documents using Optical Character Recognition (OCR). However, given the imperfect prediction of OCR and the inherent non-uniform representation of structured data, knowledge bases inevitably contain various OCR noises. In this paper, we introduce OHRBench, the first benchmark for understanding the cascading impact of OCR on RAG systems. OHRBench includes 350 carefully selected unstructured PDF documents from six real-world RAG application domains, along with Q&As derived from multimodal elements in documents, challenging existing OCR solutions used for RAG To better understand OCR's impact on RAG systems, we identify two primary types of OCR noise: Semantic Noise and Formatting Noise and apply perturbation to generate a set of structured data with varying degrees of each OCR noise. Using OHRBench, we first conduct a comprehensive evaluation of current OCR solutions and reveal that none is competent for constructing high-quality knowledge bases for RAG systems. We then systematically evaluate the impact of these two noise types and demonstrate the vulnerability of RAG systems. Furthermore, we discuss the potential of employing Vision-Language Models (VLMs) without OCR in RAG systems. Code: https://github.com/opendatalab/OHR-Bench","sentences":["Retrieval-augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge to reduce hallucinations and incorporate up-to-date information without retraining.","As an essential part of RAG, external knowledge bases are commonly built by extracting structured data from unstructured PDF documents using Optical Character Recognition (OCR).","However, given the imperfect prediction of OCR and the inherent non-uniform representation of structured data, knowledge bases inevitably contain various OCR noises.","In this paper, we introduce OHRBench, the first benchmark for understanding the cascading impact of OCR on RAG systems.","OHRBench includes 350 carefully selected unstructured PDF documents from six real-world RAG application domains, along with Q&As derived from multimodal elements in documents, challenging existing OCR solutions used for RAG To better understand OCR's impact on RAG systems, we identify two primary types of OCR noise: Semantic Noise and Formatting Noise and apply perturbation to generate a set of structured data with varying degrees of each OCR noise.","Using OHRBench, we first conduct a comprehensive evaluation of current OCR solutions and reveal that none is competent for constructing high-quality knowledge bases for RAG systems.","We then systematically evaluate the impact of these two noise types and demonstrate the vulnerability of RAG systems.","Furthermore, we discuss the potential of employing Vision-Language Models (VLMs) without OCR in RAG systems.","Code: https://github.com/opendatalab/OHR-Bench"],"url":"http://arxiv.org/abs/2412.02592v1"}
{"created":"2024-12-03 17:18:33","title":"MedTet: An Online Motion Model for 4D Heart Reconstruction","abstract":"We present a novel approach to reconstruction of 3D cardiac motion from sparse intraoperative data. While existing methods can accurately reconstruct 3D organ geometries from full 3D volumetric imaging, they cannot be used during surgical interventions where usually limited observed data, such as a few 2D frames or 1D signals, is available in real-time. We propose a versatile framework for reconstructing 3D motion from such partial data. It discretizes the 3D space into a deformable tetrahedral grid with signed distance values, providing implicit unlimited resolution while maintaining explicit control over motion dynamics. Given an initial 3D model reconstructed from pre-operative full volumetric data, our system, equipped with an universal observation encoder, can reconstruct coherent 3D cardiac motion from full 3D volumes, a few 2D MRI slices or even 1D signals. Extensive experiments on cardiac intervention scenarios demonstrate our ability to generate plausible and anatomically consistent 3D motion reconstructions from various sparse real-time observations, highlighting its potential for multimodal cardiac imaging. Our code and model will be made available at https://github.com/Scalsol/MedTet.","sentences":["We present a novel approach to reconstruction of 3D cardiac motion from sparse intraoperative data.","While existing methods can accurately reconstruct 3D organ geometries from full 3D volumetric imaging, they cannot be used during surgical interventions where usually limited observed data, such as a few 2D frames or 1D signals, is available in real-time.","We propose a versatile framework for reconstructing 3D motion from such partial data.","It discretizes the 3D space into a deformable tetrahedral grid with signed distance values, providing implicit unlimited resolution while maintaining explicit control over motion dynamics.","Given an initial 3D model reconstructed from pre-operative full volumetric data, our system, equipped with an universal observation encoder, can reconstruct coherent 3D cardiac motion from full 3D volumes, a few 2D MRI slices or even 1D signals.","Extensive experiments on cardiac intervention scenarios demonstrate our ability to generate plausible and anatomically consistent 3D motion reconstructions from various sparse real-time observations, highlighting its potential for multimodal cardiac imaging.","Our code and model will be made available at https://github.com/Scalsol/MedTet."],"url":"http://arxiv.org/abs/2412.02589v1"}
{"created":"2024-12-03 17:17:27","title":"Explainable CTR Prediction via LLM Reasoning","abstract":"Recommendation Systems have become integral to modern user experiences, but lack transparency in their decision-making processes. Existing explainable recommendation methods are hindered by reliance on a post-hoc paradigm, wherein explanation generators are trained independently of the underlying recommender models. This paradigm necessitates substantial human effort in data construction and raises concerns about explanation reliability. In this paper, we present ExpCTR, a novel framework that integrates large language model based explanation generation directly into the CTR prediction process. Inspired by recent advances in reinforcement learning, we employ two carefully designed reward mechanisms, LC alignment, which ensures explanations reflect user intentions, and IC alignment, which maintains consistency with traditional ID-based CTR models. Our approach incorporates an efficient training paradigm with LoRA and a three-stage iterative process. ExpCTR circumvents the need for extensive explanation datasets while fostering synergy between CTR prediction and explanation generation. Experimental results demonstrate that ExpCTR significantly enhances both recommendation accuracy and interpretability across three real-world datasets.","sentences":["Recommendation Systems have become integral to modern user experiences, but lack transparency in their decision-making processes.","Existing explainable recommendation methods are hindered by reliance on a post-hoc paradigm, wherein explanation generators are trained independently of the underlying recommender models.","This paradigm necessitates substantial human effort in data construction and raises concerns about explanation reliability.","In this paper, we present ExpCTR, a novel framework that integrates large language model based explanation generation directly into the CTR prediction process.","Inspired by recent advances in reinforcement learning, we employ two carefully designed reward mechanisms, LC alignment, which ensures explanations reflect user intentions, and IC alignment, which maintains consistency with traditional ID-based CTR models.","Our approach incorporates an efficient training paradigm with LoRA and a three-stage iterative process.","ExpCTR circumvents the need for extensive explanation datasets while fostering synergy between CTR prediction and explanation generation.","Experimental results demonstrate that ExpCTR significantly enhances both recommendation accuracy and interpretability across three real-world datasets."],"url":"http://arxiv.org/abs/2412.02588v1"}
{"created":"2024-12-03 17:11:35","title":"Atlantis Protocol","abstract":"This document proposes a combination of several techniques to construct anonymous and untraceable payment systems. The proposed system supports arbitrary transfer amounts and enables the simultaneous transfer of multiple assets.","sentences":["This document proposes a combination of several techniques to construct anonymous and untraceable payment systems.","The proposed system supports arbitrary transfer amounts and enables the simultaneous transfer of multiple assets."],"url":"http://arxiv.org/abs/2412.02585v1"}
{"created":"2024-12-03 17:09:13","title":"Mobile Cell-Free Massive MIMO with Multi-Agent Reinforcement Learning: A Scalable Framework","abstract":"Cell-free massive multiple-input multiple-output (mMIMO) offers significant advantages in mobility scenarios, mainly due to the elimination of cell boundaries and strong macro diversity. In this paper, we examine the downlink performance of cell-free mMIMO systems equipped with mobile-APs utilizing the concept of unmanned aerial vehicles, where mobility and power control are jointly considered to effectively enhance coverage and suppress interference. However, the high computational complexity, poor collaboration, limited scalability, and uneven reward distribution of conventional optimization schemes lead to serious performance degradation and instability. These factors complicate the provision of consistent and high-quality service across all user equipments in downlink cell-free mMIMO systems. Consequently, we propose a novel scalable framework enhanced by multi-agent reinforcement learning (MARL) to tackle these challenges. The established framework incorporates a graph neural network (GNN)-aided communication mechanism to facilitate effective collaboration among agents, a permutation architecture to improve scalability, and a directional decoupling architecture to accurately distinguish contributions. In the numerical results, we present comparisons of different optimization schemes and network architectures, which reveal that the proposed scheme can effectively enhance system performance compared to conventional schemes due to the adoption of advanced technologies. In particular, appropriately compressing the observation space of agents is beneficial for achieving a better balance between performance and convergence.","sentences":["Cell-free massive multiple-input multiple-output (mMIMO) offers significant advantages in mobility scenarios, mainly due to the elimination of cell boundaries and strong macro diversity.","In this paper, we examine the downlink performance of cell-free mMIMO systems equipped with mobile-APs utilizing the concept of unmanned aerial vehicles, where mobility and power control are jointly considered to effectively enhance coverage and suppress interference.","However, the high computational complexity, poor collaboration, limited scalability, and uneven reward distribution of conventional optimization schemes lead to serious performance degradation and instability.","These factors complicate the provision of consistent and high-quality service across all user equipments in downlink cell-free mMIMO systems.","Consequently, we propose a novel scalable framework enhanced by multi-agent reinforcement learning (MARL) to tackle these challenges.","The established framework incorporates a graph neural network (GNN)-aided communication mechanism to facilitate effective collaboration among agents, a permutation architecture to improve scalability, and a directional decoupling architecture to accurately distinguish contributions.","In the numerical results, we present comparisons of different optimization schemes and network architectures, which reveal that the proposed scheme can effectively enhance system performance compared to conventional schemes due to the adoption of advanced technologies.","In particular, appropriately compressing the observation space of agents is beneficial for achieving a better balance between performance and convergence."],"url":"http://arxiv.org/abs/2412.02581v1"}
{"created":"2024-12-03 17:04:35","title":"The Two-Center Problem of Uncertain Points on Trees","abstract":"In this paper, we consider the (weighted) two-center problem of uncertain points on a tree. Given are a tree $T$ and a set $\\calP$ of $n$ (weighted) uncertain points each of which has $m$ possible locations on $T$ associated with probabilities. The goal is to compute two points on $T$, i.e., two centers with respect to $\\calP$, so that the maximum (weighted) expected distance of $n$ uncertain points to their own expected closest center is minimized. This problem can be solved in $O(|T|+ n^{2}\\log n\\log mn + mn\\log^2 mn \\log n)$ time by the algorithm for the general $k$-center problem. In this paper, we give a more efficient and simple algorithm that solves this problem in $O(|T| + mn\\log mn)$ time.","sentences":["In this paper, we consider the (weighted) two-center problem of uncertain points on a tree.","Given are a tree $T$ and a set $\\calP$ of $n$ (weighted) uncertain points each of which has $m$ possible locations on $T$ associated with probabilities.","The goal is to compute two points on $T$, i.e., two centers with respect to $\\calP$, so that the maximum (weighted) expected distance of $n$ uncertain points to their own expected closest center is minimized.","This problem can be solved in $O(|T|+ n^{2}\\log n\\log mn","+ mn\\log^2 mn \\log n)$ time by the algorithm for the general $k$-center problem.","In this paper, we give a more efficient and simple algorithm that solves this problem in $O(|T| + mn\\log mn)$ time."],"url":"http://arxiv.org/abs/2412.02580v1"}
{"created":"2024-12-03 17:04:20","title":"Factored space models: Towards causality between levels of abstraction","abstract":"Causality plays an important role in understanding intelligent behavior, and there is a wealth of literature on mathematical models for causality, most of which is focused on causal graphs. Causal graphs are a powerful tool for a wide range of applications, in particular when the relevant variables are known and at the same level of abstraction. However, the given variables can also be unstructured data, like pixels of an image. Meanwhile, the causal variables, such as the positions of objects in the image, can be arbitrary deterministic functions of the given variables. Moreover, the causal variables may form a hierarchy of abstractions, in which the macro-level variables are deterministic functions of the micro-level variables. Causal graphs are limited when it comes to modeling this kind of situation. In the presence of deterministic relationships there is generally no causal graph that satisfies both the Markov condition and the faithfulness condition. We introduce factored space models as an alternative to causal graphs which naturally represent both probabilistic and deterministic relationships at all levels of abstraction. Moreover, we introduce structural independence and establish that it is equivalent to statistical independence in every distribution that factorizes over the factored space. This theorem generalizes the classical soundness and completeness theorem for d-separation.","sentences":["Causality plays an important role in understanding intelligent behavior, and there is a wealth of literature on mathematical models for causality, most of which is focused on causal graphs.","Causal graphs are a powerful tool for a wide range of applications, in particular when the relevant variables are known and at the same level of abstraction.","However, the given variables can also be unstructured data, like pixels of an image.","Meanwhile, the causal variables, such as the positions of objects in the image, can be arbitrary deterministic functions of the given variables.","Moreover, the causal variables may form a hierarchy of abstractions, in which the macro-level variables are deterministic functions of the micro-level variables.","Causal graphs are limited when it comes to modeling this kind of situation.","In the presence of deterministic relationships there is generally no causal graph that satisfies both the Markov condition and the faithfulness condition.","We introduce factored space models as an alternative to causal graphs which naturally represent both probabilistic and deterministic relationships at all levels of abstraction.","Moreover, we introduce structural independence and establish that it is equivalent to statistical independence in every distribution that factorizes over the factored space.","This theorem generalizes the classical soundness and completeness theorem for d-separation."],"url":"http://arxiv.org/abs/2412.02579v1"}
{"created":"2024-12-03 17:04:14","title":"Private Linear Regression with Differential Privacy and PAC Privacy","abstract":"Linear regression is a fundamental tool for statistical analysis, which has motivated the development of linear regression methods that satisfy provable privacy guarantees so that the learned model reveals little about any one data point used to construct it. Most existing privacy-preserving linear regression methods rely on the well-established framework of differential privacy, while the newly proposed PAC Privacy has not yet been explored in this context. In this paper, we systematically compare linear regression models trained with differential privacy and PAC privacy across three real-world datasets, observing several key findings that impact the performance of privacy-preserving linear regression.","sentences":["Linear regression is a fundamental tool for statistical analysis, which has motivated the development of linear regression methods that satisfy provable privacy guarantees so that the learned model reveals little about any one data point used to construct it.","Most existing privacy-preserving linear regression methods rely on the well-established framework of differential privacy, while the newly proposed PAC Privacy has not yet been explored in this context.","In this paper, we systematically compare linear regression models trained with differential privacy and PAC privacy across three real-world datasets, observing several key findings that impact the performance of privacy-preserving linear regression."],"url":"http://arxiv.org/abs/2412.02578v1"}
{"created":"2024-12-03 17:02:49","title":"The Efficacy of Transfer-based No-box Attacks on Image Watermarking: A Pragmatic Analysis","abstract":"Watermarking approaches are widely used to identify if images being circulated are authentic or AI-generated. Determining the robustness of image watermarking methods in the ``no-box'' setting, where the attacker is assumed to have no knowledge about the watermarking model, is an interesting problem. Our main finding is that evading the no-box setting is challenging: the success of optimization-based transfer attacks (involving training surrogate models) proposed in prior work~\\cite{hu2024transfer} depends on impractical assumptions, including (i) aligning the architecture and training configurations of both the victim and attacker's surrogate watermarking models, as well as (ii) a large number of surrogate models with potentially large computational requirements. Relaxing these assumptions i.e., moving to a more pragmatic threat model results in a failed attack, with an evasion rate at most $21.1\\%$. We show that when the configuration is mostly aligned, a simple non-optimization attack we propose, OFT, with one single surrogate model can already exceed the success of optimization-based efforts. Under the same $\\ell_\\infty$ norm perturbation budget of $0.25$, prior work~\\citet{hu2024transfer} is comparable to or worse than OFT in $11$ out of $12$ configurations and has a limited advantage on the remaining one. The code used for all our experiments is available at \\url{https://github.com/Ardor-Wu/transfer}.","sentences":["Watermarking approaches are widely used to identify if images being circulated are authentic or AI-generated.","Determining the robustness of image watermarking methods in the ``no-box'' setting, where the attacker is assumed to have no knowledge about the watermarking model, is an interesting problem.","Our main finding is that evading the no-box setting is challenging: the success of optimization-based transfer attacks (involving training surrogate models) proposed in prior work~\\cite{hu2024transfer} depends on impractical assumptions, including (i) aligning the architecture and training configurations of both the victim and attacker's surrogate watermarking models, as well as (ii) a large number of surrogate models with potentially large computational requirements.","Relaxing these assumptions i.e., moving to a more pragmatic threat model results in a failed attack, with an evasion rate at most $21.1\\%$. We show that when the configuration is mostly aligned, a simple non-optimization attack we propose, OFT, with one single surrogate model can already exceed the success of optimization-based efforts.","Under the same $\\ell_\\infty$ norm perturbation budget of $0.25$, prior work~\\citet{hu2024transfer} is comparable to or worse than OFT in $11$ out of $12$ configurations and has a limited advantage on the remaining one.","The code used for all our experiments is available at \\url{https://github.com/Ardor-Wu/transfer}."],"url":"http://arxiv.org/abs/2412.02576v1"}
{"created":"2024-12-03 17:02:40","title":"Copy-Move Forgery Detection and Question Answering for Remote Sensing Image","abstract":"This paper introduces the task of Remote Sensing Copy-Move Question Answering (RSCMQA). Unlike traditional Remote Sensing Visual Question Answering (RSVQA), RSCMQA focuses on interpreting complex tampering scenarios and inferring relationships between objects. Based on the practical needs of national defense security and land resource monitoring, we have developed an accurate and comprehensive global dataset for remote sensing image copy-move question answering, named RS-CMQA-2.1M. These images were collected from 29 different regions across 14 countries. Additionally, we have refined a balanced dataset, RS-CMQA-B, to address the long-standing issue of long-tail data in the remote sensing field. Furthermore, we propose a region-discriminative guided multimodal CMQA model, which enhances the accuracy of answering questions about tampered images by leveraging prompt about the differences and connections between the source and tampered domains. Extensive experiments demonstrate that our method provides a stronger benchmark for RS-CMQA compared to general VQA and RSVQA models. Our dataset and code are available at https://github.com/shenyedepisa/RSCMQA.","sentences":["This paper introduces the task of Remote Sensing Copy-Move Question Answering (RSCMQA).","Unlike traditional Remote Sensing Visual Question Answering (RSVQA), RSCMQA focuses on interpreting complex tampering scenarios and inferring relationships between objects.","Based on the practical needs of national defense security and land resource monitoring, we have developed an accurate and comprehensive global dataset for remote sensing image copy-move question answering, named RS-CMQA-2.1M.","These images were collected from 29 different regions across 14 countries.","Additionally, we have refined a balanced dataset, RS-CMQA-B, to address the long-standing issue of long-tail data in the remote sensing field.","Furthermore, we propose a region-discriminative guided multimodal CMQA model, which enhances the accuracy of answering questions about tampered images by leveraging prompt about the differences and connections between the source and tampered domains.","Extensive experiments demonstrate that our method provides a stronger benchmark for RS-CMQA compared to general VQA and RSVQA models.","Our dataset and code are available at https://github.com/shenyedepisa/RSCMQA."],"url":"http://arxiv.org/abs/2412.02575v1"}
{"created":"2024-12-03 16:59:30","title":"Generating Critical Scenarios for Testing Automated Driving Systems","abstract":"Autonomous vehicles (AVs) have demonstrated significant potential in revolutionizing transportation, yet ensuring their safety and reliability remains a critical challenge, especially when exposed to dynamic and unpredictable environments. Real-world testing of an Autonomous Driving System (ADS) is both expensive and risky, making simulation-based testing a preferred approach. In this paper, we propose AVASTRA, a Reinforcement Learning (RL)-based approach to generate realistic critical scenarios for testing ADSs in simulation environments. To capture the complexity of driving scenarios, AVASTRA comprehensively represents the environment by both the internal states of an ADS under-test (e.g., the status of the ADS's core components, speed, or acceleration) and the external states of the surrounding factors in the simulation environment (e.g., weather, traffic flow, or road condition). AVASTRA trains the RL agent to effectively configure the simulation environment that places the AV in dangerous situations and potentially leads it to collisions. We introduce a diverse set of actions that allows the RL agent to systematically configure both environmental conditions and traffic participants. Additionally, based on established safety requirements, we enforce heuristic constraints to ensure the realism and relevance of the generated test scenarios. AVASTRA is evaluated on two popular simulation maps with four different road configurations. Our results show AVASTRA's ability to outperform the state-of-the-art approach by generating 30% to 115% more collision scenarios. Compared to the baseline based on Random Search, AVASTRA achieves up to 275% better performance. These results highlight the effectiveness of AVASTRA in enhancing the safety testing of AVs through realistic comprehensive critical scenario generation.","sentences":["Autonomous vehicles (AVs) have demonstrated significant potential in revolutionizing transportation, yet ensuring their safety and reliability remains a critical challenge, especially when exposed to dynamic and unpredictable environments.","Real-world testing of an Autonomous Driving System (ADS) is both expensive and risky, making simulation-based testing a preferred approach.","In this paper, we propose AVASTRA, a Reinforcement Learning (RL)-based approach to generate realistic critical scenarios for testing ADSs in simulation environments.","To capture the complexity of driving scenarios, AVASTRA comprehensively represents the environment by both the internal states of an ADS under-test (e.g., the status of the ADS's core components, speed, or acceleration) and the external states of the surrounding factors in the simulation environment (e.g., weather, traffic flow, or road condition).","AVASTRA trains the RL agent to effectively configure the simulation environment that places the AV in dangerous situations and potentially leads it to collisions.","We introduce a diverse set of actions that allows the RL agent to systematically configure both environmental conditions and traffic participants.","Additionally, based on established safety requirements, we enforce heuristic constraints to ensure the realism and relevance of the generated test scenarios.","AVASTRA is evaluated on two popular simulation maps with four different road configurations.","Our results show AVASTRA's ability to outperform the state-of-the-art approach by generating 30% to 115% more collision scenarios.","Compared to the baseline based on Random Search, AVASTRA achieves up to 275% better performance.","These results highlight the effectiveness of AVASTRA in enhancing the safety testing of AVs through realistic comprehensive critical scenario generation."],"url":"http://arxiv.org/abs/2412.02574v1"}
{"created":"2024-12-03 16:56:10","title":"Remote Sensing Temporal Vision-Language Models: A Comprehensive Survey","abstract":"Temporal image analysis in remote sensing has traditionally centered on change detection, which identifies regions of change between images captured at different times. However, change detection remains limited by its focus on visual-level interpretation, often lacking contextual or descriptive information. The rise of Vision-Language Models (VLMs) has introduced a new dimension to remote sensing temporal image analysis by integrating visual information with natural language, creating an avenue for advanced interpretation of temporal image changes. Remote Sensing Temporal VLMs (RSTVLMs) allow for dynamic interactions, generating descriptive captions, answering questions, and providing a richer semantic understanding of temporal images. This temporal vision-language capability is particularly valuable for complex remote sensing applications, where higher-level insights are crucial. This paper comprehensively reviews the progress of RSTVLM research, with a focus on the latest VLM applications for temporal image analysis. We categorize and discuss core methodologies, datasets, and metrics, highlight recent advances in temporal vision-language tasks, and outline key challenges and future directions for research in this emerging field. This survey fills a critical gap in the literature by providing an integrated overview of RSTVLM, offering a foundation for further advancements in remote sensing temporal image understanding. We will keep tracing related works at \\url{https://github.com/Chen-Yang-Liu/Awesome-RS-Temporal-VLM}","sentences":["Temporal image analysis in remote sensing has traditionally centered on change detection, which identifies regions of change between images captured at different times.","However, change detection remains limited by its focus on visual-level interpretation, often lacking contextual or descriptive information.","The rise of Vision-Language Models (VLMs) has introduced a new dimension to remote sensing temporal image analysis by integrating visual information with natural language, creating an avenue for advanced interpretation of temporal image changes.","Remote Sensing Temporal VLMs (RSTVLMs) allow for dynamic interactions, generating descriptive captions, answering questions, and providing a richer semantic understanding of temporal images.","This temporal vision-language capability is particularly valuable for complex remote sensing applications, where higher-level insights are crucial.","This paper comprehensively reviews the progress of RSTVLM research, with a focus on the latest VLM applications for temporal image analysis.","We categorize and discuss core methodologies, datasets, and metrics, highlight recent advances in temporal vision-language tasks, and outline key challenges and future directions for research in this emerging field.","This survey fills a critical gap in the literature by providing an integrated overview of RSTVLM, offering a foundation for further advancements in remote sensing temporal image understanding.","We will keep tracing related works at \\url{https://github.com/Chen-Yang-Liu/Awesome-RS-Temporal-VLM}"],"url":"http://arxiv.org/abs/2412.02573v1"}
{"created":"2024-12-03 16:55:27","title":"TAB-Fields: A Maximum Entropy Framework for Mission-Aware Adversarial Planning","abstract":"Autonomous agents operating in adversarial scenarios face a fundamental challenge: while they may know their adversaries' high-level objectives, such as reaching specific destinations within time constraints, the exact policies these adversaries will employ remain unknown. Traditional approaches address this challenge by treating the adversary's state as a partially observable element, leading to a formulation as a Partially Observable Markov Decision Process (POMDP). However, the induced belief-space dynamics in a POMDP require knowledge of the system's transition dynamics, which, in this case, depend on the adversary's unknown policy. Our key observation is that while an adversary's exact policy is unknown, their behavior is necessarily constrained by their mission objectives and the physical environment, allowing us to characterize the space of possible behaviors without assuming specific policies. In this paper, we develop Task-Aware Behavior Fields (TAB-Fields), a representation that captures adversary state distributions over time by computing the most unbiased probability distribution consistent with known constraints. We construct TAB-Fields by solving a constrained optimization problem that minimizes additional assumptions about adversary behavior beyond mission and environmental requirements. We integrate TAB-Fields with standard planning algorithms by introducing TAB-conditioned POMCP, an adaptation of Partially Observable Monte Carlo Planning. Through experiments in simulation with underwater robots and hardware implementations with ground robots, we demonstrate that our approach achieves superior performance compared to baselines that either assume specific adversary policies or neglect mission constraints altogether. Evaluation videos and code are available at https://tab-fields.github.io.","sentences":["Autonomous agents operating in adversarial scenarios face a fundamental challenge: while they may know their adversaries' high-level objectives, such as reaching specific destinations within time constraints, the exact policies these adversaries will employ remain unknown.","Traditional approaches address this challenge by treating the adversary's state as a partially observable element, leading to a formulation as a Partially Observable Markov Decision Process (POMDP).","However, the induced belief-space dynamics in a POMDP require knowledge of the system's transition dynamics, which, in this case, depend on the adversary's unknown policy.","Our key observation is that while an adversary's exact policy is unknown, their behavior is necessarily constrained by their mission objectives and the physical environment, allowing us to characterize the space of possible behaviors without assuming specific policies.","In this paper, we develop Task-Aware Behavior Fields (TAB-Fields), a representation that captures adversary state distributions over time by computing the most unbiased probability distribution consistent with known constraints.","We construct TAB-Fields by solving a constrained optimization problem that minimizes additional assumptions about adversary behavior beyond mission and environmental requirements.","We integrate TAB-Fields with standard planning algorithms by introducing TAB-conditioned POMCP, an adaptation of Partially Observable Monte Carlo Planning.","Through experiments in simulation with underwater robots and hardware implementations with ground robots, we demonstrate that our approach achieves superior performance compared to baselines that either assume specific adversary policies or neglect mission constraints altogether.","Evaluation videos and code are available at https://tab-fields.github.io."],"url":"http://arxiv.org/abs/2412.02570v1"}
{"created":"2024-12-03 16:54:49","title":"Can I do it","abstract":"Knowledge about how well a robot can perform a specific task is currently present only in engineering reports which are inaccessible to the robot. Artificial Intelligence techniques, such as hypergraphs and automated reasoning, can provide such engineering knowledge online while enabling updates in the knowledge with new experiences. This requires a sound knowledge structure and maintenance routines for keeping this knowledge-base about the robot's capabilities truthful. A robot with such up-to-date information can reason about if and how well it can accomplish a task. This article introduces a knowledge representation that combines an ontology on system engineering, a deductive reasoning on the connections between system components, and an inductive reasoning on the performance of these components in the current system configuration. This representation is further used to derive the expected performance for the overall system based on a continuous evaluation of the actual performance per component. Our real-life implementation shows a robot that can answer questions on whether it can do a specific task with the desired performance.","sentences":["Knowledge about how well a robot can perform a specific task is currently present only in engineering reports which are inaccessible to the robot.","Artificial Intelligence techniques, such as hypergraphs and automated reasoning, can provide such engineering knowledge online while enabling updates in the knowledge with new experiences.","This requires a sound knowledge structure and maintenance routines for keeping this knowledge-base about the robot's capabilities truthful.","A robot with such up-to-date information can reason about if and how well it can accomplish a task.","This article introduces a knowledge representation that combines an ontology on system engineering, a deductive reasoning on the connections between system components, and an inductive reasoning on the performance of these components in the current system configuration.","This representation is further used to derive the expected performance for the overall system based on a continuous evaluation of the actual performance per component.","Our real-life implementation shows a robot that can answer questions on whether it can do a specific task with the desired performance."],"url":"http://arxiv.org/abs/2412.02569v1"}
{"created":"2024-12-03 16:53:58","title":"SJTU:Spatial judgments in multimodal models towards unified segmentation through coordinate detection","abstract":"Despite advances in vision-language understanding, implementing image segmentation within multimodal architectures remains a fundamental challenge in modern artificial intelligence systems. Existing vision-language models, which primarily rely on backbone architectures or CLIP-based embedding learning, demonstrate inherent limitations in fine-grained spatial localization and operational capabilities. This paper introduces SJTU: Spatial Judgments in multimodal models - Towards Unified segmentation through coordinate detection, a novel framework that leverages spatial coordinate understanding to bridge vision-language interaction and precise segmentation, enabling accurate target identification through natural language instructions. The framework proposes a novel approach for integrating segmentation techniques with vision-language models based on multimodal spatial inference. By leveraging normalized coordinate detection for bounding boxes and translating it into actionable segmentation outputs, we explore the possibility of integrating multimodal spatial and language representations. Based on the proposed technical approach, the framework demonstrates superior performance on various benchmark datasets as well as accurate object segmentation. Results on the COCO 2017 dataset for general object detection and Pascal VOC datasets for semantic segmentation demonstrate the generalization capabilities of the framework.","sentences":["Despite advances in vision-language understanding, implementing image segmentation within multimodal architectures remains a fundamental challenge in modern artificial intelligence systems.","Existing vision-language models, which primarily rely on backbone architectures or CLIP-based embedding learning, demonstrate inherent limitations in fine-grained spatial localization and operational capabilities.","This paper introduces SJTU:","Spatial Judgments in multimodal models - Towards Unified segmentation through coordinate detection, a novel framework that leverages spatial coordinate understanding to bridge vision-language interaction and precise segmentation, enabling accurate target identification through natural language instructions.","The framework proposes a novel approach for integrating segmentation techniques with vision-language models based on multimodal spatial inference.","By leveraging normalized coordinate detection for bounding boxes and translating it into actionable segmentation outputs, we explore the possibility of integrating multimodal spatial and language representations.","Based on the proposed technical approach, the framework demonstrates superior performance on various benchmark datasets as well as accurate object segmentation.","Results on the COCO 2017 dataset for general object detection and Pascal VOC datasets for semantic segmentation demonstrate the generalization capabilities of the framework."],"url":"http://arxiv.org/abs/2412.02565v1"}
{"created":"2024-12-03 16:52:06","title":"Semantic Tokens in Retrieval Augmented Generation","abstract":"Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks. However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases. Even with smaller datasets, these systems occasionally fail to address simple queries. This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs. In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses. The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability. This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems. This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability.","sentences":["Retrieval-Augmented Generation (RAG) architectures have recently garnered significant attention for their ability to improve truth grounding and coherence in natural language processing tasks.","However, the reliability of RAG systems in producing accurate answers diminishes as the volume of data they access increases.","Even with smaller datasets, these systems occasionally fail to address simple queries.","This issue arises from their dependence on state-of-the-art large language models (LLMs), which can introduce uncertainty into the system's outputs.","In this work, I propose a novel Comparative RAG system that introduces an evaluator module to bridge the gap between probabilistic RAG systems and deterministically verifiable responses.","The evaluator compares external recommendations with the retrieved document chunks, adding a decision-making layer that enhances the system's reliability.","This approach ensures that the chunks retrieved are both semantically relevant and logically consistent with deterministic insights, thereby improving the accuracy and overall efficiency of RAG systems.","This framework paves the way for more reliable and scalable question-answering applications in domains requiring high precision and verifiability."],"url":"http://arxiv.org/abs/2412.02563v1"}
{"created":"2024-12-03 16:48:53","title":"The Two-Center Problem of Uncertain Points on Cactus Graphs","abstract":"We study the two-center problem on cactus graphs in facility locations, which aims to place two facilities on the graph network to serve customers in order to minimize the maximum transportation cost. In our problem, the location of each customer is uncertain and may appear at $O(m)$ points on the network with probabilities. More specifically, given are a cactus graph $G$ and a set $\\calP$ of $n$ (weighted) uncertain points where every uncertain point has $O(m)$ possible locations on $G$ each associated with a probability and is of a non-negative weight. The problem aims to compute two centers (points) on $G$ so that the maximum (weighted) expected distance of the $n$ uncertain points to their own expected closest center is minimized. No previous algorithms are known for this problem. In this paper, we present the first algorithm for this problem and it solves the problem in $O(|G|+ m^{2}n^{2}\\log mn)$ time.","sentences":["We study the two-center problem on cactus graphs in facility locations, which aims to place two facilities on the graph network to serve customers in order to minimize the maximum transportation cost.","In our problem, the location of each customer is uncertain and may appear at $O(m)$ points on the network with probabilities.","More specifically, given are a cactus graph $G$ and a set $\\calP$ of $n$ (weighted) uncertain points where every uncertain point has $O(m)$ possible locations on $G$ each associated with a probability and is of a non-negative weight.","The problem aims to compute two centers (points) on $G$ so that the maximum (weighted) expected distance of the $n$ uncertain points to their own expected closest center is minimized.","No previous algorithms are known for this problem.","In this paper, we present the first algorithm for this problem and it solves the problem in $O(|G|+ m^{2}n^{2}\\log mn)$ time."],"url":"http://arxiv.org/abs/2412.02559v1"}
{"created":"2024-12-03 16:45:08","title":"Simple Construction of Greedy Trees and Greedy Permutations","abstract":"\\begin{abstract}   Greedy permutations, also known as Gonzalez Orderings or Farthest Point Traversals are a standard way to approximate $k$-center clustering and have many applications in sampling and approximating metric spaces.   A greedy tree is an added structure on a greedy permutation that tracks the (approximate) nearest predecessor.   Greedy trees have applications in proximity search as well as in topological data analysis.   For metrics of doubling dimension $d$, a $2^{O(d)}n\\log n$ time algorithm is known, but it is randomized and also, quite complicated.   Its construction involves a series of intermediate structures and $O(n \\log n)$ space.   In this paper, we show how to construct greedy permutations and greedy trees using a simple variation of an algorithm of Clarkson that was shown to run in $2^{O(d)}n\\log \\Delta$ time, where the spread $\\spread$ is the ratio of largest to smallest pairwise distances.   The improvement comes from the observation that the greedy tree can be constructed more easily than the greedy permutation.   This leads to a linear time algorithm for merging two approximate greedy trees and thus, an $2^{O(d)}n \\log n$ time algorithm for computing the tree.   Then, we show how to extract a $(1+\\frac{1}{n})$-approximate greedy permutation from the approximate greedy tree in the same asymptotic running time. \\end{abstract}","sentences":["\\begin{abstract}   Greedy permutations, also known as Gonzalez Orderings or Farthest Point Traversals are a standard way to approximate $k$-center clustering and have many applications in sampling and approximating metric spaces.   ","A greedy tree is an added structure on a greedy permutation that tracks the (approximate) nearest predecessor.   ","Greedy trees have applications in proximity search as well as in topological data analysis.   ","For metrics of doubling dimension $d$, a $2^{O(d)}n\\log n$ time algorithm is known, but it is randomized and also, quite complicated.   ","Its construction involves a series of intermediate structures and $O(n \\log n)$ space.   ","In this paper, we show how to construct greedy permutations and greedy trees using a simple variation of an algorithm of Clarkson that was shown to run in $2^{O(d)}n\\log \\Delta$ time, where the spread $\\spread$ is the ratio of largest to smallest pairwise distances.   ","The improvement comes from the observation that the greedy tree can be constructed more easily than the greedy permutation.   ","This leads to a linear time algorithm for merging two approximate greedy trees and thus, an $2^{O(d)}n \\log n$ time algorithm for computing the tree.   ","Then, we show how to extract a $(1+\\frac{1}{n})$-approximate greedy permutation from the approximate greedy tree in the same asymptotic running time.","\\end{abstract}"],"url":"http://arxiv.org/abs/2412.02554v1"}
{"created":"2024-12-03 16:43:42","title":"Patent-CR: A Dataset for Patent Claim Revision","abstract":"This paper presents Patent-CR, the first dataset created for the patent claim revision task in English. It includes both initial patent applications rejected by patent examiners and the final granted versions. Unlike normal text revision tasks that predominantly focus on enhancing sentence quality, such as grammar correction and coherence improvement, patent claim revision aims at ensuring the claims meet stringent legal criteria. These criteria are beyond novelty and inventiveness, including clarity of scope, technical accuracy, language precision, and legal robustness. We assess various large language models (LLMs) through professional human evaluation, including general LLMs with different sizes and architectures, text revision models, and domain-specific models. Our results indicate that LLMs often bring ineffective edits that deviate from the target revisions. In addition, domain-specific models and the method of fine-tuning show promising results. Notably, GPT-4 outperforms other tested LLMs, but further revisions are still necessary to reach the examination standard. Furthermore, we demonstrate the inconsistency between automated and human evaluation results, suggesting that GPT-4-based automated evaluation has the highest correlation with human judgment. This dataset, along with our preliminary empirical research, offers invaluable insights for further exploration in patent claim revision.","sentences":["This paper presents Patent-CR, the first dataset created for the patent claim revision task in English.","It includes both initial patent applications rejected by patent examiners and the final granted versions.","Unlike normal text revision tasks that predominantly focus on enhancing sentence quality, such as grammar correction and coherence improvement, patent claim revision aims at ensuring the claims meet stringent legal criteria.","These criteria are beyond novelty and inventiveness, including clarity of scope, technical accuracy, language precision, and legal robustness.","We assess various large language models (LLMs) through professional human evaluation, including general LLMs with different sizes and architectures, text revision models, and domain-specific models.","Our results indicate that LLMs often bring ineffective edits that deviate from the target revisions.","In addition, domain-specific models and the method of fine-tuning show promising results.","Notably, GPT-4 outperforms other tested LLMs, but further revisions are still necessary to reach the examination standard.","Furthermore, we demonstrate the inconsistency between automated and human evaluation results, suggesting that GPT-4-based automated evaluation has the highest correlation with human judgment.","This dataset, along with our preliminary empirical research, offers invaluable insights for further exploration in patent claim revision."],"url":"http://arxiv.org/abs/2412.02549v1"}
{"created":"2024-12-03 16:40:43","title":"Interaction Identification of a Heterogeneous NDS with Quadratic-Bilinear Subsystems","abstract":"This paper attacks time-domain identification for the interaction parameters of a heterogeneous networked dynamic system (NDS), with each of its subsystems being described by a continuous-time descriptor quadratic-bilinear time-invariant (QBTI) model. No restrictions are put on the sampling rate. Explicit formulas are derived respectively for the transient and steady-state responses of the NDS, provided that the probing signal is generated by a linear time invariant (LTI) system. Some relations have been derived between the NDS steady-state response and its frequency domain input-output mappings. These relations reveal that the value of some NDS associated TFMs can in principle be estimated at almost any interested point of the imaginary axis from input-output experimental data, as well as its derivatives and a right tangential interpolation along an arbitrary direction. Based on these relations, an estimation algorithm is suggested respectively for the parameters of the NDS and the values of these TFMs.","sentences":["This paper attacks time-domain identification for the interaction parameters of a heterogeneous networked dynamic system (NDS), with each of its subsystems being described by a continuous-time descriptor quadratic-bilinear time-invariant (QBTI) model.","No restrictions are put on the sampling rate.","Explicit formulas are derived respectively for the transient and steady-state responses of the NDS, provided that the probing signal is generated by a linear time invariant (LTI) system.","Some relations have been derived between the NDS steady-state response and its frequency domain input-output mappings.","These relations reveal that the value of some NDS associated TFMs can in principle be estimated at almost any interested point of the imaginary axis from input-output experimental data, as well as its derivatives and a right tangential interpolation along an arbitrary direction.","Based on these relations, an estimation algorithm is suggested respectively for the parameters of the NDS and the values of these TFMs."],"url":"http://arxiv.org/abs/2412.02547v1"}
{"created":"2024-12-03 16:39:01","title":"Fractional Order Distributed Optimization","abstract":"Distributed optimization is fundamental to modern machine learning applications like federated learning, but existing methods often struggle with ill-conditioned problems and face stability-versus-speed tradeoffs. We introduce fractional order distributed optimization (FrODO); a theoretically-grounded framework that incorporates fractional-order memory terms to enhance convergence properties in challenging optimization landscapes. Our approach achieves provable linear convergence for any strongly connected network. Through empirical validation, our results suggest that FrODO achieves up to 4 times faster convergence versus baselines on ill-conditioned problems and 2-3 times speedup in federated neural network training, while maintaining stability and theoretical guarantees.","sentences":["Distributed optimization is fundamental to modern machine learning applications like federated learning, but existing methods often struggle with ill-conditioned problems and face stability-versus-speed tradeoffs.","We introduce fractional order distributed optimization (FrODO); a theoretically-grounded framework that incorporates fractional-order memory terms to enhance convergence properties in challenging optimization landscapes.","Our approach achieves provable linear convergence for any strongly connected network.","Through empirical validation, our results suggest that FrODO achieves up to 4 times faster convergence versus baselines on ill-conditioned problems and 2-3 times speedup in federated neural network training, while maintaining stability and theoretical guarantees."],"url":"http://arxiv.org/abs/2412.02546v1"}
{"created":"2024-12-03 16:37:23","title":"ShadowHack: Hacking Shadows via Luminance-Color Divide and Conquer","abstract":"Shadows introduce challenges such as reduced brightness, texture deterioration, and color distortion in images, complicating a holistic solution. This study presents \\textbf{ShadowHack}, a divide-and-conquer strategy that tackles these complexities by decomposing the original task into luminance recovery and color remedy. To brighten shadow regions and repair the corrupted textures in the luminance space, we customize LRNet, a U-shaped network with a rectified outreach attention module, to enhance information interaction and recalibrate contaminated attention maps. With luminance recovered, CRNet then leverages cross-attention mechanisms to revive vibrant colors, producing visually compelling results. Extensive experiments on multiple datasets are conducted to demonstrate the superiority of ShadowHack over existing state-of-the-art solutions both quantitatively and qualitatively, highlighting the effectiveness of our design. Our code will be made publicly available at https://github.com/lime-j/ShadowHack","sentences":["Shadows introduce challenges such as reduced brightness, texture deterioration, and color distortion in images, complicating a holistic solution.","This study presents \\textbf{ShadowHack}, a divide-and-conquer strategy that tackles these complexities by decomposing the original task into luminance recovery and color remedy.","To brighten shadow regions and repair the corrupted textures in the luminance space, we customize LRNet, a U-shaped network with a rectified outreach attention module, to enhance information interaction and recalibrate contaminated attention maps.","With luminance recovered, CRNet then leverages cross-attention mechanisms to revive vibrant colors, producing visually compelling results.","Extensive experiments on multiple datasets are conducted to demonstrate the superiority of ShadowHack over existing state-of-the-art solutions both quantitatively and qualitatively, highlighting the effectiveness of our design.","Our code will be made publicly available at https://github.com/lime-j/ShadowHack"],"url":"http://arxiv.org/abs/2412.02545v1"}
{"created":"2024-12-03 16:34:49","title":"Unveiling Concept Attribution in Diffusion Models","abstract":"Diffusion models have shown remarkable abilities in generating realistic and high-quality images from text prompts. However, a trained model remains black-box; little do we know about the role of its components in exhibiting a concept such as objects or styles. Recent works employ causal tracing to localize layers storing knowledge in generative models without showing how those layers contribute to the target concept. In this work, we approach the model interpretability problem from a more general perspective and pose a question: \\textit{``How do model components work jointly to demonstrate knowledge?''}. We adapt component attribution to decompose diffusion models, unveiling how a component contributes to a concept. Our framework allows effective model editing, in particular, we can erase a concept from diffusion models by removing positive components while remaining knowledge of other concepts. Surprisingly, we also show there exist components that contribute negatively to a concept, which has not been discovered in the knowledge localization approach. Experimental results confirm the role of positive and negative components pinpointed by our framework, depicting a complete view of interpreting generative models. Our code is available at \\url{https://github.com/mail-research/CAD-attribution4diffusion}","sentences":["Diffusion models have shown remarkable abilities in generating realistic and high-quality images from text prompts.","However, a trained model remains black-box; little do we know about the role of its components in exhibiting a concept such as objects or styles.","Recent works employ causal tracing to localize layers storing knowledge in generative models without showing how those layers contribute to the target concept.","In this work, we approach the model interpretability problem from a more general perspective and pose a question: \\textit{``How do model components work jointly to demonstrate knowledge?''}.","We adapt component attribution to decompose diffusion models, unveiling how a component contributes to a concept.","Our framework allows effective model editing, in particular, we can erase a concept from diffusion models by removing positive components while remaining knowledge of other concepts.","Surprisingly, we also show there exist components that contribute negatively to a concept, which has not been discovered in the knowledge localization approach.","Experimental results confirm the role of positive and negative components pinpointed by our framework, depicting a complete view of interpreting generative models.","Our code is available at \\url{https://github.com/mail-research/CAD-attribution4diffusion}"],"url":"http://arxiv.org/abs/2412.02542v1"}
{"created":"2024-12-03 16:33:17","title":"Automatic State Machine Inference for Binary Protocol Reverse Engineering","abstract":"Protocol Reverse Engineering (PRE) is used to analyze protocols by inferring their structure and behavior. However, current PRE methods mainly focus on field identification within a single protocol and neglect Protocol State Machine (PSM) analysis in mixed protocol environments. This results in insufficient analysis of protocols' abnormal behavior and potential vulnerabilities, which are crucial for detecting and defending against new attack patterns. To address these challenges, we propose an automatic PSM inference framework for unknown protocols, including a fuzzy membership-based auto-converging DBSCAN algorithm for protocol format clustering, followed by a session clustering algorithm based on Needleman-Wunsch and K-Medoids algorithms to classify sessions by protocol type. Finally, we refine a probabilistic PSM algorithm to infer protocol states and the transition conditions between these states. Experimental results show that, compared with existing PRE techniques, our method can infer PSMs while enabling more precise classification of protocols.","sentences":["Protocol Reverse Engineering (PRE) is used to analyze protocols by inferring their structure and behavior.","However, current PRE methods mainly focus on field identification within a single protocol and neglect Protocol State Machine (PSM) analysis in mixed protocol environments.","This results in insufficient analysis of protocols' abnormal behavior and potential vulnerabilities, which are crucial for detecting and defending against new attack patterns.","To address these challenges, we propose an automatic PSM inference framework for unknown protocols, including a fuzzy membership-based auto-converging DBSCAN algorithm for protocol format clustering, followed by a session clustering algorithm based on Needleman-Wunsch and K-Medoids algorithms to classify sessions by protocol type.","Finally, we refine a probabilistic PSM algorithm to infer protocol states and the transition conditions between these states.","Experimental results show that, compared with existing PRE techniques, our method can infer PSMs while enabling more precise classification of protocols."],"url":"http://arxiv.org/abs/2412.02540v1"}
{"created":"2024-12-03 16:32:57","title":"Graph-Powered Defense: Controller Area Network Intrusion Detection for Unmanned Aerial Vehicles","abstract":"The network of services, including delivery, farming, and environmental monitoring, has experienced exponential expansion in the past decade with Unmanned Aerial Vehicles (UAVs). Yet, UAVs are not robust enough against cyberattacks, especially on the Controller Area Network (CAN) bus. The CAN bus is a general-purpose vehicle-bus standard to enable microcontrollers and in-vehicle computers to interact, primarily connecting different Electronic Control Units (ECUs). In this study, we focus on solving some of the most critical security weaknesses in UAVs by developing a novel graph-based intrusion detection system (IDS) leveraging the Uncomplicated Application-level Vehicular Communication and Networking (UAVCAN) protocol. First, we decode CAN messages based on UAVCAN protocol specification; second, we present a comprehensive method of transforming tabular UAVCAN messages into graph structures. Lastly, we apply various graph-based machine learning models for detecting cyber-attacks on the CAN bus, including graph convolutional neural networks (GCNNs), graph attention networks (GATs), Graph Sample and Aggregate Networks (GraphSAGE), and graph structure-based transformers. Our findings show that inductive models such as GATs, GraphSAGE, and graph-based transformers can achieve competitive and even better accuracy than transductive models like GCNNs in detecting various types of intrusions, with minimum information on protocol specification, thus providing a generic robust solution for CAN bus security for the UAVs. We also compared our results with baseline single-layer Long Short-Term Memory (LSTM) and found that all our graph-based models perform better without using any decoded features based on the UAVCAN protocol, highlighting higher detection performance with protocol-independent capability.","sentences":["The network of services, including delivery, farming, and environmental monitoring, has experienced exponential expansion in the past decade with Unmanned Aerial Vehicles (UAVs).","Yet, UAVs are not robust enough against cyberattacks, especially on the Controller Area Network (CAN) bus.","The CAN bus is a general-purpose vehicle-bus standard to enable microcontrollers and in-vehicle computers to interact, primarily connecting different Electronic Control Units (ECUs).","In this study, we focus on solving some of the most critical security weaknesses in UAVs by developing a novel graph-based intrusion detection system (IDS) leveraging the Uncomplicated Application-level Vehicular Communication and Networking (UAVCAN) protocol.","First, we decode CAN messages based on UAVCAN protocol specification; second, we present a comprehensive method of transforming tabular UAVCAN messages into graph structures.","Lastly, we apply various graph-based machine learning models for detecting cyber-attacks on the CAN bus, including graph convolutional neural networks (GCNNs), graph attention networks (GATs), Graph Sample and Aggregate Networks (GraphSAGE), and graph structure-based transformers.","Our findings show that inductive models such as GATs, GraphSAGE, and graph-based transformers can achieve competitive and even better accuracy than transductive models like GCNNs in detecting various types of intrusions, with minimum information on protocol specification, thus providing a generic robust solution for CAN bus security for the UAVs.","We also compared our results with baseline single-layer Long Short-Term Memory (LSTM) and found that all our graph-based models perform better without using any decoded features based on the UAVCAN protocol, highlighting higher detection performance with protocol-independent capability."],"url":"http://arxiv.org/abs/2412.02539v1"}
{"created":"2024-12-03 16:32:19","title":"On the Privacy, Security, and Trustworthy for Distributed Wireless Large AI Model (WLAM)","abstract":"Combining wireless communication with large artificial intelligence (AI) models can open up a myriad of novel application scenarios. In sixth generation (6G) networks, ubiquitous communication and computing resources allow large AI models to serve democratic large AI models-related services to enable real-time applications like autonomous vehicles, smart cities, and Internet of Things (IoT) ecosystems. However, the security considerations and sustainable communication resources limit the deployment of large AI models over distributed wireless networks. This paper provides a comprehensive overview of privacy, security, and trustworthy for distributed wireless large AI model (WLAM). In particular, the detailed privacy and security are analysis for distributed WLAM is fist revealed. The classifications and theoretical findings about privacy and security in distributed WLAM are discussed. Then the trustworthy and ethics for implementing distributed WLAM are described. Finally, the comprehensive applications of distributed WLAM is provided in the aspect of electromagnetic signal processing.","sentences":["Combining wireless communication with large artificial intelligence (AI) models can open up a myriad of novel application scenarios.","In sixth generation (6G) networks, ubiquitous communication and computing resources allow large AI models to serve democratic large AI models-related services to enable real-time applications like autonomous vehicles, smart cities, and Internet of Things (IoT) ecosystems.","However, the security considerations and sustainable communication resources limit the deployment of large AI models over distributed wireless networks.","This paper provides a comprehensive overview of privacy, security, and trustworthy for distributed wireless large AI model (WLAM).","In particular, the detailed privacy and security are analysis for distributed WLAM is fist revealed.","The classifications and theoretical findings about privacy and security in distributed WLAM are discussed.","Then the trustworthy and ethics for implementing distributed WLAM are described.","Finally, the comprehensive applications of distributed WLAM is provided in the aspect of electromagnetic signal processing."],"url":"http://arxiv.org/abs/2412.02538v1"}
{"created":"2024-12-03 16:26:56","title":"Defending Against Diverse Attacks in Federated Learning Through Consensus-Based Bi-Level Optimization","abstract":"Adversarial attacks pose significant challenges in many machine learning applications, particularly in the setting of distributed training and federated learning, where malicious agents seek to corrupt the training process with the goal of jeopardizing and compromising the performance and reliability of the final models. In this paper, we address the problem of robust federated learning in the presence of such attacks by formulating the training task as a bi-level optimization problem. We conduct a theoretical analysis of the resilience of consensus-based bi-level optimization (CB$^2$O), an interacting multi-particle metaheuristic optimization method, in adversarial settings. Specifically, we provide a global convergence analysis of CB$^2$O in mean-field law in the presence of malicious agents, demonstrating the robustness of CB$^2$O against a diverse range of attacks. Thereby, we offer insights into how specific hyperparameter choices enable to mitigate adversarial effects. On the practical side, we extend CB$^2$O to the clustered federated learning setting by proposing FedCB$^2$O, a novel interacting multi-particle system, and design a practical algorithm that addresses the demands of real-world applications. Extensive experiments demonstrate the robustness of the FedCB$^2$O algorithm against label-flipping attacks in decentralized clustered federated learning scenarios, showcasing its effectiveness in practical contexts.","sentences":["Adversarial attacks pose significant challenges in many machine learning applications, particularly in the setting of distributed training and federated learning, where malicious agents seek to corrupt the training process with the goal of jeopardizing and compromising the performance and reliability of the final models.","In this paper, we address the problem of robust federated learning in the presence of such attacks by formulating the training task as a bi-level optimization problem.","We conduct a theoretical analysis of the resilience of consensus-based bi-level optimization (CB$^2$O), an interacting multi-particle metaheuristic optimization method, in adversarial settings.","Specifically, we provide a global convergence analysis of CB$^2$O in mean-field law in the presence of malicious agents, demonstrating the robustness of CB$^2$O against a diverse range of attacks.","Thereby, we offer insights into how specific hyperparameter choices enable to mitigate adversarial effects.","On the practical side, we extend CB$^2$O to the clustered federated learning setting by proposing FedCB$^2$O, a novel interacting multi-particle system, and design a practical algorithm that addresses the demands of real-world applications.","Extensive experiments demonstrate the robustness of the FedCB$^2$O algorithm against label-flipping attacks in decentralized clustered federated learning scenarios, showcasing its effectiveness in practical contexts."],"url":"http://arxiv.org/abs/2412.02535v1"}
{"created":"2024-12-03 16:25:08","title":"LiDAR-based Registration against Georeferenced Models for Globally Consistent Allocentric Maps","abstract":"Modern unmanned aerial vehicles (UAVs) are irreplaceable in search and rescue (SAR) missions to obtain a situational overview or provide closeups without endangering personnel. However, UAVs heavily rely on global navigation satellite system (GNSS) for localization which works well in open spaces, but the precision drastically degrades in the vicinity of buildings. These inaccuracies hinder aggregation of diverse data from multiple sources in a unified georeferenced frame for SAR operators. In contrast, CityGML models provide approximate building shapes with accurate georeferenced poses. Besides, LiDAR works best in the vicinity of 3D structures. Hence, we refine coarse GNSS measurements by registering LiDAR maps against CityGML and digital elevation map (DEM) models as a prior for allocentric mapping. An intuitive plausibility score selects the best hypothesis based on occupancy using a 2D height map. Afterwards, we integrate the registration results in a continuous-time spline-based pose graph optimizer with LiDAR odometry and further sensing modalities to obtain globally consistent, georeferenced trajectories and maps. We evaluate the viability of our approach on multiple flights captured at two distinct testing sites. Our method successfully reduced GNSS offset errors from up-to 16 m to below 0.5 m on multiple flights. Furthermore, we obtain globally consistent maps w.r.t. prior 3D geospatial models.","sentences":["Modern unmanned aerial vehicles (UAVs) are irreplaceable in search and rescue (SAR) missions to obtain a situational overview or provide closeups without endangering personnel.","However, UAVs heavily rely on global navigation satellite system (GNSS) for localization which works well in open spaces, but the precision drastically degrades in the vicinity of buildings.","These inaccuracies hinder aggregation of diverse data from multiple sources in a unified georeferenced frame for SAR operators.","In contrast, CityGML models provide approximate building shapes with accurate georeferenced poses.","Besides, LiDAR works best in the vicinity of 3D structures.","Hence, we refine coarse GNSS measurements by registering LiDAR maps against CityGML and digital elevation map (DEM) models as a prior for allocentric mapping.","An intuitive plausibility score selects the best hypothesis based on occupancy using a 2D height map.","Afterwards, we integrate the registration results in a continuous-time spline-based pose graph optimizer with LiDAR odometry and further sensing modalities to obtain globally consistent, georeferenced trajectories and maps.","We evaluate the viability of our approach on multiple flights captured at two distinct testing sites.","Our method successfully reduced GNSS offset errors from up-to 16 m to below 0.5 m on multiple flights.","Furthermore, we obtain globally consistent maps w.r.t.","prior 3D geospatial models."],"url":"http://arxiv.org/abs/2412.02533v1"}
{"created":"2024-12-03 16:24:16","title":"Multimodal Remote Sensing Scene Classification Using VLMs and Dual-Cross Attention Networks","abstract":"Remote sensing scene classification (RSSC) is a critical task with diverse applications in land use and resource management. While unimodal image-based approaches show promise, they often struggle with limitations such as high intra-class variance and inter-class similarity. Incorporating textual information can enhance classification by providing additional context and semantic understanding, but manual text annotation is labor-intensive and costly. In this work, we propose a novel RSSC framework that integrates text descriptions generated by large vision-language models (VLMs) as an auxiliary modality without incurring expensive manual annotation costs. To fully leverage the latent complementarities between visual and textual data, we propose a dual cross-attention-based network to fuse these modalities into a unified representation. Extensive experiments with both quantitative and qualitative evaluation across five RSSC datasets demonstrate that our framework consistently outperforms baseline models. We also verify the effectiveness of VLM-generated text descriptions compared to human-annotated descriptions. Additionally, we design a zero-shot classification scenario to show that the learned multimodal representation can be effectively utilized for unseen class classification. This research opens new opportunities for leveraging textual information in RSSC tasks and provides a promising multimodal fusion structure, offering insights and inspiration for future studies. Code is available at: https://github.com/CJR7/MultiAtt-RSSC","sentences":["Remote sensing scene classification (RSSC) is a critical task with diverse applications in land use and resource management.","While unimodal image-based approaches show promise, they often struggle with limitations such as high intra-class variance and inter-class similarity.","Incorporating textual information can enhance classification by providing additional context and semantic understanding, but manual text annotation is labor-intensive and costly.","In this work, we propose a novel RSSC framework that integrates text descriptions generated by large vision-language models (VLMs) as an auxiliary modality without incurring expensive manual annotation costs.","To fully leverage the latent complementarities between visual and textual data, we propose a dual cross-attention-based network to fuse these modalities into a unified representation.","Extensive experiments with both quantitative and qualitative evaluation across five RSSC datasets demonstrate that our framework consistently outperforms baseline models.","We also verify the effectiveness of VLM-generated text descriptions compared to human-annotated descriptions.","Additionally, we design a zero-shot classification scenario to show that the learned multimodal representation can be effectively utilized for unseen class classification.","This research opens new opportunities for leveraging textual information in RSSC tasks and provides a promising multimodal fusion structure, offering insights and inspiration for future studies.","Code is available at: https://github.com/CJR7/MultiAtt-RSSC"],"url":"http://arxiv.org/abs/2412.02531v1"}
{"created":"2024-12-03 16:23:02","title":"WEM-GAN: Wavelet transform based facial expression manipulation","abstract":"Facial expression manipulation aims to change human facial expressions without affecting face recognition. In order to transform the facial expressions to target expressions, previous methods relied on expression labels to guide the manipulation process. However, these methods failed to preserve the details of facial features, which causes the weakening or the loss of identity information in the output image. In our work, we propose WEM-GAN, in short for wavelet-based expression manipulation GAN, which puts more efforts on preserving the details of the original image in the editing process. Firstly, we take advantage of the wavelet transform technique and combine it with our generator with a U-net autoencoder backbone, in order to improve the generator's ability to preserve more details of facial features. Secondly, we also implement the high-frequency component discriminator, and use high-frequency domain adversarial loss to further constrain the optimization of our model, providing the generated face image with more abundant details. Additionally, in order to narrow the gap between generated facial expressions and target expressions, we use residual connections between encoder and decoder, while also using relative action units (AUs) several times. Extensive qualitative and quantitative experiments have demonstrated that our model performs better in preserving identity features, editing capability, and image generation quality on the AffectNet dataset. It also shows superior performance in metrics such as Average Content Distance (ACD) and Expression Distance (ED).","sentences":["Facial expression manipulation aims to change human facial expressions without affecting face recognition.","In order to transform the facial expressions to target expressions, previous methods relied on expression labels to guide the manipulation process.","However, these methods failed to preserve the details of facial features, which causes the weakening or the loss of identity information in the output image.","In our work, we propose WEM-GAN, in short for wavelet-based expression manipulation GAN, which puts more efforts on preserving the details of the original image in the editing process.","Firstly, we take advantage of the wavelet transform technique and combine it with our generator with a U-net autoencoder backbone, in order to improve the generator's ability to preserve more details of facial features.","Secondly, we also implement the high-frequency component discriminator, and use high-frequency domain adversarial loss to further constrain the optimization of our model, providing the generated face image with more abundant details.","Additionally, in order to narrow the gap between generated facial expressions and target expressions, we use residual connections between encoder and decoder, while also using relative action units (AUs) several times.","Extensive qualitative and quantitative experiments have demonstrated that our model performs better in preserving identity features, editing capability, and image generation quality on the AffectNet dataset.","It also shows superior performance in metrics such as Average Content Distance (ACD) and Expression Distance (ED)."],"url":"http://arxiv.org/abs/2412.02530v1"}
{"created":"2024-12-03 16:21:37","title":"Bias Analysis of AI Models for Undergraduate Student Admissions","abstract":"Bias detection and mitigation is an active area of research in machine learning. This work extends previous research done by the authors to provide a rigorous and more complete analysis of the bias found in AI predictive models. Admissions data spanning six years was used to create an AI model to determine whether a given student would be directly admitted into the School of Science under various scenarios at a large urban research university. During this time, submission of standardized test scores as part of an application became optional which led to interesting questions about the impact of standardized test scores on admission decisions. We developed and analyzed AI models to understand which variables are important in admissions decisions, and how the decision to exclude test scores affects the demographics of the students who are admitted. We then evaluated the predictive models to detect and analyze biases these models may carry with respect to three variables chosen to represent sensitive populations: gender, race, and whether a student was the first in his or her family to attend college. We also extended our analysis to show that the biases detected were persistent. Finally, we included several fairness metrics in our analysis and discussed the uses and limitations of these metrics.","sentences":["Bias detection and mitigation is an active area of research in machine learning.","This work extends previous research done by the authors to provide a rigorous and more complete analysis of the bias found in AI predictive models.","Admissions data spanning six years was used to create an AI model to determine whether a given student would be directly admitted into the School of Science under various scenarios at a large urban research university.","During this time, submission of standardized test scores as part of an application became optional which led to interesting questions about the impact of standardized test scores on admission decisions.","We developed and analyzed AI models to understand which variables are important in admissions decisions, and how the decision to exclude test scores affects the demographics of the students who are admitted.","We then evaluated the predictive models to detect and analyze biases these models may carry with respect to three variables chosen to represent sensitive populations: gender, race, and whether a student was the first in his or her family to attend college.","We also extended our analysis to show that the biases detected were persistent.","Finally, we included several fairness metrics in our analysis and discussed the uses and limitations of these metrics."],"url":"http://arxiv.org/abs/2412.02528v1"}
{"created":"2024-12-03 16:21:00","title":"On the lifting degree of girth-8 QC-LDPC codes","abstract":"The lifting degree and the deterministic construction of quasi-cyclic low-density parity-check (QC-LDPC) codes have been extensively studied, with many construction methods in the literature, including those based on finite geometry, array-based codes, computer search, and combinatorial techniques. In this paper, we focus on the lifting degree $p$ required for achieving a girth of 8 in $(3,L)$ fully connected QC-LDPC codes, and we propose an improvement over the classical lower bound $p\\geq 2L-1$, enhancing it to $p\\geq \\sqrt{5L^2-11L+\\frac{13}{2}}+\\frac{1}{2}$. Moreover, we demonstrate that for girth-8 QC-LDPC codes containing an arithmetic row in the exponent matrix, a necessary condition for achieving a girth of 8 is $p\\geq \\frac{1}{2}L^2+\\frac{1}{2}L$. Additionally, we present a corresponding deterministic construction of $(3,L)$ QC-LDPC codes with girth 8 for any $p\\geq \\frac{1}{2}L^2+\\frac{1}{2}L+\\lfloor \\frac{L-1}{2}\\rfloor$, which approaches the lower bound of $\\frac{1}{2}L^2+\\frac{1}{2}L$. Under the same conditions, this construction achieves a smaller lifting degree compared to prior methods. To the best of our knowledge, the proposed order of lifting degree matches the smallest known, on the order of $\\frac{1}{2}L^2+\\mathcal{O} (L)$.","sentences":["The lifting degree and the deterministic construction of quasi-cyclic low-density parity-check (QC-LDPC) codes have been extensively studied, with many construction methods in the literature, including those based on finite geometry, array-based codes, computer search, and combinatorial techniques.","In this paper, we focus on the lifting degree $p$ required for achieving a girth of 8 in $(3,L)$ fully connected QC-LDPC codes, and we propose an improvement over the classical lower bound $p\\geq 2L-1$, enhancing it to $p\\geq \\sqrt{5L^2-11L+\\frac{13}{2}}+\\frac{1}{2}$.","Moreover, we demonstrate that for girth-8 QC-LDPC codes containing an arithmetic row in the exponent matrix, a necessary condition for achieving a girth of 8 is $p\\geq \\frac{1}{2}L^2+\\frac{1}{2}L$. Additionally, we present a corresponding deterministic construction of $(3,L)$ QC-LDPC codes with girth 8 for any $p\\geq \\frac{1}{2}L^2+\\frac{1}{2}L+\\lfloor \\frac{L-1}{2}\\rfloor$, which approaches the lower bound of $\\frac{1}{2}L^2+\\frac{1}{2}L$. Under the same conditions, this construction achieves a smaller lifting degree compared to prior methods.","To the best of our knowledge, the proposed order of lifting degree matches the smallest known, on the order of $\\frac{1}{2}L^2+\\mathcal{O} (L)$."],"url":"http://arxiv.org/abs/2412.02526v1"}
{"created":"2024-12-03 16:18:42","title":"LLMForecaster: Improving Seasonal Event Forecasts with Unstructured Textual Data","abstract":"Modern time-series forecasting models often fail to make full use of rich unstructured information about the time series themselves. This lack of proper conditioning can lead to obvious model failures; for example, models may be unaware of the details of a particular product, and hence fail to anticipate seasonal surges in customer demand in the lead up to major exogenous events like holidays for clearly relevant products. To address this shortcoming, this paper introduces a novel forecast post-processor -- which we call LLMForecaster -- that fine-tunes large language models (LLMs) to incorporate unstructured semantic and contextual information and historical data to improve the forecasts from an existing demand forecasting pipeline. In an industry-scale retail application, we demonstrate that our technique yields statistically significantly forecast improvements across several sets of products subject to holiday-driven demand surges.","sentences":["Modern time-series forecasting models often fail to make full use of rich unstructured information about the time series themselves.","This lack of proper conditioning can lead to obvious model failures; for example, models may be unaware of the details of a particular product, and hence fail to anticipate seasonal surges in customer demand in the lead up to major exogenous events like holidays for clearly relevant products.","To address this shortcoming, this paper introduces a novel forecast post-processor -- which we call LLMForecaster -- that fine-tunes large language models (LLMs) to incorporate unstructured semantic and contextual information and historical data to improve the forecasts from an existing demand forecasting pipeline.","In an industry-scale retail application, we demonstrate that our technique yields statistically significantly forecast improvements across several sets of products subject to holiday-driven demand surges."],"url":"http://arxiv.org/abs/2412.02525v1"}
{"created":"2024-12-03 16:13:42","title":"Cooperative Cruising: Reinforcement Learning based Time-Headway Control for Increased Traffic Efficiency","abstract":"The proliferation of Connected Automated Vehicles represents an unprecedented opportunity for improving driving efficiency and alleviating traffic congestion. However, existing research fails to address realistic multi-lane highway scenarios without assuming connectivity, perception, and control capabilities that are typically unavailable in current vehicles. This paper proposes a novel AI system that is the first to improve highway traffic efficiency compared with human-like traffic in realistic, simulated multi-lane scenarios, while relying on existing connectivity, perception, and control capabilities. At the core of our approach is a reinforcement learning based controller that dynamically communicates time-headways to automated vehicles near bottlenecks based on real-time traffic conditions. These desired time-headways are then used by Adaptive Cruise Control (ACC) systems to adjust their following distance. By (i) integrating existing traffic estimation technology and low-bandwidth vehicle-to-infrastructure connectivity, (ii) leveraging safety-certified ACC systems, and (iii) targeting localized bottleneck challenges that can be addressed independently in different locations, we propose a practical, safe, and scalable system that can positively impact numerous road users.","sentences":["The proliferation of Connected Automated Vehicles represents an unprecedented opportunity for improving driving efficiency and alleviating traffic congestion.","However, existing research fails to address realistic multi-lane highway scenarios without assuming connectivity, perception, and control capabilities that are typically unavailable in current vehicles.","This paper proposes a novel AI system that is the first to improve highway traffic efficiency compared with human-like traffic in realistic, simulated multi-lane scenarios, while relying on existing connectivity, perception, and control capabilities.","At the core of our approach is a reinforcement learning based controller that dynamically communicates time-headways to automated vehicles near bottlenecks based on real-time traffic conditions.","These desired time-headways are then used by Adaptive Cruise Control (ACC) systems to adjust their following distance.","By (i) integrating existing traffic estimation technology and low-bandwidth vehicle-to-infrastructure connectivity, (ii) leveraging safety-certified ACC systems, and (iii) targeting localized bottleneck challenges that can be addressed independently in different locations, we propose a practical, safe, and scalable system that can positively impact numerous road users."],"url":"http://arxiv.org/abs/2412.02520v1"}
{"created":"2024-12-03 15:48:33","title":"FCL-ViT: Task-Aware Attention Tuning for Continual Learning","abstract":"Continual Learning (CL) involves adapting the prior Deep Neural Network (DNN) knowledge to new tasks, without forgetting the old ones. However, modern CL techniques focus on provisioning memory capabilities to existing DNN models rather than designing new ones that are able to adapt according to the task at hand. This paper presents the novel Feedback Continual Learning Vision Transformer (FCL-ViT) that uses a feedback mechanism to generate real-time dynamic attention features tailored to the current task. The FCL-ViT operates in two Phases. In phase 1, the generic image features are produced and determine where the Transformer should attend on the current image. In phase 2, task-specific image features are generated that leverage dynamic attention. To this end, Tunable self-Attention Blocks (TABs) and Task Specific Blocks (TSBs) are introduced that operate in both phases and are responsible for tuning the TABs attention, respectively. The FCL-ViT surpasses state-of-the-art performance on Continual Learning compared to benchmark methods, while retaining a small number of trainable DNN parameters.","sentences":["Continual Learning (CL) involves adapting the prior Deep Neural Network (DNN) knowledge to new tasks, without forgetting the old ones.","However, modern CL techniques focus on provisioning memory capabilities to existing DNN models rather than designing new ones that are able to adapt according to the task at hand.","This paper presents the novel Feedback Continual Learning Vision Transformer (FCL-ViT) that uses a feedback mechanism to generate real-time dynamic attention features tailored to the current task.","The FCL-ViT operates in two Phases.","In phase 1, the generic image features are produced and determine where the Transformer should attend on the current image.","In phase 2, task-specific image features are generated that leverage dynamic attention.","To this end, Tunable self-Attention Blocks (TABs) and Task Specific Blocks (TSBs) are introduced that operate in both phases and are responsible for tuning the TABs attention, respectively.","The FCL-ViT surpasses state-of-the-art performance on Continual Learning compared to benchmark methods, while retaining a small number of trainable DNN parameters."],"url":"http://arxiv.org/abs/2412.02509v1"}
{"created":"2024-12-03 15:39:05","title":"Towards Rich Emotions in 3D Avatars: A Text-to-3D Avatar Generation Benchmark","abstract":"Producing emotionally dynamic 3D facial avatars with text derived from spoken words (Emo3D) has been a pivotal research topic in 3D avatar generation. While progress has been made in general-purpose 3D avatar generation, the exploration of generating emotional 3D avatars remains scarce, primarily due to the complexities of identifying and rendering rich emotions from spoken words. This paper reexamines Emo3D generation and draws inspiration from human processes, breaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping (T3DEM) and 3D Avatar Rendering (3DAR). T3DEM is the most crucial step in determining the quality of Emo3D generation and encompasses three key challenges: Expression Diversity, Emotion-Content Consistency, and Expression Fluidity. To address these challenges, we introduce a novel benchmark to advance research in Emo3D generation. First, we present EmoAva, a large-scale, high-quality dataset for T3DEM, comprising 15,000 text-to-3D expression mappings that characterize the aforementioned three challenges in Emo3D generation. Furthermore, we develop various metrics to effectively evaluate models against these identified challenges. Next, to effectively model the consistency, diversity, and fluidity of human expressions in the T3DEM step, we propose the Continuous Text-to-Expression Generator, which employs an autoregressive Conditional Variational Autoencoder for expression code generation, enhanced with Latent Temporal Attention and Expression-wise Attention mechanisms. Finally, to further enhance the 3DAR step on rendering higher-quality subtle expressions, we present the Globally-informed Gaussian Avatar (GiGA) model. GiGA incorporates a global information mechanism into 3D Gaussian representations, enabling the capture of subtle micro-expressions and seamless transitions between emotional states.","sentences":["Producing emotionally dynamic 3D facial avatars with text derived from spoken words (Emo3D) has been a pivotal research topic in 3D avatar generation.","While progress has been made in general-purpose 3D avatar generation, the exploration of generating emotional 3D avatars remains scarce, primarily due to the complexities of identifying and rendering rich emotions from spoken words.","This paper reexamines Emo3D generation and draws inspiration from human processes, breaking down Emo3D into two cascading steps: Text-to-3D Expression Mapping (T3DEM) and 3D Avatar Rendering (3DAR).","T3DEM is the most crucial step in determining the quality of Emo3D generation and encompasses three key challenges: Expression Diversity, Emotion-Content Consistency, and Expression Fluidity.","To address these challenges, we introduce a novel benchmark to advance research in Emo3D generation.","First, we present EmoAva, a large-scale, high-quality dataset for T3DEM, comprising 15,000 text-to-3D expression mappings that characterize the aforementioned three challenges in Emo3D generation.","Furthermore, we develop various metrics to effectively evaluate models against these identified challenges.","Next, to effectively model the consistency, diversity, and fluidity of human expressions in the T3DEM step, we propose the Continuous Text-to-Expression Generator, which employs an autoregressive Conditional Variational Autoencoder for expression code generation, enhanced with Latent Temporal Attention and Expression-wise Attention mechanisms.","Finally, to further enhance the 3DAR step on rendering higher-quality subtle expressions, we present the Globally-informed Gaussian Avatar (GiGA) model.","GiGA incorporates a global information mechanism into 3D Gaussian representations, enabling the capture of subtle micro-expressions and seamless transitions between emotional states."],"url":"http://arxiv.org/abs/2412.02508v1"}
{"created":"2024-12-03 15:34:00","title":"ROVER: A Multi-Season Dataset for Visual SLAM","abstract":"Robust Simultaneous Localization and Mapping (SLAM) is a crucial enabler for autonomous navigation in natural, unstructured environments such as parks and gardens. However, these environments present unique challenges for SLAM due to frequent seasonal changes, varying light conditions, and dense vegetation. These factors often degrade the performance of visual SLAM algorithms originally developed for structured urban environments. To address this gap, we present ROVER, a comprehensive benchmark dataset tailored for evaluating visual SLAM algorithms under diverse environmental conditions and spatial configurations. We captured the dataset with a robotic platform equipped with monocular, stereo, and RGB-D cameras, as well as inertial sensors. It covers 39 recordings across five outdoor locations, collected through all seasons and various lighting scenarios, i.e., day, dusk, and night with and without external lighting. With this novel dataset, we evaluate several traditional and deep learning-based SLAM methods and study their performance in diverse challenging conditions. The results demonstrate that while stereo-inertial and RGB-D configurations generally perform better under favorable lighting and moderate vegetation, most SLAM systems perform poorly in low-light and high-vegetation scenarios, particularly during summer and autumn. Our analysis highlights the need for improved adaptability in visual SLAM algorithms for outdoor applications, as current systems struggle with dynamic environmental factors affecting scale, feature extraction, and trajectory consistency. This dataset provides a solid foundation for advancing visual SLAM research in real-world, natural environments, fostering the development of more resilient SLAM systems for long-term outdoor localization and mapping. The dataset and the code of the benchmark are available under https://iis-esslingen.github.io/rover.","sentences":["Robust Simultaneous Localization and Mapping (SLAM) is a crucial enabler for autonomous navigation in natural, unstructured environments such as parks and gardens.","However, these environments present unique challenges for SLAM due to frequent seasonal changes, varying light conditions, and dense vegetation.","These factors often degrade the performance of visual SLAM algorithms originally developed for structured urban environments.","To address this gap, we present ROVER, a comprehensive benchmark dataset tailored for evaluating visual SLAM algorithms under diverse environmental conditions and spatial configurations.","We captured the dataset with a robotic platform equipped with monocular, stereo, and RGB-D cameras, as well as inertial sensors.","It covers 39 recordings across five outdoor locations, collected through all seasons and various lighting scenarios, i.e., day, dusk, and night with and without external lighting.","With this novel dataset, we evaluate several traditional and deep learning-based SLAM methods and study their performance in diverse challenging conditions.","The results demonstrate that while stereo-inertial and RGB-D configurations generally perform better under favorable lighting and moderate vegetation, most SLAM systems perform poorly in low-light and high-vegetation scenarios, particularly during summer and autumn.","Our analysis highlights the need for improved adaptability in visual SLAM algorithms for outdoor applications, as current systems struggle with dynamic environmental factors affecting scale, feature extraction, and trajectory consistency.","This dataset provides a solid foundation for advancing visual SLAM research in real-world, natural environments, fostering the development of more resilient SLAM systems for long-term outdoor localization and mapping.","The dataset and the code of the benchmark are available under https://iis-esslingen.github.io/rover."],"url":"http://arxiv.org/abs/2412.02506v1"}
{"created":"2024-12-03 15:30:52","title":"CA-MoE: Channel-Adapted MoE for Incremental Weather Forecasting","abstract":"Atmospheric science is intricately connected with other fields, e.g., geography and aerospace. Most existing approaches involve training a joint atmospheric and geographic model from scratch, which incurs significant computational costs and overlooks the potential for incremental learning of weather variables across different domains. In this paper, we introduce incremental learning to weather forecasting and propose a novel structure that allows for the flexible expansion of variables within the model. Specifically, our method presents a Channel-Adapted MoE (CA-MoE) that employs a divide-and-conquer strategy. This strategy assigns variable training tasks to different experts by index embedding and reduces computational complexity through a channel-wise Top-K strategy. Experiments conducted on the widely utilized ERA5 dataset reveal that our method, utilizing only approximately 15\\% of trainable parameters during the incremental stage, attains performance that is on par with state-of-the-art competitors. Notably, in the context of variable incremental experiments, our method demonstrates negligible issues with catastrophic forgetting.","sentences":["Atmospheric science is intricately connected with other fields, e.g., geography and aerospace.","Most existing approaches involve training a joint atmospheric and geographic model from scratch, which incurs significant computational costs and overlooks the potential for incremental learning of weather variables across different domains.","In this paper, we introduce incremental learning to weather forecasting and propose a novel structure that allows for the flexible expansion of variables within the model.","Specifically, our method presents a Channel-Adapted MoE (CA-MoE) that employs a divide-and-conquer strategy.","This strategy assigns variable training tasks to different experts by index embedding and reduces computational complexity through a channel-wise Top-K strategy.","Experiments conducted on the widely utilized ERA5 dataset reveal that our method, utilizing only approximately 15\\% of trainable parameters during the incremental stage, attains performance that is on par with state-of-the-art competitors.","Notably, in the context of variable incremental experiments, our method demonstrates negligible issues with catastrophic forgetting."],"url":"http://arxiv.org/abs/2412.02503v1"}
