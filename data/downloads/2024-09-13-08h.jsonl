{"created":"2024-09-12 17:59:49","title":"DreamHOI: Subject-Driven Generation of 3D Human-Object Interactions with Diffusion Priors","abstract":"We present DreamHOI, a novel method for zero-shot synthesis of human-object interactions (HOIs), enabling a 3D human model to realistically interact with any given object based on a textual description. This task is complicated by the varying categories and geometries of real-world objects and the scarcity of datasets encompassing diverse HOIs. To circumvent the need for extensive data, we leverage text-to-image diffusion models trained on billions of image-caption pairs. We optimize the articulation of a skinned human mesh using Score Distillation Sampling (SDS) gradients obtained from these models, which predict image-space edits. However, directly backpropagating image-space gradients into complex articulation parameters is ineffective due to the local nature of such gradients. To overcome this, we introduce a dual implicit-explicit representation of a skinned mesh, combining (implicit) neural radiance fields (NeRFs) with (explicit) skeleton-driven mesh articulation. During optimization, we transition between implicit and explicit forms, grounding the NeRF generation while refining the mesh articulation. We validate our approach through extensive experiments, demonstrating its effectiveness in generating realistic HOIs.","sentences":["We present DreamHOI, a novel method for zero-shot synthesis of human-object interactions (HOIs), enabling a 3D human model to realistically interact with any given object based on a textual description.","This task is complicated by the varying categories and geometries of real-world objects and the scarcity of datasets encompassing diverse HOIs.","To circumvent the need for extensive data, we leverage text-to-image diffusion models trained on billions of image-caption pairs.","We optimize the articulation of a skinned human mesh using Score Distillation Sampling (SDS) gradients obtained from these models, which predict image-space edits.","However, directly backpropagating image-space gradients into complex articulation parameters is ineffective due to the local nature of such gradients.","To overcome this, we introduce a dual implicit-explicit representation of a skinned mesh, combining (implicit) neural radiance fields (NeRFs) with (explicit) skeleton-driven mesh articulation.","During optimization, we transition between implicit and explicit forms, grounding the NeRF generation while refining the mesh articulation.","We validate our approach through extensive experiments, demonstrating its effectiveness in generating realistic HOIs."],"url":"http://arxiv.org/abs/2409.08278v1"}
{"created":"2024-09-12 17:59:46","title":"Depth on Demand: Streaming Dense Depth from a Low Frame Rate Active Sensor","abstract":"High frame rate and accurate depth estimation plays an important role in several tasks crucial to robotics and automotive perception. To date, this can be achieved through ToF and LiDAR devices for indoor and outdoor applications, respectively. However, their applicability is limited by low frame rate, energy consumption, and spatial sparsity. Depth on Demand (DoD) allows for accurate temporal and spatial depth densification achieved by exploiting a high frame rate RGB sensor coupled with a potentially lower frame rate and sparse active depth sensor. Our proposal jointly enables lower energy consumption and denser shape reconstruction, by significantly reducing the streaming requirements on the depth sensor thanks to its three core stages: i) multi-modal encoding, ii) iterative multi-modal integration, and iii) depth decoding. We present extended evidence assessing the effectiveness of DoD on indoor and outdoor video datasets, covering both environment scanning and automotive perception use cases.","sentences":["High frame rate and accurate depth estimation plays an important role in several tasks crucial to robotics and automotive perception.","To date, this can be achieved through ToF and LiDAR devices for indoor and outdoor applications, respectively.","However, their applicability is limited by low frame rate, energy consumption, and spatial sparsity.","Depth on Demand (DoD) allows for accurate temporal and spatial depth densification achieved by exploiting a high frame rate RGB sensor coupled with a potentially lower frame rate and sparse active depth sensor.","Our proposal jointly enables lower energy consumption and denser shape reconstruction, by significantly reducing the streaming requirements on the depth sensor thanks to its three core stages: i) multi-modal encoding, ii) iterative multi-modal integration, and iii) depth decoding.","We present extended evidence assessing the effectiveness of DoD on indoor and outdoor video datasets, covering both environment scanning and automotive perception use cases."],"url":"http://arxiv.org/abs/2409.08277v1"}
{"created":"2024-09-12 17:59:44","title":"AnySkin: Plug-and-play Skin Sensing for Robotic Touch","abstract":"While tactile sensing is widely accepted as an important and useful sensing modality, its use pales in comparison to other sensory modalities like vision and proprioception. AnySkin addresses the critical challenges that impede the use of tactile sensing -- versatility, replaceability, and data reusability. Building on the simplistic design of ReSkin, and decoupling the sensing electronics from the sensing interface, AnySkin simplifies integration making it as straightforward as putting on a phone case and connecting a charger. Furthermore, AnySkin is the first uncalibrated tactile-sensor with cross-instance generalizability of learned manipulation policies. To summarize, this work makes three key contributions: first, we introduce a streamlined fabrication process and a design tool for creating an adhesive-free, durable and easily replaceable magnetic tactile sensor; second, we characterize slip detection and policy learning with the AnySkin sensor; and third, we demonstrate zero-shot generalization of models trained on one instance of AnySkin to new instances, and compare it with popular existing tactile solutions like DIGIT and ReSkin.https://any-skin.github.io/","sentences":["While tactile sensing is widely accepted as an important and useful sensing modality, its use pales in comparison to other sensory modalities like vision and proprioception.","AnySkin addresses the critical challenges that impede the use of tactile sensing -- versatility, replaceability, and data reusability.","Building on the simplistic design of ReSkin, and decoupling the sensing electronics from the sensing interface, AnySkin simplifies integration making it as straightforward as putting on a phone case and connecting a charger.","Furthermore, AnySkin is the first uncalibrated tactile-sensor with cross-instance generalizability of learned manipulation policies.","To summarize, this work makes three key contributions: first, we introduce a streamlined fabrication process and a design tool for creating an adhesive-free, durable and easily replaceable magnetic tactile sensor; second, we characterize slip detection and policy learning with the AnySkin sensor; and third, we demonstrate zero-shot generalization of models trained on one instance of AnySkin to new instances, and compare it with popular existing tactile solutions like DIGIT and ReSkin.https://any-skin.github.io/"],"url":"http://arxiv.org/abs/2409.08276v1"}
{"created":"2024-09-12 17:59:07","title":"Hand-Object Interaction Pretraining from Videos","abstract":"We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories. We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories. We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions. Generative modeling on this data gives us a task-agnostic base policy. This policy captures a general yet flexible manipulation prior. We empirically demonstrate that finetuning this policy, with both reinforcement learning (RL) and behavior cloning (BC), enables sample-efficient adaptation to downstream tasks and simultaneously improves robustness and generalizability compared to prior approaches. Qualitative experiments are available at: \\url{https://hgaurav2k.github.io/hop/}.","sentences":["We present an approach to learn general robot manipulation priors from 3D hand-object interaction trajectories.","We build a framework to use in-the-wild videos to generate sensorimotor robot trajectories.","We do so by lifting both the human hand and the manipulated object in a shared 3D space and retargeting human motions to robot actions.","Generative modeling on this data gives us a task-agnostic base policy.","This policy captures a general yet flexible manipulation prior.","We empirically demonstrate that finetuning this policy, with both reinforcement learning (RL) and behavior cloning (BC), enables sample-efficient adaptation to downstream tasks and simultaneously improves robustness and generalizability compared to prior approaches.","Qualitative experiments are available at: \\url{https://hgaurav2k.github.io/hop/}."],"url":"http://arxiv.org/abs/2409.08273v1"}
{"created":"2024-09-12 17:59:04","title":"Click2Mask: Local Editing with Dynamic Mask Generation","abstract":"Recent advancements in generative models have revolutionized image generation and editing, making these tasks accessible to non-experts. This paper focuses on local image editing, particularly the task of adding new content to a loosely specified area. Existing methods often require a precise mask or a detailed description of the location, which can be cumbersome and prone to errors. We propose Click2Mask, a novel approach that simplifies the local editing process by requiring only a single point of reference (in addition to the content description). A mask is dynamically grown around this point during a Blended Latent Diffusion (BLD) process, guided by a masked CLIP-based semantic loss. Click2Mask surpasses the limitations of segmentation-based and fine-tuning dependent methods, offering a more user-friendly and contextually accurate solution. Our experiments demonstrate that Click2Mask not only minimizes user effort but also delivers competitive or superior local image manipulation results compared to SoTA methods, according to both human judgement and automatic metrics. Key contributions include the simplification of user input, the ability to freely add objects unconstrained by existing segments, and the integration potential of our dynamic mask approach within other editing methods.","sentences":["Recent advancements in generative models have revolutionized image generation and editing, making these tasks accessible to non-experts.","This paper focuses on local image editing, particularly the task of adding new content to a loosely specified area.","Existing methods often require a precise mask or a detailed description of the location, which can be cumbersome and prone to errors.","We propose Click2Mask, a novel approach that simplifies the local editing process by requiring only a single point of reference (in addition to the content description).","A mask is dynamically grown around this point during a Blended Latent Diffusion (BLD) process, guided by a masked CLIP-based semantic loss.","Click2Mask surpasses the limitations of segmentation-based and fine-tuning dependent methods, offering a more user-friendly and contextually accurate solution.","Our experiments demonstrate that Click2Mask not only minimizes user effort but also delivers competitive or superior local image manipulation results compared to SoTA methods, according to both human judgement and automatic metrics.","Key contributions include the simplification of user input, the ability to freely add objects unconstrained by existing segments, and the integration potential of our dynamic mask approach within other editing methods."],"url":"http://arxiv.org/abs/2409.08272v1"}
{"created":"2024-09-12 17:58:31","title":"DreamBeast: Distilling 3D Fantastical Animals with Part-Aware Knowledge Transfer","abstract":"We present DreamBeast, a novel method based on score distillation sampling (SDS) for generating fantastical 3D animal assets composed of distinct parts. Existing SDS methods often struggle with this generation task due to a limited understanding of part-level semantics in text-to-image diffusion models. While recent diffusion models, such as Stable Diffusion 3, demonstrate a better part-level understanding, they are prohibitively slow and exhibit other common problems associated with single-view diffusion models. DreamBeast overcomes this limitation through a novel part-aware knowledge transfer mechanism. For each generated asset, we efficiently extract part-level knowledge from the Stable Diffusion 3 model into a 3D Part-Affinity implicit representation. This enables us to instantly generate Part-Affinity maps from arbitrary camera views, which we then use to modulate the guidance of a multi-view diffusion model during SDS to create 3D assets of fantastical animals. DreamBeast significantly enhances the quality of generated 3D creatures with user-specified part compositions while reducing computational overhead, as demonstrated by extensive quantitative and qualitative evaluations.","sentences":["We present DreamBeast, a novel method based on score distillation sampling (SDS) for generating fantastical 3D animal assets composed of distinct parts.","Existing SDS methods often struggle with this generation task due to a limited understanding of part-level semantics in text-to-image diffusion models.","While recent diffusion models, such as Stable Diffusion 3, demonstrate a better part-level understanding, they are prohibitively slow and exhibit other common problems associated with single-view diffusion models.","DreamBeast overcomes this limitation through a novel part-aware knowledge transfer mechanism.","For each generated asset, we efficiently extract part-level knowledge from the Stable Diffusion 3 model into a 3D Part-Affinity implicit representation.","This enables us to instantly generate Part-Affinity maps from arbitrary camera views, which we then use to modulate the guidance of a multi-view diffusion model during SDS to create 3D assets of fantastical animals.","DreamBeast significantly enhances the quality of generated 3D creatures with user-specified part compositions while reducing computational overhead, as demonstrated by extensive quantitative and qualitative evaluations."],"url":"http://arxiv.org/abs/2409.08271v1"}
{"created":"2024-09-12 17:58:13","title":"FlashSplat: 2D to 3D Gaussian Splatting Segmentation Solved Optimally","abstract":"This study addresses the challenge of accurately segmenting 3D Gaussian Splatting from 2D masks. Conventional methods often rely on iterative gradient descent to assign each Gaussian a unique label, leading to lengthy optimization and sub-optimal solutions. Instead, we propose a straightforward yet globally optimal solver for 3D-GS segmentation. The core insight of our method is that, with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially a linear function with respect to the labels of each Gaussian. As such, the optimal label assignment can be solved via linear programming in closed form. This solution capitalizes on the alpha blending characteristic of the splatting process for single step optimization. By incorporating the background bias in our objective function, our method shows superior robustness in 3D segmentation against noises. Remarkably, our optimization completes within 30 seconds, about 50$\\times$ faster than the best existing methods. Extensive experiments demonstrate the efficiency and robustness of our method in segmenting various scenes, and its superior performance in downstream tasks such as object removal and inpainting. Demos and code will be available at https://github.com/florinshen/FlashSplat.","sentences":["This study addresses the challenge of accurately segmenting 3D Gaussian Splatting from 2D masks.","Conventional methods often rely on iterative gradient descent to assign each Gaussian a unique label, leading to lengthy optimization and sub-optimal solutions.","Instead, we propose a straightforward yet globally optimal solver for 3D-GS segmentation.","The core insight of our method is that, with a reconstructed 3D-GS scene, the rendering of the 2D masks is essentially a linear function with respect to the labels of each Gaussian.","As such, the optimal label assignment can be solved via linear programming in closed form.","This solution capitalizes on the alpha blending characteristic of the splatting process for single step optimization.","By incorporating the background bias in our objective function, our method shows superior robustness in 3D segmentation against noises.","Remarkably, our optimization completes within 30 seconds, about 50$\\times$ faster than the best existing methods.","Extensive experiments demonstrate the efficiency and robustness of our method in segmenting various scenes, and its superior performance in downstream tasks such as object removal and inpainting.","Demos and code will be available at https://github.com/florinshen/FlashSplat."],"url":"http://arxiv.org/abs/2409.08270v1"}
{"created":"2024-09-12 17:58:07","title":"Touch2Touch: Cross-Modal Tactile Generation for Object Manipulation","abstract":"Today's touch sensors come in many shapes and sizes. This has made it challenging to develop general-purpose touch processing methods since models are generally tied to one specific sensor design. We address this problem by performing cross-modal prediction between touch sensors: given the tactile signal from one sensor, we use a generative model to estimate how the same physical contact would be perceived by another sensor. This allows us to apply sensor-specific methods to the generated signal. We implement this idea by training a diffusion model to translate between the popular GelSlim and Soft Bubble sensors. As a downstream task, we perform in-hand object pose estimation using GelSlim sensors while using an algorithm that operates only on Soft Bubble signals. The dataset, the code, and additional details can be found at https://www.mmintlab.com/research/touch2touch/.","sentences":["Today's touch sensors come in many shapes and sizes.","This has made it challenging to develop general-purpose touch processing methods since models are generally tied to one specific sensor design.","We address this problem by performing cross-modal prediction between touch sensors: given the tactile signal from one sensor, we use a generative model to estimate how the same physical contact would be perceived by another sensor.","This allows us to apply sensor-specific methods to the generated signal.","We implement this idea by training a diffusion model to translate between the popular GelSlim and Soft Bubble sensors.","As a downstream task, we perform in-hand object pose estimation using GelSlim sensors while using an algorithm that operates only on Soft Bubble signals.","The dataset, the code, and additional details can be found at https://www.mmintlab.com/research/touch2touch/."],"url":"http://arxiv.org/abs/2409.08269v1"}
{"created":"2024-09-12 17:57:12","title":"CROSS: A Contributor-Project Interaction Lifecycle Model for Open Source Software","abstract":"Despite the widespread adoption of open source software (OSS), its sustainability remains a critical concern, particularly in light of security vulnerabilities and the often inadequate end-of-service (EoS) processes for OSS projects as they decline. Existing models of OSS community participation, like the Onion model and the episodic contribution model, offer valuable insights but are fundamentally incompatible and fail to provide a comprehensive picture of contributor engagement with OSS projects. This paper addresses these gaps by proposing the CROSS model, a novel contributor-project interaction lifecycle model for open source, which delineates the various lifecycle stages of contributor-project interaction along with the driving and retaining forces pertinent to each stage. By synthesizing existing research on OSS communities, organizational behavior, and human resource development, it explains a range of archetypal cases of contributor engagement and highlights research gaps, especially in EoS/offboarding scenarios. The CROSS model provides a foundation for understanding and enhancing the sustainability of OSS projects, offering a robust foundation for future research and practical application.","sentences":["Despite the widespread adoption of open source software (OSS), its sustainability remains a critical concern, particularly in light of security vulnerabilities and the often inadequate end-of-service (EoS) processes for OSS projects as they decline.","Existing models of OSS community participation, like the Onion model and the episodic contribution model, offer valuable insights but are fundamentally incompatible and fail to provide a comprehensive picture of contributor engagement with OSS projects.","This paper addresses these gaps by proposing the CROSS model, a novel contributor-project interaction lifecycle model for open source, which delineates the various lifecycle stages of contributor-project interaction along with the driving and retaining forces pertinent to each stage.","By synthesizing existing research on OSS communities, organizational behavior, and human resource development, it explains a range of archetypal cases of contributor engagement and highlights research gaps, especially in EoS/offboarding scenarios.","The CROSS model provides a foundation for understanding and enhancing the sustainability of OSS projects, offering a robust foundation for future research and practical application."],"url":"http://arxiv.org/abs/2409.08267v1"}
{"created":"2024-09-12 17:56:43","title":"Windows Agent Arena: Evaluating Multi-Modal OS Agents at Scale","abstract":"Large language models (LLMs) show remarkable potential to act as computer agents, enhancing human productivity and software accessibility in multi-modal tasks that require planning and reasoning. However, measuring agent performance in realistic environments remains a challenge since: (i) most benchmarks are limited to specific modalities or domains (e.g. text-only, web navigation, Q&A, coding) and (ii) full benchmark evaluations are slow (on order of magnitude of days) given the multi-step sequential nature of tasks. To address these challenges, we introduce the Windows Agent Arena: a reproducible, general environment focusing exclusively on the Windows operating system (OS) where agents can operate freely within a real Windows OS and use the same wide range of applications, tools, and web browsers available to human users when solving tasks. We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Windows tasks across representative domains that require agent abilities in planning, screen understanding, and tool usage. Our benchmark is scalable and can be seamlessly parallelized in Azure for a full benchmark evaluation in as little as 20 minutes. To demonstrate Windows Agent Arena's capabilities, we also introduce a new multi-modal agent, Navi. Our agent achieves a success rate of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted human. Navi also demonstrates strong performance on another popular web-based benchmark, Mind2Web. We offer extensive quantitative and qualitative analysis of Navi's performance, and provide insights into the opportunities for future research in agent development and data generation using Windows Agent Arena.   Webpage: https://microsoft.github.io/WindowsAgentArena   Code: https://github.com/microsoft/WindowsAgentArena","sentences":["Large language models (LLMs) show remarkable potential to act as computer agents, enhancing human productivity and software accessibility in multi-modal tasks that require planning and reasoning.","However, measuring agent performance in realistic environments remains a challenge since: (i) most benchmarks are limited to specific modalities or domains (e.g. text-only, web navigation, Q&A, coding) and (ii) full benchmark evaluations are slow (on order of magnitude of days) given the multi-step sequential nature of tasks.","To address these challenges, we introduce the Windows Agent Arena: a reproducible, general environment focusing exclusively on the Windows operating system (OS) where agents can operate freely within a real Windows OS and use the same wide range of applications, tools, and web browsers available to human users when solving tasks.","We adapt the OSWorld framework (Xie et al., 2024) to create 150+ diverse Windows tasks across representative domains that require agent abilities in planning, screen understanding, and tool usage.","Our benchmark is scalable and can be seamlessly parallelized in Azure for a full benchmark evaluation in as little as 20 minutes.","To demonstrate Windows Agent Arena's capabilities, we also introduce a new multi-modal agent, Navi.","Our agent achieves a success rate of 19.5% in the Windows domain, compared to 74.5% performance of an unassisted human.","Navi also demonstrates strong performance on another popular web-based benchmark, Mind2Web.","We offer extensive quantitative and qualitative analysis of Navi's performance, and provide insights into the opportunities for future research in agent development and data generation using Windows Agent Arena.   ","Webpage: https://microsoft.github.io/WindowsAgentArena   Code: https://github.com/microsoft/WindowsAgentArena"],"url":"http://arxiv.org/abs/2409.08264v1"}
{"created":"2024-09-12 17:55:44","title":"Learning incomplete factorization preconditioners for GMRES","abstract":"In this paper, we develop a data-driven approach to generate incomplete LU factorizations of large-scale sparse matrices. The learned approximate factorization is utilized as a preconditioner for the corresponding linear equation system in the GMRES method. Incomplete factorization methods are one of the most commonly applied algebraic preconditioners for sparse linear equation systems and are able to speed up the convergence of Krylov subspace methods. However, they are sensitive to hyper-parameters and might suffer from numerical breakdown or lead to slow convergence when not properly applied. We replace the typically hand-engineered algorithms with a graph neural network based approach that is trained against data to predict an approximate factorization. This allows us to learn preconditioners tailored for a specific problem distribution. We analyze and empirically evaluate different loss functions to train the learned preconditioners and show their effectiveness to decrease the number of GMRES iterations and improve the spectral properties on our synthetic dataset. The code is available at https://github.com/paulhausner/neural-incomplete-factorization.","sentences":["In this paper, we develop a data-driven approach to generate incomplete LU factorizations of large-scale sparse matrices.","The learned approximate factorization is utilized as a preconditioner for the corresponding linear equation system in the GMRES method.","Incomplete factorization methods are one of the most commonly applied algebraic preconditioners for sparse linear equation systems and are able to speed up the convergence of Krylov subspace methods.","However, they are sensitive to hyper-parameters and might suffer from numerical breakdown or lead to slow convergence when not properly applied.","We replace the typically hand-engineered algorithms with a graph neural network based approach that is trained against data to predict an approximate factorization.","This allows us to learn preconditioners tailored for a specific problem distribution.","We analyze and empirically evaluate different loss functions to train the learned preconditioners and show their effectiveness to decrease the number of GMRES iterations and improve the spectral properties on our synthetic dataset.","The code is available at https://github.com/paulhausner/neural-incomplete-factorization."],"url":"http://arxiv.org/abs/2409.08262v1"}
{"created":"2024-09-12 17:55:37","title":"Improving Text-guided Object Inpainting with Semantic Pre-inpainting","abstract":"Recent years have witnessed the success of large text-to-image diffusion models and their remarkable potential to generate high-quality images. The further pursuit of enhancing the editability of images has sparked significant interest in the downstream task of inpainting a novel object described by a text prompt within a designated region in the image. Nevertheless, the problem is not trivial from two aspects: 1) Solely relying on one single U-Net to align text prompt and visual object across all the denoising timesteps is insufficient to generate desired objects; 2) The controllability of object generation is not guaranteed in the intricate sampling space of diffusion model. In this paper, we propose to decompose the typical single-stage object inpainting into two cascaded processes: 1) semantic pre-inpainting that infers the semantic features of desired objects in a multi-modal feature space; 2) high-fieldity object generation in diffusion latent space that pivots on such inpainted semantic features. To achieve this, we cascade a Transformer-based semantic inpainter and an object inpainting diffusion model, leading to a novel CAscaded Transformer-Diffusion (CAT-Diffusion) framework for text-guided object inpainting. Technically, the semantic inpainter is trained to predict the semantic features of the target object conditioning on unmasked context and text prompt. The outputs of the semantic inpainter then act as the informative visual prompts to guide high-fieldity object generation through a reference adapter layer, leading to controllable object inpainting. Extensive evaluations on OpenImages-V6 and MSCOCO validate the superiority of CAT-Diffusion against the state-of-the-art methods. Code is available at \\url{https://github.com/Nnn-s/CATdiffusion}.","sentences":["Recent years have witnessed the success of large text-to-image diffusion models and their remarkable potential to generate high-quality images.","The further pursuit of enhancing the editability of images has sparked significant interest in the downstream task of inpainting a novel object described by a text prompt within a designated region in the image.","Nevertheless, the problem is not trivial from two aspects: 1) Solely relying on one single U-Net to align text prompt and visual object across all the denoising timesteps is insufficient to generate desired objects; 2) The controllability of object generation is not guaranteed in the intricate sampling space of diffusion model.","In this paper, we propose to decompose the typical single-stage object inpainting into two cascaded processes: 1) semantic pre-inpainting that infers the semantic features of desired objects in a multi-modal feature space; 2) high-fieldity object generation in diffusion latent space that pivots on such inpainted semantic features.","To achieve this, we cascade a Transformer-based semantic inpainter and an object inpainting diffusion model, leading to a novel CAscaded Transformer-Diffusion (CAT-Diffusion) framework for text-guided object inpainting.","Technically, the semantic inpainter is trained to predict the semantic features of the target object conditioning on unmasked context and text prompt.","The outputs of the semantic inpainter then act as the informative visual prompts to guide high-fieldity object generation through a reference adapter layer, leading to controllable object inpainting.","Extensive evaluations on OpenImages-V6 and MSCOCO validate the superiority of CAT-Diffusion against the state-of-the-art methods.","Code is available at \\url{https://github.com/Nnn-s/CATdiffusion}."],"url":"http://arxiv.org/abs/2409.08260v1"}
{"created":"2024-09-12 17:55:11","title":"Improving Virtual Try-On with Garment-focused Diffusion Models","abstract":"Diffusion models have led to the revolutionizing of generative modeling in numerous image synthesis tasks. Nevertheless, it is not trivial to directly apply diffusion models for synthesizing an image of a target person wearing a given in-shop garment, i.e., image-based virtual try-on (VTON) task. The difficulty originates from the aspect that the diffusion process should not only produce holistically high-fidelity photorealistic image of the target person, but also locally preserve every appearance and texture detail of the given garment. To address this, we shape a new Diffusion model, namely GarDiff, which triggers the garment-focused diffusion process with amplified guidance of both basic visual appearance and detailed textures (i.e., high-frequency details) derived from the given garment. GarDiff first remoulds a pre-trained latent diffusion model with additional appearance priors derived from the CLIP and VAE encodings of the reference garment. Meanwhile, a novel garment-focused adapter is integrated into the UNet of diffusion model, pursuing local fine-grained alignment with the visual appearance of reference garment and human pose. We specifically design an appearance loss over the synthesized garment to enhance the crucial, high-frequency details. Extensive experiments on VITON-HD and DressCode datasets demonstrate the superiority of our GarDiff when compared to state-of-the-art VTON approaches. Code is publicly available at: \\href{https://github.com/siqi0905/GarDiff/tree/master}{https://github.com/siqi0905/GarDiff/tree/master}.","sentences":["Diffusion models have led to the revolutionizing of generative modeling in numerous image synthesis tasks.","Nevertheless, it is not trivial to directly apply diffusion models for synthesizing an image of a target person wearing a given in-shop garment, i.e., image-based virtual try-on (VTON) task.","The difficulty originates from the aspect that the diffusion process should not only produce holistically high-fidelity photorealistic image of the target person, but also locally preserve every appearance and texture detail of the given garment.","To address this, we shape a new Diffusion model, namely GarDiff, which triggers the garment-focused diffusion process with amplified guidance of both basic visual appearance and detailed textures (i.e., high-frequency details) derived from the given garment.","GarDiff first remoulds a pre-trained latent diffusion model with additional appearance priors derived from the CLIP and VAE encodings of the reference garment.","Meanwhile, a novel garment-focused adapter is integrated into the UNet of diffusion model, pursuing local fine-grained alignment with the visual appearance of reference garment and human pose.","We specifically design an appearance loss over the synthesized garment to enhance the crucial, high-frequency details.","Extensive experiments on VITON-HD and DressCode datasets demonstrate the superiority of our GarDiff when compared to state-of-the-art VTON approaches.","Code is publicly available at: \\href{https://github.com/siqi0905/GarDiff/tree/master}{https://github.com/siqi0905/GarDiff/tree/master}."],"url":"http://arxiv.org/abs/2409.08258v1"}
{"created":"2024-09-12 17:51:25","title":"LoRID: Low-Rank Iterative Diffusion for Adversarial Purification","abstract":"This work presents an information-theoretic examination of diffusion-based purification methods, the state-of-the-art adversarial defenses that utilize diffusion models to remove malicious perturbations in adversarial examples. By theoretically characterizing the inherent purification errors associated with the Markov-based diffusion purifications, we introduce LoRID, a novel Low-Rank Iterative Diffusion purification method designed to remove adversarial perturbation with low intrinsic purification errors. LoRID centers around a multi-stage purification process that leverages multiple rounds of diffusion-denoising loops at the early time-steps of the diffusion models, and the integration of Tucker decomposition, an extension of matrix factorization, to remove adversarial noise at high-noise regimes. Consequently, LoRID increases the effective diffusion time-steps and overcomes strong adversarial attacks, achieving superior robustness performance in CIFAR-10/100, CelebA-HQ, and ImageNet datasets under both white-box and black-box settings.","sentences":["This work presents an information-theoretic examination of diffusion-based purification methods, the state-of-the-art adversarial defenses that utilize diffusion models to remove malicious perturbations in adversarial examples.","By theoretically characterizing the inherent purification errors associated with the Markov-based diffusion purifications, we introduce LoRID, a novel Low-Rank Iterative Diffusion purification method designed to remove adversarial perturbation with low intrinsic purification errors.","LoRID centers around a multi-stage purification process that leverages multiple rounds of diffusion-denoising loops at the early time-steps of the diffusion models, and the integration of Tucker decomposition, an extension of matrix factorization, to remove adversarial noise at high-noise regimes.","Consequently, LoRID increases the effective diffusion time-steps and overcomes strong adversarial attacks, achieving superior robustness performance in CIFAR-10/100, CelebA-HQ, and ImageNet datasets under both white-box and black-box settings."],"url":"http://arxiv.org/abs/2409.08255v1"}
{"created":"2024-09-12 17:50:05","title":"The Design of Informative Take-Over Requests for Semi-Autonomous Cyber-Physical Systems: Combining Spoken Language and Visual Icons in a Drone-Controller Setting","abstract":"The question of how cyber-physical systems should interact with human partners that can take over control or exert oversight is becoming more pressing, as these systems are deployed for an ever larger range of tasks. Drawing on the literatures on handing over control during semi-autonomous driving and human-robot interaction, we propose a design of a take-over request that combines an abstract pre-alert with an informative TOR: Relevant sensor information is highlighted on the controller's display, while a spoken message verbalizes the reason for the TOR. We conduct our study in the context of a semi-autonomous drone control scenario as our testbed. The goal of our online study is to assess in more detail what form a language-based TOR should take. Specifically, we compare a full sentence condition to shorter fragments, and test whether the visual highlighting should be done synchronously or asynchronously with the speech. Participants showed a higher accuracy in choosing the correct solution with our bi-modal TOR and felt that they were better able to recognize the critical situation. Using only fragments in the spoken message rather than full sentences did not lead to improved accuracy or faster reactions. Also, synchronizing the visual highlighting with the spoken message did not result in better accuracy and response times were even increased in this condition.","sentences":["The question of how cyber-physical systems should interact with human partners that can take over control or exert oversight is becoming more pressing, as these systems are deployed for an ever larger range of tasks.","Drawing on the literatures on handing over control during semi-autonomous driving and human-robot interaction, we propose a design of a take-over request that combines an abstract pre-alert with an informative TOR: Relevant sensor information is highlighted on the controller's display, while a spoken message verbalizes the reason for the TOR.","We conduct our study in the context of a semi-autonomous drone control scenario as our testbed.","The goal of our online study is to assess in more detail what form a language-based TOR should take.","Specifically, we compare a full sentence condition to shorter fragments, and test whether the visual highlighting should be done synchronously or asynchronously with the speech.","Participants showed a higher accuracy in choosing the correct solution with our bi-modal TOR and felt that they were better able to recognize the critical situation.","Using only fragments in the spoken message rather than full sentences did not lead to improved accuracy or faster reactions.","Also, synchronizing the visual highlighting with the spoken message did not result in better accuracy and response times were even increased in this condition."],"url":"http://arxiv.org/abs/2409.08253v1"}
{"created":"2024-09-12 17:48:22","title":"Dynamic Prompting of Frozen Text-to-Image Diffusion Models for Panoptic Narrative Grounding","abstract":"Panoptic narrative grounding (PNG), whose core target is fine-grained image-text alignment, requires a panoptic segmentation of referred objects given a narrative caption. Previous discriminative methods achieve only weak or coarse-grained alignment by panoptic segmentation pretraining or CLIP model adaptation. Given the recent progress of text-to-image Diffusion models, several works have shown their capability to achieve fine-grained image-text alignment through cross-attention maps and improved general segmentation performance. However, the direct use of phrase features as static prompts to apply frozen Diffusion models to the PNG task still suffers from a large task gap and insufficient vision-language interaction, yielding inferior performance. Therefore, we propose an Extractive-Injective Phrase Adapter (EIPA) bypass within the Diffusion UNet to dynamically update phrase prompts with image features and inject the multimodal cues back, which leverages the fine-grained image-text alignment capability of Diffusion models more sufficiently. In addition, we also design a Multi-Level Mutual Aggregation (MLMA) module to reciprocally fuse multi-level image and phrase features for segmentation refinement. Extensive experiments on the PNG benchmark show that our method achieves new state-of-the-art performance.","sentences":["Panoptic narrative grounding (PNG), whose core target is fine-grained image-text alignment, requires a panoptic segmentation of referred objects given a narrative caption.","Previous discriminative methods achieve only weak or coarse-grained alignment by panoptic segmentation pretraining or CLIP model adaptation.","Given the recent progress of text-to-image Diffusion models, several works have shown their capability to achieve fine-grained image-text alignment through cross-attention maps and improved general segmentation performance.","However, the direct use of phrase features as static prompts to apply frozen Diffusion models to the PNG task still suffers from a large task gap and insufficient vision-language interaction, yielding inferior performance.","Therefore, we propose an Extractive-Injective Phrase Adapter (EIPA) bypass within the Diffusion UNet to dynamically update phrase prompts with image features and inject the multimodal cues back, which leverages the fine-grained image-text alignment capability of Diffusion models more sufficiently.","In addition, we also design a Multi-Level Mutual Aggregation (MLMA) module to reciprocally fuse multi-level image and phrase features for segmentation refinement.","Extensive experiments on the PNG benchmark show that our method achieves new state-of-the-art performance."],"url":"http://arxiv.org/abs/2409.08251v1"}
{"created":"2024-09-12 17:48:08","title":"OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering","abstract":"People often capture memories through photos, screenshots, and videos. While existing AI-based tools enable querying this data using natural language, they mostly only support retrieving individual pieces of information like certain objects in photos and struggle with answering more complex queries that involve interpreting interconnected memories like event sequences. We conducted a one-month diary study to collect realistic user queries and generated a taxonomy of necessary contextual information for integrating with captured memories. We then introduce OmniQuery, a novel system that is able to answer complex personal memory-related questions that require extracting and inferring contextual information. OmniQuery augments single captured memories through integrating scattered contextual information from multiple interconnected memories, retrieves relevant memories, and uses a large language model (LLM) to comprehensive answers. In human evaluations, we show the effectiveness of OmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG system, winning or tying in 74.5% of the time.","sentences":["People often capture memories through photos, screenshots, and videos.","While existing AI-based tools enable querying this data using natural language, they mostly only support retrieving individual pieces of information like certain objects in photos and struggle with answering more complex queries that involve interpreting interconnected memories like event sequences.","We conducted a one-month diary study to collect realistic user queries and generated a taxonomy of necessary contextual information for integrating with captured memories.","We then introduce OmniQuery, a novel system that is able to answer complex personal memory-related questions that require extracting and inferring contextual information.","OmniQuery augments single captured memories through integrating scattered contextual information from multiple interconnected memories, retrieves relevant memories, and uses a large language model (LLM) to comprehensive answers.","In human evaluations, we show the effectiveness of OmniQuery with an accuracy of 71.5%, and it outperformed a conventional RAG system, winning or tying in 74.5% of the time."],"url":"http://arxiv.org/abs/2409.08250v1"}
{"created":"2024-09-12 17:47:56","title":"Quantifying Aleatoric and Epistemic Dynamics Uncertainty via Local Conformal Calibration","abstract":"Whether learned, simulated, or analytical, approximations of a robot's dynamics can be inaccurate when encountering novel environments. Many approaches have been proposed to quantify the aleatoric uncertainty of such methods, i.e. uncertainty resulting from stochasticity, however these estimates alone are not enough to properly estimate the uncertainty of a model in a novel environment, where the actual dynamics can change. Such changes can induce epistemic uncertainty, i.e. uncertainty due to a lack of information/data. Accounting for both epistemic and aleatoric dynamics uncertainty in a theoretically-grounded way remains an open problem. We introduce Local Uncertainty Conformal Calibration (LUCCa), a conformal prediction-based approach that calibrates the aleatoric uncertainty estimates provided by dynamics models to generate probabilistically-valid prediction regions of the system's state. We account for both epistemic and aleatoric uncertainty non-asymptotically, without strong assumptions about the form of the true dynamics or how it changes. The calibration is performed locally in the state-action space, leading to uncertainty estimates that are useful for planning. We validate our method by constructing probabilistically-safe plans for a double-integrator under significant changes in dynamics.","sentences":["Whether learned, simulated, or analytical, approximations of a robot's dynamics can be inaccurate when encountering novel environments.","Many approaches have been proposed to quantify the aleatoric uncertainty of such methods, i.e. uncertainty resulting from stochasticity, however these estimates alone are not enough to properly estimate the uncertainty of a model in a novel environment, where the actual dynamics can change.","Such changes can induce epistemic uncertainty, i.e. uncertainty due to a lack of information/data.","Accounting for both epistemic and aleatoric dynamics uncertainty in a theoretically-grounded way remains an open problem.","We introduce Local Uncertainty Conformal Calibration (LUCCa), a conformal prediction-based approach that calibrates the aleatoric uncertainty estimates provided by dynamics models to generate probabilistically-valid prediction regions of the system's state.","We account for both epistemic and aleatoric uncertainty non-asymptotically, without strong assumptions about the form of the true dynamics or how it changes.","The calibration is performed locally in the state-action space, leading to uncertainty estimates that are useful for planning.","We validate our method by constructing probabilistically-safe plans for a double-integrator under significant changes in dynamics."],"url":"http://arxiv.org/abs/2409.08249v1"}
{"created":"2024-09-12 17:47:51","title":"TextBoost: Towards One-Shot Personalization of Text-to-Image Models via Fine-tuning Text Encoder","abstract":"Recent breakthroughs in text-to-image models have opened up promising research avenues in personalized image generation, enabling users to create diverse images of a specific subject using natural language prompts. However, existing methods often suffer from performance degradation when given only a single reference image. They tend to overfit the input, producing highly similar outputs regardless of the text prompt. This paper addresses the challenge of one-shot personalization by mitigating overfitting, enabling the creation of controllable images through text prompts. Specifically, we propose a selective fine-tuning strategy that focuses on the text encoder. Furthermore, we introduce three key techniques to enhance personalization performance: (1) augmentation tokens to encourage feature disentanglement and alleviate overfitting, (2) a knowledge-preservation loss to reduce language drift and promote generalizability across diverse prompts, and (3) SNR-weighted sampling for efficient training. Extensive experiments demonstrate that our approach efficiently generates high-quality, diverse images using only a single reference image while significantly reducing memory and storage requirements.","sentences":["Recent breakthroughs in text-to-image models have opened up promising research avenues in personalized image generation, enabling users to create diverse images of a specific subject using natural language prompts.","However, existing methods often suffer from performance degradation when given only a single reference image.","They tend to overfit the input, producing highly similar outputs regardless of the text prompt.","This paper addresses the challenge of one-shot personalization by mitigating overfitting, enabling the creation of controllable images through text prompts.","Specifically, we propose a selective fine-tuning strategy that focuses on the text encoder.","Furthermore, we introduce three key techniques to enhance personalization performance: (1) augmentation tokens to encourage feature disentanglement and alleviate overfitting, (2) a knowledge-preservation loss to reduce language drift and promote generalizability across diverse prompts, and (3) SNR-weighted sampling for efficient training.","Extensive experiments demonstrate that our approach efficiently generates high-quality, diverse images using only a single reference image while significantly reducing memory and storage requirements."],"url":"http://arxiv.org/abs/2409.08248v1"}
{"created":"2024-09-12 17:44:07","title":"Style Based Clustering of Visual Artworks","abstract":"Clustering artworks based on style has many potential real-world applications like art recommendations, style-based search and retrieval, and the study of artistic style evolution in an artwork corpus. However, clustering artworks based on style is largely an unaddressed problem. A few present methods for clustering artworks principally rely on generic image feature representations derived from deep neural networks and do not specifically deal with the artistic style. In this paper, we introduce and deliberate over the notion of style-based clustering of visual artworks. Our main objective is to explore neural feature representations and architectures that can be used for style-based clustering and observe their impact and effectiveness. We develop different methods and assess their relative efficacy for style-based clustering through qualitative and quantitative analysis by applying them to four artwork corpora and four curated synthetically styled datasets. Our analysis provides some key novel insights on architectures, feature representations, and evaluation methods suitable for style-based clustering.","sentences":["Clustering artworks based on style has many potential real-world applications like art recommendations, style-based search and retrieval, and the study of artistic style evolution in an artwork corpus.","However, clustering artworks based on style is largely an unaddressed problem.","A few present methods for clustering artworks principally rely on generic image feature representations derived from deep neural networks and do not specifically deal with the artistic style.","In this paper, we introduce and deliberate over the notion of style-based clustering of visual artworks.","Our main objective is to explore neural feature representations and architectures that can be used for style-based clustering and observe their impact and effectiveness.","We develop different methods and assess their relative efficacy for style-based clustering through qualitative and quantitative analysis by applying them to four artwork corpora and four curated synthetically styled datasets.","Our analysis provides some key novel insights on architectures, feature representations, and evaluation methods suitable for style-based clustering."],"url":"http://arxiv.org/abs/2409.08245v1"}
{"created":"2024-09-12 17:41:19","title":"Communication Separations for Truthful Auctions: Breaking the Two-Player Barrier","abstract":"We study the communication complexity of truthful combinatorial auctions, and in particular the case where valuations are either subadditive or single-minded, which we denote with $\\mathsf{SubAdd}\\cup\\mathsf{SingleM}$. We show that for three bidders with valuations in $\\mathsf{SubAdd}\\cup\\mathsf{SingleM}$, any deterministic truthful mechanism that achieves at least a $0.366$-approximation requires $\\exp(m)$ communication. In contrast, a natural extension of [Fei09] yields a non-truthful $\\mathrm{poly}(m)$-communication protocol that achieves a $\\frac{1}{2}$-approximation, demonstrating a gap between the power of truthful mechanisms and non-truthful protocols for this problem.   Our approach follows the taxation complexity framework laid out in [Dob16b], but applies this framework in a setting not encompassed by the techniques used in past work. In particular, the only successful prior application of this framework uses a reduction to simultaneous protocols which only applies for two bidders [AKSW20], whereas our three-player lower bounds are stronger than what can possibly arise from a two-player construction (since a trivial truthful auction guarantees a $\\frac{1}{2}$-approximation for two players).","sentences":["We study the communication complexity of truthful combinatorial auctions, and in particular the case where valuations are either subadditive or single-minded, which we denote with $\\mathsf{SubAdd}\\cup\\mathsf{SingleM}$. We show that for three bidders with valuations in $\\mathsf{SubAdd}\\cup\\mathsf{SingleM}$, any deterministic truthful mechanism that achieves at least a $0.366$-approximation requires $\\exp(m)$ communication.","In contrast, a natural extension of [Fei09] yields a non-truthful $\\mathrm{poly}(m)$-communication protocol that achieves a $\\frac{1}{2}$-approximation, demonstrating a gap between the power of truthful mechanisms and non-truthful protocols for this problem.   ","Our approach follows the taxation complexity framework laid out in [Dob16b], but applies this framework in a setting not encompassed by the techniques used in past work.","In particular, the only successful prior application of this framework uses a reduction to simultaneous protocols which only applies for two bidders","[AKSW20], whereas our three-player lower bounds are stronger than what can possibly arise from a two-player construction (since a trivial truthful auction guarantees a $\\frac{1}{2}$-approximation for two players)."],"url":"http://arxiv.org/abs/2409.08241v1"}
{"created":"2024-09-12 17:39:23","title":"IFAdapter: Instance Feature Control for Grounded Text-to-Image Generation","abstract":"While Text-to-Image (T2I) diffusion models excel at generating visually appealing images of individual instances, they struggle to accurately position and control the features generation of multiple instances. The Layout-to-Image (L2I) task was introduced to address the positioning challenges by incorporating bounding boxes as spatial control signals, but it still falls short in generating precise instance features. In response, we propose the Instance Feature Generation (IFG) task, which aims to ensure both positional accuracy and feature fidelity in generated instances. To address the IFG task, we introduce the Instance Feature Adapter (IFAdapter). The IFAdapter enhances feature depiction by incorporating additional appearance tokens and utilizing an Instance Semantic Map to align instance-level features with spatial locations. The IFAdapter guides the diffusion process as a plug-and-play module, making it adaptable to various community models. For evaluation, we contribute an IFG benchmark and develop a verification pipeline to objectively compare models' abilities to generate instances with accurate positioning and features. Experimental results demonstrate that IFAdapter outperforms other models in both quantitative and qualitative evaluations.","sentences":["While Text-to-Image (T2I) diffusion models excel at generating visually appealing images of individual instances, they struggle to accurately position and control the features generation of multiple instances.","The Layout-to-Image (L2I) task was introduced to address the positioning challenges by incorporating bounding boxes as spatial control signals, but it still falls short in generating precise instance features.","In response, we propose the Instance Feature Generation (IFG) task, which aims to ensure both positional accuracy and feature fidelity in generated instances.","To address the IFG task, we introduce the Instance Feature Adapter (IFAdapter).","The IFAdapter enhances feature depiction by incorporating additional appearance tokens and utilizing an Instance Semantic Map to align instance-level features with spatial locations.","The IFAdapter guides the diffusion process as a plug-and-play module, making it adaptable to various community models.","For evaluation, we contribute an IFG benchmark and develop a verification pipeline to objectively compare models' abilities to generate instances with accurate positioning and features.","Experimental results demonstrate that IFAdapter outperforms other models in both quantitative and qualitative evaluations."],"url":"http://arxiv.org/abs/2409.08240v1"}
{"created":"2024-09-12 17:39:08","title":"Source2Synth: Synthetic Data Generation and Curation Grounded in Real Data Sources","abstract":"Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage. In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations. Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources. Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability. We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA). Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines.","sentences":["Large Language Models still struggle in challenging scenarios that leverage structured data, complex reasoning, or tool usage.","In this paper, we propose Source2Synth: a new method that can be used for teaching LLMs new skills without relying on costly human annotations.","Source2Synth takes as input a custom data source and produces synthetic data points with intermediate reasoning steps grounded in real-world sources.","Source2Synth improves the dataset quality by discarding low-quality generations based on their answerability.","We demonstrate the generality of this approach by applying it to two challenging domains: we test reasoning abilities in multi-hop question answering (MHQA), and tool usage in tabular question answering (TQA).","Our method improves performance by 25.51% for TQA on WikiSQL and 22.57% for MHQA on HotPotQA compared to the fine-tuned baselines."],"url":"http://arxiv.org/abs/2409.08239v1"}
{"created":"2024-09-12 17:36:26","title":"Multi-Model based Federated Learning Against Model Poisoning Attack: A Deep Learning Based Model Selection for MEC Systems","abstract":"Federated Learning (FL) enables training of a global model from distributed data, while preserving data privacy. However, the singular-model based operation of FL is open with uploading poisoned models compatible with the global model structure and can be exploited as a vulnerability to conduct model poisoning attacks. This paper proposes a multi-model based FL as a proactive mechanism to enhance the opportunity of model poisoning attack mitigation. A master model is trained by a set of slave models. To enhance the opportunity of attack mitigation, the structure of client models dynamically change within learning epochs, and the supporter FL protocol is provided. For a MEC system, the model selection problem is modeled as an optimization to minimize loss and recognition time, while meeting a robustness confidence. In adaption with dynamic network condition, a deep reinforcement learning based model selection is proposed. For a DDoS attack detection scenario, results illustrate a competitive accuracy gain under poisoning attack with the scenario that the system is without attack, and also a potential of recognition time improvement.","sentences":["Federated Learning (FL) enables training of a global model from distributed data, while preserving data privacy.","However, the singular-model based operation of FL is open with uploading poisoned models compatible with the global model structure and can be exploited as a vulnerability to conduct model poisoning attacks.","This paper proposes a multi-model based FL as a proactive mechanism to enhance the opportunity of model poisoning attack mitigation.","A master model is trained by a set of slave models.","To enhance the opportunity of attack mitigation, the structure of client models dynamically change within learning epochs, and the supporter FL protocol is provided.","For a MEC system, the model selection problem is modeled as an optimization to minimize loss and recognition time, while meeting a robustness confidence.","In adaption with dynamic network condition, a deep reinforcement learning based model selection is proposed.","For a DDoS attack detection scenario, results illustrate a competitive accuracy gain under poisoning attack with the scenario that the system is without attack, and also a potential of recognition time improvement."],"url":"http://arxiv.org/abs/2409.08237v1"}
{"created":"2024-09-12 17:33:06","title":"LLM Honeypot: Leveraging Large Language Models as Advanced Interactive Honeypot Systems","abstract":"The rapid evolution of cyber threats necessitates innovative solutions for detecting and analyzing malicious activity. Honeypots, which are decoy systems designed to lure and interact with attackers, have emerged as a critical component in cybersecurity. In this paper, we present a novel approach to creating realistic and interactive honeypot systems using Large Language Models (LLMs). By fine-tuning a pre-trained open-source language model on a diverse dataset of attacker-generated commands and responses, we developed a honeypot capable of sophisticated engagement with attackers. Our methodology involved several key steps: data collection and processing, prompt engineering, model selection, and supervised fine-tuning to optimize the model's performance. Evaluation through similarity metrics and live deployment demonstrated that our approach effectively generates accurate and informative responses. The results highlight the potential of LLMs to revolutionize honeypot technology, providing cybersecurity professionals with a powerful tool to detect and analyze malicious activity, thereby enhancing overall security infrastructure.","sentences":["The rapid evolution of cyber threats necessitates innovative solutions for detecting and analyzing malicious activity.","Honeypots, which are decoy systems designed to lure and interact with attackers, have emerged as a critical component in cybersecurity.","In this paper, we present a novel approach to creating realistic and interactive honeypot systems using Large Language Models (LLMs).","By fine-tuning a pre-trained open-source language model on a diverse dataset of attacker-generated commands and responses, we developed a honeypot capable of sophisticated engagement with attackers.","Our methodology involved several key steps: data collection and processing, prompt engineering, model selection, and supervised fine-tuning to optimize the model's performance.","Evaluation through similarity metrics and live deployment demonstrated that our approach effectively generates accurate and informative responses.","The results highlight the potential of LLMs to revolutionize honeypot technology, providing cybersecurity professionals with a powerful tool to detect and analyze malicious activity, thereby enhancing overall security infrastructure."],"url":"http://arxiv.org/abs/2409.08234v1"}
{"created":"2024-09-12 17:11:17","title":"Towards Instance-Optimal Euclidean Spanners","abstract":"Euclidean spanners are important geometric objects that have been extensively studied since the 1980s. The two most basic \"compactness'' measures of a Euclidean spanner $E$ are the size (number of edges) $|E|$ and the weight (sum of edge weights) $\\|E\\|$. In this paper, we initiate the study of instance optimal Euclidean spanners. Our results are two-fold.   We demonstrate that the greedy spanner is far from being instance optimal, even when allowing its stretch to grow. More concretely, we design two hard instances of point sets in the plane, where the greedy $(1+x \\epsilon)$-spanner (for basically any parameter $x \\geq 1$) has $\\Omega_x(\\epsilon^{-1/2}) \\cdot |E_\\mathrm{spa}|$ edges and weight $\\Omega_x(\\epsilon^{-1}) \\cdot \\|E_\\mathrm{light}\\|$, where $E_\\mathrm{spa}$ and $E_\\mathrm{light}$ denote the per-instance sparsest and lightest $(1+\\epsilon)$-spanners, respectively, and the $\\Omega_x$ notation suppresses a polynomial dependence on $1/x$.   As our main contribution, we design a new construction of Euclidean spanners, which is inherently different from known constructions, achieving the following bounds: a stretch of $1+\\epsilon\\cdot 2^{O(\\log^*(d/\\epsilon))}$ with $O(1) \\cdot |E_\\mathrm{spa}|$ edges and weight $O(1) \\cdot \\|E_\\mathrm{light}\\|$. In other words, we show that a slight increase to the stretch suffices for obtaining instance optimality up to an absolute constant for both sparsity and lightness. Remarkably, there is only a log-star dependence on the dimension in the stretch, and there is no dependence on it whatsoever in the number of edges and weight.","sentences":["Euclidean spanners are important geometric objects that have been extensively studied since the 1980s.","The two most basic \"compactness'' measures of a Euclidean spanner $E$ are the size (number of edges) $|E|$ and the weight (sum of edge weights) $\\|E\\|$. In this paper, we initiate the study of instance optimal Euclidean spanners.","Our results are two-fold.   ","We demonstrate that the greedy spanner is far from being instance optimal, even when allowing its stretch to grow.","More concretely, we design two hard instances of point sets in the plane, where the greedy $(1+x \\epsilon)$-spanner (for basically any parameter $x \\geq 1$) has $\\Omega_x(\\epsilon^{-1/2})","\\cdot |E_\\mathrm{spa}|$ edges and weight $\\Omega_x(\\epsilon^{-1}) \\cdot \\|E_\\mathrm{light}\\|$, where $E_\\mathrm{spa}$ and $E_\\mathrm{light}$ denote the per-instance sparsest and lightest $(1+\\epsilon)$-spanners, respectively, and the $\\Omega_x$ notation suppresses a polynomial dependence on $1/x$.   ","As our main contribution, we design a new construction of Euclidean spanners, which is inherently different from known constructions, achieving the following bounds: a stretch of $1+\\epsilon\\cdot 2^{O(\\log^*(d/\\epsilon))}$ with $O(1) \\cdot |E_\\mathrm{spa}|$ edges and weight $O(1) \\cdot \\|E_\\mathrm{light}\\|$.","In other words, we show that a slight increase to the stretch suffices for obtaining instance optimality up to an absolute constant for both sparsity and lightness.","Remarkably, there is only a log-star dependence on the dimension in the stretch, and there is no dependence on it whatsoever in the number of edges and weight."],"url":"http://arxiv.org/abs/2409.08227v1"}
{"created":"2024-09-12 17:08:41","title":"Exploring Use and Perceptions of Generative AI Art Tools by Blind Artists","abstract":"The paper explores the intersection of AI art and blindness, as existing AI research has primarily focused on AI art's reception and impact, on sighted artists and consumers. To address this gap, the researcher interviewed six blind artists from various visual art mediums and levels of blindness about the generative AI image platform Midjourney. The participants shared text prompts and discussed their reactions to the generated images with the sighted researcher. The findings highlight blind artists' interest in AI images as a collaborative tool but express concerns about cultural perceptions and labeling of AI-generated art. They also underscore unique challenges, such as potential misunderstandings and stereotypes about blindness leading to exclusion. The study advocates for greater inclusion of blind individuals in AI art, emphasizing the need to address their specific needs and experiences in developing AI art technologies.","sentences":["The paper explores the intersection of AI art and blindness, as existing AI research has primarily focused on AI art's reception and impact, on sighted artists and consumers.","To address this gap, the researcher interviewed six blind artists from various visual art mediums and levels of blindness about the generative AI image platform Midjourney.","The participants shared text prompts and discussed their reactions to the generated images with the sighted researcher.","The findings highlight blind artists' interest in AI images as a collaborative tool but express concerns about cultural perceptions and labeling of AI-generated art.","They also underscore unique challenges, such as potential misunderstandings and stereotypes about blindness leading to exclusion.","The study advocates for greater inclusion of blind individuals in AI art, emphasizing the need to address their specific needs and experiences in developing AI art technologies."],"url":"http://arxiv.org/abs/2409.08226v1"}
{"created":"2024-09-12 17:03:28","title":"Multi-Robot Coordination Induced in Hazardous Environments through an Adversarial Graph-Traversal Game","abstract":"This paper presents a game theoretic formulation of a graph traversal problem, with applications to robots moving through hazardous environments in the presence of an adversary, as in military and security applications. The blue team of robots moves in an environment modeled by a time-varying graph, attempting to reach some goal with minimum cost, while the red team controls how the graph changes to maximize the cost. The problem is formulated as a stochastic game, so that Nash equilibrium strategies can be computed numerically. Bounds are provided for the game value, with a guarantee that it solves the original problem. Numerical simulations demonstrate the results and the effectiveness of this method, particularly showing the benefit of mixing actions for both players, as well as beneficial coordinated behavior, where blue robots split up and/or synchronize to traverse risky edges.","sentences":["This paper presents a game theoretic formulation of a graph traversal problem, with applications to robots moving through hazardous environments in the presence of an adversary, as in military and security applications.","The blue team of robots moves in an environment modeled by a time-varying graph, attempting to reach some goal with minimum cost, while the red team controls how the graph changes to maximize the cost.","The problem is formulated as a stochastic game, so that Nash equilibrium strategies can be computed numerically.","Bounds are provided for the game value, with a guarantee that it solves the original problem.","Numerical simulations demonstrate the results and the effectiveness of this method, particularly showing the benefit of mixing actions for both players, as well as beneficial coordinated behavior, where blue robots split up and/or synchronize to traverse risky edges."],"url":"http://arxiv.org/abs/2409.08222v1"}
{"created":"2024-09-12 17:03:15","title":"Tweezers: A Framework for Security Event Detection via Event Attribution-centric Tweet Embedding","abstract":"Twitter is recognized as a crucial platform for the dissemination and gathering of Cyber Threat Intelligence (CTI). Its capability to provide real-time, actionable intelligence makes it an indispensable tool for detecting security events, helping security professionals cope with ever-growing threats. However, the large volume of tweets and inherent noises of human-crafted tweets pose significant challenges in accurately identifying security events. While many studies tried to filter out event-related tweets based on keywords, they are not effective due to their limitation in understanding the semantics of tweets. Another challenge in security event detection from Twitter is the comprehensive coverage of security events. Previous studies emphasized the importance of early detection of security events, but they overlooked the importance of event coverage. To cope with these challenges, in our study, we introduce a novel event attribution-centric tweet embedding method to enable the high precision and coverage of events. Our experiment result shows that the proposed method outperforms existing text and graph-based tweet embedding methods in identifying security events. Leveraging this novel embedding approach, we have developed and implemented a framework, Tweezers, that is applicable to security event detection from Twitter for CTI gathering. This framework has demonstrated its effectiveness, detecting twice as many events compared to established baselines. Additionally, we have showcased two applications, built on Tweezers for the integration and inspection of security events, i.e., security event trend analysis and informative security user identification.","sentences":["Twitter is recognized as a crucial platform for the dissemination and gathering of Cyber Threat Intelligence (CTI).","Its capability to provide real-time, actionable intelligence makes it an indispensable tool for detecting security events, helping security professionals cope with ever-growing threats.","However, the large volume of tweets and inherent noises of human-crafted tweets pose significant challenges in accurately identifying security events.","While many studies tried to filter out event-related tweets based on keywords, they are not effective due to their limitation in understanding the semantics of tweets.","Another challenge in security event detection from Twitter is the comprehensive coverage of security events.","Previous studies emphasized the importance of early detection of security events, but they overlooked the importance of event coverage.","To cope with these challenges, in our study, we introduce a novel event attribution-centric tweet embedding method to enable the high precision and coverage of events.","Our experiment result shows that the proposed method outperforms existing text and graph-based tweet embedding methods in identifying security events.","Leveraging this novel embedding approach, we have developed and implemented a framework, Tweezers, that is applicable to security event detection from Twitter for CTI gathering.","This framework has demonstrated its effectiveness, detecting twice as many events compared to established baselines.","Additionally, we have showcased two applications, built on Tweezers for the integration and inspection of security events, i.e., security event trend analysis and informative security user identification."],"url":"http://arxiv.org/abs/2409.08221v1"}
{"created":"2024-09-12 17:01:03","title":"Graph Inspection for Robotic Motion Planning: Do Arithmetic Circuits Help?","abstract":"We investigate whether algorithms based on arithmetic circuits are a viable alternative to existing solvers for Graph Inspection, a problem with direct application in robotic motion planning. Specifically, we seek to address the high memory usage of existing solvers. Aided by novel theoretical results enabling fast solution recovery, we implement a circuit-based solver for Graph Inspection which uses only polynomial space and test it on several realistic robotic motion planning datasets. In particular, we provide a comprehensive experimental evaluation of a suite of engineered algorithms for three key subroutines. While this evaluation demonstrates that circuit-based methods are not yet practically competitive for our robotics application, it also provides insights which may guide future efforts to bring circuit-based algorithms from theory to practice.","sentences":["We investigate whether algorithms based on arithmetic circuits are a viable alternative to existing solvers for Graph Inspection, a problem with direct application in robotic motion planning.","Specifically, we seek to address the high memory usage of existing solvers.","Aided by novel theoretical results enabling fast solution recovery, we implement a circuit-based solver for Graph Inspection which uses only polynomial space and test it on several realistic robotic motion planning datasets.","In particular, we provide a comprehensive experimental evaluation of a suite of engineered algorithms for three key subroutines.","While this evaluation demonstrates that circuit-based methods are not yet practically competitive for our robotics application, it also provides insights which may guide future efforts to bring circuit-based algorithms from theory to practice."],"url":"http://arxiv.org/abs/2409.08219v1"}
{"created":"2024-09-12 16:56:26","title":"CliquePH: Higher-Order Information for Graph Neural Networks through Persistent Homology on Clique Graphs","abstract":"Graph neural networks have become the default choice by practitioners for graph learning tasks such as graph classification and node classification. Nevertheless, popular graph neural network models still struggle to capture higher-order information, i.e., information that goes \\emph{beyond} pairwise interactions. Recent work has shown that persistent homology, a tool from topological data analysis, can enrich graph neural networks with topological information that they otherwise could not capture. Calculating such features is efficient for dimension 0 (connected components) and dimension 1 (cycles). However, when it comes to higher-order structures, it does not scale well, with a complexity of $O(n^d)$, where $n$ is the number of nodes and $d$ is the order of the structures. In this work, we introduce a novel method that extracts information about higher-order structures in the graph while still using the efficient low-dimensional persistent homology algorithm. On standard benchmark datasets, we show that our method can lead to up to $31\\%$ improvements in test accuracy.","sentences":["Graph neural networks have become the default choice by practitioners for graph learning tasks such as graph classification and node classification.","Nevertheless, popular graph neural network models still struggle to capture higher-order information, i.e., information that goes \\emph{beyond} pairwise interactions.","Recent work has shown that persistent homology, a tool from topological data analysis, can enrich graph neural networks with topological information that they otherwise could not capture.","Calculating such features is efficient for dimension 0 (connected components) and dimension 1 (cycles).","However, when it comes to higher-order structures, it does not scale well, with a complexity of $O(n^d)$, where $n$ is the number of nodes and $d$ is the order of the structures.","In this work, we introduce a novel method that extracts information about higher-order structures in the graph while still using the efficient low-dimensional persistent homology algorithm.","On standard benchmark datasets, we show that our method can lead to up to $31\\%$ improvements in test accuracy."],"url":"http://arxiv.org/abs/2409.08217v1"}
{"created":"2024-09-12 16:55:51","title":"LT3SD: Latent Trees for 3D Scene Diffusion","abstract":"We present LT3SD, a novel latent diffusion model for large-scale 3D scene generation. Recent advances in diffusion models have shown impressive results in 3D object generation, but are limited in spatial extent and quality when extended to 3D scenes. To generate complex and diverse 3D scene structures, we introduce a latent tree representation to effectively encode both lower-frequency geometry and higher-frequency detail in a coarse-to-fine hierarchy. We can then learn a generative diffusion process in this latent 3D scene space, modeling the latent components of a scene at each resolution level. To synthesize large-scale scenes with varying sizes, we train our diffusion model on scene patches and synthesize arbitrary-sized output 3D scenes through shared diffusion generation across multiple scene patches. Through extensive experiments, we demonstrate the efficacy and benefits of LT3SD for large-scale, high-quality unconditional 3D scene generation and for probabilistic completion for partial scene observations.","sentences":["We present LT3SD, a novel latent diffusion model for large-scale 3D scene generation.","Recent advances in diffusion models have shown impressive results in 3D object generation, but are limited in spatial extent and quality when extended to 3D scenes.","To generate complex and diverse 3D scene structures, we introduce a latent tree representation to effectively encode both lower-frequency geometry and higher-frequency detail in a coarse-to-fine hierarchy.","We can then learn a generative diffusion process in this latent 3D scene space, modeling the latent components of a scene at each resolution level.","To synthesize large-scale scenes with varying sizes, we train our diffusion model on scene patches and synthesize arbitrary-sized output 3D scenes through shared diffusion generation across multiple scene patches.","Through extensive experiments, we demonstrate the efficacy and benefits of LT3SD for large-scale, high-quality unconditional 3D scene generation and for probabilistic completion for partial scene observations."],"url":"http://arxiv.org/abs/2409.08215v1"}
{"created":"2024-09-12 16:51:58","title":"Adaptive Language-Guided Abstraction from Contrastive Explanations","abstract":"Many approaches to robot learning begin by inferring a reward function from a set of human demonstrations. To learn a good reward, it is necessary to determine which features of the environment are relevant before determining how these features should be used to compute reward. End-to-end methods for joint feature and reward learning (e.g., using deep networks or program synthesis techniques) often yield brittle reward functions that are sensitive to spurious state features. By contrast, humans can often generalizably learn from a small number of demonstrations by incorporating strong priors about what features of a demonstration are likely meaningful for a task of interest. How do we build robots that leverage this kind of background knowledge when learning from new demonstrations? This paper describes a method named ALGAE (Adaptive Language-Guided Abstraction from [Contrastive] Explanations) which alternates between using language models to iteratively identify human-meaningful features needed to explain demonstrated behavior, then standard inverse reinforcement learning techniques to assign weights to these features. Experiments across a variety of both simulated and real-world robot environments show that ALGAE learns generalizable reward functions defined on interpretable features using only small numbers of demonstrations. Importantly, ALGAE can recognize when features are missing, then extract and define those features without any human input -- making it possible to quickly and efficiently acquire rich representations of user behavior.","sentences":["Many approaches to robot learning begin by inferring a reward function from a set of human demonstrations.","To learn a good reward, it is necessary to determine which features of the environment are relevant before determining how these features should be used to compute reward.","End-to-end methods for joint feature and reward learning (e.g., using deep networks or program synthesis techniques) often yield brittle reward functions that are sensitive to spurious state features.","By contrast, humans can often generalizably learn from a small number of demonstrations by incorporating strong priors about what features of a demonstration are likely meaningful for a task of interest.","How do we build robots that leverage this kind of background knowledge when learning from new demonstrations?","This paper describes a method named ALGAE (Adaptive Language-Guided Abstraction from [Contrastive] Explanations) which alternates between using language models to iteratively identify human-meaningful features needed to explain demonstrated behavior, then standard inverse reinforcement learning techniques to assign weights to these features.","Experiments across a variety of both simulated and real-world robot environments show that ALGAE learns generalizable reward functions defined on interpretable features using only small numbers of demonstrations.","Importantly, ALGAE can recognize when features are missing, then extract and define those features without any human input -- making it possible to quickly and efficiently acquire rich representations of user behavior."],"url":"http://arxiv.org/abs/2409.08212v1"}
{"created":"2024-09-12 16:51:55","title":"Graph Laplacian-based Bayesian Multi-fidelity Modeling","abstract":"We present a novel probabilistic approach for generating multi-fidelity data while accounting for errors inherent in both low- and high-fidelity data. In this approach a graph Laplacian constructed from the low-fidelity data is used to define a multivariate Gaussian prior density for the coordinates of the true data points. In addition, few high-fidelity data points are used to construct a conjugate likelihood term. Thereafter, Bayes rule is applied to derive an explicit expression for the posterior density which is also multivariate Gaussian. The maximum \\textit{a posteriori} (MAP) estimate of this density is selected to be the optimal multi-fidelity estimate. It is shown that the MAP estimate and the covariance of the posterior density can be determined through the solution of linear systems of equations. Thereafter, two methods, one based on spectral truncation and another based on a low-rank approximation, are developed to solve these equations efficiently. The multi-fidelity approach is tested on a variety of problems in solid and fluid mechanics with data that represents vectors of quantities of interest and discretized spatial fields in one and two dimensions. The results demonstrate that by utilizing a small fraction of high-fidelity data, the multi-fidelity approach can significantly improve the accuracy of a large collection of low-fidelity data points.","sentences":["We present a novel probabilistic approach for generating multi-fidelity data while accounting for errors inherent in both low- and high-fidelity data.","In this approach a graph Laplacian constructed from the low-fidelity data is used to define a multivariate Gaussian prior density for the coordinates of the true data points.","In addition, few high-fidelity data points are used to construct a conjugate likelihood term.","Thereafter, Bayes rule is applied to derive an explicit expression for the posterior density which is also multivariate Gaussian.","The maximum \\textit{a posteriori} (MAP) estimate of this density is selected to be the optimal multi-fidelity estimate.","It is shown that the MAP estimate and the covariance of the posterior density can be determined through the solution of linear systems of equations.","Thereafter, two methods, one based on spectral truncation and another based on a low-rank approximation, are developed to solve these equations efficiently.","The multi-fidelity approach is tested on a variety of problems in solid and fluid mechanics with data that represents vectors of quantities of interest and discretized spatial fields in one and two dimensions.","The results demonstrate that by utilizing a small fraction of high-fidelity data, the multi-fidelity approach can significantly improve the accuracy of a large collection of low-fidelity data points."],"url":"http://arxiv.org/abs/2409.08211v1"}
{"created":"2024-09-12 16:47:57","title":"VI3DRM:Towards meticulous 3D Reconstruction from Sparse Views via Photo-Realistic Novel View Synthesis","abstract":"Recently, methods like Zero-1-2-3 have focused on single-view based 3D reconstruction and have achieved remarkable success. However, their predictions for unseen areas heavily rely on the inductive bias of large-scale pretrained diffusion models. Although subsequent work, such as DreamComposer, attempts to make predictions more controllable by incorporating additional views, the results remain unrealistic due to feature entanglement in the vanilla latent space, including factors such as lighting, material, and structure. To address these issues, we introduce the Visual Isotropy 3D Reconstruction Model (VI3DRM), a diffusion-based sparse views 3D reconstruction model that operates within an ID consistent and perspective-disentangled 3D latent space. By facilitating the disentanglement of semantic information, color, material properties and lighting, VI3DRM is capable of generating highly realistic images that are indistinguishable from real photographs. By leveraging both real and synthesized images, our approach enables the accurate construction of pointmaps, ultimately producing finely textured meshes or point clouds. On the NVS task, tested on the GSO dataset, VI3DRM significantly outperforms state-of-the-art method DreamComposer, achieving a PSNR of 38.61, an SSIM of 0.929, and an LPIPS of 0.027. Code will be made available upon publication.","sentences":["Recently, methods like Zero-1-2-3 have focused on single-view based 3D reconstruction and have achieved remarkable success.","However, their predictions for unseen areas heavily rely on the inductive bias of large-scale pretrained diffusion models.","Although subsequent work, such as DreamComposer, attempts to make predictions more controllable by incorporating additional views, the results remain unrealistic due to feature entanglement in the vanilla latent space, including factors such as lighting, material, and structure.","To address these issues, we introduce the Visual Isotropy 3D Reconstruction Model (VI3DRM), a diffusion-based sparse views 3D reconstruction model that operates within an ID consistent and perspective-disentangled 3D latent space.","By facilitating the disentanglement of semantic information, color, material properties and lighting, VI3DRM is capable of generating highly realistic images that are indistinguishable from real photographs.","By leveraging both real and synthesized images, our approach enables the accurate construction of pointmaps, ultimately producing finely textured meshes or point clouds.","On the NVS task, tested on the GSO dataset, VI3DRM significantly outperforms state-of-the-art method DreamComposer, achieving a PSNR of 38.61, an SSIM of 0.929, and an LPIPS of 0.027.","Code will be made available upon publication."],"url":"http://arxiv.org/abs/2409.08207v1"}
{"created":"2024-09-12 16:46:41","title":"ComAlign: Compositional Alignment in Vision-Language Models","abstract":"Vision-language models (VLMs) like CLIP have showcased a remarkable ability to extract transferable features for downstream tasks. Nonetheless, the training process of these models is usually based on a coarse-grained contrastive loss between the global embedding of images and texts which may lose the compositional structure of these modalities. Many recent studies have shown VLMs lack compositional understandings like attribute binding and identifying object relationships. Although some recent methods have tried to achieve finer-level alignments, they either are not based on extracting meaningful components of proper granularity or don't properly utilize the modalities' correspondence (especially in image-text pairs with more ingredients). Addressing these limitations, we introduce Compositional Alignment (ComAlign), a fine-grained approach to discover more exact correspondence of text and image components using only the weak supervision in the form of image-text pairs. Our methodology emphasizes that the compositional structure (including entities and relations) extracted from the text modality must also be retained in the image modality. To enforce correspondence of fine-grained concepts in image and text modalities, we train a lightweight network lying on top of existing visual and language encoders using a small dataset. The network is trained to align nodes and edges of the structure across the modalities. Experimental results on various VLMs and datasets demonstrate significant improvements in retrieval and compositional benchmarks, affirming the effectiveness of our plugin model.","sentences":["Vision-language models (VLMs) like CLIP have showcased a remarkable ability to extract transferable features for downstream tasks.","Nonetheless, the training process of these models is usually based on a coarse-grained contrastive loss between the global embedding of images and texts which may lose the compositional structure of these modalities.","Many recent studies have shown VLMs lack compositional understandings like attribute binding and identifying object relationships.","Although some recent methods have tried to achieve finer-level alignments, they either are not based on extracting meaningful components of proper granularity or don't properly utilize the modalities' correspondence (especially in image-text pairs with more ingredients).","Addressing these limitations, we introduce Compositional Alignment (ComAlign), a fine-grained approach to discover more exact correspondence of text and image components using only the weak supervision in the form of image-text pairs.","Our methodology emphasizes that the compositional structure (including entities and relations) extracted from the text modality must also be retained in the image modality.","To enforce correspondence of fine-grained concepts in image and text modalities, we train a lightweight network lying on top of existing visual and language encoders using a small dataset.","The network is trained to align nodes and edges of the structure across the modalities.","Experimental results on various VLMs and datasets demonstrate significant improvements in retrieval and compositional benchmarks, affirming the effectiveness of our plugin model."],"url":"http://arxiv.org/abs/2409.08206v1"}
{"created":"2024-09-12 16:41:47","title":"What Makes a Maze Look Like a Maze?","abstract":"A unique aspect of human visual understanding is the ability to flexibly interpret abstract concepts: acquiring lifted rules explaining what they symbolize, grounding them across familiar and unfamiliar contexts, and making predictions or reasoning about them. While off-the-shelf vision-language models excel at making literal interpretations of images (e.g., recognizing object categories such as tree branches), they still struggle to make sense of such visual abstractions (e.g., how an arrangement of tree branches may form the walls of a maze). To address this challenge, we introduce Deep Schema Grounding (DSG), a framework that leverages explicit structured representations of visual abstractions for grounding and reasoning. At the core of DSG are schemas--dependency graph descriptions of abstract concepts that decompose them into more primitive-level symbols. DSG uses large language models to extract schemas, then hierarchically grounds concrete to abstract components of the schema onto images with vision-language models. The grounded schema is used to augment visual abstraction understanding. We systematically evaluate DSG and different methods in reasoning on our new Visual Abstractions Dataset, which consists of diverse, real-world images of abstract concepts and corresponding question-answer pairs labeled by humans. We show that DSG significantly improves the abstract visual reasoning performance of vision-language models, and is a step toward human-aligned understanding of visual abstractions.","sentences":["A unique aspect of human visual understanding is the ability to flexibly interpret abstract concepts: acquiring lifted rules explaining what they symbolize, grounding them across familiar and unfamiliar contexts, and making predictions or reasoning about them.","While off-the-shelf vision-language models excel at making literal interpretations of images (e.g., recognizing object categories such as tree branches), they still struggle to make sense of such visual abstractions (e.g., how an arrangement of tree branches may form the walls of a maze).","To address this challenge, we introduce Deep Schema Grounding (DSG), a framework that leverages explicit structured representations of visual abstractions for grounding and reasoning.","At the core of DSG are schemas--dependency graph descriptions of abstract concepts that decompose them into more primitive-level symbols.","DSG uses large language models to extract schemas, then hierarchically grounds concrete to abstract components of the schema onto images with vision-language models.","The grounded schema is used to augment visual abstraction understanding.","We systematically evaluate DSG and different methods in reasoning on our new Visual Abstractions Dataset, which consists of diverse, real-world images of abstract concepts and corresponding question-answer pairs labeled by humans.","We show that DSG significantly improves the abstract visual reasoning performance of vision-language models, and is a step toward human-aligned understanding of visual abstractions."],"url":"http://arxiv.org/abs/2409.08202v1"}
{"created":"2024-09-12 16:38:20","title":"Machine Learning for Two-Sample Testing under Right-Censored Data: A Simulation Study","abstract":"The focus of this study is to evaluate the effectiveness of Machine Learning (ML) methods for two-sample testing with right-censored observations. To achieve this, we develop several ML-based methods with varying architectures and implement them as two-sample tests. Each method is an ensemble (stacking) that combines predictions from classical two-sample tests. This paper presents the results of training the proposed ML methods, examines their statistical power compared to classical two-sample tests, analyzes the distribution of test statistics for the proposed methods when the null hypothesis is true, and evaluates the significance of the features incorporated into the proposed methods. All results from numerical experiments were obtained from a synthetic dataset generated using the Smirnov transform (Inverse Transform Sampling) and replicated multiple times through Monte Carlo simulation. To test the two-sample problem with right-censored observations, one can use the proposed two-sample methods. All necessary materials (source code, example scripts, dataset, and samples) are available on GitHub and Hugging Face.","sentences":["The focus of this study is to evaluate the effectiveness of Machine Learning (ML) methods for two-sample testing with right-censored observations.","To achieve this, we develop several ML-based methods with varying architectures and implement them as two-sample tests.","Each method is an ensemble (stacking) that combines predictions from classical two-sample tests.","This paper presents the results of training the proposed ML methods, examines their statistical power compared to classical two-sample tests, analyzes the distribution of test statistics for the proposed methods when the null hypothesis is true, and evaluates the significance of the features incorporated into the proposed methods.","All results from numerical experiments were obtained from a synthetic dataset generated using the Smirnov transform (Inverse Transform Sampling) and replicated multiple times through Monte Carlo simulation.","To test the two-sample problem with right-censored observations, one can use the proposed two-sample methods.","All necessary materials (source code, example scripts, dataset, and samples) are available on GitHub and Hugging Face."],"url":"http://arxiv.org/abs/2409.08201v1"}
{"created":"2024-09-12 16:36:39","title":"AudioBERT: Audio Knowledge Augmented Language Model","abstract":"Recent studies have identified that language models, pretrained on text-only datasets, often lack elementary visual knowledge, \\textit{e.g.,} colors of everyday objects. Motivated by this observation, we ask whether a similar shortcoming exists in terms of the \\textit{auditory} knowledge. To answer this question, we construct a new dataset called AuditoryBench, which consists of two novel tasks for evaluating auditory knowledge. Based on our analysis using the benchmark, we find that language models also suffer from a severe lack of auditory knowledge. To address this limitation, we propose AudioBERT, a novel method to augment the auditory knowledge of BERT through a retrieval-based approach. First, we detect auditory knowledge spans in prompts to query our retrieval model efficiently. Then, we inject audio knowledge into BERT and switch on low-rank adaptation for effective adaptation when audio knowledge is required. Our experiments demonstrate that AudioBERT is quite effective, achieving superior performance on the AuditoryBench. The dataset and code are available at \\bulurl{https://github.com/HJ-Ok/AudioBERT}.","sentences":["Recent studies have identified that language models, pretrained on text-only datasets, often lack elementary visual knowledge, \\textit{e.g.,} colors of everyday objects.","Motivated by this observation, we ask whether a similar shortcoming exists in terms of the \\textit{auditory} knowledge.","To answer this question, we construct a new dataset called AuditoryBench, which consists of two novel tasks for evaluating auditory knowledge.","Based on our analysis using the benchmark, we find that language models also suffer from a severe lack of auditory knowledge.","To address this limitation, we propose AudioBERT, a novel method to augment the auditory knowledge of BERT through a retrieval-based approach.","First, we detect auditory knowledge spans in prompts to query our retrieval model efficiently.","Then, we inject audio knowledge into BERT and switch on low-rank adaptation for effective adaptation when audio knowledge is required.","Our experiments demonstrate that AudioBERT is quite effective, achieving superior performance on the AuditoryBench.","The dataset and code are available at \\bulurl{https://github.com/HJ-Ok/AudioBERT}."],"url":"http://arxiv.org/abs/2409.08199v1"}
{"created":"2024-09-12 16:32:24","title":"Composing Option Sequences by Adaptation: Initial Results","abstract":"Robot manipulation in real-world settings often requires adapting the robot's behavior to the current situation, such as by changing the sequences in which policies execute to achieve the desired task. Problematically, however, we show that composing a novel sequence of five deep RL options to perform a pick-and-place task is unlikely to successfully complete, even if their initiation and termination conditions align. We propose a framework to determine whether sequences will succeed a priori, and examine three approaches that adapt options to sequence successfully if they will not. Crucially, our adaptation methods consider the actual subset of points that the option is trained from or where it ends: (1) trains the second option to start where the first ends; (2) trains the first option to reach the centroid of where the second starts; and (3) trains the first option to reach the median of where the second starts. Our results show that our framework and adaptation methods have promise in adapting options to work in novel sequences.","sentences":["Robot manipulation in real-world settings often requires adapting the robot's behavior to the current situation, such as by changing the sequences in which policies execute to achieve the desired task.","Problematically, however, we show that composing a novel sequence of five deep RL options to perform a pick-and-place task is unlikely to successfully complete, even if their initiation and termination conditions align.","We propose a framework to determine whether sequences will succeed a priori, and examine three approaches that adapt options to sequence successfully if they will not.","Crucially, our adaptation methods consider the actual subset of points that the option is trained from or where it ends: (1) trains the second option to start where the first ends; (2) trains the first option to reach the centroid of where the second starts; and (3) trains the first option to reach the median of where the second starts.","Our results show that our framework and adaptation methods have promise in adapting options to work in novel sequences."],"url":"http://arxiv.org/abs/2409.08195v1"}
{"created":"2024-09-12 16:28:01","title":"A Secure Standard for NFT Fractionalization","abstract":"Non-fungible tokens (NFTs) offer a unique method for representing digital and physical assets on the blockchain. However, the NFT market has recently experienced a downturn in interest, mainly due to challenges related to high entry barriers and limited market liquidity. Fractionalization emerges as a promising solution, allowing multiple parties to hold a stake in a single NFT. By breaking down ownership into fractional shares, this approach lowers the entry barrier for investors, enhances market liquidity, and democratizes access to valuable digital assets. Despite these benefits, the current landscape of NFT fractionalization is fragmented, with no standardized framework to guide the secure and interoperable implementation of fractionalization mechanisms. This paper contributions are twofold: first, we provide a detailed analysis of the current NFT fractionalization landscape focusing on security challenges; second, we introduce a standardized approach that addresses these challenges, paving the way for more secure, interoperable, and accessible NFT fractionalization platforms.","sentences":["Non-fungible tokens (NFTs) offer a unique method for representing digital and physical assets on the blockchain.","However, the NFT market has recently experienced a downturn in interest, mainly due to challenges related to high entry barriers and limited market liquidity.","Fractionalization emerges as a promising solution, allowing multiple parties to hold a stake in a single NFT.","By breaking down ownership into fractional shares, this approach lowers the entry barrier for investors, enhances market liquidity, and democratizes access to valuable digital assets.","Despite these benefits, the current landscape of NFT fractionalization is fragmented, with no standardized framework to guide the secure and interoperable implementation of fractionalization mechanisms.","This paper contributions are twofold: first, we provide a detailed analysis of the current NFT fractionalization landscape focusing on security challenges; second, we introduce a standardized approach that addresses these challenges, paving the way for more secure, interoperable, and accessible NFT fractionalization platforms."],"url":"http://arxiv.org/abs/2409.08190v1"}
{"created":"2024-09-12 16:26:47","title":"Gaussian Garments: Reconstructing Simulation-Ready Clothing with Photorealistic Appearance from Multi-View Video","abstract":"We introduce Gaussian Garments, a novel approach for reconstructing realistic simulation-ready garment assets from multi-view videos. Our method represents garments with a combination of a 3D mesh and a Gaussian texture that encodes both the color and high-frequency surface details. This representation enables accurate registration of garment geometries to multi-view videos and helps disentangle albedo textures from lighting effects. Furthermore, we demonstrate how a pre-trained graph neural network (GNN) can be fine-tuned to replicate the real behavior of each garment. The reconstructed Gaussian Garments can be automatically combined into multi-garment outfits and animated with the fine-tuned GNN.","sentences":["We introduce Gaussian Garments, a novel approach for reconstructing realistic simulation-ready garment assets from multi-view videos.","Our method represents garments with a combination of a 3D mesh and a Gaussian texture that encodes both the color and high-frequency surface details.","This representation enables accurate registration of garment geometries to multi-view videos and helps disentangle albedo textures from lighting effects.","Furthermore, we demonstrate how a pre-trained graph neural network (GNN) can be fine-tuned to replicate the real behavior of each garment.","The reconstructed Gaussian Garments can be automatically combined into multi-garment outfits and animated with the fine-tuned GNN."],"url":"http://arxiv.org/abs/2409.08189v1"}
{"created":"2024-09-12 16:20:57","title":"Fine-tuning Large Language Models for Entity Matching","abstract":"Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and their ability to generalize to unseen entities. Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning. This paper explores the potential of fine-tuning LLMs for entity matching. We analyze fine-tuning along two dimensions: 1) The representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs. In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the model's ability to generalize to other in-domain datasets as well as across topical domains. Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed. Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer. We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o Mini.","sentences":["Generative large language models (LLMs) are a promising alternative to pre-trained language models for entity matching due to their high zero-shot performance and their ability to generalize to unseen entities.","Existing research on using LLMs for entity matching has focused on prompt engineering and in-context learning.","This paper explores the potential of fine-tuning LLMs for entity matching.","We analyze fine-tuning along two dimensions: 1) The representation of training examples, where we experiment with adding different types of LLM-generated explanations to the training set, and 2) the selection and generation of training examples using LLMs.","In addition to the matching performance on the source dataset, we investigate how fine-tuning affects the model's ability to generalize to other in-domain datasets as well as across topical domains.","Our experiments show that fine-tuning significantly improves the performance of the smaller models while the results for the larger models are mixed.","Fine-tuning also improves the generalization to in-domain datasets while hurting cross-domain transfer.","We show that adding structured explanations to the training set has a positive impact on the performance of three out of four LLMs, while the proposed example selection and generation methods only improve the performance of Llama 3.1 8B while decreasing the performance of GPT-4o Mini."],"url":"http://arxiv.org/abs/2409.08185v1"}
{"created":"2024-09-12 16:13:07","title":"Enhancing Canine Musculoskeletal Diagnoses: Leveraging Synthetic Image Data for Pre-Training AI-Models on Visual Documentations","abstract":"The examination of the musculoskeletal system in dogs is a challenging task in veterinary practice. In this work, a novel method has been developed that enables efficient documentation of a dog's condition through a visual representation. However, since the visual documentation is new, there is no existing training data. The objective of this work is therefore to mitigate the impact of data scarcity in order to develop an AI-based diagnostic support system. To this end, the potential of synthetic data that mimics realistic visual documentations of diseases for pre-training AI models is investigated. We propose a method for generating synthetic image data that mimics realistic visual documentations. Initially, a basic dataset containing three distinct classes is generated, followed by the creation of a more sophisticated dataset containing 36 different classes. Both datasets are used for the pre-training of an AI model. Subsequently, an evaluation dataset is created, consisting of 250 manually created visual documentations for five different diseases. This dataset, along with a subset containing 25 examples. The obtained results on the evaluation dataset containing 25 examples demonstrate a significant enhancement of approximately 10% in diagnosis accuracy when utilizing generated synthetic images that mimic real-world visual documentations. However, these results do not hold true for the larger evaluation dataset containing 250 examples, indicating that the advantages of using synthetic data for pre-training an AI model emerge primarily when dealing with few examples of visual documentations for a given disease. Overall, this work provides valuable insights into mitigating the limitations imposed by limited training data through the strategic use of generated synthetic data, presenting an approach applicable beyond the canine musculoskeletal assessment domain.","sentences":["The examination of the musculoskeletal system in dogs is a challenging task in veterinary practice.","In this work, a novel method has been developed that enables efficient documentation of a dog's condition through a visual representation.","However, since the visual documentation is new, there is no existing training data.","The objective of this work is therefore to mitigate the impact of data scarcity in order to develop an AI-based diagnostic support system.","To this end, the potential of synthetic data that mimics realistic visual documentations of diseases for pre-training AI models is investigated.","We propose a method for generating synthetic image data that mimics realistic visual documentations.","Initially, a basic dataset containing three distinct classes is generated, followed by the creation of a more sophisticated dataset containing 36 different classes.","Both datasets are used for the pre-training of an AI model.","Subsequently, an evaluation dataset is created, consisting of 250 manually created visual documentations for five different diseases.","This dataset, along with a subset containing 25 examples.","The obtained results on the evaluation dataset containing 25 examples demonstrate a significant enhancement of approximately 10% in diagnosis accuracy when utilizing generated synthetic images that mimic real-world visual documentations.","However, these results do not hold true for the larger evaluation dataset containing 250 examples, indicating that the advantages of using synthetic data for pre-training an AI model emerge primarily when dealing with few examples of visual documentations for a given disease.","Overall, this work provides valuable insights into mitigating the limitations imposed by limited training data through the strategic use of generated synthetic data, presenting an approach applicable beyond the canine musculoskeletal assessment domain."],"url":"http://arxiv.org/abs/2409.08181v1"}
{"created":"2024-09-12 16:06:45","title":"Co-badge: An Activity for Collaborative Engagement with Data Visualization Design Concepts","abstract":"As data visualization gains popularity and projects become more interdisciplinary, there is a growing need for methods that foster creative collaboration and inform diverse audiences about data visualisation. In this paper, we introduce Co-Badge, a 90-minute design activity where participants collaboratively construct visualizations by ideating and prioritizing relevant data types, mapping them to visual variables, and constructing data badges with stationery materials. We conducted three workshops in diverse settings with participants of different backgrounds. Our findings indicate that Co-badge facilitates a playful and engaging way to gain awareness about data visualization design principles without formal training while navigating the challenges of collaboration. Our work contributes to the field of data visualization education for diverse actors. We believe Co-Badge can serve as an engaging activity that introduces basic concepts of data visualization and collaboration.","sentences":["As data visualization gains popularity and projects become more interdisciplinary, there is a growing need for methods that foster creative collaboration and inform diverse audiences about data visualisation.","In this paper, we introduce Co-Badge, a 90-minute design activity where participants collaboratively construct visualizations by ideating and prioritizing relevant data types, mapping them to visual variables, and constructing data badges with stationery materials.","We conducted three workshops in diverse settings with participants of different backgrounds.","Our findings indicate that Co-badge facilitates a playful and engaging way to gain awareness about data visualization design principles without formal training while navigating the challenges of collaboration.","Our work contributes to the field of data visualization education for diverse actors.","We believe Co-Badge can serve as an engaging activity that introduces basic concepts of data visualization and collaboration."],"url":"http://arxiv.org/abs/2409.08175v1"}
{"created":"2024-09-12 16:03:56","title":"Low-Cost Tree Crown Dieback Estimation Using Deep Learning-Based Segmentation","abstract":"The global increase in observed forest dieback, characterised by the death of tree foliage, heralds widespread decline in forest ecosystems. This degradation causes significant changes to ecosystem services and functions, including habitat provision and carbon sequestration, which can be difficult to detect using traditional monitoring techniques, highlighting the need for large-scale and high-frequency monitoring. Contemporary developments in the instruments and methods to gather and process data at large-scales mean this monitoring is now possible. In particular, the advancement of low-cost drone technology and deep learning on consumer-level hardware provide new opportunities. Here, we use an approach based on deep learning and vegetation indices to assess crown dieback from RGB aerial data without the need for expensive instrumentation such as LiDAR. We use an iterative approach to match crown footprints predicted by deep learning with field-based inventory data from a Mediterranean ecosystem exhibiting drought-induced dieback, and compare expert field-based crown dieback estimation with vegetation index-based estimates. We obtain high overall segmentation accuracy (mAP: 0.519) without the need for additional technical development of the underlying Mask R-CNN model, underscoring the potential of these approaches for non-expert use and proving their applicability to real-world conservation. We also find colour-coordinate based estimates of dieback correlate well with expert field-based estimation. Substituting ground truth for Mask R-CNN model predictions showed negligible impact on dieback estimates, indicating robustness. Our findings demonstrate the potential of automated data collection and processing, including the application of deep learning, to improve the coverage, speed and cost of forest dieback monitoring.","sentences":["The global increase in observed forest dieback, characterised by the death of tree foliage, heralds widespread decline in forest ecosystems.","This degradation causes significant changes to ecosystem services and functions, including habitat provision and carbon sequestration, which can be difficult to detect using traditional monitoring techniques, highlighting the need for large-scale and high-frequency monitoring.","Contemporary developments in the instruments and methods to gather and process data at large-scales mean this monitoring is now possible.","In particular, the advancement of low-cost drone technology and deep learning on consumer-level hardware provide new opportunities.","Here, we use an approach based on deep learning and vegetation indices to assess crown dieback from RGB aerial data without the need for expensive instrumentation such as LiDAR.","We use an iterative approach to match crown footprints predicted by deep learning with field-based inventory data from a Mediterranean ecosystem exhibiting drought-induced dieback, and compare expert field-based crown dieback estimation with vegetation index-based estimates.","We obtain high overall segmentation accuracy (mAP: 0.519) without the need for additional technical development of the underlying Mask R-CNN model, underscoring the potential of these approaches for non-expert use and proving their applicability to real-world conservation.","We also find colour-coordinate based estimates of dieback correlate well with expert field-based estimation.","Substituting ground truth for Mask R-CNN model predictions showed negligible impact on dieback estimates, indicating robustness.","Our findings demonstrate the potential of automated data collection and processing, including the application of deep learning, to improve the coverage, speed and cost of forest dieback monitoring."],"url":"http://arxiv.org/abs/2409.08171v1"}
{"created":"2024-09-12 16:00:22","title":"Learning to Match 2D Keypoints Across Preoperative MR and Intraoperative Ultrasound","abstract":"We propose in this paper a texture-invariant 2D keypoints descriptor specifically designed for matching preoperative Magnetic Resonance (MR) images with intraoperative Ultrasound (US) images. We introduce a matching-by-synthesis strategy, where intraoperative US images are synthesized from MR images accounting for multiple MR modalities and intraoperative US variability. We build our training set by enforcing keypoints localization over all images then train a patient-specific descriptor network that learns texture-invariant discriminant features in a supervised contrastive manner, leading to robust keypoints descriptors. Our experiments on real cases with ground truth show the effectiveness of the proposed approach, outperforming the state-of-the-art methods and achieving 80.35% matching precision on average.","sentences":["We propose in this paper a texture-invariant 2D keypoints descriptor specifically designed for matching preoperative Magnetic Resonance (MR) images with intraoperative Ultrasound (US) images.","We introduce a matching-by-synthesis strategy, where intraoperative US images are synthesized from MR images accounting for multiple MR modalities and intraoperative US variability.","We build our training set by enforcing keypoints localization over all images then train a patient-specific descriptor network that learns texture-invariant discriminant features in a supervised contrastive manner, leading to robust keypoints descriptors.","Our experiments on real cases with ground truth show the effectiveness of the proposed approach, outperforming the state-of-the-art methods and achieving 80.35% matching precision on average."],"url":"http://arxiv.org/abs/2409.08169v1"}
{"created":"2024-09-12 15:58:28","title":"High-Frequency Anti-DreamBooth: Robust Defense Against Image Synthesis","abstract":"Recently, text-to-image generative models have been misused to create unauthorized malicious images of individuals, posing a growing social problem. Previous solutions, such as Anti-DreamBooth, add adversarial noise to images to protect them from being used as training data for malicious generation. However, we found that the adversarial noise can be removed by adversarial purification methods such as DiffPure. Therefore, we propose a new adversarial attack method that adds strong perturbation on the high-frequency areas of images to make it more robust to adversarial purification. Our experiment showed that the adversarial images retained noise even after adversarial purification, hindering malicious image generation.","sentences":["Recently, text-to-image generative models have been misused to create unauthorized malicious images of individuals, posing a growing social problem.","Previous solutions, such as Anti-DreamBooth, add adversarial noise to images to protect them from being used as training data for malicious generation.","However, we found that the adversarial noise can be removed by adversarial purification methods such as DiffPure.","Therefore, we propose a new adversarial attack method that adds strong perturbation on the high-frequency areas of images to make it more robust to adversarial purification.","Our experiment showed that the adversarial images retained noise even after adversarial purification, hindering malicious image generation."],"url":"http://arxiv.org/abs/2409.08167v1"}
{"created":"2024-09-12 15:57:08","title":"Collaborating for Success: Optimizing System Efficiency and Resilience Under Agile Industrial Settings","abstract":"Designing an efficient and resilient human-robot collaboration strategy that not only upholds the safety and ergonomics of shared workspace but also enhances the performance and agility of collaborative setup presents significant challenges concerning environment perception and robot control. In this research, we introduce a novel approach for collaborative environment monitoring and robot motion regulation to address this multifaceted problem. Our study proposes novel computation and division of safety monitoring zones, adhering to ISO 13855 and TS 15066 standards, utilizing 2D lasers information. These zones are not only configured in the standard three-layer arrangement but are also expanded into two adjacent quadrants, thereby enhancing system uptime and preventing unnecessary deadlocks. Moreover, we also leverage 3D visual information to track dynamic human articulations and extended intrusions. Drawing upon the fused sensory data from 2D and 3D perceptual spaces, our proposed hierarchical controller stably regulates robot velocity, validated using Lasalle in-variance principle. Empirical evaluations demonstrate that our approach significantly reduces task execution time and system response delay, resulting in improved efficiency and resilience within collaborative settings.","sentences":["Designing an efficient and resilient human-robot collaboration strategy that not only upholds the safety and ergonomics of shared workspace but also enhances the performance and agility of collaborative setup presents significant challenges concerning environment perception and robot control.","In this research, we introduce a novel approach for collaborative environment monitoring and robot motion regulation to address this multifaceted problem.","Our study proposes novel computation and division of safety monitoring zones, adhering to ISO 13855 and TS 15066 standards, utilizing 2D lasers information.","These zones are not only configured in the standard three-layer arrangement but are also expanded into two adjacent quadrants, thereby enhancing system uptime and preventing unnecessary deadlocks.","Moreover, we also leverage 3D visual information to track dynamic human articulations and extended intrusions.","Drawing upon the fused sensory data from 2D and 3D perceptual spaces, our proposed hierarchical controller stably regulates robot velocity, validated using Lasalle in-variance principle.","Empirical evaluations demonstrate that our approach significantly reduces task execution time and system response delay, resulting in improved efficiency and resilience within collaborative settings."],"url":"http://arxiv.org/abs/2409.08166v1"}
{"created":"2024-09-12 15:56:17","title":"Open Source Infrastructure for Automatic Cell Segmentation","abstract":"Automated cell segmentation is crucial for various biological and medical applications, facilitating tasks like cell counting, morphology analysis, and drug discovery. However, manual segmentation is time-consuming and prone to subjectivity, necessitating robust automated methods. This paper presents open-source infrastructure, utilizing the UNet model, a deep-learning architecture noted for its effectiveness in image segmentation tasks. This implementation is integrated into the open-source DeepChem package, enhancing accessibility and usability for researchers and practitioners. The resulting tool offers a convenient and user-friendly interface, reducing the barrier to entry for cell segmentation while maintaining high accuracy. Additionally, we benchmark this model against various datasets, demonstrating its robustness and versatility across different imaging conditions and cell types.","sentences":["Automated cell segmentation is crucial for various biological and medical applications, facilitating tasks like cell counting, morphology analysis, and drug discovery.","However, manual segmentation is time-consuming and prone to subjectivity, necessitating robust automated methods.","This paper presents open-source infrastructure, utilizing the UNet model, a deep-learning architecture noted for its effectiveness in image segmentation tasks.","This implementation is integrated into the open-source DeepChem package, enhancing accessibility and usability for researchers and practitioners.","The resulting tool offers a convenient and user-friendly interface, reducing the barrier to entry for cell segmentation while maintaining high accuracy.","Additionally, we benchmark this model against various datasets, demonstrating its robustness and versatility across different imaging conditions and cell types."],"url":"http://arxiv.org/abs/2409.08163v1"}
{"created":"2024-09-12 15:55:39","title":"Cross-Attention Based Influence Model for Manual and Nonmanual Sign Language Analysis","abstract":"Both manual (relating to the use of hands) and non-manual markers (NMM), such as facial expressions or mouthing cues, are important for providing the complete meaning of phrases in American Sign Language (ASL). Efforts have been made in advancing sign language to spoken/written language understanding, but most of these have primarily focused on manual features only. In this work, using advanced neural machine translation methods, we examine and report on the extent to which facial expressions contribute to understanding sign language phrases. We present a sign language translation architecture consisting of two-stream encoders, with one encoder handling the face and the other handling the upper body (with hands). We propose a new parallel cross-attention decoding mechanism that is useful for quantifying the influence of each input modality on the output. The two streams from the encoder are directed simultaneously to different attention stacks in the decoder. Examining the properties of the parallel cross-attention weights allows us to analyze the importance of facial markers compared to body and hand features during a translating task.","sentences":["Both manual (relating to the use of hands) and non-manual markers (NMM), such as facial expressions or mouthing cues, are important for providing the complete meaning of phrases in American Sign Language (ASL).","Efforts have been made in advancing sign language to spoken/written language understanding, but most of these have primarily focused on manual features only.","In this work, using advanced neural machine translation methods, we examine and report on the extent to which facial expressions contribute to understanding sign language phrases.","We present a sign language translation architecture consisting of two-stream encoders, with one encoder handling the face and the other handling the upper body (with hands).","We propose a new parallel cross-attention decoding mechanism that is useful for quantifying the influence of each input modality on the output.","The two streams from the encoder are directed simultaneously to different attention stacks in the decoder.","Examining the properties of the parallel cross-attention weights allows us to analyze the importance of facial markers compared to body and hand features during a translating task."],"url":"http://arxiv.org/abs/2409.08162v1"}
{"created":"2024-09-12 15:54:40","title":"A Study on Asynchronous Vote-based Blockchains","abstract":"Vote-based blockchains construct a state machine replication (SMR) system among participating nodes, using Byzantine Fault Tolerance (BFT) consensus protocols to transition from one state to another. Currently, they rely on either synchronous or partially synchronous networks with leader-based coordination or costly Asynchronous Common Subset (ACS) protocols in asynchronous settings, making them impractical for large-scale asynchronous applications.   To make Asynchronous SMR scalable, this paper proposes a \\emph{validated strong} BFT consensus model that allows leader-based coordination in asynchronous settings. Our BFT consensus model offers the same level of tolerance as binary byzantine agreement but does not demand consistency among honest nodes before they vote. An SMR using our model allows nodes to operate in different, tentative, but mutually exclusive states until they eventually converge on the same state. We propose an asynchronous BFT protocol for vote-based blockchains employing our consensus model to address several critical challenges: how to ensure that nodes eventually converge on the same state across voting rounds, how to assure that a blockchain will steadily progress through epochs while reaching consensus for previous epochs, and how to maintain robust byzantine fault tolerance.   Our protocol greatly reduces message complexity and is the first one to achieve linear view changes without relying on threshold signatures. We prove that an asynchronous blockchain built on our protocol can operate with the \\emph{same} simplicity and efficiency as partially synchronous blockchains built on, e.g. HotStuff-2. This facilitates deploying asynchronous blockchains across large-scale networks.","sentences":["Vote-based blockchains construct a state machine replication (SMR) system among participating nodes, using Byzantine Fault Tolerance (BFT) consensus protocols to transition from one state to another.","Currently, they rely on either synchronous or partially synchronous networks with leader-based coordination or costly Asynchronous Common Subset (ACS) protocols in asynchronous settings, making them impractical for large-scale asynchronous applications.   ","To make Asynchronous SMR scalable, this paper proposes a \\emph{validated strong} BFT consensus model that allows leader-based coordination in asynchronous settings.","Our BFT consensus model offers the same level of tolerance as binary byzantine agreement but does not demand consistency among honest nodes before they vote.","An SMR using our model allows nodes to operate in different, tentative, but mutually exclusive states until they eventually converge on the same state.","We propose an asynchronous BFT protocol for vote-based blockchains employing our consensus model to address several critical challenges: how to ensure that nodes eventually converge on the same state across voting rounds, how to assure that a blockchain will steadily progress through epochs while reaching consensus for previous epochs, and how to maintain robust byzantine fault tolerance.   ","Our protocol greatly reduces message complexity and is the first one to achieve linear view changes without relying on threshold signatures.","We prove that an asynchronous blockchain built on our protocol can operate with the \\emph{same} simplicity and efficiency as partially synchronous blockchains built on, e.g. HotStuff-2.","This facilitates deploying asynchronous blockchains across large-scale networks."],"url":"http://arxiv.org/abs/2409.08161v1"}
{"created":"2024-09-12 15:52:22","title":"On the Role of Context in Reading Time Prediction","abstract":"We present a new perspective on how readers integrate context during real-time language comprehension. Our proposals build on surprisal theory, which posits that the processing effort of a linguistic unit (e.g., a word) is an affine function of its in-context information content. We first observe that surprisal is only one out of many potential ways that a contextual predictor can be derived from a language model. Another one is the pointwise mutual information (PMI) between a unit and its context, which turns out to yield the same predictive power as surprisal when controlling for unigram frequency. Moreover, both PMI and surprisal are correlated with frequency. This means that neither PMI nor surprisal contains information about context alone. In response to this, we propose a technique where we project surprisal onto the orthogonal complement of frequency, yielding a new contextual predictor that is uncorrelated with frequency. Our experiments show that the proportion of variance in reading times explained by context is a lot smaller when context is represented by the orthogonalized predictor. From an interpretability standpoint, this indicates that previous studies may have overstated the role that context has in predicting reading times.","sentences":["We present a new perspective on how readers integrate context during real-time language comprehension.","Our proposals build on surprisal theory, which posits that the processing effort of a linguistic unit (e.g., a word) is an affine function of its in-context information content.","We first observe that surprisal is only one out of many potential ways that a contextual predictor can be derived from a language model.","Another one is the pointwise mutual information (PMI) between a unit and its context, which turns out to yield the same predictive power as surprisal when controlling for unigram frequency.","Moreover, both PMI and surprisal are correlated with frequency.","This means that neither PMI nor surprisal contains information about context alone.","In response to this, we propose a technique where we project surprisal onto the orthogonal complement of frequency, yielding a new contextual predictor that is uncorrelated with frequency.","Our experiments show that the proportion of variance in reading times explained by context is a lot smaller when context is represented by the orthogonalized predictor.","From an interpretability standpoint, this indicates that previous studies may have overstated the role that context has in predicting reading times."],"url":"http://arxiv.org/abs/2409.08160v1"}
{"created":"2024-09-12 15:52:08","title":"SDformer: Efficient End-to-End Transformer for Depth Completion","abstract":"Depth completion aims to predict dense depth maps with sparse depth measurements from a depth sensor. Currently, Convolutional Neural Network (CNN) based models are the most popular methods applied to depth completion tasks. However, despite the excellent high-end performance, they suffer from a limited representation area. To overcome the drawbacks of CNNs, a more effective and powerful method has been presented: the Transformer, which is an adaptive self-attention setting sequence-to-sequence model. While the standard Transformer quadratically increases the computational cost from the key-query dot-product of input resolution which improperly employs depth completion tasks. In this work, we propose a different window-based Transformer architecture for depth completion tasks named Sparse-to-Dense Transformer (SDformer). The network consists of an input module for the depth map and RGB image features extraction and concatenation, a U-shaped encoder-decoder Transformer for extracting deep features, and a refinement module. Specifically, we first concatenate the depth map features with the RGB image features through the input model. Then, instead of calculating self-attention with the whole feature maps, we apply different window sizes to extract the long-range depth dependencies. Finally, we refine the predicted features from the input module and the U-shaped encoder-decoder Transformer module to get the enriching depth features and employ a convolution layer to obtain the dense depth map. In practice, the SDformer obtains state-of-the-art results against the CNN-based depth completion models with lower computing loads and parameters on the NYU Depth V2 and KITTI DC datasets.","sentences":["Depth completion aims to predict dense depth maps with sparse depth measurements from a depth sensor.","Currently, Convolutional Neural Network (CNN) based models are the most popular methods applied to depth completion tasks.","However, despite the excellent high-end performance, they suffer from a limited representation area.","To overcome the drawbacks of CNNs, a more effective and powerful method has been presented: the Transformer, which is an adaptive self-attention setting sequence-to-sequence model.","While the standard Transformer quadratically increases the computational cost from the key-query dot-product of input resolution which improperly employs depth completion tasks.","In this work, we propose a different window-based Transformer architecture for depth completion tasks named Sparse-to-Dense Transformer (SDformer).","The network consists of an input module for the depth map and RGB image features extraction and concatenation, a U-shaped encoder-decoder Transformer for extracting deep features, and a refinement module.","Specifically, we first concatenate the depth map features with the RGB image features through the input model.","Then, instead of calculating self-attention with the whole feature maps, we apply different window sizes to extract the long-range depth dependencies.","Finally, we refine the predicted features from the input module and the U-shaped encoder-decoder Transformer module to get the enriching depth features and employ a convolution layer to obtain the dense depth map.","In practice, the SDformer obtains state-of-the-art results against the CNN-based depth completion models with lower computing loads and parameters on the NYU Depth V2 and KITTI DC datasets."],"url":"http://arxiv.org/abs/2409.08159v1"}
{"created":"2024-09-12 15:51:09","title":"MagicStyle: Portrait Stylization Based on Reference Image","abstract":"The development of diffusion models has significantly advanced the research on image stylization, particularly in the area of stylizing a content image based on a given style image, which has attracted many scholars. The main challenge in this reference image stylization task lies in how to maintain the details of the content image while incorporating the color and texture features of the style image. This challenge becomes even more pronounced when the content image is a portrait which has complex textural details. To address this challenge, we propose a diffusion model-based reference image stylization method specifically for portraits, called MagicStyle. MagicStyle consists of two phases: Content and Style DDIM Inversion (CSDI) and Feature Fusion Forward (FFF). The CSDI phase involves a reverse denoising process, where DDIM Inversion is performed separately on the content image and the style image, storing the self-attention query, key and value features of both images during the inversion process. The FFF phase executes forward denoising, harmoniously integrating the texture and color information from the pre-stored feature queries, keys and values into the diffusion generation process based on our Well-designed Feature Fusion Attention (FFA). We conducted comprehensive comparative and ablation experiments to validate the effectiveness of our proposed MagicStyle and FFA.","sentences":["The development of diffusion models has significantly advanced the research on image stylization, particularly in the area of stylizing a content image based on a given style image, which has attracted many scholars.","The main challenge in this reference image stylization task lies in how to maintain the details of the content image while incorporating the color and texture features of the style image.","This challenge becomes even more pronounced when the content image is a portrait which has complex textural details.","To address this challenge, we propose a diffusion model-based reference image stylization method specifically for portraits, called MagicStyle.","MagicStyle consists of two phases: Content and Style DDIM Inversion (CSDI) and Feature Fusion Forward (FFF).","The CSDI phase involves a reverse denoising process, where DDIM Inversion is performed separately on the content image and the style image, storing the self-attention query, key and value features of both images during the inversion process.","The FFF phase executes forward denoising, harmoniously integrating the texture and color information from the pre-stored feature queries, keys and values into the diffusion generation process based on our Well-designed Feature Fusion Attention (FFA).","We conducted comprehensive comparative and ablation experiments to validate the effectiveness of our proposed MagicStyle and FFA."],"url":"http://arxiv.org/abs/2409.08156v1"}
{"created":"2024-09-12 15:40:45","title":"LLM-POTUS Score: A Framework of Analyzing Presidential Debates with Large Language Models","abstract":"Large language models have demonstrated remarkable capabilities in natural language processing, yet their application to political discourse analysis remains underexplored. This paper introduces a novel approach to evaluating presidential debate performances using LLMs, addressing the longstanding challenge of objectively assessing debate outcomes. We propose a framework that analyzes candidates' \"Policies, Persona, and Perspective\" (3P) and how they resonate with the \"Interests, Ideologies, and Identity\" (3I) of four key audience groups: voters, businesses, donors, and politicians. Our method employs large language models to generate the LLM-POTUS Score, a quantitative measure of debate performance based on the alignment between 3P and 3I. We apply this framework to analyze transcripts from recent U.S. presidential debates, demonstrating its ability to provide nuanced, multi-dimensional assessments of candidate performances. Our results reveal insights into the effectiveness of different debating strategies and their impact on various audience segments. This study not only offers a new tool for political analysis but also explores the potential and limitations of using LLMs as impartial judges in complex social contexts. In addition, this framework provides individual citizens with an independent tool to evaluate presidential debate performances, which enhances democratic engagement and reduces reliance on potentially biased media interpretations and institutional influence, thereby strengthening the foundation of informed civic participation.","sentences":["Large language models have demonstrated remarkable capabilities in natural language processing, yet their application to political discourse analysis remains underexplored.","This paper introduces a novel approach to evaluating presidential debate performances using LLMs, addressing the longstanding challenge of objectively assessing debate outcomes.","We propose a framework that analyzes candidates' \"Policies, Persona, and Perspective\" (3P) and how they resonate with the \"Interests, Ideologies, and Identity\" (3I) of four key audience groups: voters, businesses, donors, and politicians.","Our method employs large language models to generate the LLM-POTUS Score, a quantitative measure of debate performance based on the alignment between 3P and 3I. We apply this framework to analyze transcripts from recent U.S. presidential debates, demonstrating its ability to provide nuanced, multi-dimensional assessments of candidate performances.","Our results reveal insights into the effectiveness of different debating strategies and their impact on various audience segments.","This study not only offers a new tool for political analysis but also explores the potential and limitations of using LLMs as impartial judges in complex social contexts.","In addition, this framework provides individual citizens with an independent tool to evaluate presidential debate performances, which enhances democratic engagement and reduces reliance on potentially biased media interpretations and institutional influence, thereby strengthening the foundation of informed civic participation."],"url":"http://arxiv.org/abs/2409.08147v1"}
{"created":"2024-09-12 15:34:23","title":"Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent Interconnects","abstract":"Conventional wisdom holds that an efficient interface between an OS running on a CPU and a high-bandwidth I/O device should be based on Direct Memory Access (DMA), descriptor rings, and interrupts: DMA offloads transfers from the CPU, descriptor rings provide buffering and queuing, and interrupts facilitate asynchronous interaction between cores and device with a lightweight notification mechanism. In this paper we question this wisdom in the light of modern hardware and workloads, particularly in cloud servers. We argue that the assumptions that led to this model are obsolete, and in many use-cases use of programmed I/O, where the CPU explicitly transfers data and control information to and from a device via loads and stores, actually results in a more efficient system. We quantitatively demonstrate these advantages using three use-cases: fine-grained RPC-style invocation of functions on an accelerator, offloading of operators in a streaming dataflow engine, and a network interface targeting for serverless functions. Moreover, we show that while these advantages are significant over a modern PCIe peripheral bus, a truly cache-coherent interconnect offers significant additional efficiency gains.","sentences":["Conventional wisdom holds that an efficient interface between an OS running on a CPU and a high-bandwidth I/O device should be based on Direct Memory Access (DMA), descriptor rings, and interrupts: DMA offloads transfers from the CPU, descriptor rings provide buffering and queuing, and interrupts facilitate asynchronous interaction between cores and device with a lightweight notification mechanism.","In this paper we question this wisdom in the light of modern hardware and workloads, particularly in cloud servers.","We argue that the assumptions that led to this model are obsolete, and in many use-cases use of programmed I/O, where the CPU explicitly transfers data and control information to and from a device via loads and stores, actually results in a more efficient system.","We quantitatively demonstrate these advantages using three use-cases: fine-grained RPC-style invocation of functions on an accelerator, offloading of operators in a streaming dataflow engine, and a network interface targeting for serverless functions.","Moreover, we show that while these advantages are significant over a modern PCIe peripheral bus, a truly cache-coherent interconnect offers significant additional efficiency gains."],"url":"http://arxiv.org/abs/2409.08141v1"}
{"created":"2024-09-12 15:34:23","title":"Ranked Enumeration for Database Queries","abstract":"Ranked enumeration is a query-answering paradigm where the query answers are returned incrementally in order of importance (instead of returning all answers at once). Importance is defined by a ranking function that can be specific to the application, but typically involves either a lexicographic order (e.g., \"ORDER BY R.A, S.B\" in SQL) or a weighted sum of attributes (e.g., \"ORDER BY 3*R.A + 2*S.B\"). We recently introduced any-k algorithms for (multi-way) join queries, which push ranking into joins and avoid materializing intermediate results until necessary. The top-ranked answers are returned asymptotically faster than the common join-then-rank approach of database systems, resulting in orders-of-magnitude speedup in practice.   In addition to their practical usefulness, our techniques complement a long line of theoretical research on unranked enumeration, where answers are also returned incrementally, but with no explicit ordering requirement. For a broad class of ranking functions with certain monotonicity properties, including lexicographic orders and sum-based rankings, the ordering requirement surprisingly does not increase the asymptotic time or space complexity, apart from logarithmic factors.   A key insight of our work is the connection between ranked enumeration for database queries and the fundamental task of computing the kth-shortest path in a graph. Uncovering these connections allowed us to ground our approach in the rich literature of that problem and connect ideas that had been explored in isolation before. In this article, we adopt a pragmatic approach and present a slightly simplified version of the algorithm without the shortest-path interpretation. We believe that this will benefit practitioners looking to implement and optimize any-k approaches.","sentences":["Ranked enumeration is a query-answering paradigm where the query answers are returned incrementally in order of importance (instead of returning all answers at once).","Importance is defined by a ranking function that can be specific to the application, but typically involves either a lexicographic order (e.g., \"ORDER BY R.A, S.B\" in SQL) or a weighted sum of attributes (e.g., \"ORDER BY 3*R.A + 2*S.B\").","We recently introduced any-k algorithms for (multi-way) join queries, which push ranking into joins and avoid materializing intermediate results until necessary.","The top-ranked answers are returned asymptotically faster than the common join-then-rank approach of database systems, resulting in orders-of-magnitude speedup in practice.   ","In addition to their practical usefulness, our techniques complement a long line of theoretical research on unranked enumeration, where answers are also returned incrementally, but with no explicit ordering requirement.","For a broad class of ranking functions with certain monotonicity properties, including lexicographic orders and sum-based rankings, the ordering requirement surprisingly does not increase the asymptotic time or space complexity, apart from logarithmic factors.   ","A key insight of our work is the connection between ranked enumeration for database queries and the fundamental task of computing the kth-shortest path in a graph.","Uncovering these connections allowed us to ground our approach in the rich literature of that problem and connect ideas that had been explored in isolation before.","In this article, we adopt a pragmatic approach and present a slightly simplified version of the algorithm without the shortest-path interpretation.","We believe that this will benefit practitioners looking to implement and optimize any-k approaches."],"url":"http://arxiv.org/abs/2409.08142v1"}
{"created":"2024-09-12 15:27:11","title":"The mutual pulling force of human muscle fibers can treat mild cancer and rhinitis","abstract":"Muscles can store a large amount of genetic information, and in order to transform humans into computers, we need to start by increasing muscle tension. When people with cancer go on happy trips, some cancers often heal without treatment; Rhinitis can cause blockage of the nostrils, but after running, the nostrils naturally ventilate. Both are related to exercise, and the mystery behind them can treat both conditions. Cancer belongs to systemic diseases, and the eradication method for systemic diseases should start from the entire body system, treat the symptoms and prevent recurrence. This article uses special exercise methods and detailed methods to treat diseases, and finds that treating diseases from the perspective of the human system is indeed effective. This article adopts a comparative experimental method to compare the changes in the body before and after. Through this article, it is concluded that exercise and certain methods can cure mild rhinitis and promote rapid ventilation; Explaining from the perspective of muscle pulling force that older individuals are more prone to developing cellular variant cancer; Enhancing muscle tension in the human body can promote the cure of some cancers","sentences":["Muscles can store a large amount of genetic information, and in order to transform humans into computers, we need to start by increasing muscle tension.","When people with cancer go on happy trips, some cancers often heal without treatment; Rhinitis can cause blockage of the nostrils, but after running, the nostrils naturally ventilate.","Both are related to exercise, and the mystery behind them can treat both conditions.","Cancer belongs to systemic diseases, and the eradication method for systemic diseases should start from the entire body system, treat the symptoms and prevent recurrence.","This article uses special exercise methods and detailed methods to treat diseases, and finds that treating diseases from the perspective of the human system is indeed effective.","This article adopts a comparative experimental method to compare the changes in the body before and after.","Through this article, it is concluded that exercise and certain methods can cure mild rhinitis and promote rapid ventilation; Explaining from the perspective of muscle pulling force that older individuals are more prone to developing cellular variant cancer; Enhancing muscle tension in the human body can promote the cure of some cancers"],"url":"http://arxiv.org/abs/2409.08136v1"}
{"created":"2024-09-12 15:27:09","title":"Reducing Population-level Inequality Can Improve Demographic Group Fairness: a Twitter Case Study","abstract":"Many existing fairness metrics measure group-wise demographic disparities in system behavior or model performance. Calculating these metrics requires access to demographic information, which, in industrial settings, is often unavailable. By contrast, economic inequality metrics, such as the Gini coefficient, require no demographic data to measure. However, reductions in economic inequality do not necessarily correspond to reductions in demographic disparities. In this paper, we empirically explore the relationship between demographic-free inequality metrics -- such as the Gini coefficient -- and standard demographic bias metrics that measure group-wise model performance disparities specifically in the case of engagement inequality on Twitter. We analyze tweets from 174K users over the duration of 2021 and find that demographic-free impression inequality metrics are positively correlated with gender, race, and age disparities in the average case, and weakly (but still positively) correlated with demographic bias in the worst case. We therefore recommend inequality metrics as a potentially useful proxy measure of average group-wise disparities, especially in cases where such disparities cannot be measured directly. Based on these results, we believe they can be used as part of broader efforts to improve fairness between demographic groups in scenarios like content recommendation on social media.","sentences":["Many existing fairness metrics measure group-wise demographic disparities in system behavior or model performance.","Calculating these metrics requires access to demographic information, which, in industrial settings, is often unavailable.","By contrast, economic inequality metrics, such as the Gini coefficient, require no demographic data to measure.","However, reductions in economic inequality do not necessarily correspond to reductions in demographic disparities.","In this paper, we empirically explore the relationship between demographic-free inequality metrics -- such as the Gini coefficient -- and standard demographic bias metrics that measure group-wise model performance disparities specifically in the case of engagement inequality on Twitter.","We analyze tweets from 174K users over the duration of 2021 and find that demographic-free impression inequality metrics are positively correlated with gender, race, and age disparities in the average case, and weakly (but still positively) correlated with demographic bias in the worst case.","We therefore recommend inequality metrics as a potentially useful proxy measure of average group-wise disparities, especially in cases where such disparities cannot be measured directly.","Based on these results, we believe they can be used as part of broader efforts to improve fairness between demographic groups in scenarios like content recommendation on social media."],"url":"http://arxiv.org/abs/2409.08135v1"}
{"created":"2024-09-12 15:11:35","title":"GAZEploit: Remote Keystroke Inference Attack by Gaze Estimation from Avatar Views in VR/MR Devices","abstract":"The advent and growing popularity of Virtual Reality (VR) and Mixed Reality (MR) solutions have revolutionized the way we interact with digital platforms. The cutting-edge gaze-controlled typing methods, now prevalent in high-end models of these devices, e.g., Apple Vision Pro, have not only improved user experience but also mitigated traditional keystroke inference attacks that relied on hand gestures, head movements and acoustic side-channels. However, this advancement has paradoxically given birth to a new, potentially more insidious cyber threat, GAZEploit.   In this paper, we unveil GAZEploit, a novel eye-tracking based attack specifically designed to exploit these eye-tracking information by leveraging the common use of virtual appearances in VR applications. This widespread usage significantly enhances the practicality and feasibility of our attack compared to existing methods. GAZEploit takes advantage of this vulnerability to remotely extract gaze estimations and steal sensitive keystroke information across various typing scenarios-including messages, passwords, URLs, emails, and passcodes. Our research, involving 30 participants, achieved over 80% accuracy in keystroke inference. Alarmingly, our study also identified over 15 top-rated apps in the Apple Store as vulnerable to the GAZEploit attack, emphasizing the urgent need for bolstered security measures for this state-of-the-art VR/MR text entry method.","sentences":["The advent and growing popularity of Virtual Reality (VR) and Mixed Reality (MR) solutions have revolutionized the way we interact with digital platforms.","The cutting-edge gaze-controlled typing methods, now prevalent in high-end models of these devices, e.g., Apple Vision Pro, have not only improved user experience but also mitigated traditional keystroke inference attacks that relied on hand gestures, head movements and acoustic side-channels.","However, this advancement has paradoxically given birth to a new, potentially more insidious cyber threat, GAZEploit.   ","In this paper, we unveil GAZEploit, a novel eye-tracking based attack specifically designed to exploit these eye-tracking information by leveraging the common use of virtual appearances in VR applications.","This widespread usage significantly enhances the practicality and feasibility of our attack compared to existing methods.","GAZEploit takes advantage of this vulnerability to remotely extract gaze estimations and steal sensitive keystroke information across various typing scenarios-including messages, passwords, URLs, emails, and passcodes.","Our research, involving 30 participants, achieved over 80% accuracy in keystroke inference.","Alarmingly, our study also identified over 15 top-rated apps in the Apple Store as vulnerable to the GAZEploit attack, emphasizing the urgent need for bolstered security measures for this state-of-the-art VR/MR text entry method."],"url":"http://arxiv.org/abs/2409.08122v1"}
{"created":"2024-09-12 15:07:16","title":"Anonymized Network Sensing Graph Challenge","abstract":"The MIT/IEEE/Amazon GraphChallenge encourages community approaches to developing new solutions for analyzing graphs and sparse data derived from social media, sensor feeds, and scientific data to discover relationships between events as they unfold in the field. The anonymized network sensing Graph Challenge seeks to enable large, open, community-based approaches to protecting networks. Many large-scale networking problems can only be solved with community access to very broad data sets with the highest regard for privacy and strong community buy-in. Such approaches often require community-based data sharing. In the broader networking community (commercial, federal, and academia) anonymized source-to-destination traffic matrices with standard data sharing agreements have emerged as a data product that can meet many of these requirements. This challenge provides an opportunity to highlight novel approaches for optimizing the construction and analysis of anonymized traffic matrices using over 100 billion network packets derived from the largest Internet telescope in the world (CAIDA). This challenge specifies the anonymization, construction, and analysis of these traffic matrices. A GraphBLAS reference implementation is provided, but the use of GraphBLAS is not required in this Graph Challenge. As with prior Graph Challenges the goal is to provide a well-defined context for demonstrating innovation. Graph Challenge participants are free to select (with accompanying explanation) the Graph Challenge elements that are appropriate for highlighting their innovations.","sentences":["The MIT/IEEE/Amazon GraphChallenge encourages community approaches to developing new solutions for analyzing graphs and sparse data derived from social media, sensor feeds, and scientific data to discover relationships between events as they unfold in the field.","The anonymized network sensing Graph Challenge seeks to enable large, open, community-based approaches to protecting networks.","Many large-scale networking problems can only be solved with community access to very broad data sets with the highest regard for privacy and strong community buy-in.","Such approaches often require community-based data sharing.","In the broader networking community (commercial, federal, and academia) anonymized source-to-destination traffic matrices with standard data sharing agreements have emerged as a data product that can meet many of these requirements.","This challenge provides an opportunity to highlight novel approaches for optimizing the construction and analysis of anonymized traffic matrices using over 100 billion network packets derived from the largest Internet telescope in the world (CAIDA).","This challenge specifies the anonymization, construction, and analysis of these traffic matrices.","A GraphBLAS reference implementation is provided, but the use of GraphBLAS is not required in this Graph Challenge.","As with prior Graph Challenges the goal is to provide a well-defined context for demonstrating innovation.","Graph Challenge participants are free to select (with accompanying explanation) the Graph Challenge elements that are appropriate for highlighting their innovations."],"url":"http://arxiv.org/abs/2409.08115v1"}
{"created":"2024-09-12 15:06:36","title":"Linear Complementary Dual Codes Constructed from Reinforcement Learning","abstract":"Recently, Linear Complementary Dual (LCD) codes have garnered substantial interest within coding theory research due to their diverse applications and favorable attributes. This paper directs its attention to the construction of binary and ternary LCD codes leveraging curiosity-driven reinforcement learning (RL). By establishing reward and devising well-reasoned mappings from actions to states, it aims to facilitate the successful synthesis of binary or ternary LCD codes. Experimental results indicate that LCD codes constructed using RL exhibit slightly superior error-correction performance compared to those conventionally constructed LCD codes and those developed via standard RL methodologies. The paper introduces novel binary and ternary LCD codes with enhanced minimum distance bounds. Finally, it showcases how Random Network Distillation aids agents in exploring beyond local optima, enhancing the overall performance of the models without compromising convergence.","sentences":["Recently, Linear Complementary Dual (LCD) codes have garnered substantial interest within coding theory research due to their diverse applications and favorable attributes.","This paper directs its attention to the construction of binary and ternary LCD codes leveraging curiosity-driven reinforcement learning (RL).","By establishing reward and devising well-reasoned mappings from actions to states, it aims to facilitate the successful synthesis of binary or ternary LCD codes.","Experimental results indicate that LCD codes constructed using RL exhibit slightly superior error-correction performance compared to those conventionally constructed LCD codes and those developed via standard RL methodologies.","The paper introduces novel binary and ternary LCD codes with enhanced minimum distance bounds.","Finally, it showcases how Random Network Distillation aids agents in exploring beyond local optima, enhancing the overall performance of the models without compromising convergence."],"url":"http://arxiv.org/abs/2409.08114v1"}
{"created":"2024-09-12 15:04:34","title":"Towards a graph-based foundation model for network traffic analysis","abstract":"Foundation models have shown great promise in various fields of study. A potential application of such models is in computer network traffic analysis, where these models can grasp the complexities of network traffic dynamics and adapt to any specific task or network environment with minimal fine-tuning. Previous approaches have used tokenized hex-level packet data and the model architecture of large language transformer models. We propose a new, efficient graph-based alternative at the flow-level. Our approach represents network traffic as a dynamic spatio-temporal graph, employing a self-supervised link prediction pretraining task to capture the spatial and temporal dynamics in this network graph framework. To evaluate the effectiveness of our approach, we conduct a few-shot learning experiment for three distinct downstream network tasks: intrusion detection, traffic classification, and botnet classification. Models finetuned from our pretrained base achieve an average performance increase of 6.87\\% over training from scratch, demonstrating their ability to effectively learn general network traffic dynamics during pretraining. This success suggests the potential for a large-scale version to serve as an operational foundational model.","sentences":["Foundation models have shown great promise in various fields of study.","A potential application of such models is in computer network traffic analysis, where these models can grasp the complexities of network traffic dynamics and adapt to any specific task or network environment with minimal fine-tuning.","Previous approaches have used tokenized hex-level packet data and the model architecture of large language transformer models.","We propose a new, efficient graph-based alternative at the flow-level.","Our approach represents network traffic as a dynamic spatio-temporal graph, employing a self-supervised link prediction pretraining task to capture the spatial and temporal dynamics in this network graph framework.","To evaluate the effectiveness of our approach, we conduct a few-shot learning experiment for three distinct downstream network tasks: intrusion detection, traffic classification, and botnet classification.","Models finetuned from our pretrained base achieve an average performance increase of 6.87\\% over training from scratch, demonstrating their ability to effectively learn general network traffic dynamics during pretraining.","This success suggests the potential for a large-scale version to serve as an operational foundational model."],"url":"http://arxiv.org/abs/2409.08111v1"}
{"created":"2024-09-12 15:00:58","title":"Microarchitectural comparison and in-core modeling of state-of-the-art CPUs: Grace, Sapphire Rapids, and Genoa","abstract":"With Nvidia's release of the Grace Superchip, all three big semiconductor companies in HPC (AMD, Intel, Nvidia) are currently competing in the race for the best CPU. In this work we analyze the performance of these state-of-the-art CPUs and create an accurate in-core performance model for their microarchitectures Zen 4, Golden Cove, and Neoverse V2, extending the Open Source Architecture Code Analyzer (OSACA) tool and comparing it with LLVM-MCA. Starting from the peculiarities and up- and downsides of a single core, we extend our comparison by a variety of microbenchmarks and the capabilities of a full node. The \"write-allocate (WA) evasion\" feature, which can automatically reduce the memory traffic caused by write misses, receives special attention; we show that the Grace Superchip has a next-to-optimal implementation of WA evasion, and that the only way to avoid write allocates on Zen 4 is the explicit use of non-temporal stores.","sentences":["With Nvidia's release of the Grace Superchip, all three big semiconductor companies in HPC (AMD, Intel, Nvidia) are currently competing in the race for the best CPU.","In this work we analyze the performance of these state-of-the-art CPUs and create an accurate in-core performance model for their microarchitectures Zen 4, Golden Cove, and Neoverse V2, extending the Open Source Architecture Code Analyzer (OSACA) tool and comparing it with LLVM-MCA.","Starting from the peculiarities and up- and downsides of a single core, we extend our comparison by a variety of microbenchmarks and the capabilities of a full node.","The \"write-allocate (WA) evasion\" feature, which can automatically reduce the memory traffic caused by write misses, receives special attention; we show that the Grace Superchip has a next-to-optimal implementation of WA evasion, and that the only way to avoid write allocates on Zen 4 is the explicit use of non-temporal stores."],"url":"http://arxiv.org/abs/2409.08108v1"}
{"created":"2024-09-12 15:00:56","title":"WhisperNER: Unified Open Named Entity and Speech Recognition","abstract":"Integrating named entity recognition (NER) with automatic speech recognition (ASR) can significantly enhance transcription accuracy and informativeness. In this paper, we introduce WhisperNER, a novel model that allows joint speech transcription and entity recognition. WhisperNER supports open-type NER, enabling recognition of diverse and evolving entities at inference. Building on recent advancements in open NER research, we augment a large synthetic dataset with synthetic speech samples. This allows us to train WhisperNER on a large number of examples with diverse NER tags. During training, the model is prompted with NER labels and optimized to output the transcribed utterance along with the corresponding tagged entities. To evaluate WhisperNER, we generate synthetic speech for commonly used NER benchmarks and annotate existing ASR datasets with open NER tags. Our experiments demonstrate that WhisperNER outperforms natural baselines on both out-of-domain open type NER and supervised finetuning.","sentences":["Integrating named entity recognition (NER) with automatic speech recognition (ASR) can significantly enhance transcription accuracy and informativeness.","In this paper, we introduce WhisperNER, a novel model that allows joint speech transcription and entity recognition.","WhisperNER supports open-type NER, enabling recognition of diverse and evolving entities at inference.","Building on recent advancements in open NER research, we augment a large synthetic dataset with synthetic speech samples.","This allows us to train WhisperNER on a large number of examples with diverse NER tags.","During training, the model is prompted with NER labels and optimized to output the transcribed utterance along with the corresponding tagged entities.","To evaluate WhisperNER, we generate synthetic speech for commonly used NER benchmarks and annotate existing ASR datasets with open NER tags.","Our experiments demonstrate that WhisperNER outperforms natural baselines on both out-of-domain open type NER and supervised finetuning."],"url":"http://arxiv.org/abs/2409.08107v1"}
{"created":"2024-09-12 15:00:07","title":"Hypergraph Change Point Detection using Adapted Cardinality-Based Gadgets: Applications in Dynamic Legal Structures","abstract":"Hypergraphs provide a robust framework for modeling complex systems with higher-order interactions. However, analyzing them in dynamic settings presents significant computational challenges. To address this, we introduce a novel method that adapts the cardinality-based gadget to convert hypergraphs into strongly connected weighted directed graphs, complemented by a symmetrized combinatorial Laplacian. We demonstrate that the harmonic mean of the conductance and edge expansion of the original hypergraph can be upper-bounded by the conductance of the transformed directed graph, effectively preserving crucial cut information. Additionally, we analyze how the resulting Laplacian relates to that derived from the star expansion. Our approach was validated through change point detection experiments on both synthetic and real datasets, showing superior performance over clique and star expansions in maintaining spectral information in dynamic settings. Finally, we applied our method to analyze a dynamic legal hypergraph constructed from extensive United States court opinion data.","sentences":["Hypergraphs provide a robust framework for modeling complex systems with higher-order interactions.","However, analyzing them in dynamic settings presents significant computational challenges.","To address this, we introduce a novel method that adapts the cardinality-based gadget to convert hypergraphs into strongly connected weighted directed graphs, complemented by a symmetrized combinatorial Laplacian.","We demonstrate that the harmonic mean of the conductance and edge expansion of the original hypergraph can be upper-bounded by the conductance of the transformed directed graph, effectively preserving crucial cut information.","Additionally, we analyze how the resulting Laplacian relates to that derived from the star expansion.","Our approach was validated through change point detection experiments on both synthetic and real datasets, showing superior performance over clique and star expansions in maintaining spectral information in dynamic settings.","Finally, we applied our method to analyze a dynamic legal hypergraph constructed from extensive United States court opinion data."],"url":"http://arxiv.org/abs/2409.08106v1"}
{"created":"2024-09-12 14:57:28","title":"DEMAU: Decompose, Explore, Model and Analyse Uncertainties","abstract":"Recent research in machine learning has given rise to a flourishing literature on the quantification and decomposition of model uncertainty. This information can be very useful during interactions with the learner, such as in active learning or adaptive learning, and especially in uncertainty sampling. To allow a simple representation of these total, epistemic (reducible) and aleatoric (irreducible) uncertainties, we offer DEMAU, an open-source educational, exploratory and analytical tool allowing to visualize and explore several types of uncertainty for classification models in machine learning.","sentences":["Recent research in machine learning has given rise to a flourishing literature on the quantification and decomposition of model uncertainty.","This information can be very useful during interactions with the learner, such as in active learning or adaptive learning, and especially in uncertainty sampling.","To allow a simple representation of these total, epistemic (reducible) and aleatoric (irreducible) uncertainties, we offer DEMAU, an open-source educational, exploratory and analytical tool allowing to visualize and explore several types of uncertainty for classification models in machine learning."],"url":"http://arxiv.org/abs/2409.08105v1"}
{"created":"2024-09-12 14:55:45","title":"Designing a Collaborative Platform for Advancing Supply Chain Transparency","abstract":"Enabling supply chain transparency (SCT) is essential for regulatory compliance and meeting sustainability standards. Multi-tier SCT plays a pivotal role in identifying and mitigating an organization's operational, environmental, and social (ESG) risks. While research observes increasing efforts towards SCT, a minority of companies are currently publishing supply chain information. Using the Design Science Research approach, we develop a collaborative platform for supply chain transparency. We derive design requirements, formulate design principles, and evaluate the artefact with industry experts. Our artefact is initialized with publicly available supply chain data through an automated pipeline designed to onboard future participants to our platform. This work contributes to SCT research by providing insights into the challenges and opportunities of implementing multi-tier SCT and offers a practical solution that encourages organizations to participate in a transparent ecosystem.","sentences":["Enabling supply chain transparency (SCT) is essential for regulatory compliance and meeting sustainability standards.","Multi-tier SCT plays a pivotal role in identifying and mitigating an organization's operational, environmental, and social (ESG) risks.","While research observes increasing efforts towards SCT, a minority of companies are currently publishing supply chain information.","Using the Design Science Research approach, we develop a collaborative platform for supply chain transparency.","We derive design requirements, formulate design principles, and evaluate the artefact with industry experts.","Our artefact is initialized with publicly available supply chain data through an automated pipeline designed to onboard future participants to our platform.","This work contributes to SCT research by providing insights into the challenges and opportunities of implementing multi-tier SCT and offers a practical solution that encourages organizations to participate in a transparent ecosystem."],"url":"http://arxiv.org/abs/2409.08104v1"}
{"created":"2024-09-12 14:55:33","title":"The Faetar Benchmark: Speech Recognition in a Very Under-Resourced Language","abstract":"We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark corpus designed to push the limits of current approaches to low-resource speech recognition. Faetar, a Franco-Proven\\c{c}al variety spoken primarily in Italy, has no standard orthography, has virtually no existing textual or speech resources other than what is included in the benchmark, and is quite different from other forms of Franco-Proven\\c{c}al. The corpus comes from field recordings, most of which are noisy, for which only 5 hrs have matching transcriptions, and for which forced alignment is of variable quality. The corpus contains an additional 20 hrs of unlabelled speech. We report baseline results from state-of-the-art multilingual speech foundation models with a best phone error rate of 30.4%, using a pipeline that continues pre-training on the foundation model using the unlabelled set.","sentences":["We introduce the Faetar Automatic Speech Recognition Benchmark, a benchmark corpus designed to push the limits of current approaches to low-resource speech recognition.","Faetar, a Franco-Proven\\c{c}al variety spoken primarily in Italy, has no standard orthography, has virtually no existing textual or speech resources other than what is included in the benchmark, and is quite different from other forms of Franco-Proven\\c{c}al.","The corpus comes from field recordings, most of which are noisy, for which only 5 hrs have matching transcriptions, and for which forced alignment is of variable quality.","The corpus contains an additional 20 hrs of unlabelled speech.","We report baseline results from state-of-the-art multilingual speech foundation models with a best phone error rate of 30.4%, using a pipeline that continues pre-training on the foundation model using the unlabelled set."],"url":"http://arxiv.org/abs/2409.08103v1"}
{"created":"2024-09-12 14:54:31","title":"Bayesian Self-Training for Semi-Supervised 3D Segmentation","abstract":"3D segmentation is a core problem in computer vision and, similarly to many other dense prediction tasks, it requires large amounts of annotated data for adequate training. However, densely labeling 3D point clouds to employ fully-supervised training remains too labor intensive and expensive. Semi-supervised training provides a more practical alternative, where only a small set of labeled data is given, accompanied by a larger unlabeled set. This area thus studies the effective use of unlabeled data to reduce the performance gap that arises due to the lack of annotations. In this work, inspired by Bayesian deep learning, we first propose a Bayesian self-training framework for semi-supervised 3D semantic segmentation. Employing stochastic inference, we generate an initial set of pseudo-labels and then filter these based on estimated point-wise uncertainty. By constructing a heuristic $n$-partite matching algorithm, we extend the method to semi-supervised 3D instance segmentation, and finally, with the same building blocks, to dense 3D visual grounding. We demonstrate state-of-the-art results for our semi-supervised method on SemanticKITTI and ScribbleKITTI for 3D semantic segmentation and on ScanNet and S3DIS for 3D instance segmentation. We further achieve substantial improvements in dense 3D visual grounding over supervised-only baselines on ScanRefer. Our project page is available at ouenal.github.io/bst/.","sentences":["3D segmentation is a core problem in computer vision and, similarly to many other dense prediction tasks, it requires large amounts of annotated data for adequate training.","However, densely labeling 3D point clouds to employ fully-supervised training remains too labor intensive and expensive.","Semi-supervised training provides a more practical alternative, where only a small set of labeled data is given, accompanied by a larger unlabeled set.","This area thus studies the effective use of unlabeled data to reduce the performance gap that arises due to the lack of annotations.","In this work, inspired by Bayesian deep learning, we first propose a Bayesian self-training framework for semi-supervised 3D semantic segmentation.","Employing stochastic inference, we generate an initial set of pseudo-labels and then filter these based on estimated point-wise uncertainty.","By constructing a heuristic $n$-partite matching algorithm, we extend the method to semi-supervised 3D instance segmentation, and finally, with the same building blocks, to dense 3D visual grounding.","We demonstrate state-of-the-art results for our semi-supervised method on SemanticKITTI and ScribbleKITTI for 3D semantic segmentation and on ScanNet and S3DIS for 3D instance segmentation.","We further achieve substantial improvements in dense 3D visual grounding over supervised-only baselines on ScanRefer.","Our project page is available at ouenal.github.io/bst/."],"url":"http://arxiv.org/abs/2409.08102v1"}
{"created":"2024-09-12 14:54:06","title":"Testing the Test: Observations When Assessing Visualization Literacy of Domain Experts","abstract":"Various standardized tests exist that assess individuals' visualization literacy. Their use can help to draw conclusions from studies. However, it is not taken into account that the test itself can create a pressure situation where participants might fear being exposed and assessed negatively. This is especially problematic when testing domain experts in design studies. We conducted interviews with experts from different domains performing the Mini-VLAT test for visualization literacy to identify potential problems. Our participants reported that the time limit per question, ambiguities in the questions and visualizations, and missing steps in the test procedure mainly had an impact on their performance and content. We discuss possible changes to the test design to address these issues and how such assessment methods could be integrated into existing evaluation procedures.","sentences":["Various standardized tests exist that assess individuals' visualization literacy.","Their use can help to draw conclusions from studies.","However, it is not taken into account that the test itself can create a pressure situation where participants might fear being exposed and assessed negatively.","This is especially problematic when testing domain experts in design studies.","We conducted interviews with experts from different domains performing the Mini-VLAT test for visualization literacy to identify potential problems.","Our participants reported that the time limit per question, ambiguities in the questions and visualizations, and missing steps in the test procedure mainly had an impact on their performance and content.","We discuss possible changes to the test design to address these issues and how such assessment methods could be integrated into existing evaluation procedures."],"url":"http://arxiv.org/abs/2409.08101v1"}
{"created":"2024-09-12 14:51:43","title":"The CLC-UKET Dataset: Benchmarking Case Outcome Prediction for the UK Employment Tribunal","abstract":"This paper explores the intersection of technological innovation and access to justice by developing a benchmark for predicting case outcomes in the UK Employment Tribunal (UKET). To address the challenge of extensive manual annotation, the study employs a large language model (LLM) for automatic annotation, resulting in the creation of the CLC-UKET dataset. The dataset consists of approximately 19,000 UKET cases and their metadata. Comprehensive legal annotations cover facts, claims, precedent references, statutory references, case outcomes, reasons and jurisdiction codes. Facilitated by the CLC-UKET data, we examine a multi-class case outcome prediction task in the UKET. Human predictions are collected to establish a performance reference for model comparison. Empirical results from baseline models indicate that finetuned transformer models outperform zero-shot and few-shot LLMs on the UKET prediction task. The performance of zero-shot LLMs can be enhanced by integrating task-related information into few-shot examples. We hope that the CLC-UKET dataset, along with human annotations and empirical findings, can serve as a valuable benchmark for employment-related dispute resolution.","sentences":["This paper explores the intersection of technological innovation and access to justice by developing a benchmark for predicting case outcomes in the UK Employment Tribunal (UKET).","To address the challenge of extensive manual annotation, the study employs a large language model (LLM) for automatic annotation, resulting in the creation of the CLC-UKET dataset.","The dataset consists of approximately 19,000 UKET cases and their metadata.","Comprehensive legal annotations cover facts, claims, precedent references, statutory references, case outcomes, reasons and jurisdiction codes.","Facilitated by the CLC-UKET data, we examine a multi-class case outcome prediction task in the UKET.","Human predictions are collected to establish a performance reference for model comparison.","Empirical results from baseline models indicate that finetuned transformer models outperform zero-shot and few-shot LLMs on the UKET prediction task.","The performance of zero-shot LLMs can be enhanced by integrating task-related information into few-shot examples.","We hope that the CLC-UKET dataset, along with human annotations and empirical findings, can serve as a valuable benchmark for employment-related dispute resolution."],"url":"http://arxiv.org/abs/2409.08098v1"}
{"created":"2024-09-12 14:44:45","title":"EZIGen: Enhancing zero-shot subject-driven image generation with precise subject encoding and decoupled guidance","abstract":"Zero-shot subject-driven image generation aims to produce images that incorporate a subject from a given example image. The challenge lies in preserving the subject's identity while aligning with the text prompt, which often requires modifying certain aspects of the subject's appearance. Despite advancements in diffusion model based methods, existing approaches still struggle to balance identity preservation with text prompt alignment. In this study, we conducted an in-depth investigation into this issue and uncovered key insights for achieving effective identity preservation while maintaining a strong balance. Our key findings include: (1) the design of the subject image encoder significantly impacts identity preservation quality, and (2) generating an initial layout is crucial for both text alignment and identity preservation. Building on these insights, we introduce a new approach called EZIGen, which employs two main strategies: a carefully crafted subject image Encoder based on the UNet architecture of the pretrained Stable Diffusion model to ensure high-quality identity transfer, following a process that decouples the guidance stages and iteratively refines the initial image layout. Through these strategies, EZIGen achieves state-of-the-art results on multiple subject-driven benchmarks with a unified model and 100 times less training data.","sentences":["Zero-shot subject-driven image generation aims to produce images that incorporate a subject from a given example image.","The challenge lies in preserving the subject's identity while aligning with the text prompt, which often requires modifying certain aspects of the subject's appearance.","Despite advancements in diffusion model based methods, existing approaches still struggle to balance identity preservation with text prompt alignment.","In this study, we conducted an in-depth investigation into this issue and uncovered key insights for achieving effective identity preservation while maintaining a strong balance.","Our key findings include: (1) the design of the subject image encoder significantly impacts identity preservation quality, and (2) generating an initial layout is crucial for both text alignment and identity preservation.","Building on these insights, we introduce a new approach called EZIGen, which employs two main strategies: a carefully crafted subject image Encoder based on the UNet architecture of the pretrained Stable Diffusion model to ensure high-quality identity transfer, following a process that decouples the guidance stages and iteratively refines the initial image layout.","Through these strategies, EZIGen achieves state-of-the-art results on multiple subject-driven benchmarks with a unified model and 100 times less training data."],"url":"http://arxiv.org/abs/2409.08091v1"}
{"created":"2024-09-12 14:42:08","title":"Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks","abstract":"Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.","sentences":["Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns.","This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks.","Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability.","Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises.","A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements.","The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances.","Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt.","This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field."],"url":"http://arxiv.org/abs/2409.08087v1"}
{"created":"2024-09-12 14:38:21","title":"SimMAT: Exploring Transferability from Vision Foundation Models to Any Image Modality","abstract":"Foundation models like ChatGPT and Sora that are trained on a huge scale of data have made a revolutionary social impact. However, it is extremely challenging for sensors in many different fields to collect similar scales of natural images to train strong foundation models. To this end, this work presents a simple and effective framework SimMAT to study an open problem: the transferability from vision foundation models trained on natural RGB images to other image modalities of different physical properties (e.g., polarization). SimMAT consists of a modality-agnostic transfer layer (MAT) and a pretrained foundation model. We apply SimMAT to a representative vision foundation model Segment Anything Model (SAM) to support any evaluated new image modality. Given the absence of relevant benchmarks, we construct a new benchmark to evaluate the transfer learning performance. Our experiments confirm the intriguing potential of transferring vision foundation models in enhancing other sensors' performance. Specifically, SimMAT can improve the segmentation performance (mIoU) from 22.15% to 53.88% on average for evaluated modalities and consistently outperforms other baselines. We hope that SimMAT can raise awareness of cross-modal transfer learning and benefit various fields for better results with vision foundation models.","sentences":["Foundation models like ChatGPT and Sora that are trained on a huge scale of data have made a revolutionary social impact.","However, it is extremely challenging for sensors in many different fields to collect similar scales of natural images to train strong foundation models.","To this end, this work presents a simple and effective framework SimMAT to study an open problem: the transferability from vision foundation models trained on natural RGB images to other image modalities of different physical properties (e.g., polarization).","SimMAT consists of a modality-agnostic transfer layer (MAT) and a pretrained foundation model.","We apply SimMAT to a representative vision foundation model Segment Anything Model (SAM) to support any evaluated new image modality.","Given the absence of relevant benchmarks, we construct a new benchmark to evaluate the transfer learning performance.","Our experiments confirm the intriguing potential of transferring vision foundation models in enhancing other sensors' performance.","Specifically, SimMAT can improve the segmentation performance (mIoU) from 22.15% to 53.88% on average for evaluated modalities and consistently outperforms other baselines.","We hope that SimMAT can raise awareness of cross-modal transfer learning and benefit various fields for better results with vision foundation models."],"url":"http://arxiv.org/abs/2409.08083v1"}
{"created":"2024-09-12 14:35:55","title":"SoVAR: Building Generalizable Scenarios from Accident Reports for Autonomous Driving Testing","abstract":"Autonomous driving systems (ADSs) have undergone remarkable development and are increasingly employed in safety-critical applications. However, recently reported data on fatal accidents involving ADSs suggests that the desired level of safety has not yet been fully achieved. Consequently, there is a growing need for more comprehensive and targeted testing approaches to ensure safe driving. Scenarios from real-world accident reports provide valuable resources for ADS testing, including critical scenarios and high-quality seeds. However, existing scenario reconstruction methods from accident reports often exhibit limited accuracy in information extraction. Moreover, due to the diversity and complexity of road environments, matching current accident information with the simulation map data for reconstruction poses significant challenges. In this paper, we design and implement SoVAR, a tool for automatically generating road-generalizable scenarios from accident reports. SoVAR utilizes well-designed prompts with linguistic patterns to guide the large language model in extracting accident information from textual data. Subsequently, it formulates and solves accident-related constraints in conjunction with the extracted accident information to generate accident trajectories. Finally, SoVAR reconstructs accident scenarios on various map structures and converts them into test scenarios to evaluate its capability to detect defects in industrial ADSs. We experiment with SoVAR, using accident reports from the National Highway Traffic Safety Administration's database to generate test scenarios for the industrial-grade ADS Apollo. The experimental findings demonstrate that SoVAR can effectively generate generalized accident scenarios across different road structures. Furthermore, the results confirm that SoVAR identified 5 distinct safety violation types that contributed to the crash of Baidu Apollo.","sentences":["Autonomous driving systems (ADSs) have undergone remarkable development and are increasingly employed in safety-critical applications.","However, recently reported data on fatal accidents involving ADSs suggests that the desired level of safety has not yet been fully achieved.","Consequently, there is a growing need for more comprehensive and targeted testing approaches to ensure safe driving.","Scenarios from real-world accident reports provide valuable resources for ADS testing, including critical scenarios and high-quality seeds.","However, existing scenario reconstruction methods from accident reports often exhibit limited accuracy in information extraction.","Moreover, due to the diversity and complexity of road environments, matching current accident information with the simulation map data for reconstruction poses significant challenges.","In this paper, we design and implement SoVAR, a tool for automatically generating road-generalizable scenarios from accident reports.","SoVAR utilizes well-designed prompts with linguistic patterns to guide the large language model in extracting accident information from textual data.","Subsequently, it formulates and solves accident-related constraints in conjunction with the extracted accident information to generate accident trajectories.","Finally, SoVAR reconstructs accident scenarios on various map structures and converts them into test scenarios to evaluate its capability to detect defects in industrial ADSs.","We experiment with SoVAR, using accident reports from the National Highway Traffic Safety Administration's database to generate test scenarios for the industrial-grade ADS Apollo.","The experimental findings demonstrate that SoVAR can effectively generate generalized accident scenarios across different road structures.","Furthermore, the results confirm that SoVAR identified 5 distinct safety violation types that contributed to the crash of Baidu Apollo."],"url":"http://arxiv.org/abs/2409.08081v1"}
{"created":"2024-09-12 14:31:02","title":"MosquitoMiner: A Light Weight Rover for Detecting and Eliminating Mosquito Breeding Sites","abstract":"In this paper, we present a novel approach to the development and deployment of an autonomous mosquito breeding place detector rover with the object and obstacle detection capabilities to control mosquitoes. Mosquito-borne diseases continue to pose significant health threats globally, with conventional control methods proving slow and inefficient. Amidst rising concerns over the rapid spread of these diseases, there is an urgent need for innovative and efficient strategies to manage mosquito populations and prevent disease transmission. To mitigate the limitations of manual labor and traditional methods, our rover employs autonomous control strategies. Leveraging our own custom dataset, the rover can autonomously navigate along a pre-defined path, identifying and mitigating potential breeding grounds with precision. It then proceeds to eliminate these breeding grounds by spraying a chemical agent, effectively eradicating mosquito habitats. Our project demonstrates the effectiveness that is absent in traditional ways of controlling and safeguarding public health. The code for this project is available on GitHub at - https://github.com/faiyazabdullah/MosquitoMiner","sentences":["In this paper, we present a novel approach to the development and deployment of an autonomous mosquito breeding place detector rover with the object and obstacle detection capabilities to control mosquitoes.","Mosquito-borne diseases continue to pose significant health threats globally, with conventional control methods proving slow and inefficient.","Amidst rising concerns over the rapid spread of these diseases, there is an urgent need for innovative and efficient strategies to manage mosquito populations and prevent disease transmission.","To mitigate the limitations of manual labor and traditional methods, our rover employs autonomous control strategies.","Leveraging our own custom dataset, the rover can autonomously navigate along a pre-defined path, identifying and mitigating potential breeding grounds with precision.","It then proceeds to eliminate these breeding grounds by spraying a chemical agent, effectively eradicating mosquito habitats.","Our project demonstrates the effectiveness that is absent in traditional ways of controlling and safeguarding public health.","The code for this project is available on GitHub at - https://github.com/faiyazabdullah/MosquitoMiner"],"url":"http://arxiv.org/abs/2409.08078v1"}
