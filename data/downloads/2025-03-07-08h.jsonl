{"created":"2025-03-06 18:59:48","title":"L$^2$M: Mutual Information Scaling Law for Long-Context Language Modeling","abstract":"We rigorously establish a bipartite mutual information scaling law in natural language that governs long-range dependencies. This scaling law, which we show is distinct from and scales independently of the conventional two-point mutual information, is the key to understanding long-context language modeling. Using this scaling law, we formulate the Long-context Language Modeling (L$^2$M) condition, which relates a model's capacity for effective long context length modeling to the scaling of its latent state size for storing past information. Our results are validated through experiments on both transformers and state space models. This work establishes a theoretical foundation that guides the development of large language models toward longer context lengths.","sentences":["We rigorously establish a bipartite mutual information scaling law in natural language that governs long-range dependencies.","This scaling law, which we show is distinct from and scales independently of the conventional two-point mutual information, is the key to understanding long-context language modeling.","Using this scaling law, we formulate the Long-context Language Modeling (L$^2$M) condition, which relates a model's capacity for effective long context length modeling to the scaling of its latent state size for storing past information.","Our results are validated through experiments on both transformers and state space models.","This work establishes a theoretical foundation that guides the development of large language models toward longer context lengths."],"url":"http://arxiv.org/abs/2503.04725v1"}
{"created":"2025-03-06 18:59:38","title":"LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM","abstract":"Recent advancements in speech-to-speech dialogue systems leverage LLMs for multimodal interactions, yet they remain hindered by fine-tuning requirements, high computational overhead, and text-speech misalignment. Existing speech-enabled LLMs often degrade conversational quality by modifying the LLM, thereby compromising its linguistic capabilities. In contrast, we propose LLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS system that generates high-quality speech with low latency, while fully preserving the capabilities of the base LLM. Our approach achieves a significantly lower Word Error Rate compared to speech-enabled LLMs, while operating at comparable latency and UTMOS score. By decoupling speech synthesis from LLM processing via a multi-queue token streaming system, LLMVoX supports seamless, infinite-length dialogues. Its plug-and-play design also facilitates extension to various tasks with different backbones. Furthermore, LLMVoX generalizes to new languages with only dataset adaptation, attaining a low Character Error Rate on an Arabic speech task. Additionally, we have integrated LLMVoX with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities, without requiring additional multimodal training. Our code base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .","sentences":["Recent advancements in speech-to-speech dialogue systems leverage LLMs for multimodal interactions, yet they remain hindered by fine-tuning requirements, high computational overhead, and text-speech misalignment.","Existing speech-enabled LLMs often degrade conversational quality by modifying the LLM, thereby compromising its linguistic capabilities.","In contrast, we propose LLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS system that generates high-quality speech with low latency, while fully preserving the capabilities of the base LLM.","Our approach achieves a significantly lower Word Error Rate compared to speech-enabled LLMs, while operating at comparable latency and UTMOS score.","By decoupling speech synthesis from LLM processing via a multi-queue token streaming system, LLMVoX supports seamless, infinite-length dialogues.","Its plug-and-play design also facilitates extension to various tasks with different backbones.","Furthermore, LLMVoX generalizes to new languages with only dataset adaptation, attaining a low Character Error Rate on an Arabic speech task.","Additionally, we have integrated LLMVoX with a Vision-Language Model to create an omni-model with speech, text, and vision capabilities, without requiring additional multimodal training.","Our code base and project page is available at https://mbzuai-oryx.github.io/LLMVoX ."],"url":"http://arxiv.org/abs/2503.04724v1"}
{"created":"2025-03-06 18:59:37","title":"Shifting Long-Context LLMs Research from Input to Output","abstract":"Recent advancements in long-context Large Language Models (LLMs) have primarily concentrated on processing extended input contexts, resulting in significant strides in long-context comprehension. However, the equally critical aspect of generating long-form outputs has received comparatively less attention. This paper advocates for a paradigm shift in NLP research toward addressing the challenges of long-output generation. Tasks such as novel writing, long-term planning, and complex reasoning require models to understand extensive contexts and produce coherent, contextually rich, and logically consistent extended text. These demands highlight a critical gap in current LLM capabilities. We underscore the importance of this under-explored domain and call for focused efforts to develop foundational LLMs tailored for generating high-quality, long-form outputs, which hold immense potential for real-world applications.","sentences":["Recent advancements in long-context Large Language Models (LLMs) have primarily concentrated on processing extended input contexts, resulting in significant strides in long-context comprehension.","However, the equally critical aspect of generating long-form outputs has received comparatively less attention.","This paper advocates for a paradigm shift in NLP research toward addressing the challenges of long-output generation.","Tasks such as novel writing, long-term planning, and complex reasoning require models to understand extensive contexts and produce coherent, contextually rich, and logically consistent extended text.","These demands highlight a critical gap in current LLM capabilities.","We underscore the importance of this under-explored domain and call for focused efforts to develop foundational LLMs tailored for generating high-quality, long-form outputs, which hold immense potential for real-world applications."],"url":"http://arxiv.org/abs/2503.04723v1"}
{"created":"2025-03-06 18:59:23","title":"Enough Coin Flips Can Make LLMs Act Bayesian","abstract":"Large language models (LLMs) exhibit the ability to generalize given few-shot examples in their input prompt, an emergent capability known as in-context learning (ICL). We investigate whether LLMs utilize ICL to perform structured reasoning in ways that are consistent with a Bayesian framework or rely on pattern matching. Using a controlled setting of biased coin flips, we find that: (1) LLMs often possess biased priors, causing initial divergence in zero-shot settings, (2) in-context evidence outweighs explicit bias instructions, (3) LLMs broadly follow Bayesian posterior updates, with deviations primarily due to miscalibrated priors rather than flawed updates, and (4) attention magnitude has negligible effect on Bayesian inference. With sufficient demonstrations of biased coin flips via ICL, LLMs update their priors in a Bayesian manner.","sentences":["Large language models (LLMs) exhibit the ability to generalize given few-shot examples in their input prompt, an emergent capability known as in-context learning (ICL).","We investigate whether LLMs utilize ICL to perform structured reasoning in ways that are consistent with a Bayesian framework or rely on pattern matching.","Using a controlled setting of biased coin flips, we find that: (1) LLMs often possess biased priors, causing initial divergence in zero-shot settings, (2) in-context evidence outweighs explicit bias instructions, (3) LLMs broadly follow Bayesian posterior updates, with deviations primarily due to miscalibrated priors rather than flawed updates, and (4) attention magnitude has negligible effect on Bayesian inference.","With sufficient demonstrations of biased coin flips via ICL, LLMs update their priors in a Bayesian manner."],"url":"http://arxiv.org/abs/2503.04722v1"}
{"created":"2025-03-06 18:59:16","title":"Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities","abstract":"Spoken dialogue modeling introduces unique challenges beyond text-based language modeling, demanding robust turn-taking, backchanneling, and real-time interaction. Although most Spoken Dialogue Models (SDMs) rely on half-duplex processing (handling speech one turn at a time), emerging full-duplex SDMs can listen and speak simultaneously, enabling more natural and engaging conversations. However, current evaluations of such models remain limited, often focusing on turn-based metrics or high-level corpus analyses (e.g., turn gaps, pauses). To address this gap, we present Full-Duplex-Bench, a new benchmark that systematically evaluates key conversational behaviors: pause handling, backchanneling, turn-taking, and interruption management. Our framework uses automatic metrics for consistent and reproducible assessments of SDMs' interactive performance. By offering an open and standardized evaluation benchmark, we aim to advance spoken dialogue modeling and encourage the development of more interactive and natural dialogue systems.","sentences":["Spoken dialogue modeling introduces unique challenges beyond text-based language modeling, demanding robust turn-taking, backchanneling, and real-time interaction.","Although most Spoken Dialogue Models (SDMs) rely on half-duplex processing (handling speech one turn at a time), emerging full-duplex SDMs can listen and speak simultaneously, enabling more natural and engaging conversations.","However, current evaluations of such models remain limited, often focusing on turn-based metrics or high-level corpus analyses (e.g., turn gaps, pauses).","To address this gap, we present Full-Duplex-Bench, a new benchmark that systematically evaluates key conversational behaviors: pause handling, backchanneling, turn-taking, and interruption management.","Our framework uses automatic metrics for consistent and reproducible assessments of SDMs' interactive performance.","By offering an open and standardized evaluation benchmark, we aim to advance spoken dialogue modeling and encourage the development of more interactive and natural dialogue systems."],"url":"http://arxiv.org/abs/2503.04721v1"}
{"created":"2025-03-06 18:59:06","title":"FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video","abstract":"We study reconstructing and predicting 3D fluid appearance and velocity from a single video. Current methods require multi-view videos for fluid reconstruction. We present FluidNexus, a novel framework that bridges video generation and physics simulation to tackle this task. Our key insight is to synthesize multiple novel-view videos as references for reconstruction. FluidNexus consists of two key components: (1) a novel-view video synthesizer that combines frame-wise view synthesis with video diffusion refinement for generating realistic videos, and (2) a physics-integrated particle representation coupling differentiable simulation and rendering to simultaneously facilitate 3D fluid reconstruction and prediction. To evaluate our approach, we collect two new real-world fluid datasets featuring textured backgrounds and object interactions. Our method enables dynamic novel view synthesis, future prediction, and interaction simulation from a single fluid video. Project website: https://yuegao.me/FluidNexus.","sentences":["We study reconstructing and predicting 3D fluid appearance and velocity from a single video.","Current methods require multi-view videos for fluid reconstruction.","We present FluidNexus, a novel framework that bridges video generation and physics simulation to tackle this task.","Our key insight is to synthesize multiple novel-view videos as references for reconstruction.","FluidNexus consists of two key components: (1) a novel-view video synthesizer that combines frame-wise view synthesis with video diffusion refinement for generating realistic videos, and (2) a physics-integrated particle representation coupling differentiable simulation and rendering to simultaneously facilitate 3D fluid reconstruction and prediction.","To evaluate our approach, we collect two new real-world fluid datasets featuring textured backgrounds and object interactions.","Our method enables dynamic novel view synthesis, future prediction, and interaction simulation from a single fluid video.","Project website: https://yuegao.me/FluidNexus."],"url":"http://arxiv.org/abs/2503.04720v1"}
{"created":"2025-03-06 18:58:45","title":"Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation","abstract":"Scene flow estimation is a foundational task for many robotic applications, including robust dynamic object detection, automatic labeling, and sensor synchronization. Two types of approaches to the problem have evolved: 1) Supervised and 2) optimization-based methods. Supervised methods are fast during inference and achieve high-quality results, however, they are limited by the need for large amounts of labeled training data and are susceptible to domain gaps. In contrast, unsupervised test-time optimization methods do not face the problem of domain gaps but usually suffer from substantial runtime, exhibit artifacts, or fail to converge to the right solution. In this work, we mitigate several limitations of existing optimization-based methods. To this end, we 1) introduce a simple voxel grid-based model that improves over the standard MLP-based formulation in multiple dimensions and 2) introduce a new multiframe loss formulation. 3) We combine both contributions in our new method, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only by EulerFlow among unsupervised methods while achieving comparable performance at a fraction of the computational cost. Floxels achieves a massive speedup of more than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10 minutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels achieves a speedup of ~14x.","sentences":["Scene flow estimation is a foundational task for many robotic applications, including robust dynamic object detection, automatic labeling, and sensor synchronization.","Two types of approaches to the problem have evolved: 1) Supervised and 2) optimization-based methods.","Supervised methods are fast during inference and achieve high-quality results, however, they are limited by the need for large amounts of labeled training data and are susceptible to domain gaps.","In contrast, unsupervised test-time optimization methods do not face the problem of domain gaps but usually suffer from substantial runtime, exhibit artifacts, or fail to converge to the right solution.","In this work, we mitigate several limitations of existing optimization-based methods.","To this end, we 1) introduce a simple voxel grid-based model that improves over the standard MLP-based formulation in multiple dimensions and 2) introduce a new multiframe loss formulation.","3) We combine both contributions in our new method, termed Floxels.","On the Argoverse 2 benchmark, Floxels is surpassed only by EulerFlow among unsupervised methods while achieving comparable performance at a fraction of the computational cost.","Floxels achieves a massive speedup of more than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10 minutes per sequence.","Over the faster but low-quality baseline, NSFP, Floxels achieves a speedup of ~14x."],"url":"http://arxiv.org/abs/2503.04718v1"}
{"created":"2025-03-06 18:58:29","title":"Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large Language Model Pretraining","abstract":"The impressive capabilities of Large Language Models (LLMs) across diverse tasks are now well-established, yet their effective deployment necessitates careful hyperparameter optimization. Through extensive empirical studies involving grid searches across diverse configurations, we discover universal scaling laws governing these hyperparameters: optimal learning rate follows a power-law relationship with both model parameters and data sizes, while optimal batch size scales primarily with data sizes. Our analysis reveals a convex optimization landscape for hyperparameters under fixed models and data size conditions. This convexity implies an optimal hyperparameter plateau. We contribute a universal, plug-and-play optimal hyperparameter tool for the community. Its estimated values on the test set are merely 0.07\\% away from the globally optimal LLM performance found via an exhaustive search. These laws demonstrate remarkable robustness across variations in model sparsity, training data distribution, and model shape. To our best known, this is the first work that unifies different model shapes and structures, such as Mixture-of-Experts models and dense transformers, as well as establishes optimal hyperparameter scaling laws across diverse data distributions. This exhaustive optimization process demands substantial computational resources, utilizing nearly one million NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and hyperparameters from scratch and consuming approximately 100 trillion tokens in total. To facilitate reproducibility and further research, we will progressively release all loss measurements and model checkpoints through our designated repository https://step-law.github.io/","sentences":["The impressive capabilities of Large Language Models (LLMs) across diverse tasks are now well-established, yet their effective deployment necessitates careful hyperparameter optimization.","Through extensive empirical studies involving grid searches across diverse configurations, we discover universal scaling laws governing these hyperparameters: optimal learning rate follows a power-law relationship with both model parameters and data sizes, while optimal batch size scales primarily with data sizes.","Our analysis reveals a convex optimization landscape for hyperparameters under fixed models and data size conditions.","This convexity implies an optimal hyperparameter plateau.","We contribute a universal, plug-and-play optimal hyperparameter tool for the community.","Its estimated values on the test set are merely 0.07\\%","away from the globally optimal LLM performance found via an exhaustive search.","These laws demonstrate remarkable robustness across variations in model sparsity, training data distribution, and model shape.","To our best known, this is the first work that unifies different model shapes and structures, such as Mixture-of-Experts models and dense transformers, as well as establishes optimal hyperparameter scaling laws across diverse data distributions.","This exhaustive optimization process demands substantial computational resources, utilizing nearly one million NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and hyperparameters from scratch and consuming approximately 100 trillion tokens in total.","To facilitate reproducibility and further research, we will progressively release all loss measurements and model checkpoints through our designated repository https://step-law.github.io/"],"url":"http://arxiv.org/abs/2503.04715v1"}
{"created":"2025-03-06 18:57:16","title":"Self-Supervised Models for Phoneme Recognition: Applications in Children's Speech for Reading Learning","abstract":"Child speech recognition is still an underdeveloped area of research due to the lack of data (especially on non-English languages) and the specific difficulties of this task. Having explored various architectures for child speech recognition in previous work, in this article we tackle recent self-supervised models. We first compare wav2vec 2.0, HuBERT and WavLM models adapted to phoneme recognition in French child speech, and continue our experiments with the best of them, WavLM base+. We then further adapt it by unfreezing its transformer blocks during fine-tuning on child speech, which greatly improves its performance and makes it significantly outperform our base model, a Transformer+CTC. Finally, we study in detail the behaviour of these two models under the real conditions of our application, and show that WavLM base+ is more robust to various reading tasks and noise levels. Index Terms: speech recognition, child speech, self-supervised learning","sentences":["Child speech recognition is still an underdeveloped area of research due to the lack of data (especially on non-English languages) and the specific difficulties of this task.","Having explored various architectures for child speech recognition in previous work, in this article we tackle recent self-supervised models.","We first compare wav2vec 2.0, HuBERT and WavLM models adapted to phoneme recognition in French child speech, and continue our experiments with the best of them, WavLM base+.","We then further adapt it by unfreezing its transformer blocks during fine-tuning on child speech, which greatly improves its performance and makes it significantly outperform our base model, a Transformer+CTC.","Finally, we study in detail the behaviour of these two models under the real conditions of our application, and show that WavLM base+ is more robust to various reading tasks and noise levels.","Index Terms: speech recognition, child speech, self-supervised learning"],"url":"http://arxiv.org/abs/2503.04710v1"}
{"created":"2025-03-06 18:55:21","title":"Iris Style Transfer: Enhancing Iris Recognition with Style Features and Privacy Preservation through Neural Style Transfer","abstract":"Iris texture is widely regarded as a gold standard biometric modality for authentication and identification. The demand for robust iris recognition methods, coupled with growing security and privacy concerns regarding iris attacks, has escalated recently. Inspired by neural style transfer, an advanced technique that leverages neural networks to separate content and style features, we hypothesize that iris texture's style features provide a reliable foundation for recognition and are more resilient to variations like rotation and perspective shifts than traditional approaches. Our experimental results support this hypothesis, showing a significantly higher classification accuracy compared to conventional features. Further, we propose using neural style transfer to mask identifiable iris style features, ensuring the protection of sensitive biometric information while maintaining the utility of eye images for tasks like eye segmentation and gaze estimation. This work opens new avenues for iris-oriented, secure, and privacy-aware biometric systems.","sentences":["Iris texture is widely regarded as a gold standard biometric modality for authentication and identification.","The demand for robust iris recognition methods, coupled with growing security and privacy concerns regarding iris attacks, has escalated recently.","Inspired by neural style transfer, an advanced technique that leverages neural networks to separate content and style features, we hypothesize that iris texture's style features provide a reliable foundation for recognition and are more resilient to variations like rotation and perspective shifts than traditional approaches.","Our experimental results support this hypothesis, showing a significantly higher classification accuracy compared to conventional features.","Further, we propose using neural style transfer to mask identifiable iris style features, ensuring the protection of sensitive biometric information while maintaining the utility of eye images for tasks like eye segmentation and gaze estimation.","This work opens new avenues for iris-oriented, secure, and privacy-aware biometric systems."],"url":"http://arxiv.org/abs/2503.04707v1"}
{"created":"2025-03-06 18:54:42","title":"Sample-Optimal Agnostic Boosting with Unlabeled Data","abstract":"Boosting provides a practical and provably effective framework for constructing accurate learning algorithms from inaccurate rules of thumb. It extends the promise of sample-efficient learning to settings where direct Empirical Risk Minimization (ERM) may not be implementable efficiently. In the realizable setting, boosting is known to offer this computational reprieve without compromising on sample efficiency. However, in the agnostic case, existing boosting algorithms fall short of achieving the optimal sample complexity.   This paper highlights an unexpected and previously unexplored avenue of improvement: unlabeled samples. We design a computationally efficient agnostic boosting algorithm that matches the sample complexity of ERM, given polynomially many additional unlabeled samples. In fact, we show that the total number of samples needed, unlabeled and labeled inclusive, is never more than that for the best known agnostic boosting algorithm -- so this result is never worse -- while only a vanishing fraction of these need to be labeled for the algorithm to succeed. This is particularly fortuitous for learning-theoretic applications of agnostic boosting, which often take place in the distribution-specific setting, where unlabeled samples can be availed for free. We detail other applications of this result in reinforcement learning.","sentences":["Boosting provides a practical and provably effective framework for constructing accurate learning algorithms from inaccurate rules of thumb.","It extends the promise of sample-efficient learning to settings where direct Empirical Risk Minimization (ERM) may not be implementable efficiently.","In the realizable setting, boosting is known to offer this computational reprieve without compromising on sample efficiency.","However, in the agnostic case, existing boosting algorithms fall short of achieving the optimal sample complexity.   ","This paper highlights an unexpected and previously unexplored avenue of improvement: unlabeled samples.","We design a computationally efficient agnostic boosting algorithm that matches the sample complexity of ERM, given polynomially many additional unlabeled samples.","In fact, we show that the total number of samples needed, unlabeled and labeled inclusive, is never more than that for the best known agnostic boosting algorithm -- so this result is never worse -- while only a vanishing fraction of these need to be labeled for the algorithm to succeed.","This is particularly fortuitous for learning-theoretic applications of agnostic boosting, which often take place in the distribution-specific setting, where unlabeled samples can be availed for free.","We detail other applications of this result in reinforcement learning."],"url":"http://arxiv.org/abs/2503.04706v1"}
{"created":"2025-03-06 18:54:32","title":"Universality of Layer-Level Entropy-Weighted Quantization Beyond Model Architecture and Size","abstract":"We present a novel approach to selective model quantization that transcends the limitations of architecture-specific and size-dependent compression methods for Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By analyzing the entropy distribution across transformer blocks, EWQ determines which blocks can be safely quantized without causing significant performance degradation, independent of model architecture or size. Our method outperforms uniform quantization approaches, maintaining Massive Multitask Language Understanding (MMLU) accuracy scores within 0.5% of unquantized models while reducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ across multiple architectures-from 1.6B to 70B parameters-showcasing consistent improvements in the quality-compression trade-off regardless of model scale or architectural design. A surprising finding of EWQ is its ability to reduce perplexity compared to unquantized models, suggesting the presence of beneficial regularization through selective precision reduction. This improvement holds across different model families, indicating a fundamental relationship between layer-level entropy and optimal precision requirements. Additionally, we introduce FastEWQ, a rapid method for entropy distribution analysis that eliminates the need for loading model weights. This technique leverages universal characteristics of entropy distribution that persist across various architectures and scales, enabling near-instantaneous quantization decisions while maintaining 80% classification accuracy with full entropy analysis. Our results demonstrate that effective quantization strategies can be developed independently of specific architectural choices or model sizes, opening new possibilities for efficient LLM deployment.","sentences":["We present a novel approach to selective model quantization that transcends the limitations of architecture-specific and size-dependent compression methods for Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ).","By analyzing the entropy distribution across transformer blocks, EWQ determines which blocks can be safely quantized without causing significant performance degradation, independent of model architecture or size.","Our method outperforms uniform quantization approaches, maintaining Massive Multitask Language Understanding (MMLU) accuracy scores within 0.5% of unquantized models while reducing memory usage by up to 18%.","We demonstrate the effectiveness of EWQ across multiple architectures-from 1.6B to 70B parameters-showcasing consistent improvements in the quality-compression trade-off regardless of model scale or architectural design.","A surprising finding of EWQ is its ability to reduce perplexity compared to unquantized models, suggesting the presence of beneficial regularization through selective precision reduction.","This improvement holds across different model families, indicating a fundamental relationship between layer-level entropy and optimal precision requirements.","Additionally, we introduce FastEWQ, a rapid method for entropy distribution analysis that eliminates the need for loading model weights.","This technique leverages universal characteristics of entropy distribution that persist across various architectures and scales, enabling near-instantaneous quantization decisions while maintaining 80% classification accuracy with full entropy analysis.","Our results demonstrate that effective quantization strategies can be developed independently of specific architectural choices or model sizes, opening new possibilities for efficient LLM deployment."],"url":"http://arxiv.org/abs/2503.04704v1"}
{"created":"2025-03-06 18:46:10","title":"DEAL-YOLO: Drone-based Efficient Animal Localization using YOLO","abstract":"Although advances in deep learning and aerial surveillance technology are improving wildlife conservation efforts, complex and erratic environmental conditions still pose a problem, requiring innovative solutions for cost-effective small animal detection. This work introduces DEAL-YOLO, a novel approach that improves small object detection in Unmanned Aerial Vehicle (UAV) images by using multi-objective loss functions like Wise IoU (WIoU) and Normalized Wasserstein Distance (NWD), which prioritize pixels near the centre of the bounding box, ensuring smoother localization and reducing abrupt deviations. Additionally, the model is optimized through efficient feature extraction with Linear Deformable (LD) convolutions, enhancing accuracy while maintaining computational efficiency. The Scaled Sequence Feature Fusion (SSFF) module enhances object detection by effectively capturing inter-scale relationships, improving feature representation, and boosting metrics through optimized multiscale fusion. Comparison with baseline models reveals high efficacy with up to 69.5\\% fewer parameters compared to vanilla Yolov8-N, highlighting the robustness of the proposed modifications. Through this approach, our paper aims to facilitate the detection of endangered species, animal population analysis, habitat monitoring, biodiversity research, and various other applications that enrich wildlife conservation efforts. DEAL-YOLO employs a two-stage inference paradigm for object detection, refining selected regions to improve localization and confidence. This approach enhances performance, especially for small instances with low objectness scores.","sentences":["Although advances in deep learning and aerial surveillance technology are improving wildlife conservation efforts, complex and erratic environmental conditions still pose a problem, requiring innovative solutions for cost-effective small animal detection.","This work introduces DEAL-YOLO, a novel approach that improves small object detection in Unmanned Aerial Vehicle (UAV) images by using multi-objective loss functions like Wise IoU (WIoU) and Normalized Wasserstein Distance (NWD), which prioritize pixels near the centre of the bounding box, ensuring smoother localization and reducing abrupt deviations.","Additionally, the model is optimized through efficient feature extraction with Linear Deformable (LD) convolutions, enhancing accuracy while maintaining computational efficiency.","The Scaled Sequence Feature Fusion (SSFF) module enhances object detection by effectively capturing inter-scale relationships, improving feature representation, and boosting metrics through optimized multiscale fusion.","Comparison with baseline models reveals high efficacy with up to 69.5\\% fewer parameters compared to vanilla Yolov8-N, highlighting the robustness of the proposed modifications.","Through this approach, our paper aims to facilitate the detection of endangered species, animal population analysis, habitat monitoring, biodiversity research, and various other applications that enrich wildlife conservation efforts.","DEAL-YOLO employs a two-stage inference paradigm for object detection, refining selected regions to improve localization and confidence.","This approach enhances performance, especially for small instances with low objectness scores."],"url":"http://arxiv.org/abs/2503.04698v1"}
{"created":"2025-03-06 18:43:29","title":"L1: Controlling How Long A Reasoning Model Thinks With Reinforcement Learning","abstract":"Reasoning language models have shown an uncanny ability to improve performance at test-time by ``thinking longer''-that is, by generating longer chain-of-thought sequences and hence using more compute. However, the length of their chain-of-thought reasoning is not controllable, making it impossible to allocate test-time compute to achieve a desired level of performance. We introduce Length Controlled Policy Optimization (LCPO), a simple reinforcement learning method that optimizes for accuracy and adherence to user-specified length constraints. We use LCPO to train L1, a reasoning language model that produces outputs satisfying a length constraint given in its prompt. L1's length control allows for smoothly trading off computational cost and accuracy on a wide range of tasks, and outperforms the state-of-the-art S1 method for length control. Furthermore, we uncover an unexpected short chain-of-thought capability in models trained with LCPO. For instance, our 1.5B L1 model surpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise control over reasoning length, allowing for fine-grained allocation of test-time compute and accuracy. We release code and models at https://www.cmu-l3.github.io/l1","sentences":["Reasoning language models have shown an uncanny ability to improve performance at test-time by ``thinking longer''-that is, by generating longer chain-of-thought sequences and hence using more compute.","However, the length of their chain-of-thought reasoning is not controllable, making it impossible to allocate test-time compute to achieve a desired level of performance.","We introduce Length Controlled Policy Optimization (LCPO), a simple reinforcement learning method that optimizes for accuracy and adherence to user-specified length constraints.","We use LCPO to train L1, a reasoning language model that produces outputs satisfying a length constraint given in its prompt.","L1's length control allows for smoothly trading off computational cost and accuracy on a wide range of tasks, and outperforms the state-of-the-art S1 method for length control.","Furthermore, we uncover an unexpected short chain-of-thought capability in models trained with LCPO.","For instance, our 1.5B L1 model surpasses GPT-4o at equal reasoning lengths.","Overall, LCPO enables precise control over reasoning length, allowing for fine-grained allocation of test-time compute and accuracy.","We release code and models at https://www.cmu-l3.github.io/l1"],"url":"http://arxiv.org/abs/2503.04697v1"}
{"created":"2025-03-06 18:42:36","title":"Assessing Student Adoption of Generative Artificial Intelligence across Engineering Education from 2023 to 2024","abstract":"Generative Artificial Intelligence (GenAI) tools and models have the potential to re-shape educational needs, norms, practices, and policies in all sectors of engineering education. Empirical data, rather than anecdata and assumptions, on how engineering students have adopted GenAI is essential to developing a foundational understanding of students' GenAI-related behaviors and needs during academic training. This data will also help formulate effective responses to GenAI by both academic institutions and industrial employers. We collected two representative survey samples at the Colorado School of Mines, a small engineering-focused R-1 university in the USA, in May 2023 ($n_1=601$) and September 2024 ($n_2=862$) to address research questions related to (RQ1) how GenAI has been adopted by engineering students, including motivational and demographic factors contributing to GenAI use, (RQ2) students' ethical concerns about GenAI, and (RQ3) students' perceived benefits v.s. harms for themselves, science, and society. Analysis revealed a statistically significant rise in GenAI adoption rates from 2023 to 2024. Students predominantly leverage GenAI tools to deepen understanding, enhance work quality, and stay informed about emerging technologies. Although most students assess their own usage of GenAI as ethical and beneficial, they nonetheless expressed significant concerns regarding GenAI and its impacts on society. We collected student estimates of ``P(doom)'' and discovered a bimodal distribution. Thus, we show that the student body at Mines is polarized with respect to future impacts of GenAI on the engineering workforce and society, despite being increasingly willing to explore GenAI over time. We discuss implications of these findings for future research and for integrating GenAI in engineering education.","sentences":["Generative Artificial Intelligence (GenAI) tools and models have the potential to re-shape educational needs, norms, practices, and policies in all sectors of engineering education.","Empirical data, rather than anecdata and assumptions, on how engineering students have adopted GenAI is essential to developing a foundational understanding of students' GenAI-related behaviors and needs during academic training.","This data will also help formulate effective responses to GenAI by both academic institutions and industrial employers.","We collected two representative survey samples at the Colorado School of Mines, a small engineering-focused R-1 university in the USA, in May 2023 ($n_1=601$) and September 2024 ($n_2=862$) to address research questions related to (RQ1) how GenAI has been adopted by engineering students, including motivational and demographic factors contributing to GenAI use, (RQ2) students' ethical concerns about GenAI, and (RQ3) students' perceived benefits v.s. harms for themselves, science, and society.","Analysis revealed a statistically significant rise in GenAI adoption rates from 2023 to 2024.","Students predominantly leverage GenAI tools to deepen understanding, enhance work quality, and stay informed about emerging technologies.","Although most students assess their own usage of GenAI as ethical and beneficial, they nonetheless expressed significant concerns regarding GenAI and its impacts on society.","We collected student estimates of ``P(doom)'' and discovered a bimodal distribution.","Thus, we show that the student body at Mines is polarized with respect to future impacts of GenAI on the engineering workforce and society, despite being increasingly willing to explore GenAI over time.","We discuss implications of these findings for future research and for integrating GenAI in engineering education."],"url":"http://arxiv.org/abs/2503.04696v1"}
{"created":"2025-03-06 18:40:00","title":"UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to Forgetting Targets","abstract":"Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets. LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance. Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning. In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge. To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets. Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark.","sentences":["Large Language Models (LLMs) inevitably acquire harmful information during training on massive datasets.","LLM unlearning aims to eliminate the influence of such harmful information while maintaining the model's overall performance.","Existing unlearning methods, represented by gradient ascent-based approaches, primarily focus on forgetting target data while overlooking the crucial impact of logically related knowledge on the effectiveness of unlearning.","In this paper, through both theoretical and experimental analyses, we first demonstrate that a key reason for the suboptimal unlearning performance is that models can reconstruct the target content through reasoning with logically related knowledge.","To address this issue, we propose Unlearning Improvement via Parameter Extrapolation (UIPE), a method that removes knowledge highly correlated with the forgetting targets.","Experimental results show that UIPE significantly enhances the performance of various mainstream LLM unlearning methods on the TOFU benchmark."],"url":"http://arxiv.org/abs/2503.04693v1"}
{"created":"2025-03-06 18:37:44","title":"The Influence of Prior Discourse on Conversational Agent-Driven Decision-Making","abstract":"Persuasion through conversation has been the focus of much research. Nudging is a popular strategy to influence decision-making in physical and digital settings. However, conversational agents employing \"nudging\" have not received significant attention. We explore the manifestation of cognitive biases-the underlying psychological mechanisms of nudging-and investigate how the complexity of prior dialogue tasks impacts decision-making facilitated by conversational agents. Our research used a between-group experimental design, involving 756 participants randomly assigned to either a simple or complex task before encountering a decision-making scenario. Three scenarios were adapted from Samuelson's classic experiments on status-quo bias, the underlying mechanism of default nudges. Our results aligned with previous studies in two out of three simple-task scenarios. Increasing task complexity consistently shifted effect-sizes toward our hypothesis, though bias was significant in only one case. These findings inform conversational nudging strategies and highlight inherent biases relevant to behavioural economics.","sentences":["Persuasion through conversation has been the focus of much research.","Nudging is a popular strategy to influence decision-making in physical and digital settings.","However, conversational agents employing \"nudging\" have not received significant attention.","We explore the manifestation of cognitive biases-the underlying psychological mechanisms of nudging-and investigate how the complexity of prior dialogue tasks impacts decision-making facilitated by conversational agents.","Our research used a between-group experimental design, involving 756 participants randomly assigned to either a simple or complex task before encountering a decision-making scenario.","Three scenarios were adapted from Samuelson's classic experiments on status-quo bias, the underlying mechanism of default nudges.","Our results aligned with previous studies in two out of three simple-task scenarios.","Increasing task complexity consistently shifted effect-sizes toward our hypothesis, though bias was significant in only one case.","These findings inform conversational nudging strategies and highlight inherent biases relevant to behavioural economics."],"url":"http://arxiv.org/abs/2503.04692v1"}
{"created":"2025-03-06 18:35:39","title":"Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases","abstract":"The latest reasoning-enhanced large language models (reasoning LLMs), such as DeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success. However, the application of such reasoning enhancements to the highly professional medical domain has not been clearly evaluated, particularly regarding with not only assessing the final generation but also examining the quality of their reasoning processes. In this study, we present MedR-Bench, a reasoning-focused medical evaluation benchmark comprising 1,453 structured patient cases with reasoning references mined from case reports. Our benchmark spans 13 body systems and 10 specialty disorders, encompassing both common and rare diseases. In our evaluation, we introduce a versatile framework consisting of three critical clinical stages: assessment recommendation, diagnostic decision-making, and treatment planning, comprehensively capturing the LLMs' performance across the entire patient journey in healthcare. For metrics, we propose a novel agentic system, Reasoning Evaluator, designed to automate and objectively quantify free-text reasoning responses in a scalable manner from the perspectives of efficiency, factuality, and completeness by dynamically searching and performing cross-referencing checks. As a result, we assess five state-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and others. Our results reveal that current LLMs can handle relatively simple diagnostic tasks with sufficient critical assessment results, achieving accuracy generally over 85%. However, they still struggle with more complex tasks, such as assessment recommendation and treatment planning. In reasoning, their reasoning processes are generally reliable, with factuality scores exceeding 90%, though they often omit critical reasoning steps. Our study clearly reveals further development directions for current clinical LLMs.","sentences":["The latest reasoning-enhanced large language models (reasoning LLMs), such as DeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success.","However, the application of such reasoning enhancements to the highly professional medical domain has not been clearly evaluated, particularly regarding with not only assessing the final generation but also examining the quality of their reasoning processes.","In this study, we present MedR-Bench, a reasoning-focused medical evaluation benchmark comprising 1,453 structured patient cases with reasoning references mined from case reports.","Our benchmark spans 13 body systems and 10 specialty disorders, encompassing both common and rare diseases.","In our evaluation, we introduce a versatile framework consisting of three critical clinical stages: assessment recommendation, diagnostic decision-making, and treatment planning, comprehensively capturing the LLMs' performance across the entire patient journey in healthcare.","For metrics, we propose a novel agentic system, Reasoning Evaluator, designed to automate and objectively quantify free-text reasoning responses in a scalable manner from the perspectives of efficiency, factuality, and completeness by dynamically searching and performing cross-referencing checks.","As a result, we assess five state-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and others.","Our results reveal that current LLMs can handle relatively simple diagnostic tasks with sufficient critical assessment results, achieving accuracy generally over 85%.","However, they still struggle with more complex tasks, such as assessment recommendation and treatment planning.","In reasoning, their reasoning processes are generally reliable, with factuality scores exceeding 90%, though they often omit critical reasoning steps.","Our study clearly reveals further development directions for current clinical LLMs."],"url":"http://arxiv.org/abs/2503.04691v1"}
{"created":"2025-03-06 18:31:41","title":"Teach YOLO to Remember: A Self-Distillation Approach for Continual Object Detection","abstract":"Real-time object detectors like YOLO achieve exceptional performance when trained on large datasets for multiple epochs. However, in real-world scenarios where data arrives incrementally, neural networks suffer from catastrophic forgetting, leading to a loss of previously learned knowledge. To address this, prior research has explored strategies for Class Incremental Learning (CIL) in Continual Learning for Object Detection (CLOD), with most approaches focusing on two-stage object detectors. However, existing work suggests that Learning without Forgetting (LwF) may be ineffective for one-stage anchor-free detectors like YOLO due to noisy regression outputs, which risk transferring corrupted knowledge. In this work, we introduce YOLO LwF, a self-distillation approach tailored for YOLO-based continual object detection. We demonstrate that when coupled with a replay memory, YOLO LwF significantly mitigates forgetting. Compared to previous approaches, it achieves state-of-the-art performance, improving mAP by +2.1% and +2.9% on the VOC and COCO benchmarks, respectively.","sentences":["Real-time object detectors like YOLO achieve exceptional performance when trained on large datasets for multiple epochs.","However, in real-world scenarios where data arrives incrementally, neural networks suffer from catastrophic forgetting, leading to a loss of previously learned knowledge.","To address this, prior research has explored strategies for Class Incremental Learning (CIL) in Continual Learning for Object Detection (CLOD), with most approaches focusing on two-stage object detectors.","However, existing work suggests that Learning without Forgetting (LwF) may be ineffective for one-stage anchor-free detectors like YOLO due to noisy regression outputs, which risk transferring corrupted knowledge.","In this work, we introduce YOLO LwF, a self-distillation approach tailored for YOLO-based continual object detection.","We demonstrate that when coupled with a replay memory, YOLO LwF significantly mitigates forgetting.","Compared to previous approaches, it achieves state-of-the-art performance, improving mAP by +2.1% and +2.9% on the VOC and COCO benchmarks, respectively."],"url":"http://arxiv.org/abs/2503.04688v1"}
{"created":"2025-03-06 18:29:45","title":"Compositional World Knowledge leads to High Utility Synthetic data","abstract":"Machine learning systems struggle with robustness, under subpopulation shifts. This problem becomes especially pronounced in scenarios where only a subset of attribute combinations is observed during training -a severe form of subpopulation shift, referred as compositional shift. To address this problem, we ask the following question: Can we improve the robustness by training on synthetic data, spanning all possible attribute combinations? We first show that training of conditional diffusion models on limited data lead to incorrect underlying distribution. Therefore, synthetic data sampled from such models will result in unfaithful samples and does not lead to improve performance of downstream machine learning systems. To address this problem, we propose CoInD to reflect the compositional nature of the world by enforcing conditional independence through minimizing Fisher's divergence between joint and marginal distributions. We demonstrate that synthetic data generated by CoInD is faithful and this translates to state-of-the-art worst-group accuracy on compositional shift tasks on CelebA.","sentences":["Machine learning systems struggle with robustness, under subpopulation shifts.","This problem becomes especially pronounced in scenarios where only a subset of attribute combinations is observed during training -a severe form of subpopulation shift, referred as compositional shift.","To address this problem, we ask the following question: Can we improve the robustness by training on synthetic data, spanning all possible attribute combinations?","We first show that training of conditional diffusion models on limited data lead to incorrect underlying distribution.","Therefore, synthetic data sampled from such models will result in unfaithful samples and does not lead to improve performance of downstream machine learning systems.","To address this problem, we propose CoInD to reflect the compositional nature of the world by enforcing conditional independence through minimizing Fisher's divergence between joint and marginal distributions.","We demonstrate that synthetic data generated by CoInD is faithful and this translates to state-of-the-art worst-group accuracy on compositional shift tasks on CelebA."],"url":"http://arxiv.org/abs/2503.04687v1"}
{"created":"2025-03-06 18:27:41","title":"DIMSUM: Discourse in Mathematical Reasoning as a Supervision Module","abstract":"We look at reasoning on GSM8k, a dataset of short texts presenting primary school, math problems. We find, with Mirzadeh et al. (2024), that current LLM progress on the data set may not be explained by better reasoning but by exposure to a broader pretraining data distribution. We then introduce a novel information source for helping models with less data or inferior training reason better: discourse structure. We show that discourse structure improves performance for models like Llama2 13b by up to 160%. Even for models that have most likely memorized the data set, adding discourse structural information to the model still improves predictions and dramatically improves large model performance on out of distribution examples.","sentences":["We look at reasoning on GSM8k, a dataset of short texts presenting primary school, math problems.","We find, with Mirzadeh et al. (2024), that current LLM progress on the data set may not be explained by better reasoning but by exposure to a broader pretraining data distribution.","We then introduce a novel information source for helping models with less data or inferior training reason better: discourse structure.","We show that discourse structure improves performance for models like Llama2 13b by up to 160%.","Even for models that have most likely memorized the data set, adding discourse structural information to the model still improves predictions and dramatically improves large model performance on out of distribution examples."],"url":"http://arxiv.org/abs/2503.04685v1"}
{"created":"2025-03-06 18:22:46","title":"Matrix Factorization for Inferring Associations and Missing Links","abstract":"Missing link prediction is a method for network analysis, with applications in recommender systems, biology, social sciences, cybersecurity, information retrieval, and Artificial Intelligence (AI) reasoning in Knowledge Graphs. Missing link prediction identifies unseen but potentially existing connections in a network by analyzing the observed patterns and relationships. In proliferation detection, this supports efforts to identify and characterize attempts by state and non-state actors to acquire nuclear weapons or associated technology - a notoriously challenging but vital mission for global security. Dimensionality reduction techniques like Non-Negative Matrix Factorization (NMF) and Logistic Matrix Factorization (LMF) are effective but require selection of the matrix rank parameter, that is, of the number of hidden features, k, to avoid over/under-fitting. We introduce novel Weighted (WNMFk), Boolean (BNMFk), and Recommender (RNMFk) matrix factorization methods, along with ensemble variants incorporating logistic factorization, for link prediction. Our methods integrate automatic model determination for rank estimation by evaluating stability and accuracy using a modified bootstrap methodology and uncertainty quantification (UQ), assessing prediction reliability under random perturbations. We incorporate Otsu threshold selection and k-means clustering for Boolean matrix factorization, comparing them to coordinate descent-based Boolean thresholding. Our experiments highlight the impact of rank k selection, evaluate model performance under varying test-set sizes, and demonstrate the benefits of UQ for reliable predictions using abstention. We validate our methods on three synthetic datasets (Boolean and uniformly distributed) and benchmark them against LMF and symmetric LMF (symLMF) on five real-world protein-protein interaction networks, showcasing an improved prediction performance.","sentences":["Missing link prediction is a method for network analysis, with applications in recommender systems, biology, social sciences, cybersecurity, information retrieval, and Artificial Intelligence (AI) reasoning in Knowledge Graphs.","Missing link prediction identifies unseen but potentially existing connections in a network by analyzing the observed patterns and relationships.","In proliferation detection, this supports efforts to identify and characterize attempts by state and non-state actors to acquire nuclear weapons or associated technology - a notoriously challenging but vital mission for global security.","Dimensionality reduction techniques like Non-Negative Matrix Factorization (NMF) and Logistic Matrix Factorization (LMF) are effective but require selection of the matrix rank parameter, that is, of the number of hidden features, k, to avoid over/under-fitting.","We introduce novel Weighted (WNMFk), Boolean (BNMFk), and Recommender (RNMFk) matrix factorization methods, along with ensemble variants incorporating logistic factorization, for link prediction.","Our methods integrate automatic model determination for rank estimation by evaluating stability and accuracy using a modified bootstrap methodology and uncertainty quantification (UQ), assessing prediction reliability under random perturbations.","We incorporate Otsu threshold selection and k-means clustering for Boolean matrix factorization, comparing them to coordinate descent-based Boolean thresholding.","Our experiments highlight the impact of rank k selection, evaluate model performance under varying test-set sizes, and demonstrate the benefits of UQ for reliable predictions using abstention.","We validate our methods on three synthetic datasets (Boolean and uniformly distributed) and benchmark them against LMF and symmetric LMF (symLMF) on five real-world protein-protein interaction networks, showcasing an improved prediction performance."],"url":"http://arxiv.org/abs/2503.04680v1"}
{"created":"2025-03-06 18:22:29","title":"Multi-Agent Inverse Q-Learning from Demonstrations","abstract":"When reward functions are hand-designed, deep reinforcement learning algorithms often suffer from reward misspecification, causing them to learn suboptimal policies in terms of the intended task objectives. In the single-agent case, inverse reinforcement learning (IRL) techniques attempt to address this issue by inferring the reward function from expert demonstrations. However, in multi-agent problems, misalignment between the learned and true objectives is exacerbated due to increased environment non-stationarity and variance that scales with multiple agents. As such, in multi-agent general-sum games, multi-agent IRL algorithms have difficulty balancing cooperative and competitive objectives. To address these issues, we propose Multi-Agent Marginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient framework for multi-agent IRL. For each agent, MAMQL learns a critic marginalized over the other agents' policies, allowing for a well-motivated use of Boltzmann policies in the multi-agent context. We identify a connection between optimal marginalized critics and single-agent soft-Q IRL, allowing us to apply a direct, simple optimization criterion from the single-agent domain. Across our experiments on three different simulated domains, MAMQL significantly outperforms previous multi-agent methods in average reward, sample efficiency, and reward recovery by often more than 2-5x. We make our code available at https://sites.google.com/view/mamql .","sentences":["When reward functions are hand-designed, deep reinforcement learning algorithms often suffer from reward misspecification, causing them to learn suboptimal policies in terms of the intended task objectives.","In the single-agent case, inverse reinforcement learning (IRL) techniques attempt to address this issue by inferring the reward function from expert demonstrations.","However, in multi-agent problems, misalignment between the learned and true objectives is exacerbated due to increased environment non-stationarity and variance that scales with multiple agents.","As such, in multi-agent general-sum games, multi-agent IRL algorithms have difficulty balancing cooperative and competitive objectives.","To address these issues, we propose Multi-Agent Marginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient framework for multi-agent IRL.","For each agent, MAMQL learns a critic marginalized over the other agents' policies, allowing for a well-motivated use of Boltzmann policies in the multi-agent context.","We identify a connection between optimal marginalized critics and single-agent soft-Q IRL, allowing us to apply a direct, simple optimization criterion from the single-agent domain.","Across our experiments on three different simulated domains, MAMQL significantly outperforms previous multi-agent methods in average reward, sample efficiency, and reward recovery by often more than 2-5x.","We make our code available at https://sites.google.com/view/mamql ."],"url":"http://arxiv.org/abs/2503.04679v1"}
{"created":"2025-03-06 18:12:33","title":"LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable User Satisfaction Estimation in Dialogue","abstract":"Understanding user satisfaction with conversational systems, known as User Satisfaction Estimation (USE), is essential for assessing dialogue quality and enhancing user experiences. However, existing methods for USE face challenges due to limited understanding of underlying reasons for user dissatisfaction and the high costs of annotating user intentions. To address these challenges, we propose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction Estimation), an interpretable framework for effective user satisfaction prediction. PRAISE operates through three key modules. The Strategy Planner develops strategies, which are natural language criteria for classifying user satisfaction. The Feature Retriever then incorporates knowledge on user satisfaction from Large Language Models (LLMs) and retrieves relevance features from utterances. Finally, the Score Analyzer evaluates strategy predictions and classifies user satisfaction. Experimental results demonstrate that PRAISE achieves state-of-the-art performance on three benchmarks for the USE task. Beyond its superior performance, PRAISE offers additional benefits. It enhances interpretability by providing instance-level explanations through effective alignment of utterances with strategies. Moreover, PRAISE operates more efficiently than existing approaches by eliminating the need for LLMs during the inference phase.","sentences":["Understanding user satisfaction with conversational systems, known as User Satisfaction Estimation (USE), is essential for assessing dialogue quality and enhancing user experiences.","However, existing methods for USE face challenges due to limited understanding of underlying reasons for user dissatisfaction and the high costs of annotating user intentions.","To address these challenges, we propose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction Estimation), an interpretable framework for effective user satisfaction prediction.","PRAISE operates through three key modules.","The Strategy Planner develops strategies, which are natural language criteria for classifying user satisfaction.","The Feature Retriever then incorporates knowledge on user satisfaction from Large Language Models (LLMs) and retrieves relevance features from utterances.","Finally, the Score Analyzer evaluates strategy predictions and classifies user satisfaction.","Experimental results demonstrate that PRAISE achieves state-of-the-art performance on three benchmarks for the USE task.","Beyond its superior performance, PRAISE offers additional benefits.","It enhances interpretability by providing instance-level explanations through effective alignment of utterances with strategies.","Moreover, PRAISE operates more efficiently than existing approaches by eliminating the need for LLMs during the inference phase."],"url":"http://arxiv.org/abs/2503.04675v1"}
{"created":"2025-03-06 17:59:51","title":"An Information-theoretic Multi-task Representation Learning Framework for Natural Language Understanding","abstract":"This paper proposes a new principled multi-task representation learning framework (InfoMTL) to extract noise-invariant sufficient representations for all tasks. It ensures sufficiency of shared representations for all tasks and mitigates the negative effect of redundant features, which can enhance language understanding of pre-trained language models (PLMs) under the multi-task paradigm. Firstly, a shared information maximization principle is proposed to learn more sufficient shared representations for all target tasks. It can avoid the insufficiency issue arising from representation compression in the multi-task paradigm. Secondly, a task-specific information minimization principle is designed to mitigate the negative effect of potential redundant features in the input for each task. It can compress task-irrelevant redundant information and preserve necessary information relevant to the target for multi-task prediction. Experiments on six classification benchmarks show that our method outperforms 12 comparative multi-task methods under the same multi-task settings, especially in data-constrained and noisy scenarios. Extensive experiments demonstrate that the learned representations are more sufficient, data-efficient, and robust.","sentences":["This paper proposes a new principled multi-task representation learning framework (InfoMTL) to extract noise-invariant sufficient representations for all tasks.","It ensures sufficiency of shared representations for all tasks and mitigates the negative effect of redundant features, which can enhance language understanding of pre-trained language models (PLMs) under the multi-task paradigm.","Firstly, a shared information maximization principle is proposed to learn more sufficient shared representations for all target tasks.","It can avoid the insufficiency issue arising from representation compression in the multi-task paradigm.","Secondly, a task-specific information minimization principle is designed to mitigate the negative effect of potential redundant features in the input for each task.","It can compress task-irrelevant redundant information and preserve necessary information relevant to the target for multi-task prediction.","Experiments on six classification benchmarks show that our method outperforms 12 comparative multi-task methods under the same multi-task settings, especially in data-constrained and noisy scenarios.","Extensive experiments demonstrate that the learned representations are more sufficient, data-efficient, and robust."],"url":"http://arxiv.org/abs/2503.04667v1"}
{"created":"2025-03-06 17:59:29","title":"What Are You Doing? A Closer Look at Controllable Human Video Generation","abstract":"High-quality benchmarks are crucial for driving progress in machine learning research. However, despite the growing interest in video generation, there is no comprehensive dataset to evaluate human generation. Humans can perform a wide variety of actions and interactions, but existing datasets, like TikTok and TED-Talks, lack the diversity and complexity to fully capture the capabilities of video generation models. We close this gap by introducing `What Are You Doing?' (WYD): a new benchmark for fine-grained evaluation of controllable image-to-video generation of humans. WYD consists of 1{,}544 captioned videos that have been meticulously collected and annotated with 56 fine-grained categories. These allow us to systematically measure performance across 9 aspects of human generation, including actions, interactions and motion. We also propose and validate automatic metrics that leverage our annotations and better capture human evaluations. Equipped with our dataset and metrics, we perform in-depth analyses of seven state-of-the-art models in controllable image-to-video generation, showing how WYD provides novel insights about the capabilities of these models. We release our data and code to drive forward progress in human video generation modeling at https://github.com/google-deepmind/wyd-benchmark.","sentences":["High-quality benchmarks are crucial for driving progress in machine learning research.","However, despite the growing interest in video generation, there is no comprehensive dataset to evaluate human generation.","Humans can perform a wide variety of actions and interactions, but existing datasets, like TikTok and TED-Talks, lack the diversity and complexity to fully capture the capabilities of video generation models.","We close this gap by introducing `What Are You Doing?'","(WYD): a new benchmark for fine-grained evaluation of controllable image-to-video generation of humans.","WYD consists of 1{,}544 captioned videos that have been meticulously collected and annotated with 56 fine-grained categories.","These allow us to systematically measure performance across 9 aspects of human generation, including actions, interactions and motion.","We also propose and validate automatic metrics that leverage our annotations and better capture human evaluations.","Equipped with our dataset and metrics, we perform in-depth analyses of seven state-of-the-art models in controllable image-to-video generation, showing how WYD provides novel insights about the capabilities of these models.","We release our data and code to drive forward progress in human video generation modeling at https://github.com/google-deepmind/wyd-benchmark."],"url":"http://arxiv.org/abs/2503.04666v1"}
{"created":"2025-03-06 17:58:55","title":"Implicit Neural Representation for Video and Image Super-Resolution","abstract":"We present a novel approach for super-resolution that utilizes implicit neural representation (INR) to effectively reconstruct and enhance low-resolution videos and images. By leveraging the capacity of neural networks to implicitly encode spatial and temporal features, our method facilitates high-resolution reconstruction using only low-resolution inputs and a 3D high-resolution grid. This results in an efficient solution for both image and video super-resolution. Our proposed method, SR-INR, maintains consistent details across frames and images, achieving impressive temporal stability without relying on the computationally intensive optical flow or motion estimation typically used in other video super-resolution techniques. The simplicity of our approach contrasts with the complexity of many existing methods, making it both effective and efficient. Experimental evaluations show that SR-INR delivers results on par with or superior to state-of-the-art super-resolution methods, while maintaining a more straightforward structure and reduced computational demands. These findings highlight the potential of implicit neural representations as a powerful tool for reconstructing high-quality, temporally consistent video and image signals from low-resolution data.","sentences":["We present a novel approach for super-resolution that utilizes implicit neural representation (INR) to effectively reconstruct and enhance low-resolution videos and images.","By leveraging the capacity of neural networks to implicitly encode spatial and temporal features, our method facilitates high-resolution reconstruction using only low-resolution inputs and a 3D high-resolution grid.","This results in an efficient solution for both image and video super-resolution.","Our proposed method, SR-INR, maintains consistent details across frames and images, achieving impressive temporal stability without relying on the computationally intensive optical flow or motion estimation typically used in other video super-resolution techniques.","The simplicity of our approach contrasts with the complexity of many existing methods, making it both effective and efficient.","Experimental evaluations show that SR-INR delivers results on par with or superior to state-of-the-art super-resolution methods, while maintaining a more straightforward structure and reduced computational demands.","These findings highlight the potential of implicit neural representations as a powerful tool for reconstructing high-quality, temporally consistent video and image signals from low-resolution data."],"url":"http://arxiv.org/abs/2503.04665v1"}
{"created":"2025-03-06 17:56:32","title":"Control for Coalitions in Parliamentary Elections","abstract":"The traditional election control problem focuses on the use of control to promote a single candidate. In parliamentary elections, however, the focus shifts: voters care no less about the overall governing coalition than the individual parties' seat count. This paper introduces a new problem: controlling parliamentary elections, where the goal extends beyond promoting a single party to influencing the collective seat count of coalitions of parties.   We focus on plurality rule and control through the addition or deletion of parties. Our analysis reveals that, without restrictions on voters' preferences, these control problems are W[1]-hard. In some cases, the problems are immune to control, making such efforts ineffective.   We then study the special case where preferences are symmetric single-peaked. We show that in the single-peaked setting, aggregation of voters into types allows for a compact representation of the problem. Our findings show that for the single-peaked setting, some cases are solvable in polynomial time, while others are NP-hard for the compact representation - but admit a polynomial algorithm for the extensive representation.","sentences":["The traditional election control problem focuses on the use of control to promote a single candidate.","In parliamentary elections, however, the focus shifts: voters care no less about the overall governing coalition than the individual parties' seat count.","This paper introduces a new problem: controlling parliamentary elections, where the goal extends beyond promoting a single party to influencing the collective seat count of coalitions of parties.   ","We focus on plurality rule and control through the addition or deletion of parties.","Our analysis reveals that, without restrictions on voters' preferences, these control problems are W[1]-hard.","In some cases, the problems are immune to control, making such efforts ineffective.   ","We then study the special case where preferences are symmetric single-peaked.","We show that in the single-peaked setting, aggregation of voters into types allows for a compact representation of the problem.","Our findings show that for the single-peaked setting, some cases are solvable in polynomial time, while others are NP-hard for the compact representation - but admit a polynomial algorithm for the extensive representation."],"url":"http://arxiv.org/abs/2503.04661v1"}
{"created":"2025-03-06 17:49:13","title":"CLDyB: Towards Dynamic Benchmarking for Continual Learning with Pre-trained Models","abstract":"The advent of the foundation model era has sparked significant research interest in leveraging pre-trained representations for continual learning (CL), yielding a series of top-performing CL methods on standard evaluation benchmarks. Nonetheless, there are growing concerns regarding potential data contamination during the pre-training stage. Furthermore, standard evaluation benchmarks, which are typically static, fail to capture the complexities of real-world CL scenarios, resulting in saturated performance. To address these issues, we describe CL on dynamic benchmarks (CLDyB), a general computational framework based on Markov decision processes for evaluating CL methods reliably. CLDyB dynamically identifies inherently difficult and algorithm-dependent tasks for the given CL methods, and determines challenging task orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct a joint evaluation of multiple state-of-the-art CL methods, leading to a set of commonly challenging and generalizable task sequences where existing CL methods tend to perform poorly. We then conduct separate evaluations of individual CL methods using CLDyB, discovering their respective strengths and weaknesses. The source code and generated task sequences are publicly accessible at https://github.com/szc12153/CLDyB.","sentences":["The advent of the foundation model era has sparked significant research interest in leveraging pre-trained representations for continual learning (CL), yielding a series of top-performing CL methods on standard evaluation benchmarks.","Nonetheless, there are growing concerns regarding potential data contamination during the pre-training stage.","Furthermore, standard evaluation benchmarks, which are typically static, fail to capture the complexities of real-world CL scenarios, resulting in saturated performance.","To address these issues, we describe CL on dynamic benchmarks (CLDyB), a general computational framework based on Markov decision processes for evaluating CL methods reliably.","CLDyB dynamically identifies inherently difficult and algorithm-dependent tasks for the given CL methods, and determines challenging task orders using Monte Carlo tree search.","Leveraging CLDyB, we first conduct a joint evaluation of multiple state-of-the-art CL methods, leading to a set of commonly challenging and generalizable task sequences where existing CL methods tend to perform poorly.","We then conduct separate evaluations of individual CL methods using CLDyB, discovering their respective strengths and weaknesses.","The source code and generated task sequences are publicly accessible at https://github.com/szc12153/CLDyB."],"url":"http://arxiv.org/abs/2503.04655v1"}
{"created":"2025-03-06 17:43:03","title":"RadIR: A Scalable Framework for Multi-Grained Medical Image Retrieval via Radiology Report Mining","abstract":"Developing advanced medical imaging retrieval systems is challenging due to the varying definitions of `similar images' across different medical contexts. This challenge is compounded by the lack of large-scale, high-quality medical imaging retrieval datasets and benchmarks. In this paper, we propose a novel methodology that leverages dense radiology reports to define image-wise similarity ordering at multiple granularities in a scalable and fully automatic manner. Using this approach, we construct two comprehensive medical imaging retrieval datasets: MIMIC-IR for Chest X-rays and CTRATE-IR for CT scans, providing detailed image-image ranking annotations conditioned on diverse anatomical structures. Furthermore, we develop two retrieval systems, RadIR-CXR and model-ChestCT, which demonstrate superior performance in traditional image-image and image-report retrieval tasks. These systems also enable flexible, effective image retrieval conditioned on specific anatomical structures described in text, achieving state-of-the-art results on 77 out of 78 metrics.","sentences":["Developing advanced medical imaging retrieval systems is challenging due to the varying definitions of `similar images' across different medical contexts.","This challenge is compounded by the lack of large-scale, high-quality medical imaging retrieval datasets and benchmarks.","In this paper, we propose a novel methodology that leverages dense radiology reports to define image-wise similarity ordering at multiple granularities in a scalable and fully automatic manner.","Using this approach, we construct two comprehensive medical imaging retrieval datasets: MIMIC-IR for Chest X-rays and CTRATE-IR for CT scans, providing detailed image-image ranking annotations conditioned on diverse anatomical structures.","Furthermore, we develop two retrieval systems, RadIR-CXR and model-ChestCT, which demonstrate superior performance in traditional image-image and image-report retrieval tasks.","These systems also enable flexible, effective image retrieval conditioned on specific anatomical structures described in text, achieving state-of-the-art results on 77 out of 78 metrics."],"url":"http://arxiv.org/abs/2503.04653v1"}
{"created":"2025-03-06 17:42:23","title":"Evaluation of Privacy-aware Support Vector Machine (SVM) Learning using Homomorphic Encryption","abstract":"The requirement for privacy-aware machine learning increases as we continue to use PII (Personally Identifiable Information) within machine training. To overcome these privacy issues, we can apply Fully Homomorphic Encryption (FHE) to encrypt data before it is fed into a machine learning model. This involves creating a homomorphic encryption key pair, and where the associated public key will be used to encrypt the input data, and the private key will decrypt the output. But, there is often a performance hit when we use homomorphic encryption, and so this paper evaluates the performance overhead of using the SVM machine learning technique with the OpenFHE homomorphic encryption library. This uses Python and the scikit-learn library for its implementation. The experiments include a range of variables such as multiplication depth, scale size, first modulus size, security level, batch size, and ring dimension, along with two different SVM models, SVM-Poly and SVM-Linear. Overall, the results show that the two main parameters which affect performance are the ring dimension and the modulus size, and that SVM-Poly and SVM-Linear show similar performance levels.","sentences":["The requirement for privacy-aware machine learning increases as we continue to use PII (Personally Identifiable Information) within machine training.","To overcome these privacy issues, we can apply Fully Homomorphic Encryption (FHE) to encrypt data before it is fed into a machine learning model.","This involves creating a homomorphic encryption key pair, and where the associated public key will be used to encrypt the input data, and the private key will decrypt the output.","But, there is often a performance hit when we use homomorphic encryption, and so this paper evaluates the performance overhead of using the SVM machine learning technique with the OpenFHE homomorphic encryption library.","This uses Python and the scikit-learn library for its implementation.","The experiments include a range of variables such as multiplication depth, scale size, first modulus size, security level, batch size, and ring dimension, along with two different SVM models, SVM-Poly and SVM-Linear.","Overall, the results show that the two main parameters which affect performance are the ring dimension and the modulus size, and that SVM-Poly and SVM-Linear show similar performance levels."],"url":"http://arxiv.org/abs/2503.04652v1"}
{"created":"2025-03-06 17:39:12","title":"Joint Masked Reconstruction and Contrastive Learning for Mining Interactions Between Proteins","abstract":"Protein-protein interaction (PPI) prediction is an instrumental means in elucidating the mechanisms underlying cellular operations, holding significant practical implications for the realms of pharmaceutical development and clinical treatment. Presently, the majority of research methods primarily concentrate on the analysis of amino acid sequences, while investigations predicated on protein structures remain in the nascent stages of exploration. Despite the emergence of several structure-based algorithms in recent years, these are still confronted with inherent challenges: (1) the extraction of intrinsic structural information of proteins typically necessitates the expenditure of substantial computational resources; (2) these models are overly reliant on seen protein data, struggling to effectively unearth interaction cues between unknown proteins. To further propel advancements in this domain, this paper introduces a novel PPI prediction method jointing masked reconstruction and contrastive learning, termed JmcPPI. This methodology dissects the PPI prediction task into two distinct phases: during the residue structure encoding phase, JmcPPI devises two feature reconstruction tasks and employs graph attention mechanism to capture structural information between residues; during the protein interaction inference phase, JmcPPI perturbs the original PPI graph and employs a multi-graph contrastive learning strategy to thoroughly mine extrinsic interaction information of novel proteins. Extensive experiments conducted on three widely utilized PPI datasets demonstrate that JmcPPI surpasses existing optimal baseline models across various data partition schemes. The associated code can be accessed via https://github.com/lijfrank-open/JmcPPI.","sentences":["Protein-protein interaction (PPI) prediction is an instrumental means in elucidating the mechanisms underlying cellular operations, holding significant practical implications for the realms of pharmaceutical development and clinical treatment.","Presently, the majority of research methods primarily concentrate on the analysis of amino acid sequences, while investigations predicated on protein structures remain in the nascent stages of exploration.","Despite the emergence of several structure-based algorithms in recent years, these are still confronted with inherent challenges: (1) the extraction of intrinsic structural information of proteins typically necessitates the expenditure of substantial computational resources; (2) these models are overly reliant on seen protein data, struggling to effectively unearth interaction cues between unknown proteins.","To further propel advancements in this domain, this paper introduces a novel PPI prediction method jointing masked reconstruction and contrastive learning, termed JmcPPI.","This methodology dissects the PPI prediction task into two distinct phases: during the residue structure encoding phase, JmcPPI devises two feature reconstruction tasks and employs graph attention mechanism to capture structural information between residues; during the protein interaction inference phase, JmcPPI perturbs the original PPI graph and employs a multi-graph contrastive learning strategy to thoroughly mine extrinsic interaction information of novel proteins.","Extensive experiments conducted on three widely utilized PPI datasets demonstrate that JmcPPI surpasses existing optimal baseline models across various data partition schemes.","The associated code can be accessed via https://github.com/lijfrank-open/JmcPPI."],"url":"http://arxiv.org/abs/2503.04650v1"}
{"created":"2025-03-06 17:35:37","title":"Transferable Foundation Models for Geometric Tasks on Point Cloud Representations: Geometric Neural Operators","abstract":"We introduce methods for obtaining pretrained Geometric Neural Operators (GNPs) that can serve as basal foundation models for use in obtaining geometric features. These can be used within data processing pipelines for machine learning tasks and numerical methods. We show how our GNPs can be trained to learn robust latent representations for the differential geometry of point-clouds to provide estimates of metric, curvature, and other shape-related features. We demonstrate how our pre-trained GNPs can be used (i) to estimate the geometric properties of surfaces of arbitrary shape and topologies with robustness in the presence of noise, (ii) to approximate solutions of geometric partial differential equations (PDEs) on manifolds, and (iii) to solve equations for shape deformations such as curvature driven flows. We also release a package of the codes and weights for using our pre-trained GNPs for processing point cloud representations. This allows for incorporating our pre-trained GNPs as components for reuse within existing and new data processing pipelines. The GNPs also can be used as part of numerical solvers involving geometry or as part of methods for performing inference and other geometric tasks.","sentences":["We introduce methods for obtaining pretrained Geometric Neural Operators (GNPs) that can serve as basal foundation models for use in obtaining geometric features.","These can be used within data processing pipelines for machine learning tasks and numerical methods.","We show how our GNPs can be trained to learn robust latent representations for the differential geometry of point-clouds to provide estimates of metric, curvature, and other shape-related features.","We demonstrate how our pre-trained GNPs can be used (i) to estimate the geometric properties of surfaces of arbitrary shape and topologies with robustness in the presence of noise, (ii) to approximate solutions of geometric partial differential equations (PDEs) on manifolds, and (iii) to solve equations for shape deformations such as curvature driven flows.","We also release a package of the codes and weights for using our pre-trained GNPs for processing point cloud representations.","This allows for incorporating our pre-trained GNPs as components for reuse within existing and new data processing pipelines.","The GNPs also can be used as part of numerical solvers involving geometry or as part of methods for performing inference and other geometric tasks."],"url":"http://arxiv.org/abs/2503.04649v1"}
{"created":"2025-03-06 17:33:01","title":"Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference Alignment","abstract":"Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences. While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is hampered by data scarcity. To address this, we propose a novel approach that $\\textit{captures}$ learned preferences from well-aligned English models by implicit rewards and $\\textit{transfers}$ them to other languages through iterative training. Specifically, we derive an implicit reward model from the logits of an English DPO-aligned model and its corresponding reference model. This reward model is then leveraged to annotate preference relations in cross-lingual instruction-following pairs, using English instructions to evaluate multilingual responses. The annotated data is subsequently used for multilingual DPO fine-tuning, facilitating preference knowledge transfer from English to other languages. Fine-tuning Llama3 for two iterations resulted in a 12.72% average improvement in Win Rate and a 5.97% increase in Length Control Win Rate across all training languages on the X-AlpacaEval leaderboard. Our findings demonstrate that leveraging existing English-aligned models can enable efficient and effective multilingual preference alignment, significantly reducing the need for extensive multilingual preference data. The code is available at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding","sentences":["Direct Preference Optimization (DPO) has become a prominent method for aligning Large Language Models (LLMs) with human preferences.","While DPO has enabled significant progress in aligning English LLMs, multilingual preference alignment is hampered by data scarcity.","To address this, we propose a novel approach that $\\textit{captures}$ learned preferences from well-aligned English models by implicit rewards and $\\textit{transfers}$ them to other languages through iterative training.","Specifically, we derive an implicit reward model from the logits of an English DPO-aligned model and its corresponding reference model.","This reward model is then leveraged to annotate preference relations in cross-lingual instruction-following pairs, using English instructions to evaluate multilingual responses.","The annotated data is subsequently used for multilingual DPO fine-tuning, facilitating preference knowledge transfer from English to other languages.","Fine-tuning Llama3 for two iterations resulted in a 12.72% average improvement in Win Rate and a 5.97% increase in Length Control Win Rate across all training languages on the X-AlpacaEval leaderboard.","Our findings demonstrate that leveraging existing English-aligned models can enable efficient and effective multilingual preference alignment, significantly reducing the need for extensive multilingual preference data.","The code is available at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding"],"url":"http://arxiv.org/abs/2503.04647v1"}
{"created":"2025-03-06 17:32:35","title":"Ultra-Low-Latency Edge Intelligent Sensing: A Source-Channel Tradeoff and Its Application to Coding Rate Adaptation","abstract":"The forthcoming sixth-generation (6G) mobile network is set to merge edge artificial intelligence (AI) and integrated sensing and communication (ISAC) extensively, giving rise to the new paradigm of edge intelligent sensing (EI-Sense). This paradigm leverages ubiquitous edge devices for environmental sensing and deploys AI algorithms at edge servers to interpret the observations via remote inference on wirelessly uploaded features. A significant challenge arises in designing EI-Sense systems for 6G mission-critical applications, which demand high performance under stringent latency constraints. To tackle this challenge, we focus on the end-to-end (E2E) performance of EI-Sense and characterize a source-channel tradeoff that balances source distortion and channel reliability. In this work, we establish a theoretical foundation for the source-channel tradeoff by quantifying the effects of source coding on feature discriminant gains and channel reliability on packet loss. Building on this foundation, we design the coding rate control by optimizing the tradeoff to minimize the E2E sensing error probability, leading to a low-complexity algorithm for ultra-low-latency EI-Sense. Finally, we validate our theoretical analysis and proposed coding rate control algorithm through extensive experiments on both synthetic and real datasets, demonstrating the sensing performance gain of our approach with respect to traditional reliability-centric methods.","sentences":["The forthcoming sixth-generation (6G) mobile network is set to merge edge artificial intelligence (AI) and integrated sensing and communication (ISAC) extensively, giving rise to the new paradigm of edge intelligent sensing (EI-Sense).","This paradigm leverages ubiquitous edge devices for environmental sensing and deploys AI algorithms at edge servers to interpret the observations via remote inference on wirelessly uploaded features.","A significant challenge arises in designing EI-Sense systems for 6G mission-critical applications, which demand high performance under stringent latency constraints.","To tackle this challenge, we focus on the end-to-end (E2E) performance of EI-Sense and characterize a source-channel tradeoff that balances source distortion and channel reliability.","In this work, we establish a theoretical foundation for the source-channel tradeoff by quantifying the effects of source coding on feature discriminant gains and channel reliability on packet loss.","Building on this foundation, we design the coding rate control by optimizing the tradeoff to minimize the E2E sensing error probability, leading to a low-complexity algorithm for ultra-low-latency EI-Sense.","Finally, we validate our theoretical analysis and proposed coding rate control algorithm through extensive experiments on both synthetic and real datasets, demonstrating the sensing performance gain of our approach with respect to traditional reliability-centric methods."],"url":"http://arxiv.org/abs/2503.04645v1"}
{"created":"2025-03-06 17:32:22","title":"IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in Expert-Domain Information Retrieval","abstract":"We introduce IFIR, the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains. IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature. Each subset addresses one or more domain-specific retrieval tasks, replicating real-world scenarios where customized instructions are critical. IFIR enables a detailed analysis of instruction-following retrieval capabilities by incorporating instructions at different levels of complexity. We also propose a novel LLM-based evaluation method to provide a more precise and reliable assessment of model performance in following instructions. Through extensive experiments on 15 frontier retrieval models, including those based on LLMs, our results reveal that current models face significant challenges in effectively following complex, domain-specific instructions. We further provide in-depth analyses to highlight these limitations, offering valuable insights to guide future advancements in retriever development.","sentences":["We introduce IFIR, the first comprehensive benchmark designed to evaluate instruction-following information retrieval (IR) in expert domains.","IFIR includes 2,426 high-quality examples and covers eight subsets across four specialized domains: finance, law, healthcare, and science literature.","Each subset addresses one or more domain-specific retrieval tasks, replicating real-world scenarios where customized instructions are critical.","IFIR enables a detailed analysis of instruction-following retrieval capabilities by incorporating instructions at different levels of complexity.","We also propose a novel LLM-based evaluation method to provide a more precise and reliable assessment of model performance in following instructions.","Through extensive experiments on 15 frontier retrieval models, including those based on LLMs, our results reveal that current models face significant challenges in effectively following complex, domain-specific instructions.","We further provide in-depth analyses to highlight these limitations, offering valuable insights to guide future advancements in retriever development."],"url":"http://arxiv.org/abs/2503.04644v1"}
{"created":"2025-03-06 17:31:43","title":"Simulating the Real World: A Unified Survey of Multimodal Generative Models","abstract":"Understanding and replicating the real world is a critical challenge in Artificial General Intelligence (AGI) research. To achieve this, many existing approaches, such as world models, aim to capture the fundamental principles governing the physical world, enabling more accurate simulations and meaningful interactions. However, current methods often treat different modalities, including 2D (images), videos, 3D, and 4D representations, as independent domains, overlooking their interdependencies. Additionally, these methods typically focus on isolated dimensions of reality without systematically integrating their connections. In this survey, we present a unified survey for multimodal generative models that investigate the progression of data dimensionality in real-world simulation. Specifically, this survey starts from 2D generation (appearance), then moves to video (appearance+dynamics) and 3D generation (appearance+geometry), and finally culminates in 4D generation that integrate all dimensions. To the best of our knowledge, this is the first attempt to systematically unify the study of 2D, video, 3D and 4D generation within a single framework. To guide future research, we provide a comprehensive review of datasets, evaluation metrics and future directions, and fostering insights for newcomers. This survey serves as a bridge to advance the study of multimodal generative models and real-world simulation within a unified framework.","sentences":["Understanding and replicating the real world is a critical challenge in Artificial General Intelligence (AGI) research.","To achieve this, many existing approaches, such as world models, aim to capture the fundamental principles governing the physical world, enabling more accurate simulations and meaningful interactions.","However, current methods often treat different modalities, including 2D (images), videos, 3D, and 4D representations, as independent domains, overlooking their interdependencies.","Additionally, these methods typically focus on isolated dimensions of reality without systematically integrating their connections.","In this survey, we present a unified survey for multimodal generative models that investigate the progression of data dimensionality in real-world simulation.","Specifically, this survey starts from 2D generation (appearance), then moves to video (appearance+dynamics) and 3D generation (appearance+geometry), and finally culminates in 4D generation that integrate all dimensions.","To the best of our knowledge, this is the first attempt to systematically unify the study of 2D, video, 3D and 4D generation within a single framework.","To guide future research, we provide a comprehensive review of datasets, evaluation metrics and future directions, and fostering insights for newcomers.","This survey serves as a bridge to advance the study of multimodal generative models and real-world simulation within a unified framework."],"url":"http://arxiv.org/abs/2503.04641v1"}
{"created":"2025-03-06 17:28:48","title":"Enhancing SAM with Efficient Prompting and Preference Optimization for Semi-supervised Medical Image Segmentation","abstract":"Foundational models such as the Segment Anything Model (SAM) are gaining traction in medical imaging segmentation, supporting multiple downstream tasks. However, such models are supervised in nature, still relying on large annotated datasets or prompts supplied by experts. Conventional techniques such as active learning to alleviate such limitations are limited in scope and still necessitate continuous human involvement and complex domain knowledge for label refinement or establishing reward ground truth. To address these challenges, we propose an enhanced Segment Anything Model (SAM) framework that utilizes annotation-efficient prompts generated in a fully unsupervised fashion, while still capturing essential semantic, location, and shape information through contrastive language-image pretraining and visual question answering. We adopt the direct preference optimization technique to design an optimal policy that enables the model to generate high-fidelity segmentations with simple ratings or rankings provided by a virtual annotator simulating the human annotation process. State-of-the-art performance of our framework in tasks such as lung segmentation, breast tumor segmentation, and organ segmentation across various modalities, including X-ray, ultrasound, and abdominal CT, justifies its effectiveness in low-annotation data scenarios.","sentences":["Foundational models such as the Segment Anything Model (SAM) are gaining traction in medical imaging segmentation, supporting multiple downstream tasks.","However, such models are supervised in nature, still relying on large annotated datasets or prompts supplied by experts.","Conventional techniques such as active learning to alleviate such limitations are limited in scope and still necessitate continuous human involvement and complex domain knowledge for label refinement or establishing reward ground truth.","To address these challenges, we propose an enhanced Segment Anything Model (SAM) framework that utilizes annotation-efficient prompts generated in a fully unsupervised fashion, while still capturing essential semantic, location, and shape information through contrastive language-image pretraining and visual question answering.","We adopt the direct preference optimization technique to design an optimal policy that enables the model to generate high-fidelity segmentations with simple ratings or rankings provided by a virtual annotator simulating the human annotation process.","State-of-the-art performance of our framework in tasks such as lung segmentation, breast tumor segmentation, and organ segmentation across various modalities, including X-ray, ultrasound, and abdominal CT, justifies its effectiveness in low-annotation data scenarios."],"url":"http://arxiv.org/abs/2503.04639v1"}
{"created":"2025-03-06 17:25:46","title":"No Forgetting Learning: Memory-free Continual Learning","abstract":"Continual Learning (CL) remains a central challenge in deep learning, where models must sequentially acquire new knowledge while mitigating Catastrophic Forgetting (CF) of prior tasks. Existing approaches often struggle with efficiency and scalability, requiring extensive memory or model buffers. This work introduces ``No Forgetting Learning\" (NFL), a memory-free CL framework that leverages knowledge distillation to maintain stability while preserving plasticity. Memory-free means the NFL does not rely on any memory buffer. Through extensive evaluations of three benchmark datasets, we demonstrate that NFL achieves competitive performance while utilizing approximately 14.75 times less memory than state-of-the-art methods. Furthermore, we introduce a new metric to better assess CL's plasticity-stability trade-off.","sentences":["Continual Learning (CL) remains a central challenge in deep learning, where models must sequentially acquire new knowledge while mitigating Catastrophic Forgetting (CF) of prior tasks.","Existing approaches often struggle with efficiency and scalability, requiring extensive memory or model buffers.","This work introduces ``No Forgetting Learning\" (NFL), a memory-free CL framework that leverages knowledge distillation to maintain stability while preserving plasticity.","Memory-free means the NFL does not rely on any memory buffer.","Through extensive evaluations of three benchmark datasets, we demonstrate that NFL achieves competitive performance while utilizing approximately 14.75 times less memory than state-of-the-art methods.","Furthermore, we introduce a new metric to better assess CL's plasticity-stability trade-off."],"url":"http://arxiv.org/abs/2503.04638v1"}
{"created":"2025-03-06 17:25:38","title":"When Next-Gen Sensing Meets Legacy Wi-Fi: Performance Analyses of IEEE 802.11bf and IEEE 802.11ax Coexistence","abstract":"Sensing is emerging as a vital future service in next-generation wireless networks, enabling applications such as object localization and activity recognition. The IEEE 802.11bf standard extends Wi-Fi capabilities to incorporate these sensing functionalities. However, coexistence with legacy Wi-Fi in densely populated networks poses challenges, as contention for channels can impair both sensing and communication quality. This paper develops an analytical framework and a system-level simulation in ns-3 to evaluate the coexistence of IEEE 802.11bf and legacy 802.11ax in terms of sensing delay and communication throughput. Forthis purpose, we have developed a dedicated ns-3 module forIEEE 802.11bf, which is made publicly available as open-source. We provide the first coexistence analysis between IEEE 802.11bfand IEEE 802.11ax, supported by link-level simulation in ns-3to assess the impact on sensing delay and network performance. Key parameters, including sensing intervals, access categories, network densities, and antenna configurations, are systematically analyzed to understand their influence on the sensing delay and aggregated network throughput. The evaluation is further extended to a realistic indoor office environment modeled after the 3GPP TR 38.901 standard. Our findings reveal key trade-offs between sensing intervals and throughput and the need for balanced sensing parameters to ensure effective coexistence in Wi-Fi networks.","sentences":["Sensing is emerging as a vital future service in next-generation wireless networks, enabling applications such as object localization and activity recognition.","The IEEE 802.11bf standard extends Wi-Fi capabilities to incorporate these sensing functionalities.","However, coexistence with legacy Wi-Fi in densely populated networks poses challenges, as contention for channels can impair both sensing and communication quality.","This paper develops an analytical framework and a system-level simulation in ns-3 to evaluate the coexistence of IEEE 802.11bf and legacy 802.11ax in terms of sensing delay and communication throughput.","Forthis purpose, we have developed a dedicated ns-3 module forIEEE 802.11bf, which is made publicly available as open-source.","We provide the first coexistence analysis between IEEE 802.11bfand IEEE 802.11ax, supported by link-level simulation in ns-3to assess the impact on sensing delay and network performance.","Key parameters, including sensing intervals, access categories, network densities, and antenna configurations, are systematically analyzed to understand their influence on the sensing delay and aggregated network throughput.","The evaluation is further extended to a realistic indoor office environment modeled after the 3GPP TR 38.901 standard.","Our findings reveal key trade-offs between sensing intervals and throughput and the need for balanced sensing parameters to ensure effective coexistence in Wi-Fi networks."],"url":"http://arxiv.org/abs/2503.04637v1"}
{"created":"2025-03-06 17:24:06","title":"Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models via Watermarking","abstract":"As open-source large language models (LLMs) like Llama3 become more capable, it is crucial to develop watermarking techniques to detect their potential misuse. Existing watermarking methods either add watermarks during LLM inference, which is unsuitable for open-source LLMs, or primarily target classification LLMs rather than recent generative LLMs. Adapting these watermarks to open-source LLMs for misuse detection remains an open challenge. This work defines two misuse scenarios for open-source LLMs: intellectual property (IP) violation and LLM Usage Violation. Then, we explore the application of inference-time watermark distillation and backdoor watermarking in these contexts. We propose comprehensive evaluation methods to assess the impact of various real-world further fine-tuning scenarios on watermarks and the effect of these watermarks on LLM performance. Our experiments reveal that backdoor watermarking could effectively detect IP Violation, while inference-time watermark distillation is applicable in both scenarios but less robust to further fine-tuning and has a more significant impact on LLM performance compared to backdoor watermarking. Exploring more advanced watermarking methods for open-source LLMs to detect their misuse should be an important future direction.","sentences":["As open-source large language models (LLMs) like Llama3 become more capable, it is crucial to develop watermarking techniques to detect their potential misuse.","Existing watermarking methods either add watermarks during LLM inference, which is unsuitable for open-source LLMs, or primarily target classification LLMs rather than recent generative LLMs.","Adapting these watermarks to open-source LLMs for misuse detection remains an open challenge.","This work defines two misuse scenarios for open-source LLMs: intellectual property (IP) violation and LLM Usage Violation.","Then, we explore the application of inference-time watermark distillation and backdoor watermarking in these contexts.","We propose comprehensive evaluation methods to assess the impact of various real-world further fine-tuning scenarios on watermarks and the effect of these watermarks on LLM performance.","Our experiments reveal that backdoor watermarking could effectively detect IP Violation, while inference-time watermark distillation is applicable in both scenarios but less robust to further fine-tuning and has a more significant impact on LLM performance compared to backdoor watermarking.","Exploring more advanced watermarking methods for open-source LLMs to detect their misuse should be an important future direction."],"url":"http://arxiv.org/abs/2503.04636v1"}
{"created":"2025-03-06 17:23:55","title":"3HANDS Dataset: Learning from Humans for Generating Naturalistic Handovers with Supernumerary Robotic Limbs","abstract":"Supernumerary robotic limbs (SRLs) are robotic structures integrated closely with the user's body, which augment human physical capabilities and necessitate seamless, naturalistic human-machine interaction. For effective assistance in physical tasks, enabling SRLs to hand over objects to humans is crucial. Yet, designing heuristic-based policies for robots is time-consuming, difficult to generalize across tasks, and results in less human-like motion. When trained with proper datasets, generative models are powerful alternatives for creating naturalistic handover motions. We introduce 3HANDS, a novel dataset of object handover interactions between a participant performing a daily activity and another participant enacting a hip-mounted SRL in a naturalistic manner. 3HANDS captures the unique characteristics of SRL interactions: operating in intimate personal space with asymmetric object origins, implicit motion synchronization, and the user's engagement in a primary task during the handover. To demonstrate the effectiveness of our dataset, we present three models: one that generates naturalistic handover trajectories, another that determines the appropriate handover endpoints, and a third that predicts the moment to initiate a handover. In a user study (N=10), we compare the handover interaction performed with our method compared to a baseline. The findings show that our method was perceived as significantly more natural, less physically demanding, and more comfortable.","sentences":["Supernumerary robotic limbs (SRLs) are robotic structures integrated closely with the user's body, which augment human physical capabilities and necessitate seamless, naturalistic human-machine interaction.","For effective assistance in physical tasks, enabling SRLs to hand over objects to humans is crucial.","Yet, designing heuristic-based policies for robots is time-consuming, difficult to generalize across tasks, and results in less human-like motion.","When trained with proper datasets, generative models are powerful alternatives for creating naturalistic handover motions.","We introduce 3HANDS, a novel dataset of object handover interactions between a participant performing a daily activity and another participant enacting a hip-mounted SRL in a naturalistic manner.","3HANDS captures the unique characteristics of SRL interactions: operating in intimate personal space with asymmetric object origins, implicit motion synchronization, and the user's engagement in a primary task during the handover.","To demonstrate the effectiveness of our dataset, we present three models: one that generates naturalistic handover trajectories, another that determines the appropriate handover endpoints, and a third that predicts the moment to initiate a handover.","In a user study (N=10), we compare the handover interaction performed with our method compared to a baseline.","The findings show that our method was perceived as significantly more natural, less physically demanding, and more comfortable."],"url":"http://arxiv.org/abs/2503.04635v1"}
{"created":"2025-03-06 17:21:12","title":"PathoPainter: Augmenting Histopathology Segmentation via Tumor-aware Inpainting","abstract":"Tumor segmentation plays a critical role in histopathology, but it requires costly, fine-grained image-mask pairs annotated by pathologists. Thus, synthesizing histopathology data to expand the dataset is highly desirable. Previous works suffer from inaccuracies and limited diversity in image-mask pairs, both of which affect training segmentation, particularly in small-scale datasets and the inherently complex nature of histopathology images. To address this challenge, we propose PathoPainter, which reformulates image-mask pair generation as a tumor inpainting task. Specifically, our approach preserves the background while inpainting the tumor region, ensuring precise alignment between the generated image and its corresponding mask. To enhance dataset diversity while maintaining biological plausibility, we incorporate a sampling mechanism that conditions tumor inpainting on regional embeddings from a different image. Additionally, we introduce a filtering strategy to exclude uncertain synthetic regions, further improving the quality of the generated data. Our comprehensive evaluation spans multiple datasets featuring diverse tumor types and various training data scales. As a result, segmentation improved significantly with our synthetic data, surpassing existing segmentation data synthesis approaches, e.g., 75.69% -> 77.69% on CAMELYON16. The code is available at https://github.com/HongLiuuuuu/PathoPainter.","sentences":["Tumor segmentation plays a critical role in histopathology, but it requires costly, fine-grained image-mask pairs annotated by pathologists.","Thus, synthesizing histopathology data to expand the dataset is highly desirable.","Previous works suffer from inaccuracies and limited diversity in image-mask pairs, both of which affect training segmentation, particularly in small-scale datasets and the inherently complex nature of histopathology images.","To address this challenge, we propose PathoPainter, which reformulates image-mask pair generation as a tumor inpainting task.","Specifically, our approach preserves the background while inpainting the tumor region, ensuring precise alignment between the generated image and its corresponding mask.","To enhance dataset diversity while maintaining biological plausibility, we incorporate a sampling mechanism that conditions tumor inpainting on regional embeddings from a different image.","Additionally, we introduce a filtering strategy to exclude uncertain synthetic regions, further improving the quality of the generated data.","Our comprehensive evaluation spans multiple datasets featuring diverse tumor types and various training data scales.","As a result, segmentation improved significantly with our synthetic data, surpassing existing segmentation data synthesis approaches, e.g., 75.69% -> 77.69% on CAMELYON16.","The code is available at https://github.com/HongLiuuuuu/PathoPainter."],"url":"http://arxiv.org/abs/2503.04634v1"}
{"created":"2025-03-06 17:20:06","title":"Quickly Avoiding a Random Catastrophe","abstract":"We study the problem of constructing simulations of a given randomized search algorithm \\texttt{alg} with expected running time $O( \\mathcal{O} \\log \\mathcal{O})$, where $\\mathcal{O}$ is the optimal expected running time of any such simulation. Counterintuitively, these simulators can be dramatically faster than the original algorithm in getting alg to perform a single successful run, and this is done without any knowledge about alg, its running time distribution, etc.   For example, consider an algorithm that randomly picks some integer $t$ according to some distribution over the integers, and runs for $t$ seconds. then with probability $1/2$ it stops, or else runs forever (i.e., a catastrophe). The simulators described here, for this case, all terminate in constant expected time, with exponentially decaying distribution on the running time of the simulation.   Luby et al. studied this problem before -- and our main contribution is in offering several additional simulation strategies to the one they describe. In particular, one of our (optimal) simulation strategies is strikingly simple: Randomly pick an integer $t>0$ with probability $c/t^2$ (with $c= 6/\\pi^2$). Run the algorithm for $t$ seconds. If the run of alg terminates before this threshold is met, the simulation succeeded and it exits. Otherwise, the simulator repeat the process till success.","sentences":["We study the problem of constructing simulations of a given randomized search algorithm \\texttt{alg} with expected running time $O( \\mathcal{O} \\log \\mathcal{O})$, where $\\mathcal{O}$ is the optimal expected running time of any such simulation.","Counterintuitively, these simulators can be dramatically faster than the original algorithm in getting alg to perform a single successful run, and this is done without any knowledge about alg, its running time distribution, etc.   ","For example, consider an algorithm that randomly picks some integer $t$ according to some distribution over the integers, and runs for $t$ seconds.","then with probability $1/2$ it stops, or else runs forever (i.e., a catastrophe).","The simulators described here, for this case, all terminate in constant expected time, with exponentially decaying distribution on the running time of the simulation.   ","Luby et al. studied this problem before -- and our main contribution is in offering several additional simulation strategies to the one they describe.","In particular, one of our (optimal) simulation strategies is strikingly simple: Randomly pick an integer $t>0$ with probability $c/t^2$ (with $c= 6/\\pi^2$).","Run the algorithm for $t$ seconds.","If the run of alg terminates before this threshold is met, the simulation succeeded and it exits.","Otherwise, the simulator repeat the process till success."],"url":"http://arxiv.org/abs/2503.04633v1"}
{"created":"2025-03-06 17:15:48","title":"SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and Multi-dimensional Evaluation for Automated Survey Writing","abstract":"Survey paper plays a crucial role in scientific research, especially given the rapid growth of research publications. Recently, researchers have begun using LLMs to automate survey generation for better efficiency. However, the quality gap between LLM-generated surveys and those written by human remains significant, particularly in terms of outline quality and citation accuracy. To close these gaps, we introduce SurveyForge, which first generates the outline by analyzing the logical structure of human-written outlines and referring to the retrieved domain-related articles. Subsequently, leveraging high-quality papers retrieved from memory by our scholar navigation agent, SurveyForge can automatically generate and refine the content of the generated article. Moreover, to achieve a comprehensive evaluation, we construct SurveyBench, which includes 100 human-written survey papers for win-rate comparison and assesses AI-generated survey papers across three dimensions: reference, outline, and content quality. Experiments demonstrate that SurveyForge can outperform previous works such as AutoSurvey.","sentences":["Survey paper plays a crucial role in scientific research, especially given the rapid growth of research publications.","Recently, researchers have begun using LLMs to automate survey generation for better efficiency.","However, the quality gap between LLM-generated surveys and those written by human remains significant, particularly in terms of outline quality and citation accuracy.","To close these gaps, we introduce SurveyForge, which first generates the outline by analyzing the logical structure of human-written outlines and referring to the retrieved domain-related articles.","Subsequently, leveraging high-quality papers retrieved from memory by our scholar navigation agent, SurveyForge can automatically generate and refine the content of the generated article.","Moreover, to achieve a comprehensive evaluation, we construct SurveyBench, which includes 100 human-written survey papers for win-rate comparison and assesses AI-generated survey papers across three dimensions: reference, outline, and content quality.","Experiments demonstrate that SurveyForge can outperform previous works such as AutoSurvey."],"url":"http://arxiv.org/abs/2503.04629v1"}
{"created":"2025-03-06 17:12:46","title":"IDInit: A Universal and Stable Initialization Method for Neural Network Training","abstract":"Deep neural networks have achieved remarkable accomplishments in practice. The success of these networks hinges on effective initialization methods, which are vital for ensuring stable and rapid convergence during training. Recently, initialization methods that maintain identity transition within layers have shown good efficiency in network training. These techniques (e.g., Fixup) set specific weights to zero to achieve identity control. However, settings of remaining weight (e.g., Fixup uses random values to initialize non-zero weights) will affect the inductive bias that is achieved only by a zero weight, which may be harmful to training. Addressing this concern, we introduce fully identical initialization (IDInit), a novel method that preserves identity in both the main and sub-stem layers of residual networks. IDInit employs a padded identity-like matrix to overcome rank constraints in non-square weight matrices. Furthermore, we show the convergence problem of an identity matrix can be solved by stochastic gradient descent. Additionally, we enhance the universality of IDInit by processing higher-order weights and addressing dead neuron problems. IDInit is a straightforward yet effective initialization method, with improved convergence, stability, and performance across various settings, including large-scale datasets and deep models.","sentences":["Deep neural networks have achieved remarkable accomplishments in practice.","The success of these networks hinges on effective initialization methods, which are vital for ensuring stable and rapid convergence during training.","Recently, initialization methods that maintain identity transition within layers have shown good efficiency in network training.","These techniques (e.g., Fixup) set specific weights to zero to achieve identity control.","However, settings of remaining weight (e.g., Fixup uses random values to initialize non-zero weights) will affect the inductive bias that is achieved only by a zero weight, which may be harmful to training.","Addressing this concern, we introduce fully identical initialization (IDInit), a novel method that preserves identity in both the main and sub-stem layers of residual networks.","IDInit employs a padded identity-like matrix to overcome rank constraints in non-square weight matrices.","Furthermore, we show the convergence problem of an identity matrix can be solved by stochastic gradient descent.","Additionally, we enhance the universality of IDInit by processing higher-order weights and addressing dead neuron problems.","IDInit is a straightforward yet effective initialization method, with improved convergence, stability, and performance across various settings, including large-scale datasets and deep models."],"url":"http://arxiv.org/abs/2503.04626v1"}
{"created":"2025-03-06 17:11:51","title":"START: Self-taught Reasoner with Tools","abstract":"Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT). However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes. In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools. Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs. The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'') during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data. Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM. Through this framework, we have fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview.","sentences":["Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have demonstrated remarkable capabilities in complex reasoning tasks through the utilization of long Chain-of-thought (CoT).","However, these models often suffer from hallucinations and inefficiencies due to their reliance solely on internal reasoning processes.","In this paper, we introduce START (Self-Taught Reasoner with Tools), a novel tool-integrated long CoT reasoning LLM that significantly enhances reasoning capabilities by leveraging external tools.","Through code execution, START is capable of performing complex computations, self-checking, exploring diverse methods, and self-debugging, thereby addressing the limitations of LRMs.","The core innovation of START lies in its self-learning framework, which comprises two key techniques: 1) Hint-infer: We demonstrate that inserting artificially designed hints (e.g., ``Wait, maybe using Python here is a good idea.'')","during the inference process of a LRM effectively stimulates its ability to utilize external tools without the need for any demonstration data.","Hint-infer can also serve as a simple and effective sequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning (Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and modifying the reasoning trajectories with tool invocation generated by a LRM via Hint-infer, followed by fine-tuning the LRM.","Through this framework, we have fine-tuned the QwQ-32B model to achieve START.","On PhD-level science QA (GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the competition-level code benchmark (LiveCodeBench), START achieves accuracy rates of 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively.","It significantly outperforms the base QwQ-32B and achieves performance comparable to the state-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary model o1-Preview."],"url":"http://arxiv.org/abs/2503.04625v1"}
{"created":"2025-03-06 17:05:33","title":"SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming User Sentiment Modeling","abstract":"User reviews on e-commerce platforms exhibit dynamic sentiment patterns driven by temporal and contextual factors. Traditional sentiment analysis methods focus on static reviews, failing to capture the evolving temporal relationship between user sentiment rating and textual content. Sentiment analysis on streaming reviews addresses this limitation by modeling and predicting the temporal evolution of user sentiments. However, it suffers from data sparsity, manifesting in temporal, spatial, and combined forms. In this paper, we introduce SynGraph, a novel framework designed to address data sparsity in sentiment analysis on streaming reviews. SynGraph alleviates data sparsity by categorizing users into mid-tail, long-tail, and extreme scenarios and incorporating LLM-augmented enhancements within a dynamic graph-based structure. Experiments on real-world datasets demonstrate its effectiveness in addressing sparsity and improving sentiment modeling in streaming reviews.","sentences":["User reviews on e-commerce platforms exhibit dynamic sentiment patterns driven by temporal and contextual factors.","Traditional sentiment analysis methods focus on static reviews, failing to capture the evolving temporal relationship between user sentiment rating and textual content.","Sentiment analysis on streaming reviews addresses this limitation by modeling and predicting the temporal evolution of user sentiments.","However, it suffers from data sparsity, manifesting in temporal, spatial, and combined forms.","In this paper, we introduce SynGraph, a novel framework designed to address data sparsity in sentiment analysis on streaming reviews.","SynGraph alleviates data sparsity by categorizing users into mid-tail, long-tail, and extreme scenarios and incorporating LLM-augmented enhancements within a dynamic graph-based structure.","Experiments on real-world datasets demonstrate its effectiveness in addressing sparsity and improving sentiment modeling in streaming reviews."],"url":"http://arxiv.org/abs/2503.04619v1"}
{"created":"2025-03-06 17:03:17","title":"Better Process Supervision with Bi-directional Rewarding Signals","abstract":"Process supervision, i.e., evaluating each step, is critical for complex large language model (LLM) reasoning and test-time searching with increased inference compute. Existing approaches, represented by process reward models (PRMs), primarily focus on rewarding signals up to the current step, exhibiting a one-directional nature and lacking a mechanism to model the distance to the final target. To address this problem, we draw inspiration from the A* algorithm, which states that an effective supervisory signal should simultaneously consider the incurred cost and the estimated cost for reaching the target. Building on this key insight, we introduce BiRM, a novel process supervision model that not only evaluates the correctness of previous steps but also models the probability of future success. We conduct extensive experiments on mathematical reasoning tasks and demonstrate that BiRM provides more precise evaluations of LLM reasoning steps, achieving an improvement of 3.1% on Gaokao2023 over PRM under the Best-of-N sampling method. Besides, in search-based strategies, BiRM provides more comprehensive guidance and outperforms ORM by 5.0% and PRM by 3.8% respectively on MATH-500.","sentences":["Process supervision, i.e., evaluating each step, is critical for complex large language model (LLM) reasoning and test-time searching with increased inference compute.","Existing approaches, represented by process reward models (PRMs), primarily focus on rewarding signals up to the current step, exhibiting a one-directional nature and lacking a mechanism to model the distance to the final target.","To address this problem, we draw inspiration from the A* algorithm, which states that an effective supervisory signal should simultaneously consider the incurred cost and the estimated cost for reaching the target.","Building on this key insight, we introduce BiRM, a novel process supervision model that not only evaluates the correctness of previous steps but also models the probability of future success.","We conduct extensive experiments on mathematical reasoning tasks and demonstrate that BiRM provides more precise evaluations of LLM reasoning steps, achieving an improvement of 3.1% on Gaokao2023 over PRM under the Best-of-N sampling method.","Besides, in search-based strategies, BiRM provides more comprehensive guidance and outperforms ORM by 5.0% and PRM by 3.8% respectively on MATH-500."],"url":"http://arxiv.org/abs/2503.04618v1"}
{"created":"2025-03-06 16:59:18","title":"HalluCounter: Reference-free LLM Hallucination Detection in the Wild!","abstract":"Response consistency-based, reference-free hallucination detection (RFHD) methods do not depend on internal model states, such as generation probabilities or gradients, which Grey-box models typically rely on but are inaccessible in closed-source LLMs. However, their inability to capture query-response alignment patterns often results in lower detection accuracy. Additionally, the lack of large-scale benchmark datasets spanning diverse domains remains a challenge, as most existing datasets are limited in size and scope. To this end, we propose HalluCounter, a novel reference-free hallucination detection method that utilizes both response-response and query-response consistency and alignment patterns. This enables the training of a classifier that detects hallucinations and provides a confidence score and an optimal response for user queries. Furthermore, we introduce HalluCounterEval, a benchmark dataset comprising both synthetically generated and human-curated samples across multiple domains. Our method outperforms state-of-the-art approaches by a significant margin, achieving over 90\\% average confidence in hallucination detection across datasets.","sentences":["Response consistency-based, reference-free hallucination detection (RFHD) methods do not depend on internal model states, such as generation probabilities or gradients, which Grey-box models typically rely on but are inaccessible in closed-source LLMs.","However, their inability to capture query-response alignment patterns often results in lower detection accuracy.","Additionally, the lack of large-scale benchmark datasets spanning diverse domains remains a challenge, as most existing datasets are limited in size and scope.","To this end, we propose HalluCounter, a novel reference-free hallucination detection method that utilizes both response-response and query-response consistency and alignment patterns.","This enables the training of a classifier that detects hallucinations and provides a confidence score and an optimal response for user queries.","Furthermore, we introduce HalluCounterEval, a benchmark dataset comprising both synthetically generated and human-curated samples across multiple domains.","Our method outperforms state-of-the-art approaches by a significant margin, achieving over 90\\% average confidence in hallucination detection across datasets."],"url":"http://arxiv.org/abs/2503.04615v1"}
{"created":"2025-03-06 16:59:06","title":"Whole-Body Model-Predictive Control of Legged Robots with MuJoCo","abstract":"We demonstrate the surprising real-world effectiveness of a very simple approach to whole-body model-predictive control (MPC) of quadruped and humanoid robots: the iterative LQR (iLQR) algorithm with MuJoCo dynamics and finite-difference approximated derivatives. Building upon the previous success of model-based behavior synthesis and control of locomotion and manipulation tasks with MuJoCo in simulation, we show that these policies can easily generalize to the real world with few sim-to-real considerations. Our baseline method achieves real-time whole-body MPC on a variety of hardware experiments, including dynamic quadruped locomotion, quadruped walking on two legs, and full-sized humanoid bipedal locomotion. We hope this easy-to-reproduce hardware baseline lowers the barrier to entry for real-world whole-body MPC research and contributes to accelerating research velocity in the community. Our code and experiment videos will be available online at:https://johnzhang3.github.io/mujoco_ilqr","sentences":["We demonstrate the surprising real-world effectiveness of a very simple approach to whole-body model-predictive control (MPC) of quadruped and humanoid robots: the iterative LQR (iLQR) algorithm with MuJoCo dynamics and finite-difference approximated derivatives.","Building upon the previous success of model-based behavior synthesis and control of locomotion and manipulation tasks with MuJoCo in simulation, we show that these policies can easily generalize to the real world with few sim-to-real considerations.","Our baseline method achieves real-time whole-body MPC on a variety of hardware experiments, including dynamic quadruped locomotion, quadruped walking on two legs, and full-sized humanoid bipedal locomotion.","We hope this easy-to-reproduce hardware baseline lowers the barrier to entry for real-world whole-body MPC research and contributes to accelerating research velocity in the community.","Our code and experiment videos will be available online at:https://johnzhang3.github.io/mujoco_ilqr"],"url":"http://arxiv.org/abs/2503.04613v1"}
{"created":"2025-03-06 16:57:26","title":"Towards Data-Efficient Language Models: A Child-Inspired Approach to Language Learning","abstract":"In this work, we explain our approach employed in the BabyLM Challenge, which uses various methods of training language models (LMs) with significantly less data compared to traditional large language models (LLMs) and are inspired by how human children learn. While a human child is exposed to far less linguistic input than an LLM, they still achieve remarkable language understanding and generation abilities. To this end, we develop a model trained on a curated dataset consisting of 10 million words, primarily sourced from child-directed transcripts. The 2024 BabyLM Challenge initial dataset of 10M words is filtered to 8.5M. Next, it is supplemented with a randomly selected subset of TVR dataset consisting of 1.5M words of television dialogues. The latter dataset ensures that similar to children, the model is also exposed to language through media. Furthermore, we reduce the vocabulary size to 32,000 tokens, aligning it with the limited vocabulary of children in the early stages of language acquisition. We use curriculum learning and is able to match the baseline on certain benchmarks while surpassing the baseline on others. Additionally, incorporating common LLM training datasets, such as MADLAD-400, degrades performance. These findings underscore the importance of dataset selection, vocabulary scaling, and curriculum learning in creating more data-efficient language models that better mimic human learning processes.","sentences":["In this work, we explain our approach employed in the BabyLM Challenge, which uses various methods of training language models (LMs) with significantly less data compared to traditional large language models (LLMs) and are inspired by how human children learn.","While a human child is exposed to far less linguistic input than an LLM, they still achieve remarkable language understanding and generation abilities.","To this end, we develop a model trained on a curated dataset consisting of 10 million words, primarily sourced from child-directed transcripts.","The 2024 BabyLM Challenge initial dataset of 10M words is filtered to 8.5M. Next, it is supplemented with a randomly selected subset of TVR dataset consisting of 1.5M words of television dialogues.","The latter dataset ensures that similar to children, the model is also exposed to language through media.","Furthermore, we reduce the vocabulary size to 32,000 tokens, aligning it with the limited vocabulary of children in the early stages of language acquisition.","We use curriculum learning and is able to match the baseline on certain benchmarks while surpassing the baseline on others.","Additionally, incorporating common LLM training datasets, such as MADLAD-400, degrades performance.","These findings underscore the importance of dataset selection, vocabulary scaling, and curriculum learning in creating more data-efficient language models that better mimic human learning processes."],"url":"http://arxiv.org/abs/2503.04611v1"}
{"created":"2025-03-06 16:53:14","title":"The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation","abstract":"Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.","sentences":["Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models.","However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling.","In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation.","Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos.","Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo.","Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field.","Our demo can be viewed at https://landiff.github.io/."],"url":"http://arxiv.org/abs/2503.04606v1"}
{"created":"2025-03-06 16:48:32","title":"ExoNav II: Design of a Robotic Tool with Follow-the-Leader Motion Capability for Lateral and Ventral Spinal Cord Stimulation (SCS)","abstract":"Spinal cord stimulation (SCS) electrodes are traditionally placed in the dorsal epidural space to stimulate the dorsal column fibers for pain therapy. Recently, SCS has gained attention in restoring gait. However, the motor fibers triggering locomotion are located in the ventral and lateral spinal cord. Currently, SCS electrodes are steered manually, making it difficult to navigate them to the lateral and ventral motor fibers in the spinal cord. In this work, we propose a helically micro-machined continuum robot that can bend in a helical shape when subjected to actuation tendon forces. Using a stiff outer tube and adding translational and rotational degrees of freedom, this helical continuum robot can perform follow-the-leader (FTL) motion. We propose a kinematic model to relate tendon stroke and geometric parameters of the robot's helical shape to its acquired trajectory and end-effector position. We evaluate the proposed kinematic model and the robot's FTL motion capability experimentally. The stroke-based method, which links tendon stroke values to the robot's shape, showed inaccuracies with a 19.84 mm deviation and an RMSE of 14.42 mm for 63.6 mm of robot's length bending. The position-based method, using kinematic equations to map joint space to task space, performed better with a 10.54 mm deviation and an RMSE of 8.04 mm. Follow-the-leader experiments showed deviations of 11.24 mm and 7.32 mm, with RMSE values of 8.67 mm and 5.18 mm for the stroke-based and position-based methods, respectively. Furthermore, end-effector trajectories in two FTL motion trials are compared to confirm the robot's repeatable behavior. Finally, we demonstrate the robot's operation on a 3D-printed spinal cord phantom model.","sentences":["Spinal cord stimulation (SCS) electrodes are traditionally placed in the dorsal epidural space to stimulate the dorsal column fibers for pain therapy.","Recently, SCS has gained attention in restoring gait.","However, the motor fibers triggering locomotion are located in the ventral and lateral spinal cord.","Currently, SCS electrodes are steered manually, making it difficult to navigate them to the lateral and ventral motor fibers in the spinal cord.","In this work, we propose a helically micro-machined continuum robot that can bend in a helical shape when subjected to actuation tendon forces.","Using a stiff outer tube and adding translational and rotational degrees of freedom, this helical continuum robot can perform follow-the-leader (FTL) motion.","We propose a kinematic model to relate tendon stroke and geometric parameters of the robot's helical shape to its acquired trajectory and end-effector position.","We evaluate the proposed kinematic model and the robot's FTL motion capability experimentally.","The stroke-based method, which links tendon stroke values to the robot's shape, showed inaccuracies with a 19.84 mm deviation and an RMSE of 14.42 mm for 63.6 mm of robot's length bending.","The position-based method, using kinematic equations to map joint space to task space, performed better with a 10.54 mm deviation and an RMSE of 8.04 mm.","Follow-the-leader experiments showed deviations of 11.24 mm and 7.32 mm, with RMSE values of 8.67 mm and 5.18 mm for the stroke-based and position-based methods, respectively.","Furthermore, end-effector trajectories in two FTL motion trials are compared to confirm the robot's repeatable behavior.","Finally, we demonstrate the robot's operation on a 3D-printed spinal cord phantom model."],"url":"http://arxiv.org/abs/2503.04603v1"}
{"created":"2025-03-06 16:40:48","title":"HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization","abstract":"Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization. While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm. In this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. This design not only stabilizes training but also enhances performance, particularly in the context of LLMs. Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. %Code will be made publicly available. Code is available at https://github.com/BryceZhuo/HybridNorm.","sentences":["Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs).","Despite their remarkable performance, challenges remain in training deep transformer networks, especially regarding the location of layer normalization.","While Pre-Norm structures facilitate easier training due to their more prominent identity path, they often yield suboptimal performance compared to Post-Norm.","In this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm approaches.","Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block.","This design not only stabilizes training but also enhances performance, particularly in the context of LLMs.","Comprehensive experiments in both dense and sparse architectures show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches, achieving state-of-the-art results across various benchmarks.","These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models.","%Code will be made publicly available.","Code is available at https://github.com/BryceZhuo/HybridNorm."],"url":"http://arxiv.org/abs/2503.04598v1"}
{"created":"2025-03-06 16:38:23","title":"The Next Frontier of LLM Applications: Open Ecosystems and Hardware Synergy","abstract":"Large Language Model (LLM) applications, including LLM app stores and autonomous agents, are shaping the future of AI ecosystems. However, platform silos, fragmented hardware integration, and the absence of standardized interfaces limit scalability, interoperability, and resource efficiency. While LLM app stores democratize AI, their closed ecosystems restrict modular AI reuse and cross-platform portability. Meanwhile, agent-based frameworks offer flexibility but often lack seamless integration across diverse environments. This paper envisions the future of LLM applications and proposes a three-layer decoupled architecture grounded in software engineering principles such as layered system design, service-oriented architectures, and hardware-software co-design. This architecture separates application logic, communication protocols, and hardware execution, enhancing modularity, efficiency, and cross-platform compatibility. Beyond architecture, we highlight key security and privacy challenges for safe, scalable AI deployment and outline research directions in software and security engineering. This vision aims to foster open, secure, and interoperable LLM ecosystems, guiding future advancements in AI applications.","sentences":["Large Language Model (LLM) applications, including LLM app stores and autonomous agents, are shaping the future of AI ecosystems.","However, platform silos, fragmented hardware integration, and the absence of standardized interfaces limit scalability, interoperability, and resource efficiency.","While LLM app stores democratize AI, their closed ecosystems restrict modular AI reuse and cross-platform portability.","Meanwhile, agent-based frameworks offer flexibility but often lack seamless integration across diverse environments.","This paper envisions the future of LLM applications and proposes a three-layer decoupled architecture grounded in software engineering principles such as layered system design, service-oriented architectures, and hardware-software co-design.","This architecture separates application logic, communication protocols, and hardware execution, enhancing modularity, efficiency, and cross-platform compatibility.","Beyond architecture, we highlight key security and privacy challenges for safe, scalable AI deployment and outline research directions in software and security engineering.","This vision aims to foster open, secure, and interoperable LLM ecosystems, guiding future advancements in AI applications."],"url":"http://arxiv.org/abs/2503.04596v1"}
{"created":"2025-03-06 16:38:07","title":"Boosting Blockchain Throughput: Parallel EVM Execution with Asynchronous Storage for Reddio","abstract":"The increasing adoption of blockchain technology has led to a growing demand for higher transaction throughput. Traditional blockchain platforms, such as Ethereum, execute transactions sequentially within each block, limiting scalability. Parallel execution has been proposed to enhance performance, but existing approaches either impose strict dependency annotations, rely on conservative static analysis, or suffer from high contention due to inefficient state management. Moreover, even when transaction execution is parallelized at the upper layer, storage operations remain a bottleneck due to sequential state access and I/O amplification. In this paper, we propose Reddio, a batch-based parallel transaction execution framework with asynchronous storage. Reddio processes transactions in parallel while addressing the storage bottleneck through three key techniques: (i) direct state reading, which enables efficient state access without traversing the Merkle Patricia Trie (MPT); (ii) asynchronous parallel node loading, which preloads trie nodes concurrently with execution to reduce I/O overhead; and (iii) pipelined workflow, which decouples execution, state reading, and storage updates into overlapping phases to maximize hardware utilization.","sentences":["The increasing adoption of blockchain technology has led to a growing demand for higher transaction throughput.","Traditional blockchain platforms, such as Ethereum, execute transactions sequentially within each block, limiting scalability.","Parallel execution has been proposed to enhance performance, but existing approaches either impose strict dependency annotations, rely on conservative static analysis, or suffer from high contention due to inefficient state management.","Moreover, even when transaction execution is parallelized at the upper layer, storage operations remain a bottleneck due to sequential state access and I/O amplification.","In this paper, we propose Reddio, a batch-based parallel transaction execution framework with asynchronous storage.","Reddio processes transactions in parallel while addressing the storage bottleneck through three key techniques: (i) direct state reading, which enables efficient state access without traversing the Merkle Patricia Trie (MPT); (ii) asynchronous parallel node loading, which preloads trie nodes concurrently with execution to reduce I/O overhead; and (iii) pipelined workflow, which decouples execution, state reading, and storage updates into overlapping phases to maximize hardware utilization."],"url":"http://arxiv.org/abs/2503.04595v1"}
{"created":"2025-03-06 16:31:34","title":"A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing Image Captioning","abstract":"Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision and language, aimed at automatically generating natural language descriptions of features and scenes in remote sensing imagery. Despite significant advances in developing sophisticated methods and large-scale datasets for training vision-language models (VLMs), two critical challenges persist: the scarcity of non-English descriptive datasets and the lack of multilingual capability evaluation for models. These limitations fundamentally impede the progress and practical deployment of RSIC, particularly in the era of large VLMs. To address these challenges, this paper presents several significant contributions to the field. First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image Captioning), a comprehensive bilingual dataset that enriches three established English RSIC datasets with Chinese descriptions, encompassing 13,634 images paired with 68,170 bilingual captions. Building upon this foundation, we develop a systematic evaluation framework that addresses the prevalent inconsistency in evaluation protocols, enabling rigorous assessment of model performance through standardized retraining procedures on BRSIC. Furthermore, we present an extensive empirical study of eight state-of-the-art large vision-language models (LVLMs), examining their capabilities across multiple paradigms including zero-shot inference, supervised fine-tuning, and multi-lingual training. This comprehensive evaluation provides crucial insights into the strengths and limitations of current LVLMs in handling multilingual remote sensing tasks. Additionally, our cross-dataset transfer experiments reveal interesting findings. The code and data will be available at https://github.com/mrazhou/BRSIC.","sentences":["Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision and language, aimed at automatically generating natural language descriptions of features and scenes in remote sensing imagery.","Despite significant advances in developing sophisticated methods and large-scale datasets for training vision-language models (VLMs), two critical challenges persist: the scarcity of non-English descriptive datasets and the lack of multilingual capability evaluation for models.","These limitations fundamentally impede the progress and practical deployment of RSIC, particularly in the era of large VLMs.","To address these challenges, this paper presents several significant contributions to the field.","First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image Captioning), a comprehensive bilingual dataset that enriches three established English RSIC datasets with Chinese descriptions, encompassing 13,634 images paired with 68,170 bilingual captions.","Building upon this foundation, we develop a systematic evaluation framework that addresses the prevalent inconsistency in evaluation protocols, enabling rigorous assessment of model performance through standardized retraining procedures on BRSIC.","Furthermore, we present an extensive empirical study of eight state-of-the-art large vision-language models (LVLMs), examining their capabilities across multiple paradigms including zero-shot inference, supervised fine-tuning, and multi-lingual training.","This comprehensive evaluation provides crucial insights into the strengths and limitations of current LVLMs in handling multilingual remote sensing tasks.","Additionally, our cross-dataset transfer experiments reveal interesting findings.","The code and data will be available at https://github.com/mrazhou/BRSIC."],"url":"http://arxiv.org/abs/2503.04592v1"}
{"created":"2025-03-06 16:28:02","title":"Foundations of block-parallel automata networks","abstract":"We settle the theoretical ground for the study of automata networks under block-parallel update schedules, which are somehow dual to the block-sequential ones, but allow for repetitions of automaton updates. This gain in expressivity brings new challenges, and we analyse natural equivalence classes of update schedules: those leading to the same dynamics, and to the same limit dynamics, for any automata network. Countings and enumeration algorithms are provided, for their numerical study. We also prove computational complexity bounds for many classical problems, involving fixed points, limit cycles, the recognition of subdynamics, reachability, etc. The PSPACE-completeness of computing the image of a single configuration lifts the complexity of most problems, but the landscape keeps some relief, in particular for reversible computations.","sentences":["We settle the theoretical ground for the study of automata networks under block-parallel update schedules, which are somehow dual to the block-sequential ones, but allow for repetitions of automaton updates.","This gain in expressivity brings new challenges, and we analyse natural equivalence classes of update schedules: those leading to the same dynamics, and to the same limit dynamics, for any automata network.","Countings and enumeration algorithms are provided, for their numerical study.","We also prove computational complexity bounds for many classical problems, involving fixed points, limit cycles, the recognition of subdynamics, reachability, etc.","The PSPACE-completeness of computing the image of a single configuration lifts the complexity of most problems, but the landscape keeps some relief, in particular for reversible computations."],"url":"http://arxiv.org/abs/2503.04591v1"}
{"created":"2025-03-06 16:26:22","title":"Random Testing of Model Checkers for Timed Automata with Automated Oracle Generation","abstract":"A key challenge in formal verification, particularly in Model Checking, is ensuring the correctness of the verification tools. Erroneous results on complex models can be difficult to detect, yet a high level of confidence in the outcome is expected. Indeed, these tools are frequently novel and may not have been thoroughly tested. When standard benchmarks may be insufficient or unavailable, random test case generation offers a promising approach. To scale up, random testing requires comparing actual versus expected results, i.e., solving the oracle problem. To address this challenge, this work introduces a novel theoretical framework based on a modular variant of Timed Automata (TA), called Tiled Timed Automata (TTA), for testing model checkers operating with variations of TA, by building oracles based on Weighted Automata. The framework is initially applied to verify model checkers solving the emptiness problem for Parametric TA and it is validated, in this specific scenario, by our tool, TABEC, which randomly generates tests predicting their expected outcome through automated oracle generation. Furthermore, the general nature of TTA facilitates the framework adaptation to model checkers solving other decidable problems on TA, as detailed for the minimum-cost reachability problem of Priced TA.","sentences":["A key challenge in formal verification, particularly in Model Checking, is ensuring the correctness of the verification tools.","Erroneous results on complex models can be difficult to detect, yet a high level of confidence in the outcome is expected.","Indeed, these tools are frequently novel and may not have been thoroughly tested.","When standard benchmarks may be insufficient or unavailable, random test case generation offers a promising approach.","To scale up, random testing requires comparing actual versus expected results, i.e., solving the oracle problem.","To address this challenge, this work introduces a novel theoretical framework based on a modular variant of Timed Automata (TA), called Tiled Timed Automata (TTA), for testing model checkers operating with variations of TA, by building oracles based on Weighted Automata.","The framework is initially applied to verify model checkers solving the emptiness problem for Parametric TA and it is validated, in this specific scenario, by our tool, TABEC, which randomly generates tests predicting their expected outcome through automated oracle generation.","Furthermore, the general nature of TTA facilitates the framework adaptation to model checkers solving other decidable problems on TA, as detailed for the minimum-cost reachability problem of Priced TA."],"url":"http://arxiv.org/abs/2503.04589v1"}
{"created":"2025-03-06 16:22:19","title":"Advancing Solutions for the Three-Body Problem Through Physics-Informed Neural Networks","abstract":"First formulated by Sir Isaac Newton in his work \"Philosophiae Naturalis Principia Mathematica\", the concept of the Three-Body Problem was put forth as a study of the motion of the three celestial bodies within the Earth-Sun-Moon system. In a generalized definition, it seeks to predict the motion for an isolated system composed of three point masses freely interacting under Newton's law of universal attraction. This proves to be analogous to a multitude of interactions between celestial bodies, and thus, the problem finds applicability within the studies of celestial mechanics. Despite numerous attempts by renowned physicists to solve it throughout the last three centuries, no general closed-form solutions have been reached due to its inherently chaotic nature for most initial conditions. Current state-of-the-art solutions are based on two approaches, either numerical high-precision integration or machine learning-based. Notwithstanding the breakthroughs of neural networks, these present a significant limitation, which is their ignorance of any prior knowledge of the chaotic systems presented. Thus, in this work, we propose a novel method that utilizes Physics-Informed Neural Networks (PINNs). These deep neural networks are able to incorporate any prior system knowledge expressible as an Ordinary Differential Equation (ODE) into their learning processes as a regularizing agent. Our findings showcase that PINNs surpass current state-of-the-art machine learning methods with comparable prediction quality. Despite a better prediction quality, the usability of numerical integrators suffers due to their prohibitively high computational cost. These findings confirm that PINNs are both effective and time-efficient open-form solvers of the Three-Body Problem that capitalize on the extensive knowledge we hold of classical mechanics.","sentences":["First formulated by Sir Isaac Newton in his work \"Philosophiae Naturalis Principia Mathematica\", the concept of the Three-Body Problem was put forth as a study of the motion of the three celestial bodies within the Earth-Sun-Moon system.","In a generalized definition, it seeks to predict the motion for an isolated system composed of three point masses freely interacting under Newton's law of universal attraction.","This proves to be analogous to a multitude of interactions between celestial bodies, and thus, the problem finds applicability within the studies of celestial mechanics.","Despite numerous attempts by renowned physicists to solve it throughout the last three centuries, no general closed-form solutions have been reached due to its inherently chaotic nature for most initial conditions.","Current state-of-the-art solutions are based on two approaches, either numerical high-precision integration or machine learning-based.","Notwithstanding the breakthroughs of neural networks, these present a significant limitation, which is their ignorance of any prior knowledge of the chaotic systems presented.","Thus, in this work, we propose a novel method that utilizes Physics-Informed Neural Networks (PINNs).","These deep neural networks are able to incorporate any prior system knowledge expressible as an Ordinary Differential Equation (ODE) into their learning processes as a regularizing agent.","Our findings showcase that PINNs surpass current state-of-the-art machine learning methods with comparable prediction quality.","Despite a better prediction quality, the usability of numerical integrators suffers due to their prohibitively high computational cost.","These findings confirm that PINNs are both effective and time-efficient open-form solvers of the Three-Body Problem that capitalize on the extensive knowledge we hold of classical mechanics."],"url":"http://arxiv.org/abs/2503.04585v1"}
{"created":"2025-03-06 16:20:25","title":"PSDNorm: Test-Time Temporal Normalization for Deep Learning on EEG Signals","abstract":"Distribution shift poses a significant challenge in machine learning, particularly in biomedical applications such as EEG signals collected across different subjects, institutions, and recording devices. While existing normalization layers, Batch-Norm, LayerNorm and InstanceNorm, help address distribution shifts, they fail to capture the temporal dependencies inherent in temporal signals. In this paper, we propose PSDNorm, a layer that leverages Monge mapping and temporal context to normalize feature maps in deep learning models. Notably, the proposed method operates as a test-time domain adaptation technique, addressing distribution shifts without additional training. Evaluations on 10 sleep staging datasets using the U-Time model demonstrate that PSDNorm achieves state-of-the-art performance at test time on datasets not seen during training while being 4x more data-efficient than the best baseline. Additionally, PSDNorm provides a significant improvement in robustness, achieving markedly higher F1 scores for the 20% hardest subjects.","sentences":["Distribution shift poses a significant challenge in machine learning, particularly in biomedical applications such as EEG signals collected across different subjects, institutions, and recording devices.","While existing normalization layers, Batch-Norm, LayerNorm and InstanceNorm, help address distribution shifts, they fail to capture the temporal dependencies inherent in temporal signals.","In this paper, we propose PSDNorm, a layer that leverages Monge mapping and temporal context to normalize feature maps in deep learning models.","Notably, the proposed method operates as a test-time domain adaptation technique, addressing distribution shifts without additional training.","Evaluations on 10 sleep staging datasets using the U-Time model demonstrate that PSDNorm achieves state-of-the-art performance at test time on datasets not seen during training while being 4x more data-efficient than the best baseline.","Additionally, PSDNorm provides a significant improvement in robustness, achieving markedly higher F1 scores for the 20% hardest subjects."],"url":"http://arxiv.org/abs/2503.04582v1"}
{"created":"2025-03-06 16:20:08","title":"Maestro: A 302 GFLOPS/W and 19.8GFLOPS RISC-V Vector-Tensor Architecture for Wearable Ultrasound Edge Computing","abstract":"Most Wearable Ultrasound (WUS) devices lack the computational power to process signals at the edge, instead relying on remote offload, which introduces latency, high power consumption, and privacy concerns. We present Maestro, a RISC-V SoC with unified Vector-Tensor Unit (VTU) and memory-coupled Fast Fourier Transform (FFT) accelerators targeting edge processing for wearable ultrasound devices, fabricated using low-cost TSMC 65nm CMOS technology. The VTU achieves peak 302GFLOPS/W and 19.8GFLOPS at FP16, while the multi-precision 16/32-bit floating-point FFT accelerator delivers peak 60.6GFLOPS/W and 3.6GFLOPS at FP16, We evaluate Maestro on a US-based gesture recognition task, achieving 1.62GFLOPS in signal processing at 26.68GFLOPS/W, and 19.52GFLOPS in Convolutional Neural Network (CNN) workloads at 298.03GFLOPS/W. Compared to a state-of-the-art SoC with a similar mission profile, Maestro achieves a 5x speedup while consuming only 12mW, with an energy consumption of 2.5mJ in a wearable US channel preprocessing and ML-based postprocessing pipeline.","sentences":["Most Wearable Ultrasound (WUS) devices lack the computational power to process signals at the edge, instead relying on remote offload, which introduces latency, high power consumption, and privacy concerns.","We present Maestro, a RISC-V SoC with unified Vector-Tensor Unit (VTU) and memory-coupled Fast Fourier Transform (FFT) accelerators targeting edge processing for wearable ultrasound devices, fabricated using low-cost TSMC 65nm CMOS technology.","The VTU achieves peak 302GFLOPS/W and 19.8GFLOPS at FP16, while the multi-precision 16/32-bit floating-point FFT accelerator delivers peak 60.6GFLOPS/W and 3.6GFLOPS at FP16, We evaluate Maestro on a US-based gesture recognition task, achieving 1.62GFLOPS in signal processing at 26.68GFLOPS/W, and 19.52GFLOPS in Convolutional Neural Network (CNN) workloads at 298.03GFLOPS/","W. Compared to a state-of-the-art SoC with a similar mission profile, Maestro achieves a 5x speedup while consuming only 12mW, with an energy consumption of 2.5mJ in a wearable US channel preprocessing and ML-based postprocessing pipeline."],"url":"http://arxiv.org/abs/2503.04581v1"}
{"created":"2025-03-06 16:17:48","title":"DogLegs: Robust Proprioceptive State Estimation for Legged Robots Using Multiple Leg-Mounted IMUs","abstract":"Robust and accurate proprioceptive state estimation of the main body is crucial for legged robots to execute tasks in extreme environments where exteroceptive sensors, such as LiDARs and cameras may become unreliable. In this paper, we propose DogLegs, a state estimation system for legged robots that fuses the measurements from a body-mounted inertial measurement unit (Body-IMU), joint encoders, and multiple leg-mounted IMUs (Leg-IMU) using an extended Kalman filter (EKF). The filter system contains the error states of all IMU frames. The Leg-IMUs are used to detect foot contact, thereby providing zero velocity measurements to update the state of the Leg-IMU frames. Additionally, we compute the relative position constraints between the Body-IMU and Leg-IMUs by the leg kinematics and use them to update the main body state and reduce the error drift of the individual IMU frames. Field experimental results have shown that our proposed system can achieve better state estimation accuracy compared to the traditional leg odometry method (using only Body-IMU and joint encoders) across different terrains. We make our datasets publicly available to benefit the research community.","sentences":["Robust and accurate proprioceptive state estimation of the main body is crucial for legged robots to execute tasks in extreme environments where exteroceptive sensors, such as LiDARs and cameras may become unreliable.","In this paper, we propose DogLegs, a state estimation system for legged robots that fuses the measurements from a body-mounted inertial measurement unit (Body-IMU), joint encoders, and multiple leg-mounted IMUs (Leg-IMU) using an extended Kalman filter (EKF).","The filter system contains the error states of all IMU frames.","The Leg-IMUs are used to detect foot contact, thereby providing zero velocity measurements to update the state of the Leg-IMU frames.","Additionally, we compute the relative position constraints between the Body-IMU and Leg-IMUs by the leg kinematics and use them to update the main body state and reduce the error drift of the individual IMU frames.","Field experimental results have shown that our proposed system can achieve better state estimation accuracy compared to the traditional leg odometry method (using only Body-IMU and joint encoders) across different terrains.","We make our datasets publicly available to benefit the research community."],"url":"http://arxiv.org/abs/2503.04580v1"}
{"created":"2025-03-06 16:13:32","title":"Data-augmented Learning of Geodesic Distances in Irregular Domains through Soner Boundary Conditions","abstract":"Geodesic distances play a fundamental role in robotics, as they efficiently encode global geometric information of the domain. Recent methods use neural networks to approximate geodesic distances by solving the Eikonal equation through physics-informed approaches. While effective, these approaches often suffer from unstable convergence during training in complex environments. We propose a framework to learn geodesic distances in irregular domains by using the Soner boundary condition, and systematically evaluate the impact of data losses on training stability and solution accuracy. Our experiments demonstrate that incorporating data losses significantly improves convergence robustness, reducing training instabilities and sensitivity to initialization. These findings suggest that hybrid data-physics approaches can effectively enhance the reliability of learning-based geodesic distance solvers with sparse data.","sentences":["Geodesic distances play a fundamental role in robotics, as they efficiently encode global geometric information of the domain.","Recent methods use neural networks to approximate geodesic distances by solving the Eikonal equation through physics-informed approaches.","While effective, these approaches often suffer from unstable convergence during training in complex environments.","We propose a framework to learn geodesic distances in irregular domains by using the Soner boundary condition, and systematically evaluate the impact of data losses on training stability and solution accuracy.","Our experiments demonstrate that incorporating data losses significantly improves convergence robustness, reducing training instabilities and sensitivity to initialization.","These findings suggest that hybrid data-physics approaches can effectively enhance the reliability of learning-based geodesic distance solvers with sparse data."],"url":"http://arxiv.org/abs/2503.04579v1"}
{"created":"2025-03-06 16:10:47","title":"Belonging Beyond Code: Queer Software Engineering and Humanities Student Experiences","abstract":"Queer students often encounter discrimination and a lack of belonging in their academic environments. This may be especially true in heteronormative male-dominated fields like software engineering, which already faces a diversity crisis. In contrast, disciplines like humanities have a higher proportion of queer students, suggesting a more diverse academic culture. While prior research has explored queer students' challenges in STEM fields, limited attention has been given to how experiences differ between the sociotechnical, yet highly heteronormative, field of software engineering and the socioculturally inclusive humanities. This study addresses that gap by comparing 165 queer software engineering and 119 queer humanities students experiences. Our findings reveal that queer students in software engineering are less likely to be open about their sexuality, report a significantly lower sense of belonging, and encounter more academic challenges compared to their peers in the humanities. Despite these challenges, queer software engineering students show greater determination to continue their studies. These insights suggest that software engineering could enhance inclusivity by adopting practices commonly seen in the humanities, such as integrating inclusive policies in classrooms, to create a more welcoming environment where queer students can thrive.","sentences":["Queer students often encounter discrimination and a lack of belonging in their academic environments.","This may be especially true in heteronormative male-dominated fields like software engineering, which already faces a diversity crisis.","In contrast, disciplines like humanities have a higher proportion of queer students, suggesting a more diverse academic culture.","While prior research has explored queer students' challenges in STEM fields, limited attention has been given to how experiences differ between the sociotechnical, yet highly heteronormative, field of software engineering and the socioculturally inclusive humanities.","This study addresses that gap by comparing 165 queer software engineering and 119 queer humanities students experiences.","Our findings reveal that queer students in software engineering are less likely to be open about their sexuality, report a significantly lower sense of belonging, and encounter more academic challenges compared to their peers in the humanities.","Despite these challenges, queer software engineering students show greater determination to continue their studies.","These insights suggest that software engineering could enhance inclusivity by adopting practices commonly seen in the humanities, such as integrating inclusive policies in classrooms, to create a more welcoming environment where queer students can thrive."],"url":"http://arxiv.org/abs/2503.04576v1"}
{"created":"2025-03-06 16:05:29","title":"Meta Learning not to Learn: Robustly Informing Meta-Learning under Nuisance-Varying Families","abstract":"In settings where both spurious and causal predictors are available, standard neural networks trained under the objective of empirical risk minimization (ERM) with no additional inductive biases tend to have a dependence on a spurious feature. As a result, it is necessary to integrate additional inductive biases in order to guide the network toward generalizable hypotheses. Often these spurious features are shared across related tasks, such as estimating disease prognoses from image scans coming from different hospitals, making the challenge of generalization more difficult. In these settings, it is important that methods are able to integrate the proper inductive biases to generalize across both nuisance-varying families as well as task families. Motivated by this setting, we present RIME (Robustly Informed Meta lEarning), a new method for meta learning under the presence of both positive and negative inductive biases (what to learn and what not to learn). We first develop a theoretical causal framework showing why existing approaches at knowledge integration can lead to worse performance on distributionally robust objectives. We then show that RIME is able to simultaneously integrate both biases, reaching state of the art performance under distributionally robust objectives in informed meta-learning settings under nuisance-varying families.","sentences":["In settings where both spurious and causal predictors are available, standard neural networks trained under the objective of empirical risk minimization (ERM) with no additional inductive biases tend to have a dependence on a spurious feature.","As a result, it is necessary to integrate additional inductive biases in order to guide the network toward generalizable hypotheses.","Often these spurious features are shared across related tasks, such as estimating disease prognoses from image scans coming from different hospitals, making the challenge of generalization more difficult.","In these settings, it is important that methods are able to integrate the proper inductive biases to generalize across both nuisance-varying families as well as task families.","Motivated by this setting, we present RIME (Robustly Informed Meta lEarning), a new method for meta learning under the presence of both positive and negative inductive biases (what to learn and what not to learn).","We first develop a theoretical causal framework showing why existing approaches at knowledge integration can lead to worse performance on distributionally robust objectives.","We then show that RIME is able to simultaneously integrate both biases, reaching state of the art performance under distributionally robust objectives in informed meta-learning settings under nuisance-varying families."],"url":"http://arxiv.org/abs/2503.04570v1"}
{"created":"2025-03-06 16:02:53","title":"ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making","abstract":"Despite recent advances in artificial intelligence (AI), it poses challenges to ensure personalized decision-making in tasks that are not considered in training datasets. To address this issue, we propose ValuePilot, a two-phase value-driven decision-making framework comprising a dataset generation toolkit DGT and a decision-making module DMM trained on the generated data. DGT is capable of generating scenarios based on value dimensions and closely mirroring real-world tasks, with automated filtering techniques and human curation to ensure the validity of the dataset. In the generated dataset, DMM learns to recognize the inherent values of scenarios, computes action feasibility and navigates the trade-offs between multiple value dimensions to make personalized decisions. Extensive experiments demonstrate that, given human value preferences, our DMM most closely aligns with human decisions, outperforming Claude-3.5-Sonnet, Gemini-2-flash, Llama-3.1-405b and GPT-4o. This research is a preliminary exploration of value-driven decision-making. We hope it will stimulate interest in value-driven decision-making and personalized decision-making within the community.","sentences":["Despite recent advances in artificial intelligence (AI), it poses challenges to ensure personalized decision-making in tasks that are not considered in training datasets.","To address this issue, we propose ValuePilot, a two-phase value-driven decision-making framework comprising a dataset generation toolkit DGT and a decision-making module DMM trained on the generated data.","DGT is capable of generating scenarios based on value dimensions and closely mirroring real-world tasks, with automated filtering techniques and human curation to ensure the validity of the dataset.","In the generated dataset, DMM learns to recognize the inherent values of scenarios, computes action feasibility and navigates the trade-offs between multiple value dimensions to make personalized decisions.","Extensive experiments demonstrate that, given human value preferences, our DMM most closely aligns with human decisions, outperforming Claude-3.5-Sonnet, Gemini-2-flash, Llama-3.1-405b and GPT-4o.","This research is a preliminary exploration of value-driven decision-making.","We hope it will stimulate interest in value-driven decision-making and personalized decision-making within the community."],"url":"http://arxiv.org/abs/2503.04569v1"}
{"created":"2025-03-06 15:53:42","title":"Omnidirectional Multi-Object Tracking","abstract":"Panoramic imagery, with its 360{\\deg} field of view, offers comprehensive information to support Multi-Object Tracking (MOT) in capturing spatial and temporal relationships of surrounding objects. However, most MOT algorithms are tailored for pinhole images with limited views, impairing their effectiveness in panoramic settings. Additionally, panoramic image distortions, such as resolution loss, geometric deformation, and uneven lighting, hinder direct adaptation of existing MOT methods, leading to significant performance degradation. To address these challenges, we propose OmniTrack, an omnidirectional MOT framework that incorporates Tracklet Management to introduce temporal cues, FlexiTrack Instances for object localization and association, and the CircularStatE Module to alleviate image and geometric distortions. This integration enables tracking in large field-of-view scenarios, even under rapid sensor motion. To mitigate the lack of panoramic MOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic dataset collected by a quadruped robot, featuring diverse challenges such as wide fields of view, intense motion, and complex environments. Extensive experiments on the public JRDB dataset and the newly introduced QuadTrack benchmark demonstrate the state-of-the-art performance of the proposed framework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an improvement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the baseline by 6.81%. The dataset and code will be made publicly available at https://github.com/xifen523/OmniTrack.","sentences":["Panoramic imagery, with its 360{\\deg} field of view, offers comprehensive information to support Multi-Object Tracking (MOT) in capturing spatial and temporal relationships of surrounding objects.","However, most MOT algorithms are tailored for pinhole images with limited views, impairing their effectiveness in panoramic settings.","Additionally, panoramic image distortions, such as resolution loss, geometric deformation, and uneven lighting, hinder direct adaptation of existing MOT methods, leading to significant performance degradation.","To address these challenges, we propose OmniTrack, an omnidirectional MOT framework that incorporates Tracklet Management to introduce temporal cues, FlexiTrack Instances for object localization and association, and the CircularStatE Module to alleviate image and geometric distortions.","This integration enables tracking in large field-of-view scenarios, even under rapid sensor motion.","To mitigate the lack of panoramic MOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic dataset collected by a quadruped robot, featuring diverse challenges such as wide fields of view, intense motion, and complex environments.","Extensive experiments on the public JRDB dataset and the newly introduced QuadTrack benchmark demonstrate the state-of-the-art performance of the proposed framework.","OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an improvement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the baseline by 6.81%.","The dataset and code will be made publicly available at https://github.com/xifen523/OmniTrack."],"url":"http://arxiv.org/abs/2503.04565v1"}
{"created":"2025-03-06 15:53:37","title":"Fundamental Limits of Hierarchical Secure Aggregation with Cyclic User Association","abstract":"Secure aggregation is motivated by federated learning (FL) where a cloud server aims to compute an averaged model (i.e., weights of deep neural networks) of the locally-trained models of numerous clients, while adhering to data security requirements. Hierarchical secure aggregation (HSA) extends this concept to a three-layer network, where clustered users communicate with the server through an intermediate layer of relays. In HSA, beyond conventional server security, relay security is also enforced to ensure that the relays remain oblivious to the users' inputs (an abstraction of the local models in FL). Existing study on HSA assumes that each user is associated with only one relay, limiting opportunities for coding across inter-cluster users to achieve efficient communication and key generation. In this paper, we consider HSA with a cyclic association pattern where each user is connected to $B$ consecutive relays in a wrap-around manner. We propose an efficient aggregation scheme which includes a message design for the inputs inspired by gradient coding-a well-known technique for efficient communication in distributed computing-along with a highly nontrivial security key design. We also derive novel converse bounds on the minimum achievable communication and key rates using information-theoretic arguments.","sentences":["Secure aggregation is motivated by federated learning (FL) where a cloud server aims to compute an averaged model (i.e., weights of deep neural networks) of the locally-trained models of numerous clients, while adhering to data security requirements.","Hierarchical secure aggregation (HSA) extends this concept to a three-layer network, where clustered users communicate with the server through an intermediate layer of relays.","In HSA, beyond conventional server security, relay security is also enforced to ensure that the relays remain oblivious to the users' inputs (an abstraction of the local models in FL).","Existing study on HSA assumes that each user is associated with only one relay, limiting opportunities for coding across inter-cluster users to achieve efficient communication and key generation.","In this paper, we consider HSA with a cyclic association pattern where each user is connected to $B$ consecutive relays in a wrap-around manner.","We propose an efficient aggregation scheme which includes a message design for the inputs inspired by gradient coding-a well-known technique for efficient communication in distributed computing-along with a highly nontrivial security key design.","We also derive novel converse bounds on the minimum achievable communication and key rates using information-theoretic arguments."],"url":"http://arxiv.org/abs/2503.04564v1"}
{"created":"2025-03-06 15:52:59","title":"Occlusion-Aware Consistent Model Predictive Control for Robot Navigation in Occluded Obstacle-Dense Environments","abstract":"Ensuring safety and motion consistency for robot navigation in occluded, obstacle-dense environments is a critical challenge. In this context, this study presents an occlusion-aware Consistent Model Predictive Control (CMPC) strategy. To account for the occluded obstacles, it incorporates adjustable risk regions that represent their potential future locations. Subsequently, dynamic risk boundary constraints are developed online to ensure safety. The CMPC then constructs multiple locally optimal trajectory branches (each tailored to different risk regions) to balance between exploitation and exploration. A shared consensus trunk is generated to ensure smooth transitions between branches without significant velocity fluctuations, further preserving motion consistency. To facilitate high computational efficiency and ensure coordination across local trajectories, we use the alternating direction method of multipliers (ADMM) to decompose the CMPC into manageable sub-problems for parallel solving. The proposed strategy is validated through simulation and real-world experiments on an Ackermann-steering robot platform. The results demonstrate the effectiveness of the proposed CMPC strategy through comparisons with baseline approaches in occluded, obstacle-dense environments.","sentences":["Ensuring safety and motion consistency for robot navigation in occluded, obstacle-dense environments is a critical challenge.","In this context, this study presents an occlusion-aware Consistent Model Predictive Control (CMPC) strategy.","To account for the occluded obstacles, it incorporates adjustable risk regions that represent their potential future locations.","Subsequently, dynamic risk boundary constraints are developed online to ensure safety.","The CMPC then constructs multiple locally optimal trajectory branches (each tailored to different risk regions) to balance between exploitation and exploration.","A shared consensus trunk is generated to ensure smooth transitions between branches without significant velocity fluctuations, further preserving motion consistency.","To facilitate high computational efficiency and ensure coordination across local trajectories, we use the alternating direction method of multipliers (ADMM) to decompose the CMPC into manageable sub-problems for parallel solving.","The proposed strategy is validated through simulation and real-world experiments on an Ackermann-steering robot platform.","The results demonstrate the effectiveness of the proposed CMPC strategy through comparisons with baseline approaches in occluded, obstacle-dense environments."],"url":"http://arxiv.org/abs/2503.04563v1"}
{"created":"2025-03-06 15:49:16","title":"Learning Generalizable Language-Conditioned Cloth Manipulation from Long Demonstrations","abstract":"Multi-step cloth manipulation is a challenging problem for robots due to the high-dimensional state spaces and the dynamics of cloth. Despite recent significant advances in end-to-end imitation learning for multi-step cloth manipulation skills, these methods fail to generalize to unseen tasks. Our insight in tackling the challenge of generalizable multi-step cloth manipulation is decomposition. We propose a novel pipeline that autonomously learns basic skills from long demonstrations and composes learned basic skills to generalize to unseen tasks. Specifically, our method first discovers and learns basic skills from the existing long demonstration benchmark with the commonsense knowledge of a large language model (LLM). Then, leveraging a high-level LLM-based task planner, these basic skills can be composed to complete unseen tasks. Experimental results demonstrate that our method outperforms baseline methods in learning multi-step cloth manipulation skills for both seen and unseen tasks.","sentences":["Multi-step cloth manipulation is a challenging problem for robots due to the high-dimensional state spaces and the dynamics of cloth.","Despite recent significant advances in end-to-end imitation learning for multi-step cloth manipulation skills, these methods fail to generalize to unseen tasks.","Our insight in tackling the challenge of generalizable multi-step cloth manipulation is decomposition.","We propose a novel pipeline that autonomously learns basic skills from long demonstrations and composes learned basic skills to generalize to unseen tasks.","Specifically, our method first discovers and learns basic skills from the existing long demonstration benchmark with the commonsense knowledge of a large language model (LLM).","Then, leveraging a high-level LLM-based task planner, these basic skills can be composed to complete unseen tasks.","Experimental results demonstrate that our method outperforms baseline methods in learning multi-step cloth manipulation skills for both seen and unseen tasks."],"url":"http://arxiv.org/abs/2503.04557v1"}
{"created":"2025-03-06 15:47:19","title":"Compositional Causal Reasoning Evaluation in Language Models","abstract":"Causal reasoning and compositional reasoning are two core aspirations in generative AI. Measuring the extent of these behaviors requires principled evaluation methods. We explore a unified perspective that considers both behaviors simultaneously, termed compositional causal reasoning (CCR): the ability to infer how causal measures compose and, equivalently, how causal quantities propagate through graphs. We instantiate a framework for the systematic evaluation of CCR for the average treatment effect and the probability of necessity and sufficiency. As proof of concept, we demonstrate the design of CCR tasks for language models in the LLama, Phi, and GPT families. On a math word problem, our framework revealed a range of taxonomically distinct error patterns. Additionally, CCR errors increased with the complexity of causal paths for all models except o1.","sentences":["Causal reasoning and compositional reasoning are two core aspirations in generative AI.","Measuring the extent of these behaviors requires principled evaluation methods.","We explore a unified perspective that considers both behaviors simultaneously, termed compositional causal reasoning (CCR): the ability to infer how causal measures compose and, equivalently, how causal quantities propagate through graphs.","We instantiate a framework for the systematic evaluation of CCR for the average treatment effect and the probability of necessity and sufficiency.","As proof of concept, we demonstrate the design of CCR tasks for language models in the LLama, Phi, and GPT families.","On a math word problem, our framework revealed a range of taxonomically distinct error patterns.","Additionally, CCR errors increased with the complexity of causal paths for all models except o1."],"url":"http://arxiv.org/abs/2503.04556v1"}
{"created":"2025-03-06 15:37:31","title":"Compositional Translation: A Novel LLM-based Approach for Low-resource Machine Translation","abstract":"The ability of generative large language models (LLMs) to perform in-context learning has given rise to a large body of research into how best to prompt models for various natural language processing tasks. Machine Translation (MT) has been shown to benefit from in-context examples, in particular when they are semantically similar to the sentence to translate. In this paper, we propose a new LLM-based translation paradigm, compositional translation, to replace naive few-shot MT with similarity-based demonstrations. An LLM is used to decompose a sentence into simpler phrases, and then to translate each phrase with the help of retrieved demonstrations. Finally, the LLM is prompted to translate the initial sentence with the help of the self-generated phrase-translation pairs. Our intuition is that this approach should improve translation because these shorter phrases should be intrinsically easier to translate and easier to match with relevant examples. This is especially beneficial in low-resource scenarios, and more generally whenever the selection pool is small or out of domain. We show that compositional translation boosts LLM translation performance on a wide range of popular MT benchmarks, including FLORES 200, NTREX 128 and TICO-19. Code and outputs are available at https://github.com/ArmelRandy/compositional-translation","sentences":["The ability of generative large language models (LLMs) to perform in-context learning has given rise to a large body of research into how best to prompt models for various natural language processing tasks.","Machine Translation (MT) has been shown to benefit from in-context examples, in particular when they are semantically similar to the sentence to translate.","In this paper, we propose a new LLM-based translation paradigm, compositional translation, to replace naive few-shot MT with similarity-based demonstrations.","An LLM is used to decompose a sentence into simpler phrases, and then to translate each phrase with the help of retrieved demonstrations.","Finally, the LLM is prompted to translate the initial sentence with the help of the self-generated phrase-translation pairs.","Our intuition is that this approach should improve translation because these shorter phrases should be intrinsically easier to translate and easier to match with relevant examples.","This is especially beneficial in low-resource scenarios, and more generally whenever the selection pool is small or out of domain.","We show that compositional translation boosts LLM translation performance on a wide range of popular MT benchmarks, including FLORES 200, NTREX 128 and TICO-19.","Code and outputs are available at https://github.com/ArmelRandy/compositional-translation"],"url":"http://arxiv.org/abs/2503.04554v1"}
{"created":"2025-03-06 15:36:06","title":"Benchmarking Reasoning Robustness in Large Language Models","abstract":"Despite the recent success of large language models (LLMs) in reasoning such as DeepSeek, we for the first time identify a key dilemma in reasoning robustness and generalization: significant performance degradation on novel or incomplete data, suggesting a reliance on memorized patterns rather than systematic reasoning. Our closer examination reveals four key unique limitations underlying this issue:(1) Positional bias--models favor earlier queries in multi-query inputs but answering the wrong one in the latter (e.g., GPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction sensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series and by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical fragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from 97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5 percent); and (4) Memory dependence--models resort to guesswork when missing critical data. These findings further highlight the reliance on heuristic recall over rigorous logical inference, demonstrating challenges in reasoning robustness. To comprehensively investigate these robustness challenges, this paper introduces a novel benchmark, termed as Math-RoB, that exploits hallucinations triggered by missing information to expose reasoning gaps. This is achieved by an instruction-based approach to generate diverse datasets that closely resemble training distributions, facilitating a holistic robustness assessment and advancing the development of more robust reasoning frameworks. Bad character(s) in field Abstract.","sentences":["Despite the recent success of large language models (LLMs) in reasoning such as DeepSeek, we for the first time identify a key dilemma in reasoning robustness and generalization: significant performance degradation on novel or incomplete data, suggesting a reliance on memorized patterns rather than systematic reasoning.","Our closer examination reveals four key unique limitations underlying this issue:(1) Positional bias--models favor earlier queries in multi-query inputs but answering the wrong one in the latter (e.g., GPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction sensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series and by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical fragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from 97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5 percent); and (4) Memory dependence--models resort to guesswork when missing critical data.","These findings further highlight the reliance on heuristic recall over rigorous logical inference, demonstrating challenges in reasoning robustness.","To comprehensively investigate these robustness challenges, this paper introduces a novel benchmark, termed as Math-RoB, that exploits hallucinations triggered by missing information to expose reasoning gaps.","This is achieved by an instruction-based approach to generate diverse datasets that closely resemble training distributions, facilitating a holistic robustness assessment and advancing the development of more robust reasoning frameworks.","Bad character(s) in field Abstract."],"url":"http://arxiv.org/abs/2503.04550v1"}
{"created":"2025-03-06 15:34:50","title":"Lite-PoT: Practical Powers-of-Tau Setup Ceremony","abstract":"Zero-Knowledge Succinct Non-Interactive Argument of Knowledge (zk-SNARK) schemes have gained significant adoption in privacy-preserving applications, decentralized systems (e.g., blockchain), and verifiable computation due to their efficiency. However, the most efficient zk-SNARKs often rely on a one-time trusted setup to generate a public parameter, often known as the ``Powers of Tau\" (PoT) string. The leakage of the secret parameter, $\\tau$, in the string would allow attackers to generate false proofs, compromising the soundness of all zk-SNARK systems built on it.   Prior proposals for decentralized setup ceremonies have utilized blockchain-based smart contracts to allow any party to contribute randomness to $\\tau$ while also preventing censorship of contributions. For a PoT string of $d$-degree generated by the randomness of $m$ contributors, these solutions required a total of $O(md)$ on-chain operations (i.e., in terms of both storage and cryptographic operations). These operations primarily consisted of costly group operations, particularly scalar multiplication on pairing curves, which discouraged participation and limited the impact of decentralization   In this work, we present Lite-PoT, which includes two key protocols designed to reduce participation costs: \\emph{(i)} a fraud-proof protocol to reduce the number of expensive on-chain cryptographic group operations to $O(1)$ per contributor. Our experimental results show that (with one transaction per update) our protocol enables decentralized ceremonies for PoT strings up to a $2^{15}$ degree, an $\\approx 16x$ improvement over existing on-chain solutions; \\emph{(ii)} a proof aggregation technique that batches $m$ randomness contributions into one on-chain update with only $O(d)$ on-chain operations, independent of $m$. This significantly reduces the monetary cost of on-chain updates by $m$-fold via amortization.","sentences":["Zero-Knowledge Succinct Non-Interactive Argument of Knowledge (zk-SNARK) schemes have gained significant adoption in privacy-preserving applications, decentralized systems (e.g., blockchain), and verifiable computation due to their efficiency.","However, the most efficient zk-SNARKs often rely on a one-time trusted setup to generate a public parameter, often known as the ``Powers of Tau\" (PoT) string.","The leakage of the secret parameter, $\\tau$, in the string would allow attackers to generate false proofs, compromising the soundness of all zk-SNARK systems built on it.   ","Prior proposals for decentralized setup ceremonies have utilized blockchain-based smart contracts to allow any party to contribute randomness to $\\tau$ while also preventing censorship of contributions.","For a PoT string of $d$-degree generated by the randomness of $m$ contributors, these solutions required a total of $O(md)$ on-chain operations (i.e., in terms of both storage and cryptographic operations).","These operations primarily consisted of costly group operations, particularly scalar multiplication on pairing curves, which discouraged participation and limited the impact of decentralization   In this work, we present Lite-PoT, which includes two key protocols designed to reduce participation costs: \\emph{(i)} a fraud-proof protocol to reduce the number of expensive on-chain cryptographic group operations to $O(1)$ per contributor.","Our experimental results show that (with one transaction per update) our protocol enables decentralized ceremonies for PoT strings up to a $2^{15}$ degree, an $\\approx 16x$ improvement over existing on-chain solutions; \\emph{(ii)} a proof aggregation technique that batches $m$ randomness contributions into one on-chain update with only $O(d)$ on-chain operations, independent of $m$. This significantly reduces the monetary cost of on-chain updates by $m$-fold via amortization."],"url":"http://arxiv.org/abs/2503.04549v1"}
{"created":"2025-03-06 15:34:27","title":"An Empirical Study on Eliciting and Improving R1-like Reasoning Models","abstract":"In this report, we present the third technical report on the development of slow-thinking models as part of the STILL project. As the technical pathway becomes clearer, scaling RL training has become a central technique for implementing such reasoning models. We systematically experiment with and document the effects of various factors influencing RL training, conducting experiments on both base models and fine-tuned models. Specifically, we demonstrate that our RL training approach consistently improves the Qwen2.5-32B base models, enhancing both response length and test accuracy. Furthermore, we show that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already achieved a high performance level, it can be further refined through RL training, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we also explore the use of tool manipulation, finding that it significantly boosts the reasoning performance of large reasoning models. This approach achieves a remarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its effectiveness in enhancing model capabilities. We release our resources at the STILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.","sentences":["In this report, we present the third technical report on the development of slow-thinking models as part of the STILL project.","As the technical pathway becomes clearer, scaling RL training has become a central technique for implementing such reasoning models.","We systematically experiment with and document the effects of various factors influencing RL training, conducting experiments on both base models and fine-tuned models.","Specifically, we demonstrate that our RL training approach consistently improves the Qwen2.5-32B base models, enhancing both response length and test accuracy.","Furthermore, we show that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already achieved a high performance level, it can be further refined through RL training, reaching an accuracy of 39.33% on AIME 2024.","Beyond RL training, we also explore the use of tool manipulation, finding that it significantly boosts the reasoning performance of large reasoning models.","This approach achieves a remarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its effectiveness in enhancing model capabilities.","We release our resources at the STILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs."],"url":"http://arxiv.org/abs/2503.04548v1"}
{"created":"2025-03-06 15:33:19","title":"ViT-VS: On the Applicability of Pretrained Vision Transformer Features for Generalizable Visual Servoing","abstract":"Visual servoing enables robots to precisely position their end-effector relative to a target object. While classical methods rely on hand-crafted features and thus are universally applicable without task-specific training, they often struggle with occlusions and environmental variations, whereas learning-based approaches improve robustness but typically require extensive training. We present a visual servoing approach that leverages pretrained vision transformers for semantic feature extraction, combining the advantages of both paradigms while also being able to generalize beyond the provided sample. Our approach achieves full convergence in unperturbed scenarios and surpasses classical image-based visual servoing by up to 31.2\\% relative improvement in perturbed scenarios. Even the convergence rates of learning-based methods are matched despite requiring no task- or object-specific training. Real-world evaluations confirm robust performance in end-effector positioning, industrial box manipulation, and grasping of unseen objects using only a reference from the same category. Our code and simulation environment are available at: https://alessandroscherl.github.io/ViT-VS/","sentences":["Visual servoing enables robots to precisely position their end-effector relative to a target object.","While classical methods rely on hand-crafted features and thus are universally applicable without task-specific training, they often struggle with occlusions and environmental variations, whereas learning-based approaches improve robustness but typically require extensive training.","We present a visual servoing approach that leverages pretrained vision transformers for semantic feature extraction, combining the advantages of both paradigms while also being able to generalize beyond the provided sample.","Our approach achieves full convergence in unperturbed scenarios and surpasses classical image-based visual servoing by up to 31.2\\% relative improvement in perturbed scenarios.","Even the convergence rates of learning-based methods are matched despite requiring no task- or object-specific training.","Real-world evaluations confirm robust performance in end-effector positioning, industrial box manipulation, and grasping of unseen objects using only a reference from the same category.","Our code and simulation environment are available at: https://alessandroscherl.github.io/ViT-VS/"],"url":"http://arxiv.org/abs/2503.04545v1"}
{"created":"2025-03-06 15:29:13","title":"Keeping Yourself is Important in Downstream Tuning Multimodal Large Language Model","abstract":"Multi-modal Large Language Models (MLLMs) integrate visual and linguistic reasoning to address complex tasks such as image captioning and visual question answering. While MLLMs demonstrate remarkable versatility, MLLMs appears limited performance on special applications. But tuning MLLMs for downstream tasks encounters two key challenges: Task-Expert Specialization, where distribution shifts between pre-training and target datasets constrain target performance, and Open-World Stabilization, where catastrophic forgetting erases the model general knowledge. In this work, we systematically review recent advancements in MLLM tuning methodologies, classifying them into three paradigms: (I) Selective Tuning, (II) Additive Tuning, and (III) Reparameterization Tuning. Furthermore, we benchmark these tuning strategies across popular MLLM architectures and diverse downstream tasks to establish standardized evaluation analysis and systematic tuning principles. Finally, we highlight several open challenges in this domain and propose future research directions. To facilitate ongoing progress in this rapidly evolving field, we provide a public repository that continuously tracks developments: https://github.com/WenkeHuang/Awesome-MLLM-Tuning.","sentences":["Multi-modal Large Language Models (MLLMs) integrate visual and linguistic reasoning to address complex tasks such as image captioning and visual question answering.","While MLLMs demonstrate remarkable versatility, MLLMs appears limited performance on special applications.","But tuning MLLMs for downstream tasks encounters two key challenges: Task-Expert Specialization, where distribution shifts between pre-training and target datasets constrain target performance, and Open-World Stabilization, where catastrophic forgetting erases the model general knowledge.","In this work, we systematically review recent advancements in MLLM tuning methodologies, classifying them into three paradigms: (I) Selective Tuning, (II) Additive Tuning, and (III) Reparameterization Tuning.","Furthermore, we benchmark these tuning strategies across popular MLLM architectures and diverse downstream tasks to establish standardized evaluation analysis and systematic tuning principles.","Finally, we highlight several open challenges in this domain and propose future research directions.","To facilitate ongoing progress in this rapidly evolving field, we provide a public repository that continuously tracks developments: https://github.com/WenkeHuang/Awesome-MLLM-Tuning."],"url":"http://arxiv.org/abs/2503.04543v1"}
{"created":"2025-03-06 15:28:12","title":"Inducing Efficient and Equitable Professional Networks through Link Recommendations","abstract":"Professional networks are a key determinant of individuals' labor market outcomes. They may also play a role in either exacerbating or ameliorating inequality of opportunity across demographic groups. In a theoretical model of professional network formation, we show that inequality can increase even without exogenous in-group preferences, confirming and complementing existing theoretical literature. Increased inequality emerges from the differential leverage privileged and unprivileged individuals have in forming connections due to their asymmetric ex ante prospects. This is a formalization of a source of inequality in the labor market which has not been previously explored.   We next show how inequality-aware platforms may reduce inequality by subsidizing connections, through link recommendations that reduce costs, between privileged and unprivileged individuals. Indeed, mixed-privilege connections turn out to be welfare improving, over all possible equilibria, compared to not recommending links or recommending some smaller fraction of cross-group links. Taken together, these two findings reveal a stark reality: professional networking platforms that fail to foster integration in the link formation process risk reducing the platform's utility to its users and exacerbating existing labor market inequality.","sentences":["Professional networks are a key determinant of individuals' labor market outcomes.","They may also play a role in either exacerbating or ameliorating inequality of opportunity across demographic groups.","In a theoretical model of professional network formation, we show that inequality can increase even without exogenous in-group preferences, confirming and complementing existing theoretical literature.","Increased inequality emerges from the differential leverage privileged and unprivileged individuals have in forming connections due to their asymmetric ex ante prospects.","This is a formalization of a source of inequality in the labor market which has not been previously explored.   ","We next show how inequality-aware platforms may reduce inequality by subsidizing connections, through link recommendations that reduce costs, between privileged and unprivileged individuals.","Indeed, mixed-privilege connections turn out to be welfare improving, over all possible equilibria, compared to not recommending links or recommending some smaller fraction of cross-group links.","Taken together, these two findings reveal a stark reality: professional networking platforms that fail to foster integration in the link formation process risk reducing the platform's utility to its users and exacerbating existing labor market inequality."],"url":"http://arxiv.org/abs/2503.04542v1"}
{"created":"2025-03-06 15:22:38","title":"SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks","abstract":"Enabling robots to learn novel tasks in a data-efficient manner is a long-standing challenge. Common strategies involve carefully leveraging prior experiences, especially transition data collected on related tasks. Although much progress has been made for general pick-and-place manipulation, far fewer studies have investigated contact-rich assembly tasks, where precise control is essential. We introduce SRSA (Skill Retrieval and Skill Adaptation), a novel framework designed to address this problem by utilizing a pre-existing skill library containing policies for diverse assembly tasks. The challenge lies in identifying which skill from the library is most relevant for fine-tuning on a new task. Our key hypothesis is that skills showing higher zero-shot success rates on a new task are better suited for rapid and effective fine-tuning on that task. To this end, we propose to predict the transfer success for all skills in the skill library on a novel task, and then use this prediction to guide the skill retrieval process. We establish a framework that jointly captures features of object geometry, physical dynamics, and expert actions to represent the tasks, allowing us to efficiently learn the transfer success predictor. Extensive experiments demonstrate that SRSA significantly outperforms the leading baseline. When retrieving and fine-tuning skills on unseen tasks, SRSA achieves a 19% relative improvement in success rate, exhibits 2.6x lower standard deviation across random seeds, and requires 2.4x fewer transition samples to reach a satisfactory success rate, compared to the baseline. Furthermore, policies trained with SRSA in simulation achieve a 90% mean success rate when deployed in the real world. Please visit our project webpage https://srsa2024.github.io/.","sentences":["Enabling robots to learn novel tasks in a data-efficient manner is a long-standing challenge.","Common strategies involve carefully leveraging prior experiences, especially transition data collected on related tasks.","Although much progress has been made for general pick-and-place manipulation, far fewer studies have investigated contact-rich assembly tasks, where precise control is essential.","We introduce SRSA (Skill Retrieval and Skill Adaptation), a novel framework designed to address this problem by utilizing a pre-existing skill library containing policies for diverse assembly tasks.","The challenge lies in identifying which skill from the library is most relevant for fine-tuning on a new task.","Our key hypothesis is that skills showing higher zero-shot success rates on a new task are better suited for rapid and effective fine-tuning on that task.","To this end, we propose to predict the transfer success for all skills in the skill library on a novel task, and then use this prediction to guide the skill retrieval process.","We establish a framework that jointly captures features of object geometry, physical dynamics, and expert actions to represent the tasks, allowing us to efficiently learn the transfer success predictor.","Extensive experiments demonstrate that SRSA significantly outperforms the leading baseline.","When retrieving and fine-tuning skills on unseen tasks, SRSA achieves a 19% relative improvement in success rate, exhibits 2.6x lower standard deviation across random seeds, and requires 2.4x fewer transition samples to reach a satisfactory success rate, compared to the baseline.","Furthermore, policies trained with SRSA in simulation achieve a 90% mean success rate when deployed in the real world.","Please visit our project webpage https://srsa2024.github.io/."],"url":"http://arxiv.org/abs/2503.04538v1"}
{"created":"2025-03-06 15:19:17","title":"SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning","abstract":"Large Language Models (LLMs) excel in reasoning but remain constrained by their Chain-of-Thought (CoT) approach, which struggles with complex tasks requiring more nuanced topological reasoning. We introduce SOLAR, Scalable Optimization of Large-scale Architecture for Reasoning, a framework that dynamically optimizes various reasoning topologies to enhance accuracy and efficiency.   Our Topological Annotation Generation (TAG) system automates topological dataset creation and segmentation, improving post-training and evaluation. Additionally, we propose Topological-Scaling, a reward-driven framework that aligns training and inference scaling, equipping LLMs with adaptive, task-aware reasoning.   SOLAR achieves substantial gains on MATH and GSM8K: +5% accuracy with Topological Tuning, +9% with Topological Reward, and +10.02% with Hybrid Scaling. It also reduces response length by over 5% for complex problems, lowering inference latency.   To foster the reward system, we train a multi-task Topological Reward Model (M-TRM), which autonomously selects the best reasoning topology and answer in a single pass, eliminating the need for training and inference on multiple single-task TRMs (S-TRMs), thus reducing both training cost and inference latency. In addition, in terms of performance, M-TRM surpasses all S-TRMs, improving accuracy by +10% and rank correlation by +9%.   To the best of our knowledge, SOLAR sets a new benchmark for scalable, high-precision LLM reasoning while introducing an automated annotation process and a dynamic reasoning topology competition mechanism.","sentences":["Large Language Models (LLMs) excel in reasoning but remain constrained by their Chain-of-Thought (CoT) approach, which struggles with complex tasks requiring more nuanced topological reasoning.","We introduce SOLAR, Scalable Optimization of Large-scale Architecture for Reasoning, a framework that dynamically optimizes various reasoning topologies to enhance accuracy and efficiency.   ","Our Topological Annotation Generation (TAG) system automates topological dataset creation and segmentation, improving post-training and evaluation.","Additionally, we propose Topological-Scaling, a reward-driven framework that aligns training and inference scaling, equipping LLMs with adaptive, task-aware reasoning.   ","SOLAR achieves substantial gains on MATH and GSM8K: +5% accuracy with Topological Tuning, +9% with Topological Reward, and +10.02% with Hybrid Scaling.","It also reduces response length by over 5% for complex problems, lowering inference latency.   ","To foster the reward system, we train a multi-task Topological Reward Model (M-TRM), which autonomously selects the best reasoning topology and answer in a single pass, eliminating the need for training and inference on multiple single-task TRMs (S-TRMs), thus reducing both training cost and inference latency.","In addition, in terms of performance, M-TRM surpasses all S-TRMs, improving accuracy by +10% and rank correlation by +9%.   ","To the best of our knowledge, SOLAR sets a new benchmark for scalable, high-precision LLM reasoning while introducing an automated annotation process and a dynamic reasoning topology competition mechanism."],"url":"http://arxiv.org/abs/2503.04530v1"}
{"created":"2025-03-06 15:16:57","title":"Federated Dynamic Modeling and Learning for Spatiotemporal Data Forecasting","abstract":"This paper presents an advanced Federated Learning (FL) framework for forecasting complex spatiotemporal data, improving upon recent state-of-the-art models. In the proposed approach, the original Gated Recurrent Unit (GRU) module within previous Dynamic Spatial--Temporal Graph Convolutional Recurrent Network (DSTGCRN) modeling is first replaced with a Long Short-Term Memory (LSTM) network, enabling the resulting model to more effectively capture long-term dependencies inherent to time series data. The resulting architecture significantly improves the model's capacity to handle complex temporal patterns in diverse forecasting applications. Furthermore, the proposed FL framework integrates a novel Client-Side Validation (CSV) mechanism, introducing a critical validation step at the client level before incorporating aggregated parameters from the central server into local models. This ensures that only the most effective updates are adopted, improving both the robustness and accuracy of the forecasting model across clients. The efficiency of our approach is demonstrated through extensive experiments on real-world applications, including public datasets for multimodal transport demand forecasting and private datasets for Origin-Destination (OD) matrix forecasting in urban areas. The results demonstrate substantial improvements over conventional methods, highlighting the framework's ability to capture complex spatiotemporal dependencies while preserving data privacy. This work not only provides a scalable and privacy-preserving solution for real-time, region-specific forecasting and management but also underscores the potential of leveraging distributed data sources in a FL context. We provide our algorithms as open-source on GitHub.","sentences":["This paper presents an advanced Federated Learning (FL) framework for forecasting complex spatiotemporal data, improving upon recent state-of-the-art models.","In the proposed approach, the original Gated Recurrent Unit (GRU) module within previous Dynamic Spatial--Temporal Graph Convolutional Recurrent Network (DSTGCRN) modeling is first replaced with a Long Short-Term Memory (LSTM) network, enabling the resulting model to more effectively capture long-term dependencies inherent to time series data.","The resulting architecture significantly improves the model's capacity to handle complex temporal patterns in diverse forecasting applications.","Furthermore, the proposed FL framework integrates a novel Client-Side Validation (CSV) mechanism, introducing a critical validation step at the client level before incorporating aggregated parameters from the central server into local models.","This ensures that only the most effective updates are adopted, improving both the robustness and accuracy of the forecasting model across clients.","The efficiency of our approach is demonstrated through extensive experiments on real-world applications, including public datasets for multimodal transport demand forecasting and private datasets for Origin-Destination (OD) matrix forecasting in urban areas.","The results demonstrate substantial improvements over conventional methods, highlighting the framework's ability to capture complex spatiotemporal dependencies while preserving data privacy.","This work not only provides a scalable and privacy-preserving solution for real-time, region-specific forecasting and management but also underscores the potential of leveraging distributed data sources in a FL context.","We provide our algorithms as open-source on GitHub."],"url":"http://arxiv.org/abs/2503.04528v1"}
