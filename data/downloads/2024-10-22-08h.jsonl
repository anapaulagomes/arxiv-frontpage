{"created":"2024-10-21 17:59:53","title":"FrugalNeRF: Fast Convergence for Few-shot Novel View Synthesis without Learned Priors","abstract":"Neural Radiance Fields (NeRF) face significant challenges in few-shot scenarios, primarily due to overfitting and long training times for high-fidelity rendering. Existing methods, such as FreeNeRF and SparseNeRF, use frequency regularization or pre-trained priors but struggle with complex scheduling and bias. We introduce FrugalNeRF, a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details. Our key contribution is a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors across scales. This guides training without relying on externally learned priors, enabling full utilization of the training data. It can also integrate pre-trained priors, enhancing quality without slowing convergence. Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms other few-shot NeRF methods while significantly reducing training time, making it a practical solution for efficient and accurate 3D scene reconstruction.","sentences":["Neural Radiance Fields (NeRF) face significant challenges in few-shot scenarios, primarily due to overfitting and long training times for high-fidelity rendering.","Existing methods, such as FreeNeRF and SparseNeRF, use frequency regularization or pre-trained priors but struggle with complex scheduling and bias.","We introduce FrugalNeRF, a novel few-shot NeRF framework that leverages weight-sharing voxels across multiple scales to efficiently represent scene details.","Our key contribution is a cross-scale geometric adaptation scheme that selects pseudo ground truth depth based on reprojection errors across scales.","This guides training without relying on externally learned priors, enabling full utilization of the training data.","It can also integrate pre-trained priors, enhancing quality without slowing convergence.","Experiments on LLFF, DTU, and RealEstate-10K show that FrugalNeRF outperforms other few-shot NeRF methods while significantly reducing training time, making it a practical solution for efficient and accurate 3D scene reconstruction."],"url":"http://arxiv.org/abs/2410.16271v1"}
{"created":"2024-10-21 17:59:53","title":"MvDrag3D: Drag-based Creative 3D Editing via Multi-view Generation-Reconstruction Priors","abstract":"Drag-based editing has become popular in 2D content creation, driven by the capabilities of image generative models. However, extending this technique to 3D remains a challenge. Existing 3D drag-based editing methods, whether employing explicit spatial transformations or relying on implicit latent optimization within limited-capacity 3D generative models, fall short in handling significant topology changes or generating new textures across diverse object categories. To overcome these limitations, we introduce MVDrag3D, a novel framework for more flexible and creative drag-based 3D editing that leverages multi-view generation and reconstruction priors. At the core of our approach is the usage of a multi-view diffusion model as a strong generative prior to perform consistent drag editing over multiple rendered views, which is followed by a reconstruction model that reconstructs 3D Gaussians of the edited object. While the initial 3D Gaussians may suffer from misalignment between different views, we address this via view-specific deformation networks that adjust the position of Gaussians to be well aligned. In addition, we propose a multi-view score function that distills generative priors from multiple views to further enhance the view consistency and visual quality. Extensive experiments demonstrate that MVDrag3D provides a precise, generative, and flexible solution for 3D drag-based editing, supporting more versatile editing effects across various object categories and 3D representations.","sentences":["Drag-based editing has become popular in 2D content creation, driven by the capabilities of image generative models.","However, extending this technique to 3D remains a challenge.","Existing 3D drag-based editing methods, whether employing explicit spatial transformations or relying on implicit latent optimization within limited-capacity 3D generative models, fall short in handling significant topology changes or generating new textures across diverse object categories.","To overcome these limitations, we introduce MVDrag3D, a novel framework for more flexible and creative drag-based 3D editing that leverages multi-view generation and reconstruction priors.","At the core of our approach is the usage of a multi-view diffusion model as a strong generative prior to perform consistent drag editing over multiple rendered views, which is followed by a reconstruction model that reconstructs 3D Gaussians of the edited object.","While the initial 3D Gaussians may suffer from misalignment between different views, we address this via view-specific deformation networks that adjust the position of Gaussians to be well aligned.","In addition, we propose a multi-view score function that distills generative priors from multiple views to further enhance the view consistency and visual quality.","Extensive experiments demonstrate that MVDrag3D provides a precise, generative, and flexible solution for 3D drag-based editing, supporting more versatile editing effects across various object categories and 3D representations."],"url":"http://arxiv.org/abs/2410.16272v1"}
{"created":"2024-10-21 17:59:50","title":"Reflection-Bench: probing AI intelligence with reflection","abstract":"The ability to adapt beliefs or behaviors in response to unexpected outcomes, reflection, is fundamental to intelligent systems' interaction with the world. From a cognitive science perspective, this serves as a core principle of intelligence applicable to both human and AI systems. To address the debate on the intelligence of large language models (LLMs), we propose Reflection-Bench, a comprehensive benchmark comprising 7 tasks spanning core cognitive functions crucial for reflection, including perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection. We evaluate the performances of 13 prominent LLMs such as OpenAI o1, GPT-4, Claude 3.5 Sonnet, etc. The results indicate that current LLMs still lack satisfactory reflection ability. We discuss the underlying causes of these results and suggest potential avenues for future research. In conclusion, Reflection-Bench offers both evaluation tools and inspiration for developing AI capable of reliably interacting with the environment. Our data and code are available at https://github.com/YabYum/ReflectionBench.","sentences":["The ability to adapt beliefs or behaviors in response to unexpected outcomes, reflection, is fundamental to intelligent systems' interaction with the world.","From a cognitive science perspective, this serves as a core principle of intelligence applicable to both human and AI systems.","To address the debate on the intelligence of large language models (LLMs), we propose Reflection-Bench, a comprehensive benchmark comprising 7 tasks spanning core cognitive functions crucial for reflection, including perception, memory, belief updating, decision-making, prediction, counterfactual thinking, and meta-reflection.","We evaluate the performances of 13 prominent LLMs such as OpenAI o1, GPT-4, Claude 3.5 Sonnet, etc.","The results indicate that current LLMs still lack satisfactory reflection ability.","We discuss the underlying causes of these results and suggest potential avenues for future research.","In conclusion, Reflection-Bench offers both evaluation tools and inspiration for developing AI capable of reliably interacting with the environment.","Our data and code are available at https://github.com/YabYum/ReflectionBench."],"url":"http://arxiv.org/abs/2410.16270v1"}
{"created":"2024-10-21 17:59:19","title":"SAM2Long: Enhancing SAM 2 for Long Video Segmentation with a Training-Free Memory Tree","abstract":"The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation model for object segmentation in both images and videos, paving the way for various downstream video applications. The crucial design of SAM 2 for video segmentation is its memory module, which prompts object-aware memories from previous frames for current frame prediction. However, its greedy-selection memory design suffers from the \"error accumulation\" problem, where an errored or missed mask will cascade and influence the segmentation of the subsequent frames, which limits the performance of SAM 2 toward complex long-term videos. To this end, we introduce SAM2Long, an improved training-free video object segmentation strategy, which considers the segmentation uncertainty within each frame and chooses the video-level optimal results from multiple segmentation pathways in a constrained tree search manner. In practice, we maintain a fixed number of segmentation pathways throughout the video. For each frame, multiple masks are proposed based on the existing pathways, creating various candidate branches. We then select the same fixed number of branches with higher cumulative scores as the new pathways for the next frame. After processing the final frame, the pathway with the highest cumulative score is chosen as the final segmentation result. Benefiting from its heuristic search design, SAM2Long is robust toward occlusions and object reappearances, and can effectively segment and track objects for complex long-term videos. Notably, SAM2Long achieves an average improvement of 3.0 points across all 24 head-to-head comparisons, with gains of up to 5.3 points in J&F on long-term video object segmentation benchmarks such as SA-V and LVOS. The code is released at https://github.com/Mark12Ding/SAM2Long.","sentences":["The Segment Anything Model 2 (SAM 2) has emerged as a powerful foundation model for object segmentation in both images and videos, paving the way for various downstream video applications.","The crucial design of SAM 2 for video segmentation is its memory module, which prompts object-aware memories from previous frames for current frame prediction.","However, its greedy-selection memory design suffers from the \"error accumulation\" problem, where an errored or missed mask will cascade and influence the segmentation of the subsequent frames, which limits the performance of SAM 2 toward complex long-term videos.","To this end, we introduce SAM2Long, an improved training-free video object segmentation strategy, which considers the segmentation uncertainty within each frame and chooses the video-level optimal results from multiple segmentation pathways in a constrained tree search manner.","In practice, we maintain a fixed number of segmentation pathways throughout the video.","For each frame, multiple masks are proposed based on the existing pathways, creating various candidate branches.","We then select the same fixed number of branches with higher cumulative scores as the new pathways for the next frame.","After processing the final frame, the pathway with the highest cumulative score is chosen as the final segmentation result.","Benefiting from its heuristic search design, SAM2Long is robust toward occlusions and object reappearances, and can effectively segment and track objects for complex long-term videos.","Notably, SAM2Long achieves an average improvement of 3.0 points across all 24 head-to-head comparisons, with gains of up to 5.3 points in J&F on long-term video object segmentation benchmarks such as SA-V and LVOS.","The code is released at https://github.com/Mark12Ding/SAM2Long."],"url":"http://arxiv.org/abs/2410.16268v1"}
{"created":"2024-10-21 17:59:11","title":"xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs","abstract":"We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for videos, particularly designed to efficiently capture temporal information over multiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in addition to the conventional visual tokenizer, which maps a sequence of tokens over multiple frames into a compact set of visual tokens. This enables BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32 vs. 4608 tokens). We explore different types of temporal encoders, including learnable spatio-temporal pooling as well as sequential models like Token Turing Machines. We experimentally confirm that BLIP-3-Video obtains video question-answering accuracies comparable to much larger state-of-the-art models (e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using fewer visual tokens. The project website is at https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html","sentences":["We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for videos, particularly designed to efficiently capture temporal information over multiple frames.","BLIP-3-Video takes advantage of the 'temporal encoder' in addition to the conventional visual tokenizer, which maps a sequence of tokens over multiple frames into a compact set of visual tokens.","This enables BLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32 vs. 4608 tokens).","We explore different types of temporal encoders, including learnable spatio-temporal pooling as well as sequential models like Token Turing Machines.","We experimentally confirm that BLIP-3-Video obtains video question-answering accuracies comparable to much larger state-of-the-art models (e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using fewer visual tokens.","The project website is at https://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html"],"url":"http://arxiv.org/abs/2410.16267v1"}
{"created":"2024-10-21 17:59:09","title":"3DGS-Enhancer: Enhancing Unbounded 3D Gaussian Splatting with View-consistent 2D Diffusion Priors","abstract":"Novel-view synthesis aims to generate novel views of a scene from multiple input images or videos, and recent advancements like 3D Gaussian splatting (3DGS) have achieved notable success in producing photorealistic renderings with efficient pipelines. However, generating high-quality novel views under challenging settings, such as sparse input views, remains difficult due to insufficient information in under-sampled areas, often resulting in noticeable artifacts. This paper presents 3DGS-Enhancer, a novel pipeline for enhancing the representation quality of 3DGS representations. We leverage 2D video diffusion priors to address the challenging 3D view consistency problem, reformulating it as achieving temporal consistency within a video generation process. 3DGS-Enhancer restores view-consistent latent features of rendered novel views and integrates them with the input views through a spatial-temporal decoder. The enhanced views are then used to fine-tune the initial 3DGS model, significantly improving its rendering performance. Extensive experiments on large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields superior reconstruction performance and high-fidelity rendering results compared to state-of-the-art methods. The project webpage is https://xiliu8006.github.io/3DGS-Enhancer-project .","sentences":["Novel-view synthesis aims to generate novel views of a scene from multiple input images or videos, and recent advancements like 3D Gaussian splatting (3DGS) have achieved notable success in producing photorealistic renderings with efficient pipelines.","However, generating high-quality novel views under challenging settings, such as sparse input views, remains difficult due to insufficient information in under-sampled areas, often resulting in noticeable artifacts.","This paper presents 3DGS-Enhancer, a novel pipeline for enhancing the representation quality of 3DGS representations.","We leverage 2D video diffusion priors to address the challenging 3D view consistency problem, reformulating it as achieving temporal consistency within a video generation process.","3DGS-Enhancer restores view-consistent latent features of rendered novel views and integrates them with the input views through a spatial-temporal decoder.","The enhanced views are then used to fine-tune the initial 3DGS model, significantly improving its rendering performance.","Extensive experiments on large-scale datasets of unbounded scenes demonstrate that 3DGS-Enhancer yields superior reconstruction performance and high-fidelity rendering results compared to state-of-the-art methods.","The project webpage is https://xiliu8006.github.io/3DGS-Enhancer-project ."],"url":"http://arxiv.org/abs/2410.16266v1"}
{"created":"2024-10-21 17:58:20","title":"Mini-InternVL: A Flexible-Transfer Pocket Multimodal Model with 5% Parameters and 90% Performance","abstract":"Multimodal large language models (MLLMs) have demonstrated impressive performance in vision-language tasks across a broad spectrum of domains. However, the large model scale and associated high computational costs pose significant challenges for training and deploying MLLMs on consumer-grade GPUs or edge devices, thereby hindering their widespread application. In this work, we introduce Mini-InternVL, a series of MLLMs with parameters ranging from 1B to 4B, which achieves 90% of the performance with only 5% of the parameters. This significant improvement in efficiency and effectiveness makes our models more accessible and applicable in various real-world scenarios. To further promote the adoption of our models, we develop a unified adaptation framework for Mini-InternVL, which enables our models to transfer and outperform specialized models in downstream tasks, including autonomous driving, medical images, and remote sensing. We believe that our study can provide valuable insights and resources to advance the development of efficient and effective MLLMs. Code is available at https://github.com/OpenGVLab/InternVL.","sentences":["Multimodal large language models (MLLMs) have demonstrated impressive performance in vision-language tasks across a broad spectrum of domains.","However, the large model scale and associated high computational costs pose significant challenges for training and deploying MLLMs on consumer-grade GPUs or edge devices, thereby hindering their widespread application.","In this work, we introduce Mini-InternVL, a series of MLLMs with parameters ranging from 1B to 4B, which achieves 90% of the performance with only 5% of the parameters.","This significant improvement in efficiency and effectiveness makes our models more accessible and applicable in various real-world scenarios.","To further promote the adoption of our models, we develop a unified adaptation framework for Mini-InternVL, which enables our models to transfer and outperform specialized models in downstream tasks, including autonomous driving, medical images, and remote sensing.","We believe that our study can provide valuable insights and resources to advance the development of efficient and effective MLLMs.","Code is available at https://github.com/OpenGVLab/InternVL."],"url":"http://arxiv.org/abs/2410.16261v1"}
{"created":"2024-10-21 17:57:50","title":"Agent-to-Sim: Learning Interactive Behavior Models from Casual Longitudinal Videos","abstract":"We present Agent-to-Sim (ATS), a framework for learning interactive behavior models of 3D agents from casual longitudinal video collections. Different from prior works that rely on marker-based tracking and multiview cameras, ATS learns natural behaviors of animal and human agents non-invasively through video observations recorded over a long time-span (e.g., a month) in a single environment. Modeling 3D behavior of an agent requires persistent 3D tracking (e.g., knowing which point corresponds to which) over a long time period. To obtain such data, we develop a coarse-to-fine registration method that tracks the agent and the camera over time through a canonical 3D space, resulting in a complete and persistent spacetime 4D representation. We then train a generative model of agent behaviors using paired data of perception and motion of an agent queried from the 4D reconstruction. ATS enables real-to-sim transfer from video recordings of an agent to an interactive behavior simulator. We demonstrate results on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos captured by a smartphone.","sentences":["We present Agent-to-Sim (ATS), a framework for learning interactive behavior models of 3D agents from casual longitudinal video collections.","Different from prior works that rely on marker-based tracking and multiview cameras, ATS learns natural behaviors of animal and human agents non-invasively through video observations recorded over a long time-span (e.g., a month) in a single environment.","Modeling 3D behavior of an agent requires persistent 3D tracking (e.g., knowing which point corresponds to which) over a long time period.","To obtain such data, we develop a coarse-to-fine registration method that tracks the agent and the camera over time through a canonical 3D space, resulting in a complete and persistent spacetime 4D representation.","We then train a generative model of agent behaviors using paired data of perception and motion of an agent queried from the 4D reconstruction.","ATS enables real-to-sim transfer from video recordings of an agent to an interactive behavior simulator.","We demonstrate results on pets (e.g., cat, dog, bunny) and human given monocular RGBD videos captured by a smartphone."],"url":"http://arxiv.org/abs/2410.16259v1"}
{"created":"2024-10-21 17:57:04","title":"Elucidating the design space of language models for image generation","abstract":"The success of autoregressive (AR) language models in text generation has inspired the computer vision community to adopt Large Language Models (LLMs) for image generation. However, considering the essential differences between text and image modalities, the design space of language models for image generation remains underexplored. We observe that image tokens exhibit greater randomness compared to text tokens, which presents challenges when training with token prediction. Nevertheless, AR models demonstrate their potential by effectively learning patterns even from a seemingly suboptimal optimization problem. Our analysis also reveals that while all models successfully grasp the importance of local information in image generation, smaller models struggle to capture the global context. In contrast, larger models showcase improved capabilities in this area, helping to explain the performance gains achieved when scaling up model size. We further elucidate the design space of language models for vision generation, including tokenizer choice, model choice, model scalability, vocabulary design, and sampling strategy through extensive comparative experiments. Our work is the first to analyze the optimization behavior of language models in vision generation, and we believe it can inspire more effective designs when applying LMs to other domains. Finally, our elucidated language model for image generation, termed as ELM, achieves state-of-the-art performance on the ImageNet 256*256 benchmark. The code is available at https://github.com/Pepperlll/LMforImageGeneration.git.","sentences":["The success of autoregressive (AR) language models in text generation has inspired the computer vision community to adopt Large Language Models (LLMs) for image generation.","However, considering the essential differences between text and image modalities, the design space of language models for image generation remains underexplored.","We observe that image tokens exhibit greater randomness compared to text tokens, which presents challenges when training with token prediction.","Nevertheless, AR models demonstrate their potential by effectively learning patterns even from a seemingly suboptimal optimization problem.","Our analysis also reveals that while all models successfully grasp the importance of local information in image generation, smaller models struggle to capture the global context.","In contrast, larger models showcase improved capabilities in this area, helping to explain the performance gains achieved when scaling up model size.","We further elucidate the design space of language models for vision generation, including tokenizer choice, model choice, model scalability, vocabulary design, and sampling strategy through extensive comparative experiments.","Our work is the first to analyze the optimization behavior of language models in vision generation, and we believe it can inspire more effective designs when applying LMs to other domains.","Finally, our elucidated language model for image generation, termed as ELM, achieves state-of-the-art performance on the ImageNet 256*256 benchmark.","The code is available at https://github.com/Pepperlll/LMforImageGeneration.git."],"url":"http://arxiv.org/abs/2410.16257v1"}
{"created":"2024-10-21 17:56:51","title":"CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution","abstract":"Efficient and accurate evaluation is crucial for the continuous improvement of large language models (LLMs). Among various assessment methods, subjective evaluation has garnered significant attention due to its superior alignment with real-world usage scenarios and human preferences. However, human-based evaluations are costly and lack reproducibility, making precise automated evaluators (judgers) vital in this process. In this report, we introduce \\textbf{CompassJudger-1}, the first open-source \\textbf{all-in-one} judge LLM. CompassJudger-1 is a general-purpose LLM that demonstrates remarkable versatility. It is capable of: 1. Performing unitary scoring and two-model comparisons as a reward model; 2. Conducting evaluations according to specified formats; 3. Generating critiques; 4. Executing diverse tasks like a general LLM. To assess the evaluation capabilities of different judge models under a unified setting, we have also established \\textbf{JudgerBench}, a new benchmark that encompasses various subjective evaluation tasks and covers a wide range of topics. CompassJudger-1 offers a comprehensive solution for various evaluation tasks while maintaining the flexibility to adapt to diverse requirements. Both CompassJudger and JudgerBench are released and available to the research community athttps://github.com/open-compass/CompassJudger. We believe that by open-sourcing these tools, we can foster collaboration and accelerate progress in LLM evaluation methodologies.","sentences":["Efficient and accurate evaluation is crucial for the continuous improvement of large language models (LLMs).","Among various assessment methods, subjective evaluation has garnered significant attention due to its superior alignment with real-world usage scenarios and human preferences.","However, human-based evaluations are costly and lack reproducibility, making precise automated evaluators (judgers) vital in this process.","In this report, we introduce \\textbf{CompassJudger-1}, the first open-source \\textbf{all-in-one} judge LLM. CompassJudger-1 is a general-purpose LLM that demonstrates remarkable versatility.","It is capable of: 1.","Performing unitary scoring and two-model comparisons as a reward model; 2.","Conducting evaluations according to specified formats; 3. Generating critiques; 4.","Executing diverse tasks like a general LLM.","To assess the evaluation capabilities of different judge models under a unified setting, we have also established \\textbf{JudgerBench}, a new benchmark that encompasses various subjective evaluation tasks and covers a wide range of topics.","CompassJudger-1 offers a comprehensive solution for various evaluation tasks while maintaining the flexibility to adapt to diverse requirements.","Both CompassJudger and JudgerBench are released and available to the research community athttps://github.com/open-compass/CompassJudger.","We believe that by open-sourcing these tools, we can foster collaboration and accelerate progress in LLM evaluation methodologies."],"url":"http://arxiv.org/abs/2410.16256v1"}
{"created":"2024-10-21 17:56:47","title":"Revisiting Deep Feature Reconstruction for Logical and Structural Industrial Anomaly Detection","abstract":"Industrial anomaly detection is crucial for quality control and predictive maintenance, but it presents challenges due to limited training data, diverse anomaly types, and external factors that alter object appearances. Existing methods commonly detect structural anomalies, such as dents and scratches, by leveraging multi-scale features from image patches extracted through deep pre-trained networks. However, significant memory and computational demands often limit their practical application. Additionally, detecting logical anomalies-such as images with missing or excess elements-requires an understanding of spatial relationships that traditional patch-based methods fail to capture. In this work, we address these limitations by focusing on Deep Feature Reconstruction (DFR), a memory- and compute-efficient approach for detecting structural anomalies. We further enhance DFR into a unified framework, called ULSAD, which is capable of detecting both structural and logical anomalies. Specifically, we refine the DFR training objective to improve performance in structural anomaly detection, while introducing an attention-based loss mechanism using a global autoencoder-like network to handle logical anomaly detection. Our empirical evaluation across five benchmark datasets demonstrates the performance of ULSAD in detecting and localizing both structural and logical anomalies, outperforming eight state-of-the-art methods. An extensive ablation study further highlights the contribution of each component to the overall performance improvement. Our code is available at https://github.com/sukanyapatra1997/ULSAD-2024.git","sentences":["Industrial anomaly detection is crucial for quality control and predictive maintenance, but it presents challenges due to limited training data, diverse anomaly types, and external factors that alter object appearances.","Existing methods commonly detect structural anomalies, such as dents and scratches, by leveraging multi-scale features from image patches extracted through deep pre-trained networks.","However, significant memory and computational demands often limit their practical application.","Additionally, detecting logical anomalies-such as images with missing or excess elements-requires an understanding of spatial relationships that traditional patch-based methods fail to capture.","In this work, we address these limitations by focusing on Deep Feature Reconstruction (DFR), a memory- and compute-efficient approach for detecting structural anomalies.","We further enhance DFR into a unified framework, called ULSAD, which is capable of detecting both structural and logical anomalies.","Specifically, we refine the DFR training objective to improve performance in structural anomaly detection, while introducing an attention-based loss mechanism using a global autoencoder-like network to handle logical anomaly detection.","Our empirical evaluation across five benchmark datasets demonstrates the performance of ULSAD in detecting and localizing both structural and logical anomalies, outperforming eight state-of-the-art methods.","An extensive ablation study further highlights the contribution of each component to the overall performance improvement.","Our code is available at https://github.com/sukanyapatra1997/ULSAD-2024.git"],"url":"http://arxiv.org/abs/2410.16255v1"}
{"created":"2024-10-21 17:56:09","title":"Distribution Learning with Valid Outputs Beyond the Worst-Case","abstract":"Generative models at times produce \"invalid\" outputs, such as images with generation artifacts and unnatural sounds. Validity-constrained distribution learning attempts to address this problem by requiring that the learned distribution have a provably small fraction of its mass in invalid parts of space -- something which standard loss minimization does not always ensure. To this end, a learner in this model can guide the learning via \"validity queries\", which allow it to ascertain the validity of individual examples. Prior work on this problem takes a worst-case stance, showing that proper learning requires an exponential number of validity queries, and demonstrating an improper algorithm which -- while generating guarantees in a wide-range of settings -- makes an atypical polynomial number of validity queries. In this work, we take a first step towards characterizing regimes where guaranteeing validity is easier than in the worst-case. We show that when the data distribution lies in the model class and the log-loss is minimized, the number of samples required to ensure validity has a weak dependence on the validity requirement. Additionally, we show that when the validity region belongs to a VC-class, a limited number of validity queries are often sufficient.","sentences":["Generative models at times produce \"invalid\" outputs, such as images with generation artifacts and unnatural sounds.","Validity-constrained distribution learning attempts to address this problem by requiring that the learned distribution have a provably small fraction of its mass in invalid parts of space -- something which standard loss minimization does not always ensure.","To this end, a learner in this model can guide the learning via \"validity queries\", which allow it to ascertain the validity of individual examples.","Prior work on this problem takes a worst-case stance, showing that proper learning requires an exponential number of validity queries, and demonstrating an improper algorithm which -- while generating guarantees in a wide-range of settings -- makes an atypical polynomial number of validity queries.","In this work, we take a first step towards characterizing regimes where guaranteeing validity is easier than in the worst-case.","We show that when the data distribution lies in the model class and the log-loss is minimized, the number of samples required to ensure validity has a weak dependence on the validity requirement.","Additionally, we show that when the validity region belongs to a VC-class, a limited number of validity queries are often sufficient."],"url":"http://arxiv.org/abs/2410.16253v1"}
{"created":"2024-10-21 17:55:54","title":"Can Knowledge Editing Really Correct Hallucinations?","abstract":"Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks. Meanwhile, knowledge editing has been developed as a new popular paradigm to correct the erroneous factual knowledge encoded in LLMs with the advantage of avoiding retraining from scratch. However, one common issue of existing evaluation datasets for knowledge editing is that they do not ensure LLMs actually generate hallucinated answers to the evaluation questions before editing. When LLMs are evaluated on such datasets after being edited by different techniques, it is hard to directly adopt the performance to assess the effectiveness of different knowledge editing methods in correcting hallucinations. Thus, the fundamental question remains insufficiently validated: Can knowledge editing really correct hallucinations in LLMs? We proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations. First, we rigorously construct a massive hallucination dataset with 9 domains, 26 topics and more than 6,000 hallucinations. Then, we assess the performance of knowledge editing methods in a holistic way on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness. Through HalluEditBench, we have provided new insights into the potentials and limitations of different knowledge editing methods in correcting hallucinations, which could inspire future improvements and facilitate the progress in the field of knowledge editing.","sentences":["Large Language Models (LLMs) suffer from hallucinations, referring to the non-factual information in generated content, despite their superior capacities across tasks.","Meanwhile, knowledge editing has been developed as a new popular paradigm to correct the erroneous factual knowledge encoded in LLMs with the advantage of avoiding retraining from scratch.","However, one common issue of existing evaluation datasets for knowledge editing is that they do not ensure LLMs actually generate hallucinated answers to the evaluation questions before editing.","When LLMs are evaluated on such datasets after being edited by different techniques, it is hard to directly adopt the performance to assess the effectiveness of different knowledge editing methods in correcting hallucinations.","Thus, the fundamental question remains insufficiently validated: Can knowledge editing really correct hallucinations in LLMs?","We proposed HalluEditBench to holistically benchmark knowledge editing methods in correcting real-world hallucinations.","First, we rigorously construct a massive hallucination dataset with 9 domains, 26 topics and more than 6,000 hallucinations.","Then, we assess the performance of knowledge editing methods in a holistic way on five dimensions including Efficacy, Generalization, Portability, Locality, and Robustness.","Through HalluEditBench, we have provided new insights into the potentials and limitations of different knowledge editing methods in correcting hallucinations, which could inspire future improvements and facilitate the progress in the field of knowledge editing."],"url":"http://arxiv.org/abs/2410.16251v1"}
{"created":"2024-10-21 17:52:01","title":"Implicit Regularization for Tubal Tensor Factorizations via Gradient Descent","abstract":"We provide a rigorous analysis of implicit regularization in an overparametrized tensor factorization problem beyond the lazy training regime. For matrix factorization problems, this phenomenon has been studied in a number of works. A particular challenge has been to design universal initialization strategies which provably lead to implicit regularization in gradient-descent methods. At the same time, it has been argued by Cohen et. al. 2016 that more general classes of neural networks can be captured by considering tensor factorizations. However, in the tensor case, implicit regularization has only been rigorously established for gradient flow or in the lazy training regime. In this paper, we prove the first tensor result of its kind for gradient descent rather than gradient flow. We focus on the tubal tensor product and the associated notion of low tubal rank, encouraged by the relevance of this model for image data. We establish that gradient descent in an overparametrized tensor factorization model with a small random initialization exhibits an implicit bias towards solutions of low tubal rank. Our theoretical findings are illustrated in an extensive set of numerical simulations show-casing the dynamics predicted by our theory as well as the crucial role of using a small random initialization.","sentences":["We provide a rigorous analysis of implicit regularization in an overparametrized tensor factorization problem beyond the lazy training regime.","For matrix factorization problems, this phenomenon has been studied in a number of works.","A particular challenge has been to design universal initialization strategies which provably lead to implicit regularization in gradient-descent methods.","At the same time, it has been argued by Cohen et.","al. 2016 that more general classes of neural networks can be captured by considering tensor factorizations.","However, in the tensor case, implicit regularization has only been rigorously established for gradient flow or in the lazy training regime.","In this paper, we prove the first tensor result of its kind for gradient descent rather than gradient flow.","We focus on the tubal tensor product and the associated notion of low tubal rank, encouraged by the relevance of this model for image data.","We establish that gradient descent in an overparametrized tensor factorization model with a small random initialization exhibits an implicit bias towards solutions of low tubal rank.","Our theoretical findings are illustrated in an extensive set of numerical simulations show-casing the dynamics predicted by our theory as well as the crucial role of using a small random initialization."],"url":"http://arxiv.org/abs/2410.16247v1"}
{"created":"2024-10-21 17:51:41","title":"Analyzing Context Contributions in LLM-based Machine Translation","abstract":"Large language models (LLMs) have achieved state-of-the-art performance in machine translation (MT) and demonstrated the ability to leverage in-context learning through few-shot examples. However, the mechanisms by which LLMs use different parts of the input context remain largely unexplored. In this work, we provide a comprehensive analysis of context utilization in MT, studying how LLMs use various context parts, such as few-shot examples and the source text, when generating translations. We highlight several key findings: (1) the source part of few-shot examples appears to contribute more than its corresponding targets, irrespective of translation direction; (2) finetuning LLMs with parallel data alters the contribution patterns of different context parts; and (3) there is a positional bias where earlier few-shot examples have higher contributions to the translated sequence. Finally, we demonstrate that inspecting anomalous context contributions can potentially uncover pathological translations, such as hallucinations. Our findings shed light on the internal workings of LLM-based MT which go beyond those known for standard encoder-decoder MT models.","sentences":["Large language models (LLMs) have achieved state-of-the-art performance in machine translation (MT) and demonstrated the ability to leverage in-context learning through few-shot examples.","However, the mechanisms by which LLMs use different parts of the input context remain largely unexplored.","In this work, we provide a comprehensive analysis of context utilization in MT, studying how LLMs use various context parts, such as few-shot examples and the source text, when generating translations.","We highlight several key findings: (1) the source part of few-shot examples appears to contribute more than its corresponding targets, irrespective of translation direction; (2) finetuning LLMs with parallel data alters the contribution patterns of different context parts; and (3) there is a positional bias where earlier few-shot examples have higher contributions to the translated sequence.","Finally, we demonstrate that inspecting anomalous context contributions can potentially uncover pathological translations, such as hallucinations.","Our findings shed light on the internal workings of LLM-based MT which go beyond those known for standard encoder-decoder MT models."],"url":"http://arxiv.org/abs/2410.16246v1"}
{"created":"2024-10-21 17:42:41","title":"MoRE: Multi-Modal Contrastive Pre-training with Transformers on X-Rays, ECGs, and Diagnostic Report","abstract":"In this paper, we introduce a novel Multi-Modal Contrastive Pre-training Framework that synergistically combines X-rays, electrocardiograms (ECGs), and radiology/cardiology reports. Our approach leverages transformers to encode these diverse modalities into a unified representation space, aiming to enhance diagnostic accuracy and facilitate comprehensive patient assessments. We utilize LoRA-Peft to significantly reduce trainable parameters in the LLM and incorporate recent linear attention dropping strategy in the Vision Transformer(ViT) for smoother attention. Furthermore, we provide novel multimodal attention explanations and retrieval for our model. To the best of our knowledge, we are the first to propose an integrated model that combines X-ray, ECG, and Radiology/Cardiology Report with this approach. By utilizing contrastive loss, MoRE effectively aligns modality-specific features into a coherent embedding, which supports various downstream tasks such as zero-shot classification and multimodal retrieval. Employing our proposed methodology, we achieve state-of-the-art (SOTA) on the Mimic-IV, CheXpert, Edema Severity, and PtbXl downstream datasets, surpassing existing multimodal approaches. Our proposed framework shows significant improvements in capturing intricate inter-modal relationships and its robustness in medical diagnosis that establishes a framework for future research in multimodal learning in the healthcare sector.","sentences":["In this paper, we introduce a novel Multi-Modal Contrastive Pre-training Framework that synergistically combines X-rays, electrocardiograms (ECGs), and radiology/cardiology reports.","Our approach leverages transformers to encode these diverse modalities into a unified representation space, aiming to enhance diagnostic accuracy and facilitate comprehensive patient assessments.","We utilize LoRA-Peft to significantly reduce trainable parameters in the LLM and incorporate recent linear attention dropping strategy in the Vision Transformer(ViT) for smoother attention.","Furthermore, we provide novel multimodal attention explanations and retrieval for our model.","To the best of our knowledge, we are the first to propose an integrated model that combines X-ray, ECG, and Radiology/Cardiology Report with this approach.","By utilizing contrastive loss, MoRE effectively aligns modality-specific features into a coherent embedding, which supports various downstream tasks such as zero-shot classification and multimodal retrieval.","Employing our proposed methodology, we achieve state-of-the-art (SOTA) on the Mimic-IV, CheXpert, Edema Severity, and PtbXl downstream datasets, surpassing existing multimodal approaches.","Our proposed framework shows significant improvements in capturing intricate inter-modal relationships and its robustness in medical diagnosis that establishes a framework for future research in multimodal learning in the healthcare sector."],"url":"http://arxiv.org/abs/2410.16239v1"}
{"created":"2024-10-21 17:41:42","title":"IBGP: Imperfect Byzantine Generals Problem for Zero-Shot Robustness in Communicative Multi-Agent Systems","abstract":"As large language model (LLM) agents increasingly integrate into our infrastructure, their robust coordination and message synchronization become vital. The Byzantine Generals Problem (BGP) is a critical model for constructing resilient multi-agent systems (MAS) under adversarial attacks. It describes a scenario where malicious agents with unknown identities exist in the system-situations that, in our context, could result from LLM agents' hallucinations or external attacks. In BGP, the objective of the entire system is to reach a consensus on the action to be taken. Traditional BGP requires global consensus among all agents; however, in practical scenarios, global consensus is not always necessary and can even be inefficient. Therefore, there is a pressing need to explore a refined version of BGP that aligns with the local coordination patterns observed in MAS. We refer to this refined version as Imperfect BGP (IBGP) in our research, aiming to address this discrepancy. To tackle this issue, we propose a framework that leverages consensus protocols within general MAS settings, providing provable resilience against communication attacks and adaptability to changing environments, as validated by empirical results. Additionally, we present a case study in a sensor network environment to illustrate the practical application of our protocol.","sentences":["As large language model (LLM) agents increasingly integrate into our infrastructure, their robust coordination and message synchronization become vital.","The Byzantine Generals Problem (BGP) is a critical model for constructing resilient multi-agent systems (MAS) under adversarial attacks.","It describes a scenario where malicious agents with unknown identities exist in the system-situations that, in our context, could result from LLM agents' hallucinations or external attacks.","In BGP, the objective of the entire system is to reach a consensus on the action to be taken.","Traditional BGP requires global consensus among all agents; however, in practical scenarios, global consensus is not always necessary and can even be inefficient.","Therefore, there is a pressing need to explore a refined version of BGP that aligns with the local coordination patterns observed in MAS.","We refer to this refined version as Imperfect BGP (IBGP) in our research, aiming to address this discrepancy.","To tackle this issue, we propose a framework that leverages consensus protocols within general MAS settings, providing provable resilience against communication attacks and adaptability to changing environments, as validated by empirical results.","Additionally, we present a case study in a sensor network environment to illustrate the practical application of our protocol."],"url":"http://arxiv.org/abs/2410.16237v1"}
{"created":"2024-10-21 17:41:28","title":"LLaVA-KD: A Framework of Distilling Multimodal Large Language Models","abstract":"The success of Large Language Models (LLM) has led researchers to explore Multimodal Large Language Models (MLLM) for unified visual and linguistic understanding. However, the increasing model size and computational complexity of MLLM limit their use in resource-constrained environments. Small-scale MLLM (s-MLLM) aims to retain the capabilities of the large-scale model (l-MLLM) while reducing computational demands, but resulting in a significant decline in performance. To address the aforementioned issues, we propose a novel LLaVA-KD framework to transfer knowledge from l-MLLM to s-MLLM. Specifically, we introduce Multimodal Distillation (MDist) to minimize the divergence between the visual-textual output distributions of l-MLLM and s-MLLM, and Relation Distillation (RDist) to transfer l-MLLM's ability to model correlations between visual features. Additionally, we propose a three-stage training scheme to fully exploit the potential of s-MLLM: 1) Distilled Pre-Training to align visual-textual representations, 2) Supervised Fine-Tuning to equip the model with multimodal understanding, and 3) Distilled Fine-Tuning to further transfer l-MLLM capabilities. Our approach significantly improves performance without altering the small model's architecture. Extensive experiments and ablation studies validate the effectiveness of each proposed component. Code will be available at https://github.com/caiyuxuan1120/LLaVA-KD.","sentences":["The success of Large Language Models (LLM) has led researchers to explore Multimodal Large Language Models (MLLM) for unified visual and linguistic understanding.","However, the increasing model size and computational complexity of MLLM limit their use in resource-constrained environments.","Small-scale MLLM (s-MLLM) aims to retain the capabilities of the large-scale model (l-MLLM) while reducing computational demands, but resulting in a significant decline in performance.","To address the aforementioned issues, we propose a novel LLaVA-KD framework to transfer knowledge from l-MLLM to s-MLLM.","Specifically, we introduce Multimodal Distillation (MDist) to minimize the divergence between the visual-textual output distributions of l-MLLM and s-MLLM, and Relation Distillation (RDist) to transfer l-MLLM's ability to model correlations between visual features.","Additionally, we propose a three-stage training scheme to fully exploit the potential of s-MLLM: 1) Distilled Pre-Training to align visual-textual representations, 2) Supervised Fine-Tuning to equip the model with multimodal understanding, and 3) Distilled Fine-Tuning to further transfer l-MLLM capabilities.","Our approach significantly improves performance without altering the small model's architecture.","Extensive experiments and ablation studies validate the effectiveness of each proposed component.","Code will be available at https://github.com/caiyuxuan1120/LLaVA-KD."],"url":"http://arxiv.org/abs/2410.16236v1"}
{"created":"2024-10-21 17:41:11","title":"ToW: Thoughts of Words Improve Reasoning in Large Language Models","abstract":"We introduce thoughts of words (ToW), a novel training-time data-augmentation method for next-word prediction. ToW views next-word prediction as a core reasoning task and injects fine-grained thoughts explaining what the next word should be and how it is related to the previous contexts in pre-training texts. Our formulation addresses two fundamental drawbacks of existing next-word prediction learning schemes: they induce factual hallucination and are inefficient for models to learn the implicit reasoning processes in raw texts. While there are many ways to acquire such thoughts of words, we explore the first step of acquiring ToW annotations through distilling from larger models. After continual pre-training with only 70K ToW annotations, we effectively improve models' reasoning performances by 7% to 9% on average and reduce model hallucination by up to 10%. At the same time, ToW is entirely agnostic to tasks and applications, introducing no additional biases on labels or semantics.","sentences":["We introduce thoughts of words (ToW), a novel training-time data-augmentation method for next-word prediction.","ToW views next-word prediction as a core reasoning task and injects fine-grained thoughts explaining what the next word should be and how it is related to the previous contexts in pre-training texts.","Our formulation addresses two fundamental drawbacks of existing next-word prediction learning schemes: they induce factual hallucination and are inefficient for models to learn the implicit reasoning processes in raw texts.","While there are many ways to acquire such thoughts of words, we explore the first step of acquiring ToW annotations through distilling from larger models.","After continual pre-training with only 70K ToW annotations, we effectively improve models' reasoning performances by 7% to 9% on average and reduce model hallucination by up to 10%.","At the same time, ToW is entirely agnostic to tasks and applications, introducing no additional biases on labels or semantics."],"url":"http://arxiv.org/abs/2410.16235v1"}
{"created":"2024-10-21 17:39:49","title":"Sketch2Code: Evaluating Vision-Language Models for Interactive Web Design Prototyping","abstract":"Sketches are a natural and accessible medium for UI designers to conceptualize early-stage ideas. However, existing research on UI/UX automation often requires high-fidelity inputs like Figma designs or detailed screenshots, limiting accessibility and impeding efficient design iteration. To bridge this gap, we introduce Sketch2Code, a benchmark that evaluates state-of-the-art Vision Language Models (VLMs) on automating the conversion of rudimentary sketches into webpage prototypes. Beyond end-to-end benchmarking, Sketch2Code supports interactive agent evaluation that mimics real-world design workflows, where a VLM-based agent iteratively refines its generations by communicating with a simulated user, either passively receiving feedback instructions or proactively asking clarification questions. We comprehensively analyze ten commercial and open-source models, showing that Sketch2Code is challenging for existing VLMs; even the most capable models struggle to accurately interpret sketches and formulate effective questions that lead to steady improvement. Nevertheless, a user study with UI/UX experts reveals a significant preference for proactive question-asking over passive feedback reception, highlighting the need to develop more effective paradigms for multi-turn conversational agents.","sentences":["Sketches are a natural and accessible medium for UI designers to conceptualize early-stage ideas.","However, existing research on UI/UX automation often requires high-fidelity inputs like Figma designs or detailed screenshots, limiting accessibility and impeding efficient design iteration.","To bridge this gap, we introduce Sketch2Code, a benchmark that evaluates state-of-the-art Vision Language Models (VLMs) on automating the conversion of rudimentary sketches into webpage prototypes.","Beyond end-to-end benchmarking, Sketch2Code supports interactive agent evaluation that mimics real-world design workflows, where a VLM-based agent iteratively refines its generations by communicating with a simulated user, either passively receiving feedback instructions or proactively asking clarification questions.","We comprehensively analyze ten commercial and open-source models, showing that Sketch2Code is challenging for existing VLMs; even the most capable models struggle to accurately interpret sketches and formulate effective questions that lead to steady improvement.","Nevertheless, a user study with UI/UX experts reveals a significant preference for proactive question-asking over passive feedback reception, highlighting the need to develop more effective paradigms for multi-turn conversational agents."],"url":"http://arxiv.org/abs/2410.16232v1"}
{"created":"2024-10-21 17:34:39","title":"Building A Coding Assistant via the Retrieval-Augmented Language Model","abstract":"Pretrained language models have shown strong effectiveness in code-related tasks, such as code retrieval, code generation, code summarization, and code completion tasks. In this paper, we propose COde assistaNt viA retrieval-augmeNted language model (CONAN), which aims to build a code assistant by mimicking the knowledge-seeking behaviors of humans during coding. Specifically, it consists of a code structure aware retriever (CONAN-R) and a dual-view code representation-based retrieval-augmented generation model (CONAN-G). CONAN-R pretrains CodeT5 using Code-Documentation Alignment and Masked Entity Prediction tasks to make language models code structure-aware and learn effective representations for code snippets and documentation. Then CONAN-G designs a dual-view code representation mechanism for implementing a retrieval-augmented code generation model. CONAN-G regards the code documentation descriptions as prompts, which help language models better understand the code semantics. Our experiments show that CONAN achieves convincing performance on different code generation tasks and significantly outperforms previous retrieval augmented code generation models. Our further analyses show that CONAN learns tailored representations for both code snippets and documentation by aligning code-documentation data pairs and capturing structural semantics by masking and predicting entities in the code data. Additionally, the retrieved code snippets and documentation provide necessary information from both program language and natural language to assist the code generation process. CONAN can also be used as an assistant for Large Language Models (LLMs), providing LLMs with external knowledge in shorter code document lengths to improve their effectiveness on various code tasks. It shows the ability of CONAN to extract necessary information and help filter out the noise from retrieved code documents.","sentences":["Pretrained language models have shown strong effectiveness in code-related tasks, such as code retrieval, code generation, code summarization, and code completion tasks.","In this paper, we propose COde assistaNt viA retrieval-augmeNted language model (CONAN), which aims to build a code assistant by mimicking the knowledge-seeking behaviors of humans during coding.","Specifically, it consists of a code structure aware retriever (CONAN-R) and a dual-view code representation-based retrieval-augmented generation model (CONAN-G).","CONAN-R pretrains CodeT5 using Code-Documentation Alignment and Masked Entity Prediction tasks to make language models code structure-aware and learn effective representations for code snippets and documentation.","Then CONAN-G designs a dual-view code representation mechanism for implementing a retrieval-augmented code generation model.","CONAN-G regards the code documentation descriptions as prompts, which help language models better understand the code semantics.","Our experiments show that CONAN achieves convincing performance on different code generation tasks and significantly outperforms previous retrieval augmented code generation models.","Our further analyses show that CONAN learns tailored representations for both code snippets and documentation by aligning code-documentation data pairs and capturing structural semantics by masking and predicting entities in the code data.","Additionally, the retrieved code snippets and documentation provide necessary information from both program language and natural language to assist the code generation process.","CONAN can also be used as an assistant for Large Language Models (LLMs), providing LLMs with external knowledge in shorter code document lengths to improve their effectiveness on various code tasks.","It shows the ability of CONAN to extract necessary information and help filter out the noise from retrieved code documents."],"url":"http://arxiv.org/abs/2410.16229v1"}
{"created":"2024-10-21 17:32:36","title":"Managing Bandwidth: The Key to Cloud-Assisted Autonomous Driving","abstract":"Prevailing wisdom asserts that one cannot rely on the cloud for critical real-time control systems like self-driving cars. We argue that we can, and must. Following the trends of increasing model sizes, improvements in hardware, and evolving mobile networks, we identify an opportunity to offload parts of time-sensitive and latency-critical compute to the cloud. Doing so requires carefully allocating bandwidth to meet strict latency SLOs, while maximizing benefit to the car.","sentences":["Prevailing wisdom asserts that one cannot rely on the cloud for critical real-time control systems like self-driving cars.","We argue that we can, and must.","Following the trends of increasing model sizes, improvements in hardware, and evolving mobile networks, we identify an opportunity to offload parts of time-sensitive and latency-critical compute to the cloud.","Doing so requires carefully allocating bandwidth to meet strict latency SLOs, while maximizing benefit to the car."],"url":"http://arxiv.org/abs/2410.16227v1"}
{"created":"2024-10-21 17:27:01","title":"A Realistic Threat Model for Large Language Model Jailbreaks","abstract":"A plethora of jailbreaking attacks have been proposed to obtain harmful responses from safety-tuned LLMs. In their original settings, these methods all largely succeed in coercing the target output, but their attacks vary substantially in fluency and computational effort. In this work, we propose a unified threat model for the principled comparison of these methods. Our threat model combines constraints in perplexity, measuring how far a jailbreak deviates from natural text, and computational budget, in total FLOPs. For the former, we build an N-gram model on 1T tokens, which, in contrast to model-based perplexity, allows for an LLM-agnostic and inherently interpretable evaluation. We adapt popular attacks to this new, realistic threat model, with which we, for the first time, benchmark these attacks on equal footing. After a rigorous comparison, we not only find attack success rates against safety-tuned modern models to be lower than previously presented but also find that attacks based on discrete optimization significantly outperform recent LLM-based attacks. Being inherently interpretable, our threat model allows for a comprehensive analysis and comparison of jailbreak attacks. We find that effective attacks exploit and abuse infrequent N-grams, either selecting N-grams absent from real-world text or rare ones, e.g. specific to code datasets.","sentences":["A plethora of jailbreaking attacks have been proposed to obtain harmful responses from safety-tuned LLMs.","In their original settings, these methods all largely succeed in coercing the target output, but their attacks vary substantially in fluency and computational effort.","In this work, we propose a unified threat model for the principled comparison of these methods.","Our threat model combines constraints in perplexity, measuring how far a jailbreak deviates from natural text, and computational budget, in total FLOPs.","For the former, we build an N-gram model on 1T tokens, which, in contrast to model-based perplexity, allows for an LLM-agnostic and inherently interpretable evaluation.","We adapt popular attacks to this new, realistic threat model, with which we, for the first time, benchmark these attacks on equal footing.","After a rigorous comparison, we not only find attack success rates against safety-tuned modern models to be lower than previously presented but also find that attacks based on discrete optimization significantly outperform recent LLM-based attacks.","Being inherently interpretable, our threat model allows for a comprehensive analysis and comparison of jailbreak attacks.","We find that effective attacks exploit and abuse infrequent N-grams, either selecting N-grams absent from real-world text or rare ones, e.g. specific to code datasets."],"url":"http://arxiv.org/abs/2410.16222v1"}
{"created":"2024-10-21 17:25:32","title":"On Creating an English-Thai Code-switched Machine Translation in Medical Domain","abstract":"Machine translation (MT) in the medical domain plays a pivotal role in enhancing healthcare quality and disseminating medical knowledge. Despite advancements in English-Thai MT technology, common MT approaches often underperform in the medical field due to their inability to precisely translate medical terminologies. Our research prioritizes not merely improving translation accuracy but also maintaining medical terminology in English within the translated text through code-switched (CS) translation. We developed a method to produce CS medical translation data, fine-tuned a CS translation model with this data, and evaluated its performance against strong baselines, such as Google Neural Machine Translation (NMT) and GPT-3.5/GPT-4. Our model demonstrated competitive performance in automatic metrics and was highly favored in human preference evaluations. Our evaluation result also shows that medical professionals significantly prefer CS translations that maintain critical English terms accurately, even if it slightly compromises fluency. Our code and test set are publicly available https://github.com/preceptorai-org/NLLB_CS_EM_NLP2024.","sentences":["Machine translation (MT) in the medical domain plays a pivotal role in enhancing healthcare quality and disseminating medical knowledge.","Despite advancements in English-Thai MT technology, common MT approaches often underperform in the medical field due to their inability to precisely translate medical terminologies.","Our research prioritizes not merely improving translation accuracy but also maintaining medical terminology in English within the translated text through code-switched (CS) translation.","We developed a method to produce CS medical translation data, fine-tuned a CS translation model with this data, and evaluated its performance against strong baselines, such as Google Neural Machine Translation (NMT) and GPT-3.5/GPT-4.","Our model demonstrated competitive performance in automatic metrics and was highly favored in human preference evaluations.","Our evaluation result also shows that medical professionals significantly prefer CS translations that maintain critical English terms accurately, even if it slightly compromises fluency.","Our code and test set are publicly available https://github.com/preceptorai-org/NLLB_CS_EM_NLP2024."],"url":"http://arxiv.org/abs/2410.16221v1"}
{"created":"2024-10-21 17:16:13","title":"Pre-training Distillation for Large Language Models: A Design Space Exploration","abstract":"Knowledge distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. Previous work applying KD in the field of large language models (LLMs) typically focused on the post-training phase, where the student LLM learns directly from instructions and corresponding responses generated by the teacher model. In this paper, we extend KD to the pre-training phase of LLMs, named pre-training distillation (PD). We first conduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a 1.9B parameter student LLM, validating the effectiveness of PD. Considering the key impact factors of distillation, we systematically explore the design space of pre-training distillation across four aspects: logits processing, loss selection, scaling law, and offline or online logits. We conduct extensive experiments to explore the design space of pre-training distillation and find better configurations and interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation, while a larger teacher LLM does not necessarily guarantee better results. We hope our exploration of the design space will inform future practices in pre-training distillation.","sentences":["Knowledge distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model.","Previous work applying KD in the field of large language models (LLMs) typically focused on the post-training phase, where the student LLM learns directly from instructions and corresponding responses generated by the teacher model.","In this paper, we extend KD to the pre-training phase of LLMs, named pre-training distillation (PD).","We first conduct a preliminary experiment using GLM-4-9B as the teacher LLM to distill a 1.9B parameter student LLM, validating the effectiveness of PD.","Considering the key impact factors of distillation, we systematically explore the design space of pre-training distillation across four aspects: logits processing, loss selection, scaling law, and offline or online logits.","We conduct extensive experiments to explore the design space of pre-training distillation and find better configurations and interesting conclusions, such as larger student LLMs generally benefiting more from pre-training distillation, while a larger teacher LLM does not necessarily guarantee better results.","We hope our exploration of the design space will inform future practices in pre-training distillation."],"url":"http://arxiv.org/abs/2410.16215v1"}
{"created":"2024-10-21 17:12:06","title":"Comprehensive benchmarking of large language models for RNA secondary structure prediction","abstract":"Inspired by the success of large language models (LLM) for DNA and proteins, several LLM for RNA have been developed recently. RNA-LLM uses large datasets of RNA sequences to learn, in a self-supervised way, how to represent each RNA base with a semantically rich numerical vector. This is done under the hypothesis that obtaining high-quality RNA representations can enhance data-costly downstream tasks. Among them, predicting the secondary structure is a fundamental task for uncovering RNA functional mechanisms. In this work we present a comprehensive experimental analysis of several pre-trained RNA-LLM, comparing them for the RNA secondary structure prediction task in an unified deep learning framework. The RNA-LLM were assessed with increasing generalization difficulty on benchmark datasets. Results showed that two LLM clearly outperform the other models, and revealed significant challenges for generalization in low-homology scenarios.","sentences":["Inspired by the success of large language models (LLM) for DNA and proteins, several LLM for RNA have been developed recently.","RNA-LLM uses large datasets of RNA sequences to learn, in a self-supervised way, how to represent each RNA base with a semantically rich numerical vector.","This is done under the hypothesis that obtaining high-quality RNA representations can enhance data-costly downstream tasks.","Among them, predicting the secondary structure is a fundamental task for uncovering RNA functional mechanisms.","In this work we present a comprehensive experimental analysis of several pre-trained RNA-LLM, comparing them for the RNA secondary structure prediction task in an unified deep learning framework.","The RNA-LLM were assessed with increasing generalization difficulty on benchmark datasets.","Results showed that two LLM clearly outperform the other models, and revealed significant challenges for generalization in low-homology scenarios."],"url":"http://arxiv.org/abs/2410.16212v1"}
{"created":"2024-10-21 17:11:31","title":"CiteClick: A Browser Extension for Real-Time Scholar Citation Tracking","abstract":"This technical report presents CiteClick, a browser extension designed to monitor and track Google Scholar citation counts for multiple researchers in real-time. We discuss the motivation behind the tool, its key features, implementation details, and potential impact on the academic community. The report covers installation procedures, usage guidelines, and customization options, concluding with a discussion on future work and potential improvements. By automating the process of citation tracking, CiteClick aims to enhance research evaluation processes and facilitate more informed decision-making in academic contexts.","sentences":["This technical report presents CiteClick, a browser extension designed to monitor and track Google Scholar citation counts for multiple researchers in real-time.","We discuss the motivation behind the tool, its key features, implementation details, and potential impact on the academic community.","The report covers installation procedures, usage guidelines, and customization options, concluding with a discussion on future work and potential improvements.","By automating the process of citation tracking, CiteClick aims to enhance research evaluation processes and facilitate more informed decision-making in academic contexts."],"url":"http://arxiv.org/abs/2410.16211v1"}
{"created":"2024-10-21 17:11:21","title":"Compute-Constrained Data Selection","abstract":"Data selection can reduce the amount of training data needed to finetune LLMs; however, the efficacy of data selection scales directly with its compute. Motivated by the practical challenge of compute-constrained finetuning, we consider the setting in which both the cost of selecting data and training are budgeted for. We first formalize the problem of data selection with a cost-aware utility function, and model the data selection problem as trading off initial-selection cost for training gain. We run a comprehensive sweep of experiments across multiple tasks, varying compute budget by scaling finetuning tokens, model sizes, and data selection compute. These experiments show the validity of this model in real-world experiments. Interestingly we find that many powerful data selection methods are almost never compute-optimal, and that cheaper data selection alternatives dominate both from a theoretical and empirical perspective.","sentences":["Data selection can reduce the amount of training data needed to finetune LLMs; however, the efficacy of data selection scales directly with its compute.","Motivated by the practical challenge of compute-constrained finetuning, we consider the setting in which both the cost of selecting data and training are budgeted for.","We first formalize the problem of data selection with a cost-aware utility function, and model the data selection problem as trading off initial-selection cost for training gain.","We run a comprehensive sweep of experiments across multiple tasks, varying compute budget by scaling finetuning tokens, model sizes, and data selection compute.","These experiments show the validity of this model in real-world experiments.","Interestingly we find that many powerful data selection methods are almost never compute-optimal, and that cheaper data selection alternatives dominate both from a theoretical and empirical perspective."],"url":"http://arxiv.org/abs/2410.16208v1"}
{"created":"2024-10-21 17:10:43","title":"CoT-TL: Low-Resource Temporal Knowledge Representation of Planning Instructions Using Chain-of-Thought Reasoning","abstract":"Autonomous agents often face the challenge of interpreting uncertain natural language instructions for planning tasks. Representing these instructions as Linear Temporal Logic (LTL) enables planners to synthesize actionable plans. We introduce CoT-TL, a data-efficient in-context learning framework for translating natural language specifications into LTL representations. CoT-TL addresses the limitations of large language models, which typically rely on extensive fine-tuning data, by extending chain-of-thought reasoning and semantic roles to align with the requirements of formal logic creation. This approach enhances the transparency and rationale behind LTL generation, fostering user trust. CoT-TL achieves state-of-the-art accuracy across three diverse datasets in low-data scenarios, outperforming existing methods without fine-tuning or intermediate translations. To improve reliability and minimize hallucinations, we incorporate model checking to validate the syntax of the generated LTL output. We further demonstrate CoT-TL's effectiveness through ablation studies and evaluations on unseen LTL structures and formulas in a new dataset. Finally, we validate CoT-TL's practicality by integrating it into a QuadCopter for multi-step drone planning based on natural language instructions.","sentences":["Autonomous agents often face the challenge of interpreting uncertain natural language instructions for planning tasks.","Representing these instructions as Linear Temporal Logic (LTL) enables planners to synthesize actionable plans.","We introduce CoT-TL, a data-efficient in-context learning framework for translating natural language specifications into LTL representations.","CoT-TL addresses the limitations of large language models, which typically rely on extensive fine-tuning data, by extending chain-of-thought reasoning and semantic roles to align with the requirements of formal logic creation.","This approach enhances the transparency and rationale behind LTL generation, fostering user trust.","CoT-TL achieves state-of-the-art accuracy across three diverse datasets in low-data scenarios, outperforming existing methods without fine-tuning or intermediate translations.","To improve reliability and minimize hallucinations, we incorporate model checking to validate the syntax of the generated LTL output.","We further demonstrate CoT-TL's effectiveness through ablation studies and evaluations on unseen LTL structures and formulas in a new dataset.","Finally, we validate CoT-TL's practicality by integrating it into a QuadCopter for multi-step drone planning based on natural language instructions."],"url":"http://arxiv.org/abs/2410.16207v1"}
{"created":"2024-10-21 17:08:10","title":"Comparative analysis of 3D-CNN models, GARCH-ANN, and VAR models for determining equity prices","abstract":"Financial models have increasingly become popular in recent times, and the focus of researchers has been to find the perfect model which fits all circumstances; however, this has not been thoroughly achieved, and as a result, many financial models have been created. Artificial Intelligence modelling has increasingly become more popular in the financial space as an answer to the weakness of the advanced mathematical models studied in Economics. This paper introduces three commonly used models and tests them on the S&P500 to give a strong projection as to the future values of the prices. It then introduces various error metrics like the root mean square error (RMSE) to ascertain the viability of the models. The results show that a longer-term forecast indeed has more arduous consequences as there is a veer between the actual and the forecasted readings. The models can produce a strong growth estimate for businesses and policymakers to plan more appropriately in the short term. Overall, the GARCH-LSTM model produced the least RMSE which also shows that complex models tend to perform better than simpler models","sentences":["Financial models have increasingly become popular in recent times, and the focus of researchers has been to find the perfect model which fits all circumstances; however, this has not been thoroughly achieved, and as a result, many financial models have been created.","Artificial Intelligence modelling has increasingly become more popular in the financial space as an answer to the weakness of the advanced mathematical models studied in Economics.","This paper introduces three commonly used models and tests them on the S&P500 to give a strong projection as to the future values of the prices.","It then introduces various error metrics like the root mean square error (RMSE) to ascertain the viability of the models.","The results show that a longer-term forecast indeed has more arduous consequences as there is a veer between the actual and the forecasted readings.","The models can produce a strong growth estimate for businesses and policymakers to plan more appropriately in the short term.","Overall, the GARCH-LSTM model produced the least RMSE which also shows that complex models tend to perform better than simpler models"],"url":"http://arxiv.org/abs/2410.16205v1"}
{"created":"2024-10-21 17:05:50","title":"Systematic Review: Text Processing Algorithms in Machine Learning and Deep Learning for Mental Health Detection on Social Media","abstract":"The global rise in depression necessitates innovative detection methods for early intervention. Social media provides a unique opportunity to identify depression through user-generated posts. This systematic review evaluates machine learning (ML) models for depression detection on social media, focusing on biases and methodological challenges throughout the ML lifecycle. A search of PubMed, IEEE Xplore, and Google Scholar identified 47 relevant studies published after 2010. The Prediction model Risk Of Bias ASsessment Tool (PROBAST) was utilized to assess methodological quality and risk of bias. Significant biases impacting model reliability and generalizability were found. There is a predominant reliance on Twitter (63.8%) and English-language content (over 90%), with most studies focusing on users from the United States and Europe. Non-probability sampling methods (approximately 80%) limit representativeness. Only 23% of studies explicitly addressed linguistic nuances like negations, crucial for accurate sentiment analysis. Inconsistent hyperparameter tuning was observed, with only 27.7% properly tuning models. About 17% did not adequately partition data into training, validation, and test sets, risking overfitting. While 74.5% used appropriate evaluation metrics for imbalanced data, others relied on accuracy without addressing class imbalance, potentially skewing results. Reporting transparency varied, often lacking critical methodological details. These findings highlight the need to diversify data sources, standardize preprocessing protocols, ensure consistent model development practices, address class imbalance, and enhance reporting transparency. By overcoming these challenges, future research can develop more robust and generalizable ML models for depression detection on social media, contributing to improved mental health outcomes globally.","sentences":["The global rise in depression necessitates innovative detection methods for early intervention.","Social media provides a unique opportunity to identify depression through user-generated posts.","This systematic review evaluates machine learning (ML) models for depression detection on social media, focusing on biases and methodological challenges throughout the ML lifecycle.","A search of PubMed, IEEE Xplore, and Google Scholar identified 47 relevant studies published after 2010.","The Prediction model Risk Of Bias ASsessment Tool (PROBAST) was utilized to assess methodological quality and risk of bias.","Significant biases impacting model reliability and generalizability were found.","There is a predominant reliance on Twitter (63.8%) and English-language content (over 90%), with most studies focusing on users from the United States and Europe.","Non-probability sampling methods (approximately 80%) limit representativeness.","Only 23% of studies explicitly addressed linguistic nuances like negations, crucial for accurate sentiment analysis.","Inconsistent hyperparameter tuning was observed, with only 27.7% properly tuning models.","About 17% did not adequately partition data into training, validation, and test sets, risking overfitting.","While 74.5% used appropriate evaluation metrics for imbalanced data, others relied on accuracy without addressing class imbalance, potentially skewing results.","Reporting transparency varied, often lacking critical methodological details.","These findings highlight the need to diversify data sources, standardize preprocessing protocols, ensure consistent model development practices, address class imbalance, and enhance reporting transparency.","By overcoming these challenges, future research can develop more robust and generalizable ML models for depression detection on social media, contributing to improved mental health outcomes globally."],"url":"http://arxiv.org/abs/2410.16204v1"}
{"created":"2024-10-21 17:04:28","title":"Musinger: Communication of Music over a Distance with Wearable Haptic Display and Touch Sensitive Surface","abstract":"This study explores the integration of auditory and tactile experiences in musical haptics, focusing on enhancing sensory dimensions of music through touch. Addressing the gap in translating auditory signals to meaningful tactile feedback, our research introduces a novel method involving a touch-sensitive recorder and a wearable haptic display that captures musical interactions via force sensors and converts these into tactile sensations. Previous studies have shown the potential of haptic feedback to enhance musical expressivity, yet challenges remain in conveying complex musical nuances. Our method aims to expand music accessibility for individuals with hearing impairments and deepen digital musical interactions. Experimental results reveal high accuracy ($98\\%$ without noise, 93% with white noise) in melody recognition through tactile feedback, demonstrating effective transmission and perception of musical information. The findings highlight the potential of haptic technology to bridge sensory gaps, offering significant implications for music therapy, education, and remote musical collaboration, advancing the field of musical haptics and multi-sensory technology applications.","sentences":["This study explores the integration of auditory and tactile experiences in musical haptics, focusing on enhancing sensory dimensions of music through touch.","Addressing the gap in translating auditory signals to meaningful tactile feedback, our research introduces a novel method involving a touch-sensitive recorder and a wearable haptic display that captures musical interactions via force sensors and converts these into tactile sensations.","Previous studies have shown the potential of haptic feedback to enhance musical expressivity, yet challenges remain in conveying complex musical nuances.","Our method aims to expand music accessibility for individuals with hearing impairments and deepen digital musical interactions.","Experimental results reveal high accuracy ($98\\%$ without noise, 93% with white noise) in melody recognition through tactile feedback, demonstrating effective transmission and perception of musical information.","The findings highlight the potential of haptic technology to bridge sensory gaps, offering significant implications for music therapy, education, and remote musical collaboration, advancing the field of musical haptics and multi-sensory technology applications."],"url":"http://arxiv.org/abs/2410.16202v1"}
{"created":"2024-10-21 17:00:06","title":"Improve Vision Language Model Chain-of-thought Reasoning","abstract":"Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness. However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales. In this work, we show that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses. To address this, we propose a two-fold approach. First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance. Second, we apply reinforcement learning to further calibrate reasoning quality. Specifically, we construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers. Using this pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities. Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well. This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs.","sentences":["Chain-of-thought (CoT) reasoning in vision language models (VLMs) is crucial for improving interpretability and trustworthiness.","However, current training recipes lack robust CoT reasoning data, relying on datasets dominated by short annotations with minimal rationales.","In this work, we show that training VLM on short answers does not generalize well to reasoning tasks that require more detailed responses.","To address this, we propose a two-fold approach.","First, we distill rationales from GPT-4o model to enrich the training data and fine-tune VLMs, boosting their CoT performance.","Second, we apply reinforcement learning to further calibrate reasoning quality.","Specifically, we construct positive (correct) and negative (incorrect) pairs of model-generated reasoning chains, by comparing their predictions with annotated short answers.","Using this pairwise data, we apply the Direct Preference Optimization algorithm to refine the model's reasoning abilities.","Our experiments demonstrate significant improvements in CoT reasoning on benchmark datasets and better generalization to direct answer prediction as well.","This work emphasizes the importance of incorporating detailed rationales in training and leveraging reinforcement learning to strengthen the reasoning capabilities of VLMs."],"url":"http://arxiv.org/abs/2410.16198v1"}
{"created":"2024-10-21 17:00:03","title":"LASER: Script Execution by Autonomous Agents for On-demand Traffic Simulation","abstract":"Autonomous Driving Systems (ADS) require diverse and safety-critical traffic scenarios for effective training and testing, but the existing data generation methods struggle to provide flexibility and scalability. We propose LASER, a novel frame-work that leverage large language models (LLMs) to conduct traffic simulations based on natural language inputs. The framework operates in two stages: it first generates scripts from user-provided descriptions and then executes them using autonomous agents in real time. Validated in the CARLA simulator, LASER successfully generates complex, on-demand driving scenarios, significantly improving ADS training and testing data generation.","sentences":["Autonomous Driving Systems (ADS) require diverse and safety-critical traffic scenarios for effective training and testing, but the existing data generation methods struggle to provide flexibility and scalability.","We propose LASER, a novel frame-work that leverage large language models (LLMs) to conduct traffic simulations based on natural language inputs.","The framework operates in two stages: it first generates scripts from user-provided descriptions and then executes them using autonomous agents in real time.","Validated in the CARLA simulator, LASER successfully generates complex, on-demand driving scenarios, significantly improving ADS training and testing data generation."],"url":"http://arxiv.org/abs/2410.16197v1"}
{"created":"2024-10-21 16:59:25","title":"Information for Conversation Generation: Proposals Utilising Knowledge Graphs","abstract":"LLMs are frequently used tools for conversational generation. Without additional information LLMs can generate lower quality responses due to lacking relevant content and hallucinations, as well as the perception of poor emotional capability, and an inability to maintain a consistent character. Knowledge graphs are commonly used forms of external knowledge and may provide solutions to these challenges. This paper introduces three proposals, utilizing knowledge graphs to enhance LLM generation. Firstly, dynamic knowledge graph embeddings and recommendation could allow for the integration of new information and the selection of relevant knowledge for response generation. Secondly, storing entities with emotional values as additional features may provide knowledge that is better emotionally aligned with the user input. Thirdly, integrating character information through narrative bubbles would maintain character consistency, as well as introducing a structure that would readily incorporate new information.","sentences":["LLMs are frequently used tools for conversational generation.","Without additional information LLMs can generate lower quality responses due to lacking relevant content and hallucinations, as well as the perception of poor emotional capability, and an inability to maintain a consistent character.","Knowledge graphs are commonly used forms of external knowledge and may provide solutions to these challenges.","This paper introduces three proposals, utilizing knowledge graphs to enhance LLM generation.","Firstly, dynamic knowledge graph embeddings and recommendation could allow for the integration of new information and the selection of relevant knowledge for response generation.","Secondly, storing entities with emotional values as additional features may provide knowledge that is better emotionally aligned with the user input.","Thirdly, integrating character information through narrative bubbles would maintain character consistency, as well as introducing a structure that would readily incorporate new information."],"url":"http://arxiv.org/abs/2410.16196v1"}
{"created":"2024-10-21 16:59:01","title":"A Trust-Region Method for Graphical Stein Variational Inference","abstract":"Stein variational inference (SVI) is a sample-based approximate Bayesian inference technique that generates a sample set by jointly optimizing the samples' locations to minimize an information-theoretic measure of discrepancy with the target probability distribution. SVI thus provides a fast and significantly more sample-efficient approach to Bayesian inference than traditional (random-sampling-based) alternatives. However, the optimization techniques employed in existing SVI methods struggle to address problems in which the target distribution is high-dimensional, poorly-conditioned, or non-convex, which severely limits the range of their practical applicability. In this paper, we propose a novel trust-region optimization approach for SVI that successfully addresses each of these challenges. Our method builds upon prior work in SVI by leveraging conditional independences in the target distribution (to achieve high-dimensional scaling) and second-order information (to address poor conditioning), while additionally providing an effective adaptive step control procedure, which is essential for ensuring convergence on challenging non-convex optimization problems. Experimental results show our method achieves superior numerical performance, both in convergence rate and sample accuracy, and scales better in high-dimensional distributions, than previous SVI techniques.","sentences":["Stein variational inference (SVI) is a sample-based approximate Bayesian inference technique that generates a sample set by jointly optimizing the samples' locations to minimize an information-theoretic measure of discrepancy with the target probability distribution.","SVI thus provides a fast and significantly more sample-efficient approach to Bayesian inference than traditional (random-sampling-based) alternatives.","However, the optimization techniques employed in existing SVI methods struggle to address problems in which the target distribution is high-dimensional, poorly-conditioned, or non-convex, which severely limits the range of their practical applicability.","In this paper, we propose a novel trust-region optimization approach for SVI that successfully addresses each of these challenges.","Our method builds upon prior work in SVI by leveraging conditional independences in the target distribution (to achieve high-dimensional scaling) and second-order information (to address poor conditioning), while additionally providing an effective adaptive step control procedure, which is essential for ensuring convergence on challenging non-convex optimization problems.","Experimental results show our method achieves superior numerical performance, both in convergence rate and sample accuracy, and scales better in high-dimensional distributions, than previous SVI techniques."],"url":"http://arxiv.org/abs/2410.16195v1"}
{"created":"2024-10-21 16:52:44","title":"Training Better Deep Learning Models Using Human Saliency","abstract":"This work explores how human judgement about salient regions of an image can be introduced into deep convolutional neural network (DCNN) training. Traditionally, training of DCNNs is purely data-driven. This often results in learning features of the data that are only coincidentally correlated with class labels. Human saliency can guide network training using our proposed new component of the loss function that ConveYs Brain Oversight to Raise Generalization (CYBORG) and penalizes the model for using non-salient regions. This mechanism produces DCNNs achieving higher accuracy and generalization compared to using the same training data without human salience. Experimental results demonstrate that CYBORG applies across multiple network architectures and problem domains (detection of synthetic faces, iris presentation attacks and anomalies in chest X-rays), while requiring significantly less data than training without human saliency guidance. Visualizations show that CYBORG-trained models' saliency is more consistent across independent training runs than traditionally-trained models, and also in better agreement with humans. To lower the cost of collecting human annotations, we also explore using deep learning to provide automated annotations. CYBORG training of CNNs addresses important issues such as reducing the appetite for large training sets, increasing interpretability, and reducing fragility by generalizing better to new types of data.","sentences":["This work explores how human judgement about salient regions of an image can be introduced into deep convolutional neural network (DCNN) training.","Traditionally, training of DCNNs is purely data-driven.","This often results in learning features of the data that are only coincidentally correlated with class labels.","Human saliency can guide network training using our proposed new component of the loss function that ConveYs Brain Oversight to Raise Generalization (CYBORG) and penalizes the model for using non-salient regions.","This mechanism produces DCNNs achieving higher accuracy and generalization compared to using the same training data without human salience.","Experimental results demonstrate that CYBORG applies across multiple network architectures and problem domains (detection of synthetic faces, iris presentation attacks and anomalies in chest X-rays), while requiring significantly less data than training without human saliency guidance.","Visualizations show that CYBORG-trained models' saliency is more consistent across independent training runs than traditionally-trained models, and also in better agreement with humans.","To lower the cost of collecting human annotations, we also explore using deep learning to provide automated annotations.","CYBORG training of CNNs addresses important issues such as reducing the appetite for large training sets, increasing interpretability, and reducing fragility by generalizing better to new types of data."],"url":"http://arxiv.org/abs/2410.16190v1"}
{"created":"2024-10-21 16:49:35","title":"Contamination Report for Multilingual Benchmarks","abstract":"Benchmark contamination refers to the presence of test datasets in Large Language Model (LLM) pre-training or post-training data. Contamination can lead to inflated scores on benchmarks, compromising evaluation results and making it difficult to determine the capabilities of models. In this work, we study the contamination of popular multilingual benchmarks in LLMs that support multiple languages. We use the Black Box test to determine whether $7$ frequently used multilingual benchmarks are contaminated in $7$ popular open and closed LLMs and find that almost all models show signs of being contaminated with almost all the benchmarks we test. Our findings can help the community determine the best set of benchmarks to use for multilingual evaluation.","sentences":["Benchmark contamination refers to the presence of test datasets in Large Language Model (LLM) pre-training or post-training data.","Contamination can lead to inflated scores on benchmarks, compromising evaluation results and making it difficult to determine the capabilities of models.","In this work, we study the contamination of popular multilingual benchmarks in LLMs that support multiple languages.","We use the Black Box test to determine whether $7$ frequently used multilingual benchmarks are contaminated in $7$ popular open and closed LLMs and find that almost all models show signs of being contaminated with almost all the benchmarks we test.","Our findings can help the community determine the best set of benchmarks to use for multilingual evaluation."],"url":"http://arxiv.org/abs/2410.16186v1"}
{"created":"2024-10-21 16:48:26","title":"RM-Bench: Benchmarking Reward Models of Language Models with Subtlety and Style","abstract":"Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses. Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power. However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance. To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases. Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively. We evaluate nearly 40 reward models on RM-Bench. Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference. These findings highlight the significant room for improvement in current reward models. Related code and data are available at https://github.com/THU-KEG/RM-Bench.","sentences":["Reward models are critical in techniques like Reinforcement Learning from Human Feedback (RLHF) and Inference Scaling Laws, where they guide language model alignment and select optimal responses.","Despite their importance, existing reward model benchmarks often evaluate models by asking them to distinguish between responses generated by models of varying power.","However, this approach fails to assess reward models on subtle but critical content changes and variations in style, resulting in a low correlation with policy model performance.","To this end, we introduce RM-Bench, a novel benchmark designed to evaluate reward models based on their sensitivity to subtle content differences and resistance to style biases.","Extensive experiments demonstrate that RM-Bench strongly correlates with policy model performance, making it a reliable reference for selecting reward models to align language models effectively.","We evaluate nearly 40 reward models on RM-Bench.","Our results reveal that even state-of-the-art models achieve an average performance of only 46.6%, which falls short of random-level accuracy (50%) when faced with style bias interference.","These findings highlight the significant room for improvement in current reward models.","Related code and data are available at https://github.com/THU-KEG/RM-Bench."],"url":"http://arxiv.org/abs/2410.16184v1"}
{"created":"2024-10-21 16:44:51","title":"MagicPIG: LSH Sampling for Efficient LLM Generation","abstract":"Large language models (LLMs) with long context windows have gained significant attention. However, the KV cache, stored to avoid re-computation, becomes a bottleneck. Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse. In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected. Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output. To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH). MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy. MagicPIG can improve decoding throughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens. The code is available at \\url{https://github.com/Infini-AI-Lab/MagicPIG}.","sentences":["Large language models (LLMs) with long context windows have gained significant attention.","However, the KV cache, stored to avoid re-computation, becomes a bottleneck.","Various dynamic sparse or TopK-based attention approximation methods have been proposed to leverage the common insight that attention is sparse.","In this paper, we first show that TopK attention itself suffers from quality degradation in certain downstream tasks because attention is not always as sparse as expected.","Rather than selecting the keys and values with the highest attention scores, sampling with theoretical guarantees can provide a better estimation for attention output.","To make the sampling-based approximation practical in LLM generation, we propose MagicPIG, a heterogeneous system based on Locality Sensitive Hashing (LSH).","MagicPIG significantly reduces the workload of attention computation while preserving high accuracy for diverse tasks.","MagicPIG stores the LSH hash tables and runs the attention computation on the CPU, which allows it to serve longer contexts and larger batch sizes with high approximation accuracy.","MagicPIG can improve decoding throughput by $1.9\\sim3.9\\times$ across various GPU hardware and achieve 110ms decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a context of 96k tokens.","The code is available at \\url{https://github.com/Infini-AI-Lab/MagicPIG}."],"url":"http://arxiv.org/abs/2410.16179v1"}
{"created":"2024-10-21 16:43:16","title":"A Framework for Evaluating Predictive Models Using Synthetic Image Covariates and Longitudinal Data","abstract":"We present a novel framework for synthesizing patient data with complex covariates (e.g., eye scans) paired with longitudinal observations (e.g., visual acuity over time), addressing privacy concerns in healthcare research. Our approach introduces controlled association in latent spaces generating each data modality, enabling the creation of complex covariate-longitudinal observation pairs. This framework facilitates the development of predictive models and provides openly available benchmarking datasets for healthcare research. We demonstrate our framework using optical coherence tomography (OCT) scans, though it is applicable across domains. Using 109,309 2D OCT scan slices, we trained an image generative model combining a variational autoencoder and a diffusion model. Longitudinal observations were simulated using a nonlinear mixed effect (NLME) model from a low-dimensional space of random effects. We generated 1.1M OCT scan slices paired with five sets of longitudinal observations at controlled association levels (100%, 50%, 10%, 5.26%, and 2% of between-subject variability). To assess the framework, we modeled synthetic longitudinal observations with another NLME model, computed empirical Bayes estimates of random effects, and trained a ResNet to predict these estimates from synthetic OCT scans. We then incorporated ResNet predictions into the NLME model for patient-individualized predictions. Prediction accuracy on withheld data declined as intended with reduced association between images and longitudinal measurements. Notably, in all but the 2% case, we achieved within 50% of the theoretical best possible prediction on withheld data, demonstrating our ability to detect even weak signals. This confirms the effectiveness of our framework in generating synthetic data with controlled levels of association, providing a valuable tool for healthcare research.","sentences":["We present a novel framework for synthesizing patient data with complex covariates (e.g., eye scans) paired with longitudinal observations (e.g., visual acuity over time), addressing privacy concerns in healthcare research.","Our approach introduces controlled association in latent spaces generating each data modality, enabling the creation of complex covariate-longitudinal observation pairs.","This framework facilitates the development of predictive models and provides openly available benchmarking datasets for healthcare research.","We demonstrate our framework using optical coherence tomography (OCT) scans, though it is applicable across domains.","Using 109,309 2D OCT scan slices, we trained an image generative model combining a variational autoencoder and a diffusion model.","Longitudinal observations were simulated using a nonlinear mixed effect (NLME) model from a low-dimensional space of random effects.","We generated 1.1M OCT scan slices paired with five sets of longitudinal observations at controlled association levels (100%, 50%, 10%, 5.26%, and 2% of between-subject variability).","To assess the framework, we modeled synthetic longitudinal observations with another NLME model, computed empirical Bayes estimates of random effects, and trained a ResNet to predict these estimates from synthetic OCT scans.","We then incorporated ResNet predictions into the NLME model for patient-individualized predictions.","Prediction accuracy on withheld data declined as intended with reduced association between images and longitudinal measurements.","Notably, in all but the 2% case, we achieved within 50% of the theoretical best possible prediction on withheld data, demonstrating our ability to detect even weak signals.","This confirms the effectiveness of our framework in generating synthetic data with controlled levels of association, providing a valuable tool for healthcare research."],"url":"http://arxiv.org/abs/2410.16177v1"}
{"created":"2024-10-21 16:41:35","title":"Spiking Neural Networks as a Controller for Emergent Swarm Agents","abstract":"Drones which can swarm and loiter in a certain area cost hundreds of dollars, but mosquitos can do the same and are essentially worthless. To control swarms of low-cost robots, researchers may end up spending countless hours brainstorming robot configurations and policies to ``organically\" create behaviors which do not need expensive sensors and perception. Existing research explores the possible emergent behaviors in swarms of robots with only a binary sensor and a simple but hand-picked controller structure. Even agents in this highly limited sensing, actuation, and computational capability class can exhibit relatively complex global behaviors such as aggregation, milling, and dispersal, but finding the local interaction rules that enable more collective behaviors remains a significant challenge. This paper investigates the feasibility of training spiking neural networks to find those local interaction rules that result in particular emergent behaviors. In this paper, we focus on simulating a specific milling behavior already known to be producible using very simple binary sensing and acting agents. To do this, we use evolutionary algorithms to evolve not only the parameters (the weights, biases, and delays) of a spiking neural network, but also its structure. To create a baseline, we also show an evolutionary search strategy over the parameters for the incumbent hand-picked binary controller structure. Our simulations show that spiking neural networks can be evolved in binary sensing agents to form a mill.","sentences":["Drones which can swarm and loiter in a certain area cost hundreds of dollars, but mosquitos can do the same and are essentially worthless.","To control swarms of low-cost robots, researchers may end up spending countless hours brainstorming robot configurations and policies to ``organically\" create behaviors which do not need expensive sensors and perception.","Existing research explores the possible emergent behaviors in swarms of robots with only a binary sensor and a simple but hand-picked controller structure.","Even agents in this highly limited sensing, actuation, and computational capability class can exhibit relatively complex global behaviors such as aggregation, milling, and dispersal, but finding the local interaction rules that enable more collective behaviors remains a significant challenge.","This paper investigates the feasibility of training spiking neural networks to find those local interaction rules that result in particular emergent behaviors.","In this paper, we focus on simulating a specific milling behavior already known to be producible using very simple binary sensing and acting agents.","To do this, we use evolutionary algorithms to evolve not only the parameters (the weights, biases, and delays) of a spiking neural network, but also its structure.","To create a baseline, we also show an evolutionary search strategy over the parameters for the incumbent hand-picked binary controller structure.","Our simulations show that spiking neural networks can be evolved in binary sensing agents to form a mill."],"url":"http://arxiv.org/abs/2410.16175v1"}
{"created":"2024-10-21 16:35:58","title":"Learning How to Vote With Principles: Axiomatic Insights Into the Collective Decisions of Neural Networks","abstract":"Can neural networks be applied in voting theory, while satisfying the need for transparency in collective decisions? We propose axiomatic deep voting: a framework to build and evaluate neural networks that aggregate preferences, using the well-established axiomatic method of voting theory. Our findings are: (1) Neural networks, despite being highly accurate, often fail to align with the core axioms of voting rules, revealing a disconnect between mimicking outcomes and reasoning. (2) Training with axiom-specific data does not enhance alignment with those axioms. (3) By solely optimizing axiom satisfaction, neural networks can synthesize new voting rules that often surpass and substantially differ from existing ones. This offers insights for both fields: For AI, important concepts like bias and value-alignment are studied in a mathematically rigorous way; for voting theory, new areas of the space of voting rules are explored.","sentences":["Can neural networks be applied in voting theory, while satisfying the need for transparency in collective decisions?","We propose axiomatic deep voting: a framework to build and evaluate neural networks that aggregate preferences, using the well-established axiomatic method of voting theory.","Our findings are: (1) Neural networks, despite being highly accurate, often fail to align with the core axioms of voting rules, revealing a disconnect between mimicking outcomes and reasoning.","(2) Training with axiom-specific data does not enhance alignment with those axioms.","(3) By solely optimizing axiom satisfaction, neural networks can synthesize new voting rules that often surpass and substantially differ from existing ones.","This offers insights for both fields: For AI, important concepts like bias and value-alignment are studied in a mathematically rigorous way; for voting theory, new areas of the space of voting rules are explored."],"url":"http://arxiv.org/abs/2410.16170v1"}
{"created":"2024-10-21 16:33:16","title":"Exploring Pretraining via Active Forgetting for Improving Cross Lingual Transfer for Decoder Language Models","abstract":"Large Language Models (LLMs) demonstrate exceptional capabilities in a multitude of NLP tasks. However, the efficacy of such models to languages other than English is often limited. Prior works have shown that encoder-only models such as BERT or XLM-RoBERTa show impressive cross lingual transfer of their capabilities from English to other languages. In this work, we propose a pretraining strategy that uses active forgetting to achieve similar cross lingual transfer in decoder-only LLMs. We show that LLMs pretrained with active forgetting are highly effective when adapting to new and unseen languages. Through extensive experimentation, we find that LLMs pretrained with active forgetting are able to learn better multilingual representations which translates to better performance in many downstream tasks.","sentences":["Large Language Models (LLMs) demonstrate exceptional capabilities in a multitude of NLP tasks.","However, the efficacy of such models to languages other than English is often limited.","Prior works have shown that encoder-only models such as BERT or XLM-RoBERTa show impressive cross lingual transfer of their capabilities from English to other languages.","In this work, we propose a pretraining strategy that uses active forgetting to achieve similar cross lingual transfer in decoder-only LLMs.","We show that LLMs pretrained with active forgetting are highly effective when adapting to new and unseen languages.","Through extensive experimentation, we find that LLMs pretrained with active forgetting are able to learn better multilingual representations which translates to better performance in many downstream tasks."],"url":"http://arxiv.org/abs/2410.16168v1"}
{"created":"2024-10-21 16:32:41","title":"Beyond Filtering: Adaptive Image-Text Quality Enhancement for MLLM Pretraining","abstract":"Multimodal large language models (MLLMs) have made significant strides by integrating visual and textual modalities. A critical factor in training MLLMs is the quality of image-text pairs within multimodal pretraining datasets. However, $\\textit {de facto}$ filter-based data quality enhancement paradigms often discard a substantial portion of high-quality image data due to inadequate semantic alignment between images and texts, leading to inefficiencies in data utilization and scalability. In this paper, we propose the Adaptive Image-Text Quality Enhancer (AITQE), a model that dynamically assesses and enhances the quality of image-text pairs. AITQE employs a text rewriting mechanism for low-quality pairs and incorporates a negative sample learning strategy to improve evaluative capabilities by integrating deliberately selected low-quality samples during training. Unlike prior approaches that significantly alter text distributions, our method minimally adjusts text to preserve data volume while enhancing quality. Experimental results demonstrate that AITQE surpasses existing methods on various benchmark, effectively leveraging raw data and scaling efficiently with increasing data volumes. We hope our work will inspire future works. The code and model are available at: https://github.com/hanhuang22/AITQE.","sentences":["Multimodal large language models (MLLMs) have made significant strides by integrating visual and textual modalities.","A critical factor in training MLLMs is the quality of image-text pairs within multimodal pretraining datasets.","However, $\\textit {de facto}$ filter-based data quality enhancement paradigms often discard a substantial portion of high-quality image data due to inadequate semantic alignment between images and texts, leading to inefficiencies in data utilization and scalability.","In this paper, we propose the Adaptive Image-Text Quality Enhancer (AITQE), a model that dynamically assesses and enhances the quality of image-text pairs.","AITQE employs a text rewriting mechanism for low-quality pairs and incorporates a negative sample learning strategy to improve evaluative capabilities by integrating deliberately selected low-quality samples during training.","Unlike prior approaches that significantly alter text distributions, our method minimally adjusts text to preserve data volume while enhancing quality.","Experimental results demonstrate that AITQE surpasses existing methods on various benchmark, effectively leveraging raw data and scaling efficiently with increasing data volumes.","We hope our work will inspire future works.","The code and model are available at: https://github.com/hanhuang22/AITQE."],"url":"http://arxiv.org/abs/2410.16166v1"}
{"created":"2024-10-21 16:31:23","title":"From Tokens to Materials: Leveraging Language Models for Scientific Discovery","abstract":"Exploring the predictive capabilities of language models in material science is an ongoing interest. This study investigates the application of language model embeddings to enhance material property prediction in materials science. By evaluating various contextual embedding methods and pre-trained models, including Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformers (GPT), we demonstrate that domain-specific models, particularly MatBERT significantly outperform general-purpose models in extracting implicit knowledge from compound names and material properties. Our findings reveal that information-dense embeddings from the third layer of MatBERT, combined with a context-averaging approach, offer the most effective method for capturing material-property relationships from the scientific literature. We also identify a crucial \"tokenizer effect,\" highlighting the importance of specialized text processing techniques that preserve complete compound names while maintaining consistent token counts. These insights underscore the value of domain-specific training and tokenization in materials science applications and offer a promising pathway for accelerating the discovery and development of new materials through AI-driven approaches.","sentences":["Exploring the predictive capabilities of language models in material science is an ongoing interest.","This study investigates the application of language model embeddings to enhance material property prediction in materials science.","By evaluating various contextual embedding methods and pre-trained models, including Bidirectional Encoder Representations from Transformers (BERT) and Generative Pre-trained Transformers (GPT), we demonstrate that domain-specific models, particularly MatBERT significantly outperform general-purpose models in extracting implicit knowledge from compound names and material properties.","Our findings reveal that information-dense embeddings from the third layer of MatBERT, combined with a context-averaging approach, offer the most effective method for capturing material-property relationships from the scientific literature.","We also identify a crucial \"tokenizer effect,\" highlighting the importance of specialized text processing techniques that preserve complete compound names while maintaining consistent token counts.","These insights underscore the value of domain-specific training and tokenization in materials science applications and offer a promising pathway for accelerating the discovery and development of new materials through AI-driven approaches."],"url":"http://arxiv.org/abs/2410.16165v1"}
{"created":"2024-10-21 16:31:16","title":"GenAI Assisting Medical Training","abstract":"Medical procedures such as venipuncture and cannulation are essential for nurses and require precise skills. Learning this skill, in turn, is a challenge for educators due to the number of teachers per class and the complexity of the task. The study aims to help students with skill acquisition and alleviate the educator's workload by integrating generative AI methods to provide real-time feedback on medical procedures such as venipuncture and cannulation.","sentences":["Medical procedures such as venipuncture and cannulation are essential for nurses and require precise skills.","Learning this skill, in turn, is a challenge for educators due to the number of teachers per class and the complexity of the task.","The study aims to help students with skill acquisition and alleviate the educator's workload by integrating generative AI methods to provide real-time feedback on medical procedures such as venipuncture and cannulation."],"url":"http://arxiv.org/abs/2410.16164v1"}
{"created":"2024-10-21 16:30:29","title":"Griffon-G: Bridging Vision-Language and Vision-Centric Tasks via Large Multimodal Models","abstract":"Large Multimodal Models (LMMs) have achieved significant breakthroughs in various vision-language and vision-centric tasks based on auto-regressive modeling. However, these models typically focus on either vision-centric tasks, such as visual grounding and region description, or vision-language tasks, like image caption and multi-scenario VQAs. None of the LMMs have yet comprehensively unified both types of tasks within a single model, as seen in Large Language Models in the natural language processing field. Furthermore, even with abundant multi-task instruction-following data, directly stacking these data for universal capabilities extension remains challenging. To address these issues, we introduce a novel multi-dimension curated and consolidated multimodal dataset, named CCMD-8M, which overcomes the data barriers of unifying vision-centric and vision-language tasks through multi-level data curation and multi-task consolidation. More importantly, we present Griffon-G, a general large multimodal model that addresses both vision-centric and vision-language tasks within a single end-to-end paradigm. Griffon-G resolves the training collapse issue encountered during the joint optimization of these tasks, achieving better training efficiency. Evaluations across multimodal benchmarks, general Visual Question Answering (VQA) tasks, scene text-centric VQA tasks, document-related VQA tasks, Referring Expression Comprehension, and object detection demonstrate that Griffon-G surpasses the advanced LMMs and achieves expert-level performance in complicated vision-centric tasks.","sentences":["Large Multimodal Models (LMMs) have achieved significant breakthroughs in various vision-language and vision-centric tasks based on auto-regressive modeling.","However, these models typically focus on either vision-centric tasks, such as visual grounding and region description, or vision-language tasks, like image caption and multi-scenario VQAs.","None of the LMMs have yet comprehensively unified both types of tasks within a single model, as seen in Large Language Models in the natural language processing field.","Furthermore, even with abundant multi-task instruction-following data, directly stacking these data for universal capabilities extension remains challenging.","To address these issues, we introduce a novel multi-dimension curated and consolidated multimodal dataset, named CCMD-8M, which overcomes the data barriers of unifying vision-centric and vision-language tasks through multi-level data curation and multi-task consolidation.","More importantly, we present Griffon-G, a general large multimodal model that addresses both vision-centric and vision-language tasks within a single end-to-end paradigm.","Griffon-G resolves the training collapse issue encountered during the joint optimization of these tasks, achieving better training efficiency.","Evaluations across multimodal benchmarks, general Visual Question Answering (VQA) tasks, scene text-centric VQA tasks, document-related VQA tasks, Referring Expression Comprehension, and object detection demonstrate that Griffon-G surpasses the advanced LMMs and achieves expert-level performance in complicated vision-centric tasks."],"url":"http://arxiv.org/abs/2410.16163v1"}
{"created":"2024-10-21 16:26:09","title":"Sparkle: Mastering Basic Spatial Capabilities in Vision Language Models Elicits Generalization to Composite Spatial Reasoning","abstract":"Vision language models (VLMs) have demonstrated impressive performance across a wide range of downstream tasks. However, their proficiency in spatial reasoning remains limited, despite its crucial role in tasks involving navigation and interaction with physical environments. Specifically, much of the spatial reasoning in these tasks occurs in two-dimensional (2D) environments, and our evaluation reveals that state-of-the-art VLMs frequently generate implausible and incorrect responses to composite spatial reasoning problems, including simple pathfinding tasks that humans can solve effortlessly at a glance. To address this, we explore an effective approach to enhance 2D spatial reasoning within VLMs by training the model on basic spatial capabilities. We begin by disentangling the key components of 2D spatial reasoning: direction comprehension, distance estimation, and localization. Our central hypothesis is that mastering these basic spatial capabilities can significantly enhance a model's performance on composite spatial tasks requiring advanced spatial understanding and combinatorial problem-solving. To investigate this hypothesis, we introduce Sparkle, a framework that fine-tunes VLMs on these three basic spatial capabilities by synthetic data generation and targeted supervision to form an instruction dataset for each capability. Our experiments demonstrate that VLMs fine-tuned with Sparkle achieve significant performance gains, not only in the basic tasks themselves but also in generalizing to composite and out-of-distribution spatial reasoning tasks (e.g., improving from 13.5% to 40.0% on the shortest path problem). These findings underscore the effectiveness of mastering basic spatial capabilities in enhancing composite spatial problem-solving, offering insights for improving VLMs' spatial reasoning capabilities.","sentences":["Vision language models (VLMs) have demonstrated impressive performance across a wide range of downstream tasks.","However, their proficiency in spatial reasoning remains limited, despite its crucial role in tasks involving navigation and interaction with physical environments.","Specifically, much of the spatial reasoning in these tasks occurs in two-dimensional (2D) environments, and our evaluation reveals that state-of-the-art VLMs frequently generate implausible and incorrect responses to composite spatial reasoning problems, including simple pathfinding tasks that humans can solve effortlessly at a glance.","To address this, we explore an effective approach to enhance 2D spatial reasoning within VLMs by training the model on basic spatial capabilities.","We begin by disentangling the key components of 2D spatial reasoning: direction comprehension, distance estimation, and localization.","Our central hypothesis is that mastering these basic spatial capabilities can significantly enhance a model's performance on composite spatial tasks requiring advanced spatial understanding and combinatorial problem-solving.","To investigate this hypothesis, we introduce Sparkle, a framework that fine-tunes VLMs on these three basic spatial capabilities by synthetic data generation and targeted supervision to form an instruction dataset for each capability.","Our experiments demonstrate that VLMs fine-tuned with Sparkle achieve significant performance gains, not only in the basic tasks themselves but also in generalizing to composite and out-of-distribution spatial reasoning tasks (e.g., improving from 13.5% to 40.0% on the shortest path problem).","These findings underscore the effectiveness of mastering basic spatial capabilities in enhancing composite spatial problem-solving, offering insights for improving VLMs' spatial reasoning capabilities."],"url":"http://arxiv.org/abs/2410.16162v1"}
{"created":"2024-10-21 16:25:14","title":"DMM: Distributed Matrix Mechanism for Differentially-Private Federated Learning using Packed Secret Sharing","abstract":"Federated Learning (FL) has gained lots of traction recently, both in industry and academia. In FL, a machine learning model is trained using data from various end-users arranged in committees across several rounds. Since such data can often be sensitive, a primary challenge in FL is providing privacy while still retaining utility of the model. Differential Privacy (DP) has become the main measure of privacy in the FL setting. DP comes in two flavors: central and local. In the former, a centralized server is trusted to receive the users' raw gradients from a training step, and then perturb their aggregation with some noise before releasing the next version of the model. In the latter (more private) setting, noise is applied on users' local devices, and only the aggregation of users' noisy gradients is revealed even to the server. Great strides have been made in increasing the privacy-utility trade-off in the central DP setting, by utilizing the so-called matrix mechanism. However, progress has been mostly stalled in the local DP setting. In this work, we introduce the distributed matrix mechanism to achieve the best-of-both-worlds; local DP and also better privacy-utility trade-off from the matrix mechanism. We accomplish this by proposing a cryptographic protocol that securely transfers sensitive values across rounds, which makes use of packed secret sharing. This protocol accommodates the dynamic participation of users per training round required by FL, including those that may drop out from the computation. We provide experiments which show that our mechanism indeed significantly improves the privacy-utility trade-off of FL models compared to previous local DP mechanisms, with little added overhead.","sentences":["Federated Learning (FL) has gained lots of traction recently, both in industry and academia.","In FL, a machine learning model is trained using data from various end-users arranged in committees across several rounds.","Since such data can often be sensitive, a primary challenge in FL is providing privacy while still retaining utility of the model.","Differential Privacy (DP) has become the main measure of privacy in the FL setting.","DP comes in two flavors: central and local.","In the former, a centralized server is trusted to receive the users' raw gradients from a training step, and then perturb their aggregation with some noise before releasing the next version of the model.","In the latter (more private) setting, noise is applied on users' local devices, and only the aggregation of users' noisy gradients is revealed even to the server.","Great strides have been made in increasing the privacy-utility trade-off in the central DP setting, by utilizing the so-called matrix mechanism.","However, progress has been mostly stalled in the local DP setting.","In this work, we introduce the distributed matrix mechanism to achieve the best-of-both-worlds; local DP and also better privacy-utility trade-off from the matrix mechanism.","We accomplish this by proposing a cryptographic protocol that securely transfers sensitive values across rounds, which makes use of packed secret sharing.","This protocol accommodates the dynamic participation of users per training round required by FL, including those that may drop out from the computation.","We provide experiments which show that our mechanism indeed significantly improves the privacy-utility trade-off of FL models compared to previous local DP mechanisms, with little added overhead."],"url":"http://arxiv.org/abs/2410.16161v1"}
{"created":"2024-10-21 16:22:19","title":"Metric as Transform: Exploring beyond Affine Transform for Interpretable Neural Network","abstract":"Artificial Neural Networks of varying architectures are generally paired with affine transformation at the core. However, we find dot product neurons with global influence less interpretable as compared to local influence of euclidean distance (as used in Radial Basis Function Network). In this work, we explore the generalization of dot product neurons to $l^p$-norm, metrics, and beyond. We find that metrics as transform performs similarly to affine transform when used in MultiLayer Perceptron or Convolutional Neural Network. Moreover, we explore various properties of Metrics, compare it with Affine, and present multiple cases where metrics seem to provide better interpretability. We develop an interpretable local dictionary based Neural Networks and use it to understand and reject adversarial examples.","sentences":["Artificial Neural Networks of varying architectures are generally paired with affine transformation at the core.","However, we find dot product neurons with global influence less interpretable as compared to local influence of euclidean distance (as used in Radial Basis Function Network).","In this work, we explore the generalization of dot product neurons to $l^p$-norm, metrics, and beyond.","We find that metrics as transform performs similarly to affine transform when used in MultiLayer Perceptron or Convolutional Neural Network.","Moreover, we explore various properties of Metrics, compare it with Affine, and present multiple cases where metrics seem to provide better interpretability.","We develop an interpretable local dictionary based Neural Networks and use it to understand and reject adversarial examples."],"url":"http://arxiv.org/abs/2410.16159v1"}
{"created":"2024-10-21 16:21:45","title":"Limpeh ga li gong: Challenges in Singlish Annotations","abstract":"Singlish, or Colloquial Singapore English, is a language formed from oral and social communication within multicultural Singapore. In this work, we work on a fundamental Natural Language Processing (NLP) task: Parts-Of-Speech (POS) tagging of Singlish sentences. For our analysis, we build a parallel Singlish dataset containing direct English translations and POS tags, with translation and POS annotation done by native Singlish speakers. Our experiments show that automatic transition- and transformer- based taggers perform with only $\\sim 80\\%$ accuracy when evaluated against human-annotated POS labels, suggesting that there is indeed room for improvement on computation analysis of the language. We provide an exposition of challenges in Singlish annotation: its inconsistencies in form and semantics, the highly context-dependent particles of the language, its structural unique expressions, and the variation of the language on different mediums. Our task definition, resultant labels and results reflects the challenges in analysing colloquial languages formulated from a variety of dialects, and paves the way for future studies beyond POS tagging.","sentences":["Singlish, or Colloquial Singapore English, is a language formed from oral and social communication within multicultural Singapore.","In this work, we work on a fundamental Natural Language Processing (NLP) task: Parts-Of-Speech (POS) tagging of Singlish sentences.","For our analysis, we build a parallel Singlish dataset containing direct English translations and POS tags, with translation and POS annotation done by native Singlish speakers.","Our experiments show that automatic transition- and transformer- based taggers perform with only $\\sim","80\\%$ accuracy when evaluated against human-annotated POS labels,","suggesting that there is indeed room for improvement on computation analysis of the language.","We provide an exposition of challenges in Singlish annotation: its inconsistencies in form and semantics, the highly context-dependent particles of the language, its structural unique expressions, and the variation of the language on different mediums.","Our task definition, resultant labels and results reflects the challenges in analysing colloquial languages formulated from a variety of dialects, and paves the way for future studies beyond POS tagging."],"url":"http://arxiv.org/abs/2410.16156v1"}
{"created":"2024-10-21 16:21:24","title":"A Troublemaker with Contagious Jailbreak Makes Chaos in Honest Towns","abstract":"With the development of large language models, they are widely used as agents in various fields. A key component of agents is memory, which stores vital information but is susceptible to jailbreak attacks. Existing research mainly focuses on single-agent attacks and shared memory attacks. However, real-world scenarios often involve independent memory. In this paper, we propose the Troublemaker Makes Chaos in Honest Town (TMCHT) task, a large-scale, multi-agent, multi-topology text-based attack evaluation framework. TMCHT involves one attacker agent attempting to mislead an entire society of agents. We identify two major challenges in multi-agent attacks: (1) Non-complete graph structure, (2) Large-scale systems. We attribute these challenges to a phenomenon we term toxicity disappearing. To address these issues, we propose an Adversarial Replication Contagious Jailbreak (ARCJ) method, which optimizes the retrieval suffix to make poisoned samples more easily retrieved and optimizes the replication suffix to make poisoned samples have contagious ability. We demonstrate the superiority of our approach in TMCHT, with 23.51%, 18.95%, and 52.93% improvements in line topology, star topology, and 100-agent settings. Encourage community attention to the security of multi-agent systems.","sentences":["With the development of large language models, they are widely used as agents in various fields.","A key component of agents is memory, which stores vital information but is susceptible to jailbreak attacks.","Existing research mainly focuses on single-agent attacks and shared memory attacks.","However, real-world scenarios often involve independent memory.","In this paper, we propose the Troublemaker Makes Chaos in Honest Town (TMCHT) task, a large-scale, multi-agent, multi-topology text-based attack evaluation framework.","TMCHT involves one attacker agent attempting to mislead an entire society of agents.","We identify two major challenges in multi-agent attacks: (1) Non-complete graph structure, (2) Large-scale systems.","We attribute these challenges to a phenomenon we term toxicity disappearing.","To address these issues, we propose an Adversarial Replication Contagious Jailbreak (ARCJ) method, which optimizes the retrieval suffix to make poisoned samples more easily retrieved and optimizes the replication suffix to make poisoned samples have contagious ability.","We demonstrate the superiority of our approach in TMCHT, with 23.51%, 18.95%, and 52.93% improvements in line topology, star topology, and 100-agent settings.","Encourage community attention to the security of multi-agent systems."],"url":"http://arxiv.org/abs/2410.16155v1"}
{"created":"2024-10-21 16:21:09","title":"Unsupervised Replay Strategies for Continual Learning with Limited Data","abstract":"Artificial neural networks (ANNs) show limited performance with scarce or imbalanced training data and face challenges with continuous learning, such as forgetting previously learned data after new tasks training. In contrast, the human brain can learn continuously and from just a few examples. This research explores the impact of 'sleep', an unsupervised phase incorporating stochastic activation with local Hebbian learning rules, on ANNs trained incrementally with limited and imbalanced datasets, specifically MNIST and Fashion MNIST. We discovered that introducing a sleep phase significantly enhanced accuracy in models trained with limited data. When a few tasks were trained sequentially, sleep replay not only rescued previously learned information that had been catastrophically forgetting following new task training but often enhanced performance in prior tasks, especially those trained with limited data. This study highlights the multifaceted role of sleep replay in augmenting learning efficiency and facilitating continual learning in ANNs.","sentences":["Artificial neural networks (ANNs) show limited performance with scarce or imbalanced training data and face challenges with continuous learning, such as forgetting previously learned data after new tasks training.","In contrast, the human brain can learn continuously and from just a few examples.","This research explores the impact of 'sleep', an unsupervised phase incorporating stochastic activation with local Hebbian learning rules, on ANNs trained incrementally with limited and imbalanced datasets, specifically MNIST and Fashion MNIST.","We discovered that introducing a sleep phase significantly enhanced accuracy in models trained with limited data.","When a few tasks were trained sequentially, sleep replay not only rescued previously learned information that had been catastrophically forgetting following new task training but often enhanced performance in prior tasks, especially those trained with limited data.","This study highlights the multifaceted role of sleep replay in augmenting learning efficiency and facilitating continual learning in ANNs."],"url":"http://arxiv.org/abs/2410.16154v1"}
{"created":"2024-10-21 16:19:41","title":"Pangea: A Fully Open Multilingual Multimodal LLM for 39 Languages","abstract":"Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world's languages and diverse cultural contexts underrepresented. This paper introduces Pangea, a multilingual multimodal LLM trained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages. PangeaIns features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage. To rigorously assess models' capabilities, we introduce PangeaBench, a holistic evaluation suite encompassing 14 datasets covering 47 languages. Results show that Pangea significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts. Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance. We fully open-source our data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum.","sentences":["Despite recent advances in multimodal large language models (MLLMs), their development has predominantly focused on English- and western-centric datasets and tasks, leaving most of the world's languages and diverse cultural contexts underrepresented.","This paper introduces Pangea, a multilingual multimodal LLM trained on PangeaIns, a diverse 6M instruction dataset spanning 39 languages.","PangeaIns features: 1) high-quality English instructions, 2) carefully machine-translated instructions, and 3) culturally relevant multimodal tasks to ensure cross-cultural coverage.","To rigorously assess models' capabilities, we introduce PangeaBench, a holistic evaluation suite encompassing 14 datasets covering 47 languages.","Results show that Pangea significantly outperforms existing open-source models in multilingual settings and diverse cultural contexts.","Ablation studies further reveal the importance of English data proportions, language popularity, and the number of multimodal training samples on overall performance.","We fully open-source our data, code, and trained checkpoints, to facilitate the development of inclusive and robust multilingual MLLMs, promoting equity and accessibility across a broader linguistic and cultural spectrum."],"url":"http://arxiv.org/abs/2410.16153v1"}
{"created":"2024-10-21 16:19:34","title":"Warped Diffusion: Solving Video Inverse Problems with Image Diffusion Models","abstract":"Using image models naively for solving inverse video problems often suffers from flickering, texture-sticking, and temporal inconsistency in generated videos. To tackle these problems, in this paper, we view frames as continuous functions in the 2D space, and videos as a sequence of continuous warping transformations between different frames. This perspective allows us to train function space diffusion models only on images and utilize them to solve temporally correlated inverse problems. The function space diffusion models need to be equivariant with respect to the underlying spatial transformations. To ensure temporal consistency, we introduce a simple post-hoc test-time guidance towards (self)-equivariant solutions. Our method allows us to deploy state-of-the-art latent diffusion models such as Stable Diffusion XL to solve video inverse problems. We demonstrate the effectiveness of our method for video inpainting and $8\\times$ video super-resolution, outperforming existing techniques based on noise transformations. We provide generated video results: https://giannisdaras.github.io/warped\\_diffusion.github.io/.","sentences":["Using image models naively for solving inverse video problems often suffers from flickering, texture-sticking, and temporal inconsistency in generated videos.","To tackle these problems, in this paper, we view frames as continuous functions in the 2D space, and videos as a sequence of continuous warping transformations between different frames.","This perspective allows us to train function space diffusion models only on images and utilize them to solve temporally correlated inverse problems.","The function space diffusion models need to be equivariant with respect to the underlying spatial transformations.","To ensure temporal consistency, we introduce a simple post-hoc test-time guidance towards (self)-equivariant solutions.","Our method allows us to deploy state-of-the-art latent diffusion models such as Stable Diffusion XL to solve video inverse problems.","We demonstrate the effectiveness of our method for video inpainting and $8\\times$ video super-resolution, outperforming existing techniques based on noise transformations.","We provide generated video results: https://giannisdaras.github.io/warped\\_diffusion.github.io/."],"url":"http://arxiv.org/abs/2410.16152v1"}
{"created":"2024-10-21 16:18:31","title":"Small Contributions, Small Networks: Efficient Neural Network Pruning Based on Relative Importance","abstract":"Recent advancements have scaled neural networks to unprecedented sizes, achieving remarkable performance across a wide range of tasks. However, deploying these large-scale models on resource-constrained devices poses significant challenges due to substantial storage and computational requirements. Neural network pruning has emerged as an effective technique to mitigate these limitations by reducing model size and complexity. In this paper, we introduce an intuitive and interpretable pruning method based on activation statistics, rooted in information theory and statistical analysis. Our approach leverages the statistical properties of neuron activations to identify and remove weights with minimal contributions to neuron outputs. Specifically, we build a distribution of weight contributions across the dataset and utilize its parameters to guide the pruning process. Furthermore, we propose a Pruning-aware Training strategy that incorporates an additional regularization term to enhance the effectiveness of our pruning method. Extensive experiments on multiple datasets and network architectures demonstrate that our method consistently outperforms several baseline and state-of-the-art pruning techniques.","sentences":["Recent advancements have scaled neural networks to unprecedented sizes, achieving remarkable performance across a wide range of tasks.","However, deploying these large-scale models on resource-constrained devices poses significant challenges due to substantial storage and computational requirements.","Neural network pruning has emerged as an effective technique to mitigate these limitations by reducing model size and complexity.","In this paper, we introduce an intuitive and interpretable pruning method based on activation statistics, rooted in information theory and statistical analysis.","Our approach leverages the statistical properties of neuron activations to identify and remove weights with minimal contributions to neuron outputs.","Specifically, we build a distribution of weight contributions across the dataset and utilize its parameters to guide the pruning process.","Furthermore, we propose a Pruning-aware Training strategy that incorporates an additional regularization term to enhance the effectiveness of our pruning method.","Extensive experiments on multiple datasets and network architectures demonstrate that our method consistently outperforms several baseline and state-of-the-art pruning techniques."],"url":"http://arxiv.org/abs/2410.16151v1"}
{"created":"2024-10-21 16:18:19","title":"Modelling Structured Data Learning with Restricted Boltzmann Machines in the Teacher-Student Setting","abstract":"Restricted Boltzmann machines (RBM) are generative models capable to learn data with a rich underlying structure. We study the teacher-student setting where a student RBM learns structured data generated by a teacher RBM. The amount of structure in the data is controlled by adjusting the number of hidden units of the teacher and the correlations in the rows of the weights, a.k.a. patterns. In the absence of correlations, we validate the conjecture that the performance is independent of the number of teacher patters and hidden units of the student RBMs, and we argue that the teacher-student setting can be used as a toy model for studying the lottery ticket hypothesis. Beyond this regime, we find that the critical amount of data required to learn the teacher patterns decreases with both their number and correlations. In both regimes, we find that, even with an relatively large dataset, it becomes impossible to learn the teacher patterns if the inference temperature used for regularization is kept too low. In our framework, the student can learn teacher patterns one-to-one or many-to-one, generalizing previous findings about the teacher-student setting with two hidden units to any arbitrary finite number of hidden units.","sentences":["Restricted Boltzmann machines (RBM) are generative models capable to learn data with a rich underlying structure.","We study the teacher-student setting where a student RBM learns structured data generated by a teacher RBM.","The amount of structure in the data is controlled by adjusting the number of hidden units of the teacher and the correlations in the rows of the weights, a.k.a. patterns.","In the absence of correlations, we validate the conjecture that the performance is independent of the number of teacher patters and hidden units of the student RBMs, and we argue that the teacher-student setting can be used as a toy model for studying the lottery ticket hypothesis.","Beyond this regime, we find that the critical amount of data required to learn the teacher patterns decreases with both their number and correlations.","In both regimes, we find that, even with an relatively large dataset, it becomes impossible to learn the teacher patterns if the inference temperature used for regularization is kept too low.","In our framework, the student can learn teacher patterns one-to-one or many-to-one, generalizing previous findings about the teacher-student setting with two hidden units to any arbitrary finite number of hidden units."],"url":"http://arxiv.org/abs/2410.16150v1"}
{"created":"2024-10-21 16:17:22","title":"PODTILE: Facilitating Podcast Episode Browsing with Auto-generated Chapters","abstract":"Listeners of long-form talk-audio content, such as podcast episodes, often find it challenging to understand the overall structure and locate relevant sections. A practical solution is to divide episodes into chapters--semantically coherent segments labeled with titles and timestamps. Since most episodes on our platform at Spotify currently lack creator-provided chapters, automating the creation of chapters is essential. Scaling the chapterization of podcast episodes presents unique challenges. First, episodes tend to be less structured than written texts, featuring spontaneous discussions with nuanced transitions. Second, the transcripts are usually lengthy, averaging about 16,000 tokens, which necessitates efficient processing that can preserve context. To address these challenges, we introduce PODTILE, a fine-tuned encoder-decoder transformer to segment conversational data. The model simultaneously generates chapter transitions and titles for the input transcript. To preserve context, each input text is augmented with global context, including the episode's title, description, and previous chapter titles. In our intrinsic evaluation, PODTILE achieved an 11% improvement in ROUGE score over the strongest baseline. Additionally, we provide insights into the practical benefits of auto-generated chapters for listeners navigating episode content. Our findings indicate that auto-generated chapters serve as a useful tool for engaging with less popular podcasts. Finally, we present empirical evidence that using chapter titles can enhance effectiveness of sparse retrieval in search tasks.","sentences":["Listeners of long-form talk-audio content, such as podcast episodes, often find it challenging to understand the overall structure and locate relevant sections.","A practical solution is to divide episodes into chapters--semantically coherent segments labeled with titles and timestamps.","Since most episodes on our platform at Spotify currently lack creator-provided chapters, automating the creation of chapters is essential.","Scaling the chapterization of podcast episodes presents unique challenges.","First, episodes tend to be less structured than written texts, featuring spontaneous discussions with nuanced transitions.","Second, the transcripts are usually lengthy, averaging about 16,000 tokens, which necessitates efficient processing that can preserve context.","To address these challenges, we introduce PODTILE, a fine-tuned encoder-decoder transformer to segment conversational data.","The model simultaneously generates chapter transitions and titles for the input transcript.","To preserve context, each input text is augmented with global context, including the episode's title, description, and previous chapter titles.","In our intrinsic evaluation, PODTILE achieved an 11% improvement in ROUGE score over the strongest baseline.","Additionally, we provide insights into the practical benefits of auto-generated chapters for listeners navigating episode content.","Our findings indicate that auto-generated chapters serve as a useful tool for engaging with less popular podcasts.","Finally, we present empirical evidence that using chapter titles can enhance effectiveness of sparse retrieval in search tasks."],"url":"http://arxiv.org/abs/2410.16148v1"}
{"created":"2024-10-21 16:17:01","title":"Towards Combating Frequency Simplicity-biased Learning for Domain Generalization","abstract":"Domain generalization methods aim to learn transferable knowledge from source domains that can generalize well to unseen target domains. Recent studies show that neural networks frequently suffer from a simplicity-biased learning behavior which leads to over-reliance on specific frequency sets, namely as frequency shortcuts, instead of semantic information, resulting in poor generalization performance. Despite previous data augmentation techniques successfully enhancing generalization performances, they intend to apply more frequency shortcuts, thereby causing hallucinations of generalization improvement. In this paper, we aim to prevent such learning behavior of applying frequency shortcuts from a data-driven perspective. Given the theoretical justification of models' biased learning behavior on different spatial frequency components, which is based on the dataset frequency properties, we argue that the learning behavior on various frequency components could be manipulated by changing the dataset statistical structure in the Fourier domain. Intuitively, as frequency shortcuts are hidden in the dominant and highly dependent frequencies of dataset structure, dynamically perturbating the over-reliance frequency components could prevent the application of frequency shortcuts. To this end, we propose two effective data augmentation modules designed to collaboratively and adaptively adjust the frequency characteristic of the dataset, aiming to dynamically influence the learning behavior of the model and ultimately serving as a strategy to mitigate shortcut learning. Code is available at AdvFrequency (https://github.com/C0notSilly/AdvFrequency).","sentences":["Domain generalization methods aim to learn transferable knowledge from source domains that can generalize well to unseen target domains.","Recent studies show that neural networks frequently suffer from a simplicity-biased learning behavior which leads to over-reliance on specific frequency sets, namely as frequency shortcuts, instead of semantic information, resulting in poor generalization performance.","Despite previous data augmentation techniques successfully enhancing generalization performances, they intend to apply more frequency shortcuts, thereby causing hallucinations of generalization improvement.","In this paper, we aim to prevent such learning behavior of applying frequency shortcuts from a data-driven perspective.","Given the theoretical justification of models' biased learning behavior on different spatial frequency components, which is based on the dataset frequency properties, we argue that the learning behavior on various frequency components could be manipulated by changing the dataset statistical structure in the Fourier domain.","Intuitively, as frequency shortcuts are hidden in the dominant and highly dependent frequencies of dataset structure, dynamically perturbating the over-reliance frequency components could prevent the application of frequency shortcuts.","To this end, we propose two effective data augmentation modules designed to collaboratively and adaptively adjust the frequency characteristic of the dataset, aiming to dynamically influence the learning behavior of the model and ultimately serving as a strategy to mitigate shortcut learning.","Code is available at AdvFrequency (https://github.com/C0notSilly/AdvFrequency)."],"url":"http://arxiv.org/abs/2410.16146v1"}
{"created":"2024-10-21 16:14:57","title":"1-bit AI Infra: Part 1.1, Fast and Lossless BitNet b1.58 Inference on CPUs","abstract":"Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and BitNet b1.58, present a promising approach to enhancing the efficiency of LLMs in terms of speed and energy consumption. These developments also enable local LLM deployment across a broad range of devices. In this work, we introduce bitnet.cpp, a tailored software stack designed to unlock the full potential of 1-bit LLMs. Specifically, we develop a set of kernels to support fast and lossless inference of ternary BitNet b1.58 LLMs on CPUs. Extensive experiments demonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x to 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model sizes. The code is available at https://github.com/microsoft/BitNet.","sentences":["Recent advances in 1-bit Large Language Models (LLMs), such as BitNet and BitNet b1.58, present a promising approach to enhancing the efficiency of LLMs in terms of speed and energy consumption.","These developments also enable local LLM deployment across a broad range of devices.","In this work, we introduce bitnet.cpp, a tailored software stack designed to unlock the full potential of 1-bit LLMs.","Specifically, we develop a set of kernels to support fast and lossless inference of ternary BitNet b1.58 LLMs on CPUs.","Extensive experiments demonstrate that bitnet.cpp achieves significant speedups, ranging from 2.37x to 6.17x on x86 CPUs and from 1.37x to 5.07x on ARM CPUs, across various model sizes.","The code is available at https://github.com/microsoft/BitNet."],"url":"http://arxiv.org/abs/2410.16144v1"}
{"created":"2024-10-21 16:08:20","title":"AdChain: Decentralized Header Bidding","abstract":"Due to the involvement of multiple intermediaries without trusted parties, lack of proper regulations, and a complicated supply chain, ad impression discrepancy affects online advertising. This issue causes up to $82 billion annual revenue loss for honest parties. The loss can be significantly reduced with a precise and trusted decentralized mechanism. This paper presents AdChain, a decentralized, distributed, and verifiable solution that detects and minimizes online advertisement impression discrepancies. AdChain establishes trust by employing multiple independent agents to receive and record log-level data, along with a consensus protocol to validate each ad data. AdChain is scalable, efficient, and compatible with the current infrastructure. Our experimental evaluation, using over half a million ad data points, identifies system parameters that achieve 98% accuracy, reducing the ad discrepancy rate from 20% to 2%. Our cost analysis shows that active nodes on AdChain can generate profits comparable to miners on major blockchain networks like Bitcoin.","sentences":["Due to the involvement of multiple intermediaries without trusted parties, lack of proper regulations, and a complicated supply chain, ad impression discrepancy affects online advertising.","This issue causes up to $82 billion annual revenue loss for honest parties.","The loss can be significantly reduced with a precise and trusted decentralized mechanism.","This paper presents AdChain, a decentralized, distributed, and verifiable solution that detects and minimizes online advertisement impression discrepancies.","AdChain establishes trust by employing multiple independent agents to receive and record log-level data, along with a consensus protocol to validate each ad data.","AdChain is scalable, efficient, and compatible with the current infrastructure.","Our experimental evaluation, using over half a million ad data points, identifies system parameters that achieve 98% accuracy, reducing the ad discrepancy rate from 20% to 2%.","Our cost analysis shows that active nodes on AdChain can generate profits comparable to miners on major blockchain networks like Bitcoin."],"url":"http://arxiv.org/abs/2410.16141v1"}
{"created":"2024-10-21 16:07:08","title":"Cooperative Multistatic Target Detection in Cell-Free Communication Networks","abstract":"In this work, we consider the target detection problem in a multistatic integrated sensing and communication (ISAC) scenario characterized by the cell-free MIMO communication network deployment, where multiple radio units (RUs) in the network cooperate with each other for the sensing task. By exploiting the angle resolution from multiple arrays deployed in the network and the delay resolution from the communication signals, i.e., orthogonal frequency division multiplexing (OFDM) signals, we formulate a cooperative sensing problem with coherent data fusion of multiple RUs' observations and propose a sparse Bayesian learning (SBL)-based method, where the global coordinates of target locations are directly detected. Intensive numerical results indicate promising target detection performance of the proposed SBL-based method. Additionally, a theoretical analysis of the considered cooperative multistatic sensing task is provided using the pairwise error probability (PEP) analysis, which can be used to provide design insights, e.g., illumination and beam patterns, for the considered problem.","sentences":["In this work, we consider the target detection problem in a multistatic integrated sensing and communication (ISAC) scenario characterized by the cell-free MIMO communication network deployment, where multiple radio units (RUs) in the network cooperate with each other for the sensing task.","By exploiting the angle resolution from multiple arrays deployed in the network and the delay resolution from the communication signals, i.e., orthogonal frequency division multiplexing (OFDM) signals, we formulate a cooperative sensing problem with coherent data fusion of multiple RUs' observations and propose a sparse Bayesian learning (SBL)-based method, where the global coordinates of target locations are directly detected.","Intensive numerical results indicate promising target detection performance of the proposed SBL-based method.","Additionally, a theoretical analysis of the considered cooperative multistatic sensing task is provided using the pairwise error probability (PEP) analysis, which can be used to provide design insights, e.g., illumination and beam patterns, for the considered problem."],"url":"http://arxiv.org/abs/2410.16140v1"}
{"created":"2024-10-21 16:05:58","title":"A Psycholinguistic Evaluation of Language Models' Sensitivity to Argument Roles","abstract":"We present a systematic evaluation of large language models' sensitivity to argument roles, i.e., who did what to whom, by replicating psycholinguistic studies on human argument role processing. In three experiments, we find that language models are able to distinguish verbs that appear in plausible and implausible contexts, where plausibility is determined through the relation between the verb and its preceding arguments. However, none of the models capture the same selective patterns that human comprehenders exhibit during real-time verb prediction. This indicates that language models' capacity to detect verb plausibility does not arise from the same mechanism that underlies human real-time sentence processing.","sentences":["We present a systematic evaluation of large language models' sensitivity to argument roles, i.e., who did what to whom, by replicating psycholinguistic studies on human argument role processing.","In three experiments, we find that language models are able to distinguish verbs that appear in plausible and implausible contexts, where plausibility is determined through the relation between the verb and its preceding arguments.","However, none of the models capture the same selective patterns that human comprehenders exhibit during real-time verb prediction.","This indicates that language models' capacity to detect verb plausibility does not arise from the same mechanism that underlies human real-time sentence processing."],"url":"http://arxiv.org/abs/2410.16139v1"}
{"created":"2024-10-21 16:04:50","title":"Theoretical Insights into Line Graph Transformation on Graph Learning","abstract":"Line graph transformation has been widely studied in graph theory, where each node in a line graph corresponds to an edge in the original graph. This has inspired a series of graph neural networks (GNNs) applied to transformed line graphs, which have proven effective in various graph representation learning tasks. However, there is limited theoretical study on how line graph transformation affects the expressivity of GNN models. In this study, we focus on two types of graphs known to be challenging to the Weisfeiler-Leman (WL) tests: Cai-F\\\"urer-Immerman (CFI) graphs and strongly regular graphs, and show that applying line graph transformation helps exclude these challenging graph properties, thus potentially assist WL tests in distinguishing these graphs. We empirically validate our findings by conducting a series of experiments that compare the accuracy and efficiency of graph isomorphism tests and GNNs on both line-transformed and original graphs across these graph structure types.","sentences":["Line graph transformation has been widely studied in graph theory, where each node in a line graph corresponds to an edge in the original graph.","This has inspired a series of graph neural networks (GNNs) applied to transformed line graphs, which have proven effective in various graph representation learning tasks.","However, there is limited theoretical study on how line graph transformation affects the expressivity of GNN models.","In this study, we focus on two types of graphs known to be challenging to the Weisfeiler-Leman (WL) tests: Cai-F\\\"urer-Immerman (CFI) graphs and strongly regular graphs, and show that applying line graph transformation helps exclude these challenging graph properties, thus potentially assist WL tests in distinguishing these graphs.","We empirically validate our findings by conducting a series of experiments that compare the accuracy and efficiency of graph isomorphism tests and GNNs on both line-transformed and original graphs across these graph structure types."],"url":"http://arxiv.org/abs/2410.16138v1"}
{"created":"2024-10-21 16:03:18","title":"Privacy as Social Norm: Systematically Reducing Dysfunctional Privacy Concerns on Social Media","abstract":"Privacy is essential to fully enjoying the benefits of social media. While fear around privacy risks can sometimes motivate privacy management, the negative impact of such fear, particularly when it is perceived as unaddressable (i.e., \"dysfunctional\" fear), can significantly harm teen well-being. In a co-design study with 136 participants aged 13-18, we explored how teens can protect their privacy without experiencing heightened fear. We identified seven different sources of dysfunctional fear, such as `fear of a hostile environment' and `fear of overstepping privacy norms.' We also evaluated ten designs, co-created with teen participants, that address these fears. Our findings suggest that social media platforms can mitigate dysfunctional fear without compromising privacy by creating a culture where privacy protection is the norm through default privacy-protective features. However, we also found that even the most effective privacy features are not likely to be adopted unless they balance the multifaceted and diverse needs of teens. Individual teens have different needs -- for example, public and private account users have different needs -- and teens often want to enjoy the benefits they get from slightly reducing privacy and widening their social reach. Given these considerations, augmenting default privacy features by allowing them to be toggled on and off will allow individual users to choose their own balance while still maintaining a privacy-focused norm.","sentences":["Privacy is essential to fully enjoying the benefits of social media.","While fear around privacy risks can sometimes motivate privacy management, the negative impact of such fear, particularly when it is perceived as unaddressable (i.e., \"dysfunctional\" fear), can significantly harm teen well-being.","In a co-design study with 136 participants aged 13-18, we explored how teens can protect their privacy without experiencing heightened fear.","We identified seven different sources of dysfunctional fear, such as `fear of a hostile environment' and `fear of overstepping privacy norms.'","We also evaluated ten designs, co-created with teen participants, that address these fears.","Our findings suggest that social media platforms can mitigate dysfunctional fear without compromising privacy by creating a culture where privacy protection is the norm through default privacy-protective features.","However, we also found that even the most effective privacy features are not likely to be adopted unless they balance the multifaceted and diverse needs of teens.","Individual teens have different needs -- for example, public and private account users have different needs -- and teens often want to enjoy the benefits they get from slightly reducing privacy and widening their social reach.","Given these considerations, augmenting default privacy features by allowing them to be toggled on and off will allow individual users to choose their own balance while still maintaining a privacy-focused norm."],"url":"http://arxiv.org/abs/2410.16137v1"}
{"created":"2024-10-21 16:00:04","title":"Beyond 2:4: exploring V:N:M sparsity for efficient transformer inference on GPUs","abstract":"To date, 2:4 sparsity has stood as the only sparse pattern that can be accelerated using sparse tensor cores on GPUs. In practice, 2:4 sparsity often possesses low actual speedups ($\\leq 1.3$) and requires fixed sparse ratios, meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity, do not incur any speedups on GPUs. Recent studies suggest that V:N:M sparsity is promising in addressing these limitations of 2:4 sparsity. However, regarding accuracy, the effects of V:N:M sparsity on broader Transformer models, such as vision Transformers and large language models (LLMs), are largely unexamined. Moreover, Some specific issues related to V:N:M sparsity, such as how to select appropriate V and M values, remain unresolved. In this study, we thoroughly investigate the application of V:N:M sparsity in vision models and LLMs across multiple tasks, from pertaining to downstream tasks. We propose three key approaches to enhance the applicability and accuracy of V:N:M-sparse Transformers, including heuristic V and M selection, V:N:M-specific channel permutation, and three-staged LoRA training techniques. Experimental results show that, with our methods, the DeiT-small achieves lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy even at 64:2:8 sparsity. In addition, the fine-tuned LLama2-7B at 64:2:5 sparsity performs comparably or better than training-free 2:4 sparse alternatives on downstream tasks. More importantly, V:N:M-sparse Transformers offer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity. Overall, our exploration largely facilitates the V:N:M sparsity to act as a truly effective acceleration solution for Transformers in cost-sensitive inference scenarios.","sentences":["To date, 2:4 sparsity has stood as the only sparse pattern that can be accelerated using sparse tensor cores on GPUs.","In practice, 2:4 sparsity often possesses low actual speedups ($\\leq 1.3$) and requires fixed sparse ratios, meaning that other ratios, such as 4:8, 8:16, or those exceeding 50% sparsity, do not incur any speedups on GPUs.","Recent studies suggest that V:N:M sparsity is promising in addressing these limitations of 2:4 sparsity.","However, regarding accuracy, the effects of V:N:M sparsity on broader Transformer models, such as vision Transformers and large language models (LLMs), are largely unexamined.","Moreover, Some specific issues related to V:N:M sparsity, such as how to select appropriate V and M values, remain unresolved.","In this study, we thoroughly investigate the application of V:N:M sparsity in vision models and LLMs across multiple tasks, from pertaining to downstream tasks.","We propose three key approaches to enhance the applicability and accuracy of V:N:M-sparse Transformers, including heuristic V and M selection, V:N:M-specific channel permutation, and three-staged LoRA training techniques.","Experimental results show that, with our methods, the DeiT-small achieves lossless accuracy at 64:2:5 sparsity, while the DeiT-base maintains accuracy even at 64:2:8 sparsity.","In addition, the fine-tuned LLama2-7B at 64:2:5 sparsity performs comparably or better than training-free 2:4 sparse alternatives on downstream tasks.","More importantly, V:N:M-sparse Transformers offer a wider range of speedup-accuracy trade-offs compared to 2:4 sparsity.","Overall, our exploration largely facilitates the V:N:M sparsity to act as a truly effective acceleration solution for Transformers in cost-sensitive inference scenarios."],"url":"http://arxiv.org/abs/2410.16135v1"}
{"created":"2024-10-21 15:56:17","title":"A Data-driven Crowd Simulation Framework Integrating Physics-informed Machine Learning with Navigation Potential Fields","abstract":"Traditional rule-based physical models are limited by their reliance on singular physical formulas and parameters, making it difficult to effectively tackle the intricate tasks associated with crowd simulation. Recent research has introduced deep learning methods to tackle these issues, but most current approaches focus primarily on generating pedestrian trajectories, often lacking interpretability and failing to provide real-time dynamic simulations.To address the aforementioned issues, we propose a novel data-driven crowd simulation framework that integrates Physics-informed Machine Learning (PIML) with navigation potential fields. Our approach leverages the strengths of both physical models and PIML. Specifically, we design an innovative Physics-informed Spatio-temporal Graph Convolutional Network (PI-STGCN) as a data-driven module to predict pedestrian movement trends based on crowd spatio-temporal data. Additionally, we construct a physical model of navigation potential fields based on flow field theory to guide pedestrian movements, thereby reinforcing physical constraints during the simulation. In our framework, navigation potential fields are dynamically computed and updated based on the movement trends predicted by the PI-STGCN, while the updated crowd dynamics, guided by these fields, subsequently feed back into the PI-STGCN. Comparative experiments on two publicly available large-scale real-world datasets across five scenes demonstrate that our proposed framework outperforms existing rule-based methods in accuracy and fidelity. The similarity between simulated and actual pedestrian trajectories increases by 10.8%, while the average error is reduced by 4%. Moreover, our framework exhibits greater adaptability and better interpretability compared to methods that rely solely on deep learning for trajectory generation.","sentences":["Traditional rule-based physical models are limited by their reliance on singular physical formulas and parameters, making it difficult to effectively tackle the intricate tasks associated with crowd simulation.","Recent research has introduced deep learning methods to tackle these issues, but most current approaches focus primarily on generating pedestrian trajectories, often lacking interpretability and failing to provide real-time dynamic simulations.","To address the aforementioned issues, we propose a novel data-driven crowd simulation framework that integrates Physics-informed Machine Learning (PIML) with navigation potential fields.","Our approach leverages the strengths of both physical models and PIML.","Specifically, we design an innovative Physics-informed Spatio-temporal Graph Convolutional Network (PI-STGCN) as a data-driven module to predict pedestrian movement trends based on crowd spatio-temporal data.","Additionally, we construct a physical model of navigation potential fields based on flow field theory to guide pedestrian movements, thereby reinforcing physical constraints during the simulation.","In our framework, navigation potential fields are dynamically computed and updated based on the movement trends predicted by the PI-STGCN, while the updated crowd dynamics, guided by these fields, subsequently feed back into the PI-STGCN.","Comparative experiments on two publicly available large-scale real-world datasets across five scenes demonstrate that our proposed framework outperforms existing rule-based methods in accuracy and fidelity.","The similarity between simulated and actual pedestrian trajectories increases by 10.8%, while the average error is reduced by 4%.","Moreover, our framework exhibits greater adaptability and better interpretability compared to methods that rely solely on deep learning for trajectory generation."],"url":"http://arxiv.org/abs/2410.16132v1"}
{"created":"2024-10-21 15:56:08","title":"Constrained Truthful Obnoxious Two-Facility Location with Optional Preferences","abstract":"We consider a truthful facility location problem with agents that have private positions on the line of real numbers and known optional preferences over two obnoxious facilities that must be placed at locations chosen from a given set of candidate ones. Each agent wants to be as far away as possible from the facilities that affect her, and our goal is to design mechanisms that decide where to place the facilities so as to maximize the total happiness of the agents as well as provide the right incentives to them to truthfully report their positions. We consider separately the setting in which all agents are affected by both facilities (i.e., they have non-optional preferences) and the general optional setting. We show tight bounds on the approximation ratio of deterministic strategyproof mechanisms for both settings, and almost tight bounds for randomized mechanisms.","sentences":["We consider a truthful facility location problem with agents that have private positions on the line of real numbers and known optional preferences over two obnoxious facilities that must be placed at locations chosen from a given set of candidate ones.","Each agent wants to be as far away as possible from the facilities that affect her, and our goal is to design mechanisms that decide where to place the facilities so as to maximize the total happiness of the agents as well as provide the right incentives to them to truthfully report their positions.","We consider separately the setting in which all agents are affected by both facilities (i.e., they have non-optional preferences) and the general optional setting.","We show tight bounds on the approximation ratio of deterministic strategyproof mechanisms for both settings, and almost tight bounds for randomized mechanisms."],"url":"http://arxiv.org/abs/2410.16131v1"}
{"created":"2024-10-21 15:55:04","title":"SMART: Self-learning Meta-strategy Agent for Reasoning Tasks","abstract":"Tasks requiring deductive reasoning, especially those involving multiple steps, often demand adaptive strategies such as intermediate generation of rationales or programs, as no single approach is universally optimal. While Language Models (LMs) can enhance their outputs through iterative self-refinement and strategy adjustments, they frequently fail to apply the most effective strategy in their first attempt. This inefficiency raises the question: Can LMs learn to select the optimal strategy in the first attempt, without a need for refinement? To address this challenge, we introduce SMART (Self-learning Meta-strategy Agent for Reasoning Tasks), a novel framework that enables LMs to autonomously learn and select the most effective strategies for various reasoning tasks. We model the strategy selection process as a Markov Decision Process and leverage reinforcement learning-driven continuous self-improvement to allow the model to find the suitable strategy to solve a given task. Unlike traditional self-refinement methods that rely on multiple inference passes or external feedback, SMART allows an LM to internalize the outcomes of its own reasoning processes and adjust its strategy accordingly, aiming for correct solutions on the first attempt. Our experiments across various reasoning datasets and with different model architectures demonstrate that SMART significantly enhances the ability of models to choose optimal strategies without external guidance (+15 points on the GSM8K dataset). By achieving higher accuracy with a single inference pass, SMART not only improves performance but also reduces computational costs for refinement-based strategies, paving the way for more efficient and intelligent reasoning in LMs.","sentences":["Tasks requiring deductive reasoning, especially those involving multiple steps, often demand adaptive strategies such as intermediate generation of rationales or programs, as no single approach is universally optimal.","While Language Models (LMs) can enhance their outputs through iterative self-refinement and strategy adjustments, they frequently fail to apply the most effective strategy in their first attempt.","This inefficiency raises the question: Can LMs learn to select the optimal strategy in the first attempt, without a need for refinement?","To address this challenge, we introduce SMART (Self-learning Meta-strategy Agent for Reasoning Tasks), a novel framework that enables LMs to autonomously learn and select the most effective strategies for various reasoning tasks.","We model the strategy selection process as a Markov Decision Process and leverage reinforcement learning-driven continuous self-improvement to allow the model to find the suitable strategy to solve a given task.","Unlike traditional self-refinement methods that rely on multiple inference passes or external feedback, SMART allows an LM to internalize the outcomes of its own reasoning processes and adjust its strategy accordingly, aiming for correct solutions on the first attempt.","Our experiments across various reasoning datasets and with different model architectures demonstrate that SMART significantly enhances the ability of models to choose optimal strategies without external guidance (+15 points on the GSM8K dataset).","By achieving higher accuracy with a single inference pass, SMART not only improves performance but also reduces computational costs for refinement-based strategies, paving the way for more efficient and intelligent reasoning in LMs."],"url":"http://arxiv.org/abs/2410.16128v1"}
{"created":"2024-10-21 15:51:30","title":"MNIST-Nd: a set of naturalistic datasets to benchmark clustering across dimensions","abstract":"Driven by advances in recording technology, large-scale high-dimensional datasets have emerged across many scientific disciplines. Especially in biology, clustering is often used to gain insights into the structure of such datasets, for instance to understand the organization of different cell types. However, clustering is known to scale poorly to high dimensions, even though the exact impact of dimensionality is unclear as current benchmark datasets are mostly two-dimensional. Here we propose MNIST-Nd, a set of synthetic datasets that share a key property of real-world datasets, namely that individual samples are noisy and clusters do not perfectly separate. MNIST-Nd is obtained by training mixture variational autoencoders with 2 to 64 latent dimensions on MNIST, resulting in six datasets with comparable structure but varying dimensionality. It thus offers the chance to disentangle the impact of dimensionality on clustering. Preliminary common clustering algorithm benchmarks on MNIST-Nd suggest that Leiden is the most robust for growing dimensions.","sentences":["Driven by advances in recording technology, large-scale high-dimensional datasets have emerged across many scientific disciplines.","Especially in biology, clustering is often used to gain insights into the structure of such datasets, for instance to understand the organization of different cell types.","However, clustering is known to scale poorly to high dimensions, even though the exact impact of dimensionality is unclear as current benchmark datasets are mostly two-dimensional.","Here we propose MNIST-Nd, a set of synthetic datasets that share a key property of real-world datasets, namely that individual samples are noisy and clusters do not perfectly separate.","MNIST-Nd is obtained by training mixture variational autoencoders with 2 to 64 latent dimensions on MNIST, resulting in six datasets with comparable structure but varying dimensionality.","It thus offers the chance to disentangle the impact of dimensionality on clustering.","Preliminary common clustering algorithm benchmarks on MNIST-Nd suggest that Leiden is the most robust for growing dimensions."],"url":"http://arxiv.org/abs/2410.16124v1"}
{"created":"2024-10-21 15:48:34","title":"Extracting Spatiotemporal Data from Gradients with Large Language Models","abstract":"Recent works show that sensitive user data can be reconstructed from gradient updates, breaking the key privacy promise of federated learning. While success was demonstrated primarily on image data, these methods do not directly transfer to other domains, such as spatiotemporal data. To understand privacy risks in spatiotemporal federated learning, we first propose Spatiotemporal Gradient Inversion Attack (ST-GIA), a gradient attack algorithm tailored to spatiotemporal data that successfully reconstructs the original location from gradients. Furthermore, the absence of priors in attacks on spatiotemporal data has hindered the accurate reconstruction of real client data. To address this limitation, we propose ST-GIA+, which utilizes an auxiliary language model to guide the search for potential locations, thereby successfully reconstructing the original data from gradients. In addition, we design an adaptive defense strategy to mitigate gradient inversion attacks in spatiotemporal federated learning. By dynamically adjusting the perturbation levels, we can offer tailored protection for varying rounds of training data, thereby achieving a better trade-off between privacy and utility than current state-of-the-art methods. Through intensive experimental analysis on three real-world datasets, we reveal that the proposed defense strategy can well preserve the utility of spatiotemporal federated learning with effective security protection.","sentences":["Recent works show that sensitive user data can be reconstructed from gradient updates, breaking the key privacy promise of federated learning.","While success was demonstrated primarily on image data, these methods do not directly transfer to other domains, such as spatiotemporal data.","To understand privacy risks in spatiotemporal federated learning, we first propose Spatiotemporal Gradient Inversion Attack (ST-GIA), a gradient attack algorithm tailored to spatiotemporal data that successfully reconstructs the original location from gradients.","Furthermore, the absence of priors in attacks on spatiotemporal data has hindered the accurate reconstruction of real client data.","To address this limitation, we propose ST-GIA+, which utilizes an auxiliary language model to guide the search for potential locations, thereby successfully reconstructing the original data from gradients.","In addition, we design an adaptive defense strategy to mitigate gradient inversion attacks in spatiotemporal federated learning.","By dynamically adjusting the perturbation levels, we can offer tailored protection for varying rounds of training data, thereby achieving a better trade-off between privacy and utility than current state-of-the-art methods.","Through intensive experimental analysis on three real-world datasets, we reveal that the proposed defense strategy can well preserve the utility of spatiotemporal federated learning with effective security protection."],"url":"http://arxiv.org/abs/2410.16121v1"}
{"created":"2024-10-21 15:47:58","title":"Learning SQL from within: integrating database exercises into the database itself","abstract":"SQL adventure builder (SQLab) is an open-source framework for creating SQL games that are embedded within the very database they query. Students' answers are evaluated using query fingerprinting, a novel technique that allows for better feedback than traditional SQL online judge systems. Fingerprints act as tokens that are used to unlock messages encrypted in an isolated auxiliary table. These messages may include hints, answer keys, examples, explanations, or narrative elements. They can also contain the problem statement of the next task, which turns them into nodes in a virtual DAG with queries as edges. This makes it possible to design a coherent adventure with a storyline of arbitrary complexity.   This paper describes the theoretical underpinnings of SQLab's query fingerprinting model, its implementation challenges, and its potential to improve SQL education through game-based learning. The underlying concepts are fully cross-vendor, and support for SQLite, PostgreSQL and MySQL is already available. As a proof of concept, two games, 30 exercises and one mock exam were tested over a three-year period with about 300 students.","sentences":["SQL adventure builder (SQLab) is an open-source framework for creating SQL games that are embedded within the very database they query.","Students' answers are evaluated using query fingerprinting, a novel technique that allows for better feedback than traditional SQL online judge systems.","Fingerprints act as tokens that are used to unlock messages encrypted in an isolated auxiliary table.","These messages may include hints, answer keys, examples, explanations, or narrative elements.","They can also contain the problem statement of the next task, which turns them into nodes in a virtual DAG with queries as edges.","This makes it possible to design a coherent adventure with a storyline of arbitrary complexity.   ","This paper describes the theoretical underpinnings of SQLab's query fingerprinting model, its implementation challenges, and its potential to improve SQL education through game-based learning.","The underlying concepts are fully cross-vendor, and support for SQLite, PostgreSQL and MySQL is already available.","As a proof of concept, two games, 30 exercises and one mock exam were tested over a three-year period with about 300 students."],"url":"http://arxiv.org/abs/2410.16120v1"}
{"created":"2024-10-21 15:47:03","title":"SeaDAG: Semi-autoregressive Diffusion for Conditional Directed Acyclic Graph Generation","abstract":"We introduce SeaDAG, a semi-autoregressive diffusion model for conditional generation of Directed Acyclic Graphs (DAGs). Considering their inherent layer-wise structure, we simulate layer-wise autoregressive generation by designing different denoising speed for different layers. Unlike conventional autoregressive generation that lacks a global graph structure view, our method maintains a complete graph structure at each diffusion step, enabling operations such as property control that require the full graph structure. Leveraging this capability, we evaluate the DAG properties during training by employing a graph property decoder. We explicitly train the model to learn graph conditioning with a condition loss, which enhances the diffusion model's capacity to generate graphs that are both realistic and aligned with specified properties. We evaluate our method on two representative conditional DAG generation tasks: (1) circuit generation from truth tables, where precise DAG structures are crucial for realizing circuit functionality, and (2) molecule generation based on quantum properties. Our approach demonstrates promising results, generating high-quality and realistic DAGs that closely align with given conditions.","sentences":["We introduce SeaDAG, a semi-autoregressive diffusion model for conditional generation of Directed Acyclic Graphs (DAGs).","Considering their inherent layer-wise structure, we simulate layer-wise autoregressive generation by designing different denoising speed for different layers.","Unlike conventional autoregressive generation that lacks a global graph structure view, our method maintains a complete graph structure at each diffusion step, enabling operations such as property control that require the full graph structure.","Leveraging this capability, we evaluate the DAG properties during training by employing a graph property decoder.","We explicitly train the model to learn graph conditioning with a condition loss, which enhances the diffusion model's capacity to generate graphs that are both realistic and aligned with specified properties.","We evaluate our method on two representative conditional DAG generation tasks: (1) circuit generation from truth tables, where precise DAG structures are crucial for realizing circuit functionality, and (2) molecule generation based on quantum properties.","Our approach demonstrates promising results, generating high-quality and realistic DAGs that closely align with given conditions."],"url":"http://arxiv.org/abs/2410.16119v1"}
{"created":"2024-10-21 15:42:27","title":"Increasing Interpretability of Neural Networks By Approximating Human Visual Saliency","abstract":"Understanding specifically where a model focuses on within an image is critical for human interpretability of the decision-making process. Deep learning-based solutions are prone to learning coincidental correlations in training datasets, causing over-fitting and reducing the explainability. Recent advances have shown that guiding models to human-defined regions of saliency within individual images significantly increases performance and interpretability. Human-guided models also exhibit greater generalization capabilities, as coincidental dataset features are avoided. Results show that models trained with saliency incorporation display an increase in interpretability of up to 30% over models trained without saliency information. The collection of this saliency information, however, can be costly, laborious and in some cases infeasible. To address this limitation, we propose a combination strategy of saliency incorporation and active learning to reduce the human annotation data required by 80% while maintaining the interpretability and performance increase from human saliency. Extensive experimentation outlines the effectiveness of the proposed approach across five public datasets and six active learning criteria.","sentences":["Understanding specifically where a model focuses on within an image is critical for human interpretability of the decision-making process.","Deep learning-based solutions are prone to learning coincidental correlations in training datasets, causing over-fitting and reducing the explainability.","Recent advances have shown that guiding models to human-defined regions of saliency within individual images significantly increases performance and interpretability.","Human-guided models also exhibit greater generalization capabilities, as coincidental dataset features are avoided.","Results show that models trained with saliency incorporation display an increase in interpretability of up to 30% over models trained without saliency information.","The collection of this saliency information, however, can be costly, laborious and in some cases infeasible.","To address this limitation, we propose a combination strategy of saliency incorporation and active learning to reduce the human annotation data required by 80% while maintaining the interpretability and performance increase from human saliency.","Extensive experimentation outlines the effectiveness of the proposed approach across five public datasets and six active learning criteria."],"url":"http://arxiv.org/abs/2410.16115v1"}
{"created":"2024-10-21 15:38:38","title":"DUMBO: Making durable read-only transactions fly on hardware transactional memory","abstract":"Despite the recent improvements in supporting Persistent Hardware Transactions (PHTs) on emerging persistent memories (PM), the poor performance of Read-Only (RO) transactions remains largely overlooked. We propose DUMBO, a new design for PHT that eliminates the two most crucial bottlenecks that hinder RO transactions in state-of-the-art PHT. At its core, DUMBO exploits advanced instructions that some contemporary HTMs provide to suspend (and resume) transactional access tracking. Our experimental evaluation with an IBM POWER9 system using the TPC-C benchmark shows that DUMBO can outperform the state of the art designs for persistent hardware (SPHT) and software memory transactions (Pisces), by up to 4.0x.","sentences":["Despite the recent improvements in supporting Persistent Hardware Transactions (PHTs) on emerging persistent memories (PM), the poor performance of Read-Only (RO) transactions remains largely overlooked.","We propose DUMBO, a new design for PHT that eliminates the two most crucial bottlenecks that hinder RO transactions in state-of-the-art PHT.","At its core, DUMBO exploits advanced instructions that some contemporary HTMs provide to suspend (and resume) transactional access tracking.","Our experimental evaluation with an IBM POWER9 system using the TPC-C benchmark shows that DUMBO can outperform the state of the art designs for persistent hardware (SPHT) and software memory transactions (Pisces), by up to 4.0x."],"url":"http://arxiv.org/abs/2410.16110v1"}
{"created":"2024-10-21 15:35:44","title":"Do LLMs write like humans? Variation in grammatical and rhetorical styles","abstract":"Large language models (LLMs) are capable of writing grammatical text that follows instructions, answers questions, and solves problems. As they have advanced, it has become difficult to distinguish their output from human-written text. While past research has found some differences in surface features such as word choice and punctuation, and developed classifiers to detect LLM output, none has studied the rhetorical styles of LLMs.   Using several variants of Llama 3 and GPT-4o, we construct two parallel corpora of human- and LLM-written texts from common prompts. Using Douglas Biber's set of lexical, grammatical, and rhetorical features, we identify systematic differences between LLMs and humans and between different LLMs. These differences persist when moving from smaller models to larger ones, and are larger for instruction-tuned models than base models. This demonstrates that despite their advanced abilities, LLMs struggle to match human styles, and hence more advanced linguistic features can detect patterns in their behavior not previously recognized.","sentences":["Large language models (LLMs) are capable of writing grammatical text that follows instructions, answers questions, and solves problems.","As they have advanced, it has become difficult to distinguish their output from human-written text.","While past research has found some differences in surface features such as word choice and punctuation, and developed classifiers to detect LLM output, none has studied the rhetorical styles of LLMs.   ","Using several variants of Llama 3 and GPT-4o, we construct two parallel corpora of human- and LLM-written texts from common prompts.","Using Douglas Biber's set of lexical, grammatical, and rhetorical features, we identify systematic differences between LLMs and humans and between different LLMs.","These differences persist when moving from smaller models to larger ones, and are larger for instruction-tuned models than base models.","This demonstrates that despite their advanced abilities, LLMs struggle to match human styles, and hence more advanced linguistic features can detect patterns in their behavior not previously recognized."],"url":"http://arxiv.org/abs/2410.16107v1"}
{"created":"2024-10-21 15:34:33","title":"Addressing Spectral Bias of Deep Neural Networks by Multi-Grade Deep Learning","abstract":"Deep neural networks (DNNs) suffer from the spectral bias, wherein DNNs typically exhibit a tendency to prioritize the learning of lower-frequency components of a function, struggling to capture its high-frequency features. This paper is to address this issue. Notice that a function having only low frequency components may be well-represented by a shallow neural network (SNN), a network having only a few layers. By observing that composition of low frequency functions can effectively approximate a high-frequency function, we propose to learn a function containing high-frequency components by composing several SNNs, each of which learns certain low-frequency information from the given data. We implement the proposed idea by exploiting the multi-grade deep learning (MGDL) model, a recently introduced model that trains a DNN incrementally, grade by grade, a current grade learning from the residue of the previous grade only an SNN composed with the SNNs trained in the preceding grades as features. We apply MGDL to synthetic, manifold, colored images, and MNIST datasets, all characterized by presence of high-frequency features. Our study reveals that MGDL excels at representing functions containing high-frequency information. Specifically, the neural networks learned in each grade adeptly capture some low-frequency information, allowing their compositions with SNNs learned in the previous grades effectively representing the high-frequency features. Our experimental results underscore the efficacy of MGDL in addressing the spectral bias inherent in DNNs. By leveraging MGDL, we offer insights into overcoming spectral bias limitation of DNNs, thereby enhancing the performance and applicability of deep learning models in tasks requiring the representation of high-frequency information. This study confirms that the proposed method offers a promising solution to address the spectral bias of DNNs.","sentences":["Deep neural networks (DNNs) suffer from the spectral bias, wherein DNNs typically exhibit a tendency to prioritize the learning of lower-frequency components of a function, struggling to capture its high-frequency features.","This paper is to address this issue.","Notice that a function having only low frequency components may be well-represented by a shallow neural network (SNN), a network having only a few layers.","By observing that composition of low frequency functions can effectively approximate a high-frequency function, we propose to learn a function containing high-frequency components by composing several SNNs, each of which learns certain low-frequency information from the given data.","We implement the proposed idea by exploiting the multi-grade deep learning (MGDL) model, a recently introduced model that trains a DNN incrementally, grade by grade, a current grade learning from the residue of the previous grade only an SNN composed with the SNNs trained in the preceding grades as features.","We apply MGDL to synthetic, manifold, colored images, and MNIST datasets, all characterized by presence of high-frequency features.","Our study reveals that MGDL excels at representing functions containing high-frequency information.","Specifically, the neural networks learned in each grade adeptly capture some low-frequency information, allowing their compositions with SNNs learned in the previous grades effectively representing the high-frequency features.","Our experimental results underscore the efficacy of MGDL in addressing the spectral bias inherent in DNNs.","By leveraging MGDL, we offer insights into overcoming spectral bias limitation of DNNs, thereby enhancing the performance and applicability of deep learning models in tasks requiring the representation of high-frequency information.","This study confirms that the proposed method offers a promising solution to address the spectral bias of DNNs."],"url":"http://arxiv.org/abs/2410.16105v1"}
{"created":"2024-10-21 15:32:42","title":"A Deep Unfolding-Based Scalarization Approach for Power Control in D2D Networks","abstract":"Optimizing network utility in device-to-device networks is typically formulated as a non-convex optimization problem. This paper addresses the scenario where the optimization variables are from a bounded but continuous set, allowing each device to perform power control. The power at each link is optimized to maximize a desired network utility. Specifically, we consider the weighted-sum-rate. The state of the art benchmark for this problem is fractional programming with quadratic transform, known as FPLinQ. We propose a scalarization approach to transform the weighted-sum-rate, developing an iterative algorithm that depends on step sizes, a reference, and a direction vector. By employing the deep unfolding approach, we optimize these parameters by presenting the iterative algorithm as a finite sequence of steps, enabling it to be trained as a deep neural network. Numerical experiments demonstrate that the unfolded algorithm performs comparably to the benchmark in most cases while exhibiting lower complexity. Furthermore, the unfolded algorithm shows strong generalizability in terms of varying the number of users, the signal-to-noise ratio and arbitrary weights. The weighted-sum-rate maximizer can be integrated into a low-complexity fairness scheduler, updating priority weights via virtual queues and Lyapunov Drift Plus Penalty. This is demonstrated through experiments using proportional and max-min fairness.","sentences":["Optimizing network utility in device-to-device networks is typically formulated as a non-convex optimization problem.","This paper addresses the scenario where the optimization variables are from a bounded but continuous set, allowing each device to perform power control.","The power at each link is optimized to maximize a desired network utility.","Specifically, we consider the weighted-sum-rate.","The state of the art benchmark for this problem is fractional programming with quadratic transform, known as FPLinQ.","We propose a scalarization approach to transform the weighted-sum-rate, developing an iterative algorithm that depends on step sizes, a reference, and a direction vector.","By employing the deep unfolding approach, we optimize these parameters by presenting the iterative algorithm as a finite sequence of steps, enabling it to be trained as a deep neural network.","Numerical experiments demonstrate that the unfolded algorithm performs comparably to the benchmark in most cases while exhibiting lower complexity.","Furthermore, the unfolded algorithm shows strong generalizability in terms of varying the number of users, the signal-to-noise ratio and arbitrary weights.","The weighted-sum-rate maximizer can be integrated into a low-complexity fairness scheduler, updating priority weights via virtual queues and Lyapunov Drift Plus Penalty.","This is demonstrated through experiments using proportional and max-min fairness."],"url":"http://arxiv.org/abs/2410.16104v1"}
{"created":"2024-10-21 15:31:06","title":"LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics","abstract":"We introduce LDAdam, a memory-efficient optimizer for training large models, that performs adaptive optimization steps within lower dimensional subspaces, while consistently exploring the full parameter space during training. This strategy keeps the optimizer's memory footprint to a fraction of the model size. LDAdam relies on a new projection-aware update rule for the optimizer states that allows for transitioning between subspaces, i.e., estimation of the statistics of the projected gradients. To mitigate the errors due to low-rank projection, LDAdam integrates a new generalized error feedback mechanism, which explicitly accounts for both gradient and optimizer state compression. We prove the convergence of LDAdam under standard assumptions, and show that LDAdam allows for accurate and efficient fine-tuning and pre-training of language models.","sentences":["We introduce LDAdam, a memory-efficient optimizer for training large models, that performs adaptive optimization steps within lower dimensional subspaces, while consistently exploring the full parameter space during training.","This strategy keeps the optimizer's memory footprint to a fraction of the model size.","LDAdam relies on a new projection-aware update rule for the optimizer states that allows for transitioning between subspaces, i.e., estimation of the statistics of the projected gradients.","To mitigate the errors due to low-rank projection, LDAdam integrates a new generalized error feedback mechanism, which explicitly accounts for both gradient and optimizer state compression.","We prove the convergence of LDAdam under standard assumptions, and show that LDAdam allows for accurate and efficient fine-tuning and pre-training of language models."],"url":"http://arxiv.org/abs/2410.16103v1"}
{"created":"2024-10-21 15:30:46","title":"Semantics of Sets of Programs","abstract":"Applications like program synthesis sometimes require proving that a property holds for all of the infinitely many programs described by a grammar - i.e., an inductively defined set of programs. Current verification frameworks overapproximate programs' behavior when sets of programs contain loops, including two Hoare-style logics that fail to be relatively complete when loops are allowed. In this work, we prove that compositionally verifying simple properties for infinite sets of programs requires tracking distinct program behaviors over unboundedly many executions. Tracking this information is both necessary and sufficient for verification. We prove this fact in a general, reusable theory of denotational semantics that can model the expressivity and compositionality of verification techniques over infinite sets of programs. We construct the minimal compositional semantics that captures simple properties of sets of programs and use it to derive the first sound and relatively complete Hoare-style logic for infinite sets of programs. Thus, our methods can be used to design minimally complex, compositional verification techniques for sets of programs.","sentences":["Applications like program synthesis sometimes require proving that a property holds for all of the infinitely many programs described by a grammar - i.e., an inductively defined set of programs.","Current verification frameworks overapproximate programs' behavior when sets of programs contain loops, including two Hoare-style logics that fail to be relatively complete when loops are allowed",".","In this work, we prove that compositionally verifying simple properties for infinite sets of programs requires tracking distinct program behaviors over unboundedly many executions.","Tracking this information is both necessary and sufficient for verification.","We prove this fact in a general, reusable theory of denotational semantics that can model the expressivity and compositionality of verification techniques over infinite sets of programs.","We construct the minimal compositional semantics that captures simple properties of sets of programs and use it to derive the first sound and relatively complete Hoare-style logic for infinite sets of programs.","Thus, our methods can be used to design minimally complex, compositional verification techniques for sets of programs."],"url":"http://arxiv.org/abs/2410.16102v1"}
{"created":"2024-10-21 15:27:18","title":"ExDBN: Exact learning of Dynamic Bayesian Networks","abstract":"Causal learning from data has received much attention in recent years. One way of capturing causal relationships is by utilizing Bayesian networks. There, one recovers a weighted directed acyclic graph, in which random variables are represented by vertices, and the weights associated with each edge represent the strengths of the causal relationships between them. This concept is extended to capture dynamic effects by introducing a dependency on past data, which may be captured by the structural equation model, which is utilized in the present contribution to formulate a score-based learning approach. A mixed-integer quadratic program is formulated and an algorithmic solution proposed, in which the pre-generation of exponentially many acyclicity constraints is avoided by utilizing the so-called branch-and-cut (\"lazy constraint\") method. Comparing the novel approach to the state of the art, we show that the proposed approach turns out to produce excellent results when applied to small and medium-sized synthetic instances of up to 25 time-series. Lastly, two interesting applications in bio-science and finance, to which the method is directly applied, further stress the opportunities in developing highly accurate, globally convergent solvers that can handle modest instances.","sentences":["Causal learning from data has received much attention in recent years.","One way of capturing causal relationships is by utilizing Bayesian networks.","There, one recovers a weighted directed acyclic graph, in which random variables are represented by vertices, and the weights associated with each edge represent the strengths of the causal relationships between them.","This concept is extended to capture dynamic effects by introducing a dependency on past data, which may be captured by the structural equation model, which is utilized in the present contribution to formulate a score-based learning approach.","A mixed-integer quadratic program is formulated and an algorithmic solution proposed, in which the pre-generation of exponentially many acyclicity constraints is avoided by utilizing the so-called branch-and-cut (\"lazy constraint\") method.","Comparing the novel approach to the state of the art, we show that the proposed approach turns out to produce excellent results when applied to small and medium-sized synthetic instances of up to 25 time-series.","Lastly, two interesting applications in bio-science and finance, to which the method is directly applied, further stress the opportunities in developing highly accurate, globally convergent solvers that can handle modest instances."],"url":"http://arxiv.org/abs/2410.16100v1"}
{"created":"2024-10-21 15:23:58","title":"Defending Against Attack on the Cloned: In-Band Active Man-in-the-Middle Detection for the Signal Protocol","abstract":"With Signal's position as one of the most popular secure messaging protocols in use today, the threat of government coercion and mass surveillance, i.e., active Man-in-the-Middle (MitM) attacks, are more relevant than ever. On the other hand, studies [29, 33, 37, 38] have shown that user awareness is very poor when it comes to authenticating keys in instant messaging applications, e.g., comparing key fingerprints out-of-band. The ideal solution to this problem should not require the active participation of the users. Our solution to active MitM attacks builds directly on Signal. We automate the process of key confirmation without relying on the intervention of users, and without using an out-of-band communication channel, at the cost of slightly altered trust assumptions on the server. We consider a powerful active MitM that not only controls the communication channel, but also has (one time) access to all secrets on one of the clients, i.e., can perform a key compromise attack. Our solution utilises the server to keep track of the changes in the clients key fingerprint as ratcheting is performed. Given that the server can keep a message log already, we find that any impact on deniability is minimal in practice. We present our detailed modifications to Signal, and document the new security guarantees while preserving the existing security guarantees of Signal. Our proof-of-concept implementation, which is based on the open-source Signal library used in real-world instant messaging applications, shows that our solution is practical and integrates well with the library. Our experimental results further show that our solution only has a tiny performance overhead when compared to Signal.","sentences":["With Signal's position as one of the most popular secure messaging protocols in use today, the threat of government coercion and mass surveillance, i.e., active Man-in-the-Middle (MitM) attacks, are more relevant than ever.","On the other hand, studies [29, 33, 37, 38] have shown that user awareness is very poor when it comes to authenticating keys in instant messaging applications, e.g., comparing key fingerprints out-of-band.","The ideal solution to this problem should not require the active participation of the users.","Our solution to active MitM attacks builds directly on Signal.","We automate the process of key confirmation without relying on the intervention of users, and without using an out-of-band communication channel, at the cost of slightly altered trust assumptions on the server.","We consider a powerful active MitM that not only controls the communication channel, but also has (one time) access to all secrets on one of the clients, i.e., can perform a key compromise attack.","Our solution utilises the server to keep track of the changes in the clients key fingerprint as ratcheting is performed.","Given that the server can keep a message log already, we find that any impact on deniability is minimal in practice.","We present our detailed modifications to Signal, and document the new security guarantees while preserving the existing security guarantees of Signal.","Our proof-of-concept implementation, which is based on the open-source Signal library used in real-world instant messaging applications, shows that our solution is practical and integrates well with the library.","Our experimental results further show that our solution only has a tiny performance overhead when compared to Signal."],"url":"http://arxiv.org/abs/2410.16098v1"}
{"created":"2024-10-21 15:20:02","title":"LMHaze: Intensity-aware Image Dehazing with a Large-scale Multi-intensity Real Haze Dataset","abstract":"Image dehazing has drawn a significant attention in recent years. Learning-based methods usually require paired hazy and corresponding ground truth (haze-free) images for training. However, it is difficult to collect real-world image pairs, which prevents developments of existing methods. Although several works partially alleviate this issue by using synthetic datasets or small-scale real datasets. The haze intensity distribution bias and scene homogeneity in existing datasets limit the generalization ability of these methods, particularly when encountering images with previously unseen haze intensities. In this work, we present LMHaze, a large-scale, high-quality real-world dataset. LMHaze comprises paired hazy and haze-free images captured in diverse indoor and outdoor environments, spanning multiple scenarios and haze intensities. It contains over 5K high-resolution image pairs, surpassing the size of the biggest existing real-world dehazing dataset by over 25 times. Meanwhile, to better handle images with different haze intensities, we propose a mixture-of-experts model based on Mamba (MoE-Mamba) for dehazing, which dynamically adjusts the model parameters according to the haze intensity. Moreover, with our proposed dataset, we conduct a new large multimodal model (LMM)-based benchmark study to simulate human perception for evaluating dehazed images. Experiments demonstrate that LMHaze dataset improves the dehazing performance in real scenarios and our dehazing method provides better results compared to state-of-the-art methods.","sentences":["Image dehazing has drawn a significant attention in recent years.","Learning-based methods usually require paired hazy and corresponding ground truth (haze-free) images for training.","However, it is difficult to collect real-world image pairs, which prevents developments of existing methods.","Although several works partially alleviate this issue by using synthetic datasets or small-scale real datasets.","The haze intensity distribution bias and scene homogeneity in existing datasets limit the generalization ability of these methods, particularly when encountering images with previously unseen haze intensities.","In this work, we present LMHaze, a large-scale, high-quality real-world dataset.","LMHaze comprises paired hazy and haze-free images captured in diverse indoor and outdoor environments, spanning multiple scenarios and haze intensities.","It contains over 5K high-resolution image pairs, surpassing the size of the biggest existing real-world dehazing dataset by over 25 times.","Meanwhile, to better handle images with different haze intensities, we propose a mixture-of-experts model based on Mamba (MoE-Mamba) for dehazing, which dynamically adjusts the model parameters according to the haze intensity.","Moreover, with our proposed dataset, we conduct a new large multimodal model (LMM)-based benchmark study to simulate human perception for evaluating dehazed images.","Experiments demonstrate that LMHaze dataset improves the dehazing performance in real scenarios and our dehazing method provides better results compared to state-of-the-art methods."],"url":"http://arxiv.org/abs/2410.16095v1"}
{"created":"2024-10-21 15:18:21","title":"Streaming and Communication Complexity of Load-Balancing via Matching Contractors","abstract":"In the load-balancing problem, we have an $n$-vertex bipartite graph $G=(L, R, E)$ between a set of clients and servers. The goal is to find an assignment of all clients to the servers, while minimizing the maximum load on each server, where load of a server is the number of clients assigned to it. We study load-balancing in the one-way communication model: the edges of the input graph are partitioned between Alice and Bob, and Alice needs to send a message to Bob for him to output the solution.   We show that settling the one-way communication complexity of load-balancing is equivalent to a natural sparsification problem for load-balancing. We then prove a dual interpretation of this sparsifier, showing that the minimum density of a sparsifier is effectively the same as the maximum density one can achieve for an extremal graph family that is new to this paper, called Matching-Contractors; these graphs are intimately connected to the well-known Ruzsa-Szemeredi graphs and generalize them in certain aspects. Our chain of equivalences thus shows that the one-way communication complexity of load-balancing can be reduced to a purely graph theoretic question: what is the maximum density of a Matching-Contractor on $n$ vertices?   Finally, we present a novel combinatorial construction of some-what dense Matching-Contractors, which implies a strong one-way communication lower bound for load-balancing: any one-way protocol (even randomized) with $\\tilde{O}(n)$ communication cannot achieve a better than $n^{\\frac14-o(1)}$-approximation. Previously, no non-trivial lower bounds were known for protocols with even $O(n\\log{n})$ bits of communication. Our result also implies the first non-trivial lower bounds for semi-streaming load-balancing in the edge-arrival model, ruling out $n^{\\frac14-o(1)}$-approximation in a single-pass.","sentences":["In the load-balancing problem, we have an $n$-vertex bipartite graph $G=(L, R, E)$ between a set of clients and servers.","The goal is to find an assignment of all clients to the servers, while minimizing the maximum load on each server, where load of a server is the number of clients assigned to it.","We study load-balancing in the one-way communication model: the edges of the input graph are partitioned between Alice and Bob, and Alice needs to send a message to Bob for him to output the solution.   ","We show that settling the one-way communication complexity of load-balancing is equivalent to a natural sparsification problem for load-balancing.","We then prove a dual interpretation of this sparsifier, showing that the minimum density of a sparsifier is effectively the same as the maximum density one can achieve for an extremal graph family that is new to this paper, called Matching-Contractors; these graphs are intimately connected to the well-known Ruzsa-Szemeredi graphs and generalize them in certain aspects.","Our chain of equivalences thus shows that the one-way communication complexity of load-balancing can be reduced to a purely graph theoretic question: what is the maximum density of a Matching-Contractor on $n$ vertices?   ","Finally, we present a novel combinatorial construction of some-what dense Matching-Contractors, which implies a strong one-way communication lower bound for load-balancing: any one-way protocol (even randomized) with $\\tilde{O}(n)$ communication cannot achieve a better than $n^{\\frac14-o(1)}$-approximation.","Previously, no non-trivial lower bounds were known for protocols with even $O(n\\log{n})$ bits of communication.","Our result also implies the first non-trivial lower bounds for semi-streaming load-balancing in the edge-arrival model, ruling out $n^{\\frac14-o(1)}$-approximation in a single-pass."],"url":"http://arxiv.org/abs/2410.16094v1"}
{"created":"2024-10-21 15:16:00","title":"Final Report for CHESS: Cloud, High-Performance Computing, and Edge for Science and Security","abstract":"Automating the theory-experiment cycle requires effective distributed workflows that utilize a computing continuum spanning lab instruments, edge sensors, computing resources at multiple facilities, data sets distributed across multiple information sources, and potentially cloud. Unfortunately, the obvious methods for constructing continuum platforms, orchestrating workflow tasks, and curating datasets over time fail to achieve scientific requirements for performance, energy, security, and reliability. Furthermore, achieving the best use of continuum resources depends upon the efficient composition and execution of workflow tasks, i.e., combinations of numerical solvers, data analytics, and machine learning. Pacific Northwest National Laboratory's LDRD \"Cloud, High-Performance Computing (HPC), and Edge for Science and Security\" (CHESS) has developed a set of interrelated capabilities for enabling distributed scientific workflows and curating datasets. This report describes the results and successes of CHESS from the perspective of open science.","sentences":["Automating the theory-experiment cycle requires effective distributed workflows that utilize a computing continuum spanning lab instruments, edge sensors, computing resources at multiple facilities, data sets distributed across multiple information sources, and potentially cloud.","Unfortunately, the obvious methods for constructing continuum platforms, orchestrating workflow tasks, and curating datasets over time fail to achieve scientific requirements for performance, energy, security, and reliability.","Furthermore, achieving the best use of continuum resources depends upon the efficient composition and execution of workflow tasks, i.e., combinations of numerical solvers, data analytics, and machine learning.","Pacific Northwest National Laboratory's LDRD \"Cloud, High-Performance Computing (HPC), and Edge for Science and Security\" (CHESS) has developed a set of interrelated capabilities for enabling distributed scientific workflows and curating datasets.","This report describes the results and successes of CHESS from the perspective of open science."],"url":"http://arxiv.org/abs/2410.16093v1"}
{"created":"2024-10-21 15:13:32","title":"ChangeGuard: Validating Code Changes via Pairwise Learning-Guided Execution","abstract":"Code changes are an integral part of the software development process. Many code changes are meant to improve the code without changing its functional behavior, e.g., refactorings and performance improvements. Unfortunately, validating whether a code change preserves the behavior is non-trivial, particularly when the code change is performed deep inside a complex project. This paper presents ChangeGuard, an approach that uses learning-guided execution to compare the runtime behavior of a modified function. The approach is enabled by the novel concept of pairwise learning-guided execution and by a set of techniques that improve the robustness and coverage of the state-of-the-art learning-guided execution technique. Our evaluation applies ChangeGuard to a dataset of 224 manually annotated code changes from popular Python open-source projects and to three datasets of code changes obtained by applying automated code transformations. Our results show that the approach identifies semantics-changing code changes with a precision of 77.1% and a recall of 69.5%, and that it detects unexpected behavioral changes introduced by automatic code refactoring tools. In contrast, the existing regression tests of the analyzed projects miss the vast majority of semantics-changing code changes, with a recall of only 7.6%. We envision our approach being useful for detecting unintended behavioral changes early in the development process and for improving the quality of automated code transformations.","sentences":["Code changes are an integral part of the software development process.","Many code changes are meant to improve the code without changing its functional behavior, e.g., refactorings and performance improvements.","Unfortunately, validating whether a code change preserves the behavior is non-trivial, particularly when the code change is performed deep inside a complex project.","This paper presents ChangeGuard, an approach that uses learning-guided execution to compare the runtime behavior of a modified function.","The approach is enabled by the novel concept of pairwise learning-guided execution and by a set of techniques that improve the robustness and coverage of the state-of-the-art learning-guided execution technique.","Our evaluation applies ChangeGuard to a dataset of 224 manually annotated code changes from popular Python open-source projects and to three datasets of code changes obtained by applying automated code transformations.","Our results show that the approach identifies semantics-changing code changes with a precision of 77.1% and a recall of 69.5%, and that it detects unexpected behavioral changes introduced by automatic code refactoring tools.","In contrast, the existing regression tests of the analyzed projects miss the vast majority of semantics-changing code changes, with a recall of only 7.6%.","We envision our approach being useful for detecting unintended behavioral changes early in the development process and for improving the quality of automated code transformations."],"url":"http://arxiv.org/abs/2410.16092v1"}
{"created":"2024-10-21 15:12:51","title":"Analysing the Residual Stream of Language Models Under Knowledge Conflicts","abstract":"Large language models (LLMs) can store a significant amount of factual knowledge in their parameters. However, their parametric knowledge may conflict with the information provided in the context. Such conflicts can lead to undesirable model behaviour, such as reliance on outdated or incorrect information. In this work, we investigate whether LLMs can identify knowledge conflicts and whether it is possible to know which source of knowledge the model will rely on by analysing the residual stream of the LLM. Through probing tasks, we find that LLMs can internally register the signal of knowledge conflict in the residual stream, which can be accurately detected by probing the intermediate model activations. This allows us to detect conflicts within the residual stream before generating the answers without modifying the input or model parameters. Moreover, we find that the residual stream shows significantly different patterns when the model relies on contextual knowledge versus parametric knowledge to resolve conflicts. This pattern can be employed to estimate the behaviour of LLMs when conflict happens and prevent unexpected answers before producing the answers. Our analysis offers insights into how LLMs internally manage knowledge conflicts and provides a foundation for developing methods to control the knowledge selection processes.","sentences":["Large language models (LLMs) can store a significant amount of factual knowledge in their parameters.","However, their parametric knowledge may conflict with the information provided in the context.","Such conflicts can lead to undesirable model behaviour, such as reliance on outdated or incorrect information.","In this work, we investigate whether LLMs can identify knowledge conflicts and whether it is possible to know which source of knowledge the model will rely on by analysing the residual stream of the LLM.","Through probing tasks, we find that LLMs can internally register the signal of knowledge conflict in the residual stream, which can be accurately detected by probing the intermediate model activations.","This allows us to detect conflicts within the residual stream before generating the answers without modifying the input or model parameters.","Moreover, we find that the residual stream shows significantly different patterns when the model relies on contextual knowledge versus parametric knowledge to resolve conflicts.","This pattern can be employed to estimate the behaviour of LLMs when conflict happens and prevent unexpected answers before producing the answers.","Our analysis offers insights into how LLMs internally manage knowledge conflicts and provides a foundation for developing methods to control the knowledge selection processes."],"url":"http://arxiv.org/abs/2410.16090v1"}
{"created":"2024-10-21 15:12:37","title":"Multi-Sensor Fusion for UAV Classification Based on Feature Maps of Image and Radar Data","abstract":"The unique cost, flexibility, speed, and efficiency of modern UAVs make them an attractive choice in many applications in contemporary society. This, however, causes an ever-increasing number of reported malicious or accidental incidents, rendering the need for the development of UAV detection and classification mechanisms essential. We propose a methodology for developing a system that fuses already processed multi-sensor data into a new Deep Neural Network to increase its classification accuracy towards UAV detection. The DNN model fuses high-level features extracted from individual object detection and classification models associated with thermal, optronic, and radar data. Additionally, emphasis is given to the model's Convolutional Neural Network (CNN) based architecture that combines the features of the three sensor modalities by stacking the extracted image features of the thermal and optronic sensor achieving higher classification accuracy than each sensor alone.","sentences":["The unique cost, flexibility, speed, and efficiency of modern UAVs make them an attractive choice in many applications in contemporary society.","This, however, causes an ever-increasing number of reported malicious or accidental incidents, rendering the need for the development of UAV detection and classification mechanisms essential.","We propose a methodology for developing a system that fuses already processed multi-sensor data into a new Deep Neural Network to increase its classification accuracy towards UAV detection.","The DNN model fuses high-level features extracted from individual object detection and classification models associated with thermal, optronic, and radar data.","Additionally, emphasis is given to the model's Convolutional Neural Network (CNN) based architecture that combines the features of the three sensor modalities by stacking the extracted image features of the thermal and optronic sensor achieving higher classification accuracy than each sensor alone."],"url":"http://arxiv.org/abs/2410.16089v1"}
{"created":"2024-10-21 15:12:20","title":"Fine-Tuning LLMs for Reliable Medical Question-Answering Services","abstract":"We present an advanced approach to medical question-answering (QA) services, using fine-tuned Large Language Models (LLMs) to improve the accuracy and reliability of healthcare information. Our study focuses on optimizing models like LLaMA-2 and Mistral, which have shown great promise in delivering precise, reliable medical answers. By leveraging comprehensive datasets, we applied fine-tuning techniques such as rsDoRA+ and ReRAG. rsDoRA+ enhances model performance through a combination of decomposed model weights, varied learning rates for low-rank matrices, and rank stabilization, leading to improved efficiency. ReRAG, which integrates retrieval on demand and question rewriting, further refines the accuracy of the responses. This approach enables healthcare providers to access fast, dependable information, aiding in more efficient decision-making and fostering greater patient trust. Our work highlights the potential of fine-tuned LLMs to significantly improve the quality and accessibility of medical information services, ultimately contributing to better healthcare outcomes for all.","sentences":["We present an advanced approach to medical question-answering (QA) services, using fine-tuned Large Language Models (LLMs) to improve the accuracy and reliability of healthcare information.","Our study focuses on optimizing models like LLaMA-2 and Mistral, which have shown great promise in delivering precise, reliable medical answers.","By leveraging comprehensive datasets, we applied fine-tuning techniques such as rsDoRA+ and ReRAG.","rsDoRA+ enhances model performance through a combination of decomposed model weights, varied learning rates for low-rank matrices, and rank stabilization, leading to improved efficiency.","ReRAG, which integrates retrieval on demand and question rewriting, further refines the accuracy of the responses.","This approach enables healthcare providers to access fast, dependable information, aiding in more efficient decision-making and fostering greater patient trust.","Our work highlights the potential of fine-tuned LLMs to significantly improve the quality and accessibility of medical information services, ultimately contributing to better healthcare outcomes for all."],"url":"http://arxiv.org/abs/2410.16088v1"}
{"created":"2024-10-21 15:02:30","title":"Critical Example Mining for Vehicle Trajectory Prediction using Flow-based Generative Models","abstract":"Precise trajectory prediction in complex driving scenarios is essential for autonomous vehicles. In practice, different driving scenarios present varying levels of difficulty for trajectory prediction models. However, most existing research focuses on the average precision of prediction results, while ignoring the underlying distribution of the input scenarios. This paper proposes a critical example mining method that utilizes a data-driven approach to estimate the rareness of the trajectories. By combining the rareness estimation of observations with whole trajectories, the proposed method effectively identifies a subset of data that is relatively hard to predict BEFORE feeding them to a specific prediction model. The experimental results show that the mined subset has higher prediction error when applied to different downstream prediction models, which reaches +108.1% error (greater than two times compared to the average on dataset) when mining 5% samples. Further analysis indicates that the mined critical examples include uncommon cases such as sudden brake and cancelled lane-change, which helps to better understand and improve the performance of prediction models.","sentences":["Precise trajectory prediction in complex driving scenarios is essential for autonomous vehicles.","In practice, different driving scenarios present varying levels of difficulty for trajectory prediction models.","However, most existing research focuses on the average precision of prediction results, while ignoring the underlying distribution of the input scenarios.","This paper proposes a critical example mining method that utilizes a data-driven approach to estimate the rareness of the trajectories.","By combining the rareness estimation of observations with whole trajectories, the proposed method effectively identifies a subset of data that is relatively hard to predict BEFORE feeding them to a specific prediction model.","The experimental results show that the mined subset has higher prediction error when applied to different downstream prediction models, which reaches +108.1% error (greater than two times compared to the average on dataset) when mining 5% samples.","Further analysis indicates that the mined critical examples include uncommon cases such as sudden brake and cancelled lane-change, which helps to better understand and improve the performance of prediction models."],"url":"http://arxiv.org/abs/2410.16083v1"}
{"created":"2024-10-21 14:58:38","title":"Unleashing the Potential of Multi-Channel Fusion in Retrieval for Personalized Recommendations","abstract":"Recommender systems (RS) are pivotal in managing information overload in modern digital services. A key challenge in RS is efficiently processing vast item pools to deliver highly personalized recommendations under strict latency constraints. Multi-stage cascade ranking addresses this by employing computationally efficient retrieval methods to cover diverse user interests, followed by more precise ranking models to refine the results. In the retrieval stage, multi-channel retrieval is often used to generate distinct item subsets from different candidate generators, leveraging the complementary strengths of these methods to maximize coverage. However, forwarding all retrieved items overwhelms downstream rankers, necessitating truncation. Despite advancements in individual retrieval methods, multi-channel fusion, the process of efficiently merging multi-channel retrieval results, remains underexplored. We are the first to identify and systematically investigate multi-channel fusion in the retrieval stage. Current industry practices often rely on heuristic approaches and manual designs, which often lead to suboptimal performance. Moreover, traditional gradient-based methods like SGD are unsuitable for this task due to the non-differentiable nature of the selection process. In this paper, we explore advanced channel fusion strategies by assigning systematically optimized weights to each channel. We utilize black-box optimization techniques, including the Cross Entropy Method and Bayesian Optimization for global weight optimization, alongside policy gradient-based approaches for personalized merging. Our methods enhance both personalization and flexibility, achieving significant performance improvements across multiple datasets and yielding substantial gains in real-world deployments, offering a scalable solution for optimizing multi-channel fusion in retrieval.","sentences":["Recommender systems (RS) are pivotal in managing information overload in modern digital services.","A key challenge in RS is efficiently processing vast item pools to deliver highly personalized recommendations under strict latency constraints.","Multi-stage cascade ranking addresses this by employing computationally efficient retrieval methods to cover diverse user interests, followed by more precise ranking models to refine the results.","In the retrieval stage, multi-channel retrieval is often used to generate distinct item subsets from different candidate generators, leveraging the complementary strengths of these methods to maximize coverage.","However, forwarding all retrieved items overwhelms downstream rankers, necessitating truncation.","Despite advancements in individual retrieval methods, multi-channel fusion, the process of efficiently merging multi-channel retrieval results, remains underexplored.","We are the first to identify and systematically investigate multi-channel fusion in the retrieval stage.","Current industry practices often rely on heuristic approaches and manual designs, which often lead to suboptimal performance.","Moreover, traditional gradient-based methods like SGD are unsuitable for this task due to the non-differentiable nature of the selection process.","In this paper, we explore advanced channel fusion strategies by assigning systematically optimized weights to each channel.","We utilize black-box optimization techniques, including the Cross Entropy Method and Bayesian Optimization for global weight optimization, alongside policy gradient-based approaches for personalized merging.","Our methods enhance both personalization and flexibility, achieving significant performance improvements across multiple datasets and yielding substantial gains in real-world deployments, offering a scalable solution for optimizing multi-channel fusion in retrieval."],"url":"http://arxiv.org/abs/2410.16080v1"}
{"created":"2024-10-21 14:55:59","title":"CartesianMoE: Boosting Knowledge Sharing among Experts via Cartesian Product Routing in Mixture-of-Experts","abstract":"Large language models (LLM) have been attracting much attention from the community recently, due to their remarkable performance in all kinds of downstream tasks. According to the well-known scaling law, scaling up a dense LLM enhances its capabilities, but also significantly increases the computational complexity. Mixture-of-Experts (MoE) models address that by allowing the model size to grow without substantially raising training or inference costs. Yet MoE models face challenges regarding knowledge sharing among experts, making their performance somehow sensitive to routing accuracy. To tackle that, previous works introduced shared experts and combined their outputs with those of the top $K$ routed experts in an ``addition'' manner. In this paper, inspired by collective matrix factorization to learn shared knowledge among data, we propose CartesianMoE, which implements more effective knowledge sharing among experts in more like a ``multiplication'' manner. Extensive experimental results indicate that CartesianMoE outperforms previous MoE models for building LLMs, in terms of both perplexity and downstream task performance. And we also find that CartesianMoE achieves better expert routing robustness.","sentences":["Large language models (LLM) have been attracting much attention from the community recently, due to their remarkable performance in all kinds of downstream tasks.","According to the well-known scaling law, scaling up a dense LLM enhances its capabilities, but also significantly increases the computational complexity.","Mixture-of-Experts (MoE) models address that by allowing the model size to grow without substantially raising training or inference costs.","Yet MoE models face challenges regarding knowledge sharing among experts, making their performance somehow sensitive to routing accuracy.","To tackle that, previous works introduced shared experts and combined their outputs with those of the top $K$ routed experts in an ``addition'' manner.","In this paper, inspired by collective matrix factorization to learn shared knowledge among data, we propose CartesianMoE, which implements more effective knowledge sharing among experts in more like a ``multiplication'' manner.","Extensive experimental results indicate that CartesianMoE outperforms previous MoE models for building LLMs, in terms of both perplexity and downstream task performance.","And we also find that CartesianMoE achieves better expert routing robustness."],"url":"http://arxiv.org/abs/2410.16077v1"}
